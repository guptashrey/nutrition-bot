{"title": "Implementation of deep learning artificial intelligence in vision-threatening disease screenings for an underserved community during COVID-19.", "abstract": "Age-related macular degeneration, diabetic retinopathy, and glaucoma are vision-threatening diseases that are leading causes of vision loss. Many studies have validated deep learning artificial intelligence for image-based diagnosis of vision-threatening diseases. Our study prospectively investigated deep learning artificial intelligence applications in student-run non-mydriatic screenings for an underserved, primarily Hispanic community during COVID-19.\nFive supervised student-run community screenings were held in West New York, New Jersey. Participants underwent non-mydriatic 45-degree retinal imaging by medical students. Images were uploaded to a cloud-based deep learning artificial intelligence for vision-threatening disease referral. An on-site tele-ophthalmology grader and remote clinical ophthalmologist graded images, with adjudication by a senior ophthalmologist to establish the gold standard diagnosis, which was used to assess the performance of deep learning artificial intelligence.\nA total of 385 eyes from 195 screening participants were included (mean age 52.43\u2009\u2009\u00b1\u2009\u200914.5 years, 40.0% female). A total of 48 participants were referred for at least one vision-threatening disease. Deep learning artificial intelligence marked 150/385 (38.9%) eyes as ungradable, compared to 10/385 (2.6%) ungradable as per the human gold standard (\nDeep learning artificial intelligence can increase the efficiency and accessibility of vision-threatening disease screenings, particularly in underserved communities. Deep learning artificial intelligence should be adaptable to different environments. Consideration should be given to how deep learning artificial intelligence can best be utilized in a real-world application, whether in computer-aided or autonomous diagnosis.", "journal": "Journal of telemedicine and telecare", "date": "2023-03-14", "authors": ["ArethaZhu", "PriyaTailor", "RashikaVerma", "IsisZhang", "BrianSchott", "CatherineYe", "BernardSzirth", "MiriamHabiel", "Albert SKhouri"], "doi": "10.1177/1357633X231158832"}
{"title": "COVID-Net USPro: An Explainable Few-Shot Deep Prototypical Network for COVID-19 Screening Using Point-of-Care Ultrasound.", "abstract": "As the Coronavirus Disease 2019 (COVID-19) continues to impact many aspects of life and the global healthcare systems, the adoption of rapid and effective screening methods to prevent the further spread of the virus and lessen the burden on healthcare providers is a necessity. As a cheap and widely accessible medical image modality, point-of-care ultrasound (POCUS) imaging allows radiologists to identify symptoms and assess severity through visual inspection of the chest ultrasound images. Combined with the recent advancements in computer science, applications of deep learning techniques in medical image analysis have shown promising results, demonstrating that artificial intelligence-based solutions can accelerate the diagnosis of COVID-19 and lower the burden on healthcare professionals. However, the lack of large, well annotated datasets poses a challenge in developing effective deep neural networks, especially in the case of rare diseases and new pandemics. To address this issue, we present COVID-Net USPro, an explainable few-shot deep prototypical network that is designed to detect COVID-19 cases from very few ultrasound images. Through intensive quantitative and qualitative assessments, the network not only demonstrates high performance in identifying COVID-19 positive cases, using an explainability component, but it is also shown that the network makes decisions based on the actual representative patterns of the disease. Specifically, COVID-Net USPro achieves 99.55% overall accuracy, 99.93% recall, and 99.83% precision for COVID-19-positive cases when trained with only five shots. In addition to the quantitative performance assessment, our contributing clinician with extensive experience in POCUS interpretation verified the analytic pipeline and results, ensuring that the network's decisions are based on clinically relevant image patterns integral to COVID-19 diagnosis. We believe that network explainability and clinical validation are integral components for the successful adoption of deep learning in the medical field. As part of the COVID-Net initiative, and to promote reproducibility and foster further innovation, the network is open-sourced and available to the public.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-03-12", "authors": ["JessySong", "AshkanEbadi", "AdrianFlorea", "PengchengXi", "St\u00e9phaneTremblay", "AlexanderWong"], "doi": "10.3390/s23052621\n10.31083/j.fbl2707198\n10.1002/14651858.CD013705.pub2\n10.1038/s41598-021-99015-3\n10.1038/s41598-020-76550-z\n10.3389/fmed.2021.729287\n10.3389/fmed.2020.608525\n10.18653/v1/D19-1045\n10.1109/ICCV.2017.74\n10.3389/fmed.2021.821120\n10.1016/j.compbiomed.2020.103792\n10.1016/j.patrec.2020.09.010\n10.1016/j.bspc.2021.102920\n10.1371/journal.pone.0255886\n10.1016/j.patcog.2020.107700\n10.48550/ARXIV.2109.03793\n10.1007/s13534-017-0021-8\n10.1186/s12911-020-01332-6\n10.1109/TPAMI.2019.2918284\n10.1378/chest.09-0001"}
{"title": "On the Implementation of a Post-Pandemic Deep Learning Algorithm Based on a Hybrid CT-Scan/X-ray Images Classification Applied to Pneumonia Categories.", "abstract": "The identification and characterization of lung diseases is one of the most interesting research topics in recent years. They require accurate and rapid diagnosis. Although lung imaging techniques have many advantages for disease diagnosis, the interpretation of medial lung images has always been a major problem for physicians and radiologists due to diagnostic errors. This has encouraged the use of modern artificial intelligence techniques such as deep learning. In this paper, a deep learning architecture based on EfficientNetB7, known as the most advanced architecture among convolutional networks, has been constructed for classification of medical X-ray and CT images of lungs into three classes namely: common pneumonia, coronavirus pneumonia and normal cases. In terms of accuracy, the proposed model is compared with recent pneumonia detection techniques. The results provided robust and consistent features to this system for pneumonia detection with predictive accuracy according to the three classes mentioned above for both imaging modalities: radiography at 99.81% and CT at 99.88%. This work implements an accurate computer-aided system for the analysis of radiographic and CT medical images. The results of the classification are promising and will certainly improve the diagnosis and decision making of lung diseases that keep appearing over time.", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-03-12", "authors": ["AbdelghaniMoussaid", "NabilaZrira", "IbtissamBenmiloud", "ZinebFarahat", "YoussefKarmoun", "YasmineBenzidia", "SoumayaMouline", "BahiaEl Abdi", "Jamal EddineBourkadi", "NabilNgote"], "doi": "10.3390/healthcare11050662\n10.1128/CMR.00028-20\n10.1016/j.ajem.2022.03.036\n10.1016/j.rmr.2021.11.004\n10.1016/j.gie.2020.06.040\n10.1371/journal.pone.0072457\n10.1016/j.ophtha.2017.02.008\n10.1371/journal.pone.0174944\n10.1016/j.bbe.2021.06.011\n10.1038/s41598-020-74539-2\n10.1016/j.mehy.2020.109761\n10.1109/TMI.2020.3040950\n10.1371/journal.pone.0262052\n10.1016/j.chemolab.2021.104256\n10.1007/s42979-021-00695-5\n10.1016/j.bspc.2021.103441\n10.1007/s11548-021-02317-0\n10.1088/1361-6560/abe838\n10.3389/frai.2021.694875\n10.1016/j.compbiomed.2021.104835\n10.17632/fvk7h5dg2p.3"}
{"title": "A hybrid deep learning approach for COVID-19 detection based on genomic image processing techniques.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has been spreading quickly, threatening the public health system. Consequently, positive COVID-19 cases must be rapidly detected and treated. Automatic detection systems are essential for controlling the COVID-19 pandemic. Molecular techniques and medical imaging scans are among the most effective approaches for detecting COVID-19. Although these approaches are crucial for controlling the COVID-19 pandemic, they have certain limitations. This study proposes an effective hybrid approach based on genomic image processing (GIP) techniques to rapidly detect COVID-19 while avoiding the limitations of traditional detection techniques, using whole and partial genome sequences of human coronavirus (HCoV) diseases. In this work, the GIP techniques convert the genome sequences of HCoVs into genomic grayscale images using a genomic image mapping technique known as the frequency chaos game representation. Then, the pre-trained convolution neural network, AlexNet, is used to extract deep features from these images using the last convolution (conv5) and second fully-connected (fc7) layers. The most significant features were obtained by removing the redundant ones using the ReliefF and least absolute shrinkage and selection operator (LASSO) algorithms. These features are then passed to two classifiers: decision trees and k-nearest neighbors (KNN). Results showed that extracting deep features from the fc7 layer, selecting the most significant features using the LASSO algorithm, and executing the classification process using the KNN classifier is the best hybrid approach. The proposed hybrid deep learning approach detected COVID-19, among other HCoV diseases, with 99.71% accuracy, 99.78% specificity, and 99.62% sensitivity.", "journal": "Scientific reports", "date": "2023-03-11", "authors": ["Muhammed SHammad", "Vidan FGhoneim", "Mai SMabrouk", "Walid IAl-Atabany"], "doi": "10.1038/s41598-023-30941-0\n10.1038/s41586-020-2008-3\n10.14309/ajg.0000000000000620\n10.1213/ANE.0000000000004845\n10.3390/pathogens9030186\n10.1038/s41591-020-0820-9\n10.1016/S0140-6736(20)30251-8\n10.1016/j.jinf.2020.03.041\n10.1109/ACCESS.2021.3076158\n10.1038/s41598-021-88807-2\n10.1016/j.eswa.2020.113909\n10.1002/ima.22469\n10.1016/j.compbiomed.2020.103805\n10.1007/s10489-020-01888-w\n10.1109/JIOT.2021.3055804\n10.1016/j.asoc.2020.106642\n10.1016/j.asoc.2022.108780\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020200642\n10.1021/acsnano.0c02624\n10.1016/j.cie.2021.107666\n10.1038/s41598-020-80363-5\n10.3389/fgene.2021.569120\n10.1007/s11517-022-02591-3\n10.1038/s41598-021-90766-7\n10.1093/bib/bbaa170\n10.1371/journal.pone.0232391\n10.1016/j.bspc.2022.104192\n10.1016/j.compbiomed.2021.104650\n10.13053/rcs-148-3-9\n10.1016/j.jmgm.2020.107603\n10.1016/j.aej.2022.08.023\n10.1093/bioinformatics/17.5.429\n10.1016/j.gene.2004.10.021\n10.1016/j.neucom.2020.10.068\n10.1016/j.compbiomed.2017.08.001\n10.1145/3065386\n10.1109/TMI.2016.2535302\n10.1007/s42979-021-00815-1\n10.1109/ACCESS.2019.2919122\n10.1016/j.neucom.2017.11.077\n10.1109/ACCESS.2021.3053759\n10.1016/j.jbi.2018.07.014\n10.1111/j.1467-9868.2011.00771.x\n10.1111/j.1467-9868.2007.00577.x\n10.1016/j.ipm.2009.03.002"}
{"title": "A hybrid CNN and ensemble model for COVID-19 lung infection detection on chest CT scans.", "abstract": "COVID-19 is highly infectious and causes acute respiratory disease. Machine learning (ML) and deep learning (DL) models are vital in detecting disease from computerized chest tomography (CT) scans. The DL models outperformed the ML models. For COVID-19 detection from CT scan images, DL models are used as end-to-end models. Thus, the performance of the model is evaluated for the quality of the extracted feature and classification accuracy. There are four contributions included in this work. First, this research is motivated by studying the quality of the extracted feature from the DL by feeding these extracted to an ML model. In other words, we proposed comparing the end-to-end DL model performance against the approach of using DL for feature extraction and ML for the classification of COVID-19 CT scan images. Second, we proposed studying the effect of fusing extracted features from image descriptors, e.g., Scale-Invariant Feature Transform (SIFT), with extracted features from DL models. Third, we proposed a new Convolutional Neural Network (CNN) to be trained from scratch and then compared to the deep transfer learning on the same classification problem. Finally, we studied the performance gap between classic ML models against ensemble learning models. The proposed framework is evaluated using a CT dataset, where the obtained results are evaluated using five different metrics The obtained results revealed that using the proposed CNN model is better than using the well-known DL model for the purpose of feature extraction. Moreover, using a DL model for feature extraction and an ML model for the classification task achieved better results in comparison to using an end-to-end DL model for detecting COVID-19 CT scan images. Of note, the accuracy rate of the former method improved by using ensemble learning models instead of the classic ML models. The proposed method achieved the best accuracy rate of 99.39%.", "journal": "PloS one", "date": "2023-03-10", "authors": ["Ahmed AAkl", "Khalid MHosny", "Mostafa MFouda", "AhmadSalah"], "doi": "10.1371/journal.pone.0282608\n10.1016/j.compbiomed.2020.103795\n10.1097/RLI.0000000000000700\n10.1371/journal.pone.0236621\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105532\n10.1038/nature14539\n10.1109/JPROC.2020.3004555\n10.1371/journal.pone.0235187\n10.1371/journal.pone.0250688\n10.2196/19569\n10.1007/s10489-020-02055-x\n10.1007/s00500-020-05275-y\n10.1109/ACCESS.2020.3016780\n10.1016/j.eswa.2021.116377\n10.1007/s10723-022-09615-0\n10.1002/ima.22706\n10.1109/TMI.2017.2712367\n10.1016/j.aquaeng.2020.102117\n10.1023/B:VISI.0000029664.99615.94\n10.7717/peerj.10086\n10.1016/j.patrec.2020.10.001\n10.1007/s10489-020-01826-w\n10.1007/s00521-020-05437-x"}
{"title": "COVID-19 imaging, where do we go from here? Bibliometric analysis of medical imaging in COVID-19.", "abstract": "We conducted a systematic and comprehensive bibliometric analysis of COVID-19-related medical imaging to determine the current status and indicate possible future directions.\nThis research provides an analysis of Web of Science Core Collection (WoSCC) indexed articles on COVID-19 and medical imaging published between 1 January 2020 and 30 June 2022, using the search terms \"COVID-19\" and medical imaging terms (such as \"X-ray\" or \"CT\"). Publications based solely on COVID-19 themes or medical image themes were excluded. CiteSpace was used to identify the predominant topics and generate a visual map of countries, institutions, authors, and keyword networks.\nThe search included 4444 publications. The journal with the most publications was European Radiology, and the most co-cited journal was Radiology. China was the most frequently cited country in terms of co-authorship, with the Huazhong University of Science and Technology being the institution contributing with the highest number of relevant co-authorships. Research trends and leading topics included: assessment of initial COVID-19-related clinical imaging features, differential diagnosis using artificial intelligence (AI) technology and model interpretability, diagnosis systems construction, COVID-19 vaccination, complications, and predicting prognosis.\nThis bibliometric analysis of COVID-19-related medical imaging helps clarify the current research situation and developmental trends. Subsequent trends in COVID-19 imaging are likely to shift from lung structure to function, from lung tissue to other related organs, and from COVID-19 to the impact of COVID-19 on the diagnosis and treatment of other diseases. Key Points \u2022\u00a0We conducted a systematic and comprehensive bibliometric analysis of COVID-19-related medical imaging from 1 January 2020 to 30 June 2022. \u2022 Research trends and leading topics included assessment of initial COVID-19-related clinical imaging features, differential diagnosis using AI technology and model interpretability, diagnosis systems construction, COVID-19 vaccination, complications, and predicting prognosis. \u2022 Future trends in COVID-19-related imaging are likely to involve a shift from lung structure to function, from lung tissue to other related organs, and from COVID-19 to the impact of COVID-19 on the diagnosis and treatment of other diseases.", "journal": "European radiology", "date": "2023-03-10", "authors": ["RuWen", "MudanZhang", "RuiXu", "YingmingGao", "LinLiu", "HuiChen", "XingangWang", "WenyanZhu", "HuafangLin", "ChenLiu", "XianchunZeng"], "doi": "10.1007/s00330-023-09498-z\n10.1016/j.diii.2020.03.014\n10.1007/s00330-020-06934-2\n10.1007/s00330-020-06801-0\n10.1016/S0140-6736(20)30185-9\n10.1016/j.clinimag.2021.01.019\n10.1371/journal.pone.0223994\n10.1109/RBME.2020.2990959\n10.3389/frma.2020.607286\n10.1002/asi.20317\n10.1517/14712598.2014.920813\n10.1080/14789450.2019.1703678\n10.1016/j.scs.2021.102729\n10.1016/j.dsx.2021.102325\n10.21037/atm-20-4235\n10.1016/j.biopha.2020.110451\n10.1016/j.dsx.2020.06.063\n10.1016/j.tmaid.2020.101566\n10.1097/MD.0000000000022849\n10.2174/1573405618666211230105631\n10.2196/30692\n10.1007/s40336-021-00460-x\n10.1148/radiol.2021204417\n10.1093/bioinformatics/btm554\n10.1002/asi.21309\n10.1016/S0140-6736(20)30183-5\n10.1016/j.jamda.2021.01.073\n10.1016/j.compbiomed.2020.103869\n10.1016/j.patcog.2020.107613\n10.1016/j.chaos.2020.110170\n10.1007/s10044-021-00984-y:1-14\n10.1186/s12967-020-02324-w\n10.1109/TMI.2021.3104474\n10.1097/PHM.0000000000001729\n10.1016/S0140-6736(20)32656-8\n10.1038/s41746-021-00496-3\n10.1186/s12916-021-02056-8\n10.1111/j.1440-1843.2010.01720.x"}
{"title": "Results of the COVID-19 mental health international for the health professionals (COMET-HP) study: depression, suicidal tendencies and conspiracism.", "abstract": "The current study aimed to investigate the rates of anxiety, clinical depression, and suicidality and their changes in health professionals during the COVID-19 outbreak.\nThe data came from the larger COMET-G study. The study sample includes 12,792 health professionals from 40 countries (62.40% women aged 39.76\u2009\u00b1\u200911.70; 36.81% men aged 35.91\u2009\u00b1\u200911.00 and 0.78% non-binary gender aged 35.15\u2009\u00b1\u200913.03). Distress and clinical depression were identified with the use of a previously developed cut-off and algorithm, respectively.\nDescriptive statistics were calculated. Chi-square tests, multiple forward stepwise linear regression analyses, and Factorial Analysis of Variance (ANOVA) tested relations among variables.\nClinical depression was detected in 13.16% with male doctors and 'non-binary genders' having the lowest rates (7.89 and 5.88% respectively) and 'non-binary gender' nurses and administrative staff had the highest (37.50%); distress was present in 15.19%. A significant percentage reported a deterioration in mental state, family dynamics, and everyday lifestyle. Persons with a history of mental disorders had higher rates of current depression (24.64% vs. 9.62%; p\u2009<\u20090.0001). Suicidal tendencies were at least doubled in terms of RASS scores. Approximately one-third of participants were accepting (at least to a moderate degree) a non-bizarre conspiracy. The highest Relative Risk (RR) to develop clinical depression was associated with a history of Bipolar disorder (RR\u2009=\u20094.23).\nThe current study reported findings in health care professionals similar in magnitude and quality to those reported earlier in the general population although rates of clinical depression, suicidal tendencies, and adherence to conspiracy theories were much lower. However, the general model of factors interplay seems to be the same and this could be of practical utility since many of these factors are modifiable.", "journal": "Social psychiatry and psychiatric epidemiology", "date": "2023-03-04", "authors": ["KonstantinosN Fountoulakis", "GrigoriosN Karakatsoulis", "SeriAbraham", "KristinaAdorjan", "Helal UddinAhmed", "Renato DAlarc\u00f3n", "KiyomiArai", "Sani SalihuAuwal", "JulioBobes", "TeresaBobes-Bascaran", "JulieBourgin-Duchesnay", "Cristina AnaBredicean", "LaurynasBukelskis", "AkakiBurkadze", "Indira IndianaCabrera Abud", "RubyCastilla-Puentes", "MarceloCetkovich", "HectorColon-Rivera", "RicardoCorral", "CarlaCortez-Vergara", "PiirikaCrepin", "Domenicode Berardis", "SergioZamora Delgado", "Davidde Lucena", "Avinashde Sousa", "Ramonadi Stefano", "SeetalDodd", "Livia PriyankaElek", "AnnaElissa", "BertaErdelyi-Hamza", "GamzeErzin", "Martin JEtchevers", "PeterFalkai", "AdrianaFarcas", "IlyaFedotov", "ViktoriiaFilatova", "Nikolaos KFountoulakis", "IrynaFrankova", "FrancescoFranza", "PedroFrias", "TatianaGalako", "Cristian JGaray", "LeticiaGarcia-\u00c1lvarez", "PazGarc\u00eda-Portilla", "XeniaGonda", "Tomasz MGondek", "DanielaMorera Gonz\u00e1lez", "HilaryGould", "PaoloGrandinetti", "ArturoGrau", "VioletaGroudeva", "MichalHagin", "TakayukiHarada", "Tasdik MHasan", "NurulAzreen Hashim", "JanHilbig", "SahadatHossain", "RossitzaIakimova", "MonaIbrahim", "FeliciaIftene", "YuliaIgnatenko", "MatiasIrarrazaval", "ZalihaIsmail", "JamilaIsmayilova", "AsafJacobs", "MiroJakovljevi\u0107", "NenadJak\u0161i\u0107", "AfzalJaved", "HelinYilmaz Kafali", "SagarKaria", "OlgaKazakova", "DoaaKhalifa", "OlenaKhaustova", "SteveKoh", "SvetlanaKopishinskaia", "KorneliiaKosenko", "Sotirios AKoupidis", "IllesKovacs", "BarbaraKulig", "AlishaLalljee", "JustineLiewig", "AbdulMajid", "EvgeniiaMalashonkova", "KhameliaMalik", "NajmaIqbal Malik", "GulayMammadzada", "BilveshMandalia", "DonatellaMarazziti", "DarkoMar\u010dinko", "StephanieMartinez", "EimantasMatiekus", "GabrielaMejia", "Roha SaeedMemon", "Xarah ElenneMeza Mart\u00ednez", "DaliaMickevi\u010di\u016bt\u0117", "RoumenMilev", "MuftauMohammed", "AlejandroMolina-L\u00f3pez", "PetrMorozov", "Nuru SuleimanMuhammad", "FilipMusta\u010d", "Mika SNaor", "AmiraNassieb", "AlvydasNavickas", "TarekOkasha", "MilenaPandova", "Anca-LiviaPanfil", "LiliyaPanteleeva", "IonPapava", "Mikaella EPatsali", "AlexeyPavlichenko", "BojanaPejuskovic", "MarianaPinto da Costa", "MikhailPopkov", "DinaPopovic", "Nor Jannah NasutionRaduan", "FranciscaVargas Ram\u00edrez", "ElmarsRancans", "SalmiRazali", "FedericoRebok", "AnnaRewekant", "Elena NinoskaReyes Flores", "Mar\u00eda TeresaRivera-Encinas", "Pilar ASaiz", "ManuelS\u00e1nchez de Carmona", "DavidSaucedo Mart\u00ednez", "Jo AnneSaw", "G\u00f6rkemSaygili", "PatriciaSchneidereit", "BhumikaShah", "TomohiroShirasaka", "KetevanSilagadze", "SattiSitanggang", "OlegSkugarevsky", "AnnaSpikina", "Sridevi SiraMahalingappa", "MariaStoyanova", "AnnaSzczegielniak", "Simona ClaudiaTamasan", "GiuseppeTavormina", "Maurilio Giuseppe MariaTavormina", "Pavlos NTheodorakis", "MauricioTohen", "Eva-MariaTsapakis", "DinaTukhvatullina", "IrfanUllah", "RatnarajVaidya", "Johann MVega-Dienstmaier", "JelenaVrublevska", "OliveraVukovic", "OlgaVysotska", "NataliaWidiasih", "AnnaYashikhina", "Panagiotis EPrezerakos", "MichaelBerk", "SarahLevaj", "DariaSmirnova"], "doi": "10.1007/s00127-023-02438-8\n10.2196/19458\n10.1017/S003329172000224X\n10.1017/S0033291721001434\n10.1037/0022-3514.41.6.1129\n10.1037//0096-3445.108.4.441\n10.1186/s12888-020-02864-x\n10.1002/brb3.1881\n10.3389/fpsyg.2021.646572\n10.1016/j.janxdis.2021.102368\n10.3389/fpubh.2021.610623\n10.1037//0021-843x.96.3.179\n10.1002/brb3.1964\n10.3389/fpsyg.2020.565128\n10.1111/bjso.12397\n10.1097/QAI.0b013e3181c57dbc\n10.1016/j.jad.2021.01.013\n10.1159/000513733\n10.3389/fpsyg.2020.597624\n10.1016/j.psychres.2020.113599\n10.2196/20737\n10.3390/healthcare10060979\n10.3390/healthcare9050510\n10.1016/j.euroneuro.2021.06.005\n10.1016/j.pnpbp.2020.110062\n10.1016/j.jad.2021.02.054\n10.3389/fpsyg.2021.646394\n10.1111/nyas.14506\n10.1007/s00038-020-01412-4\n10.3390/healthcare8030190\n10.1192/j.eurpsy.2020.59\n10.1186/s12910-021-00641-3\n10.3390/ijerph17217818\n10.1002/da.23162\n10.1080/10615806.2021.1878158\n10.1016/S2215-0366(20)30482-X\n10.1192/j.eurpsy.2020.35\n10.1186/1471-244x-1-3\n10.1016/j.jad.2020.10.061\n10.1016/j.euroneuro.2021.10.004\n10.1016/j.jad.2011.12.045\n10.1017/S0033291720005188\n10.18071/isz.72.0337\n10.3389/fpsyt.2020.568664\n10.1016/j.comppsych.2020.152222\n10.1016/j.comppsych.2020.152214\n10.3389/fpsyg.2020.01684\n10.3390/ijerph18073843\n10.3390/ijerph17134779\n10.1016/j.jpsychires.2020.07.024\n10.1186/s12888-021-03291-2\n10.1176/appi.ajp.2020.20070979\n10.1186/s12913-021-06555-5\n10.1111/jan.15175\n10.1016/j.ajp.2020.102052\n10.1080/13548506.2020.1754438\n10.1016/j.comppsych.2020.152213\n10.1371/journal.pone.0089177\n10.1111/bjso.12394\n10.1016/j.paid.2020.110216\n10.1186/s40359-021-00565-y\n10.1016/j.paid.2021.110771\n10.1016/j.paid.2021.110704\n10.1002/wps.20758\n10.1016/j.jpsychires.2019.08.002\n10.3389/fpsyt.2020.598712\n10.1037//0021-843x.88.1.33\n10.1177/00131640021970871\n10.1002/brb3.2318\n10.1186/s12889-021-10873-y\n10.1111/bjhp.12449\n10.1136/bmjopen-2020-042555\n10.1016/j.bbi.2020.04.048\n10.1002/da.23129\n10.1002/brb3.1730\n10.1037//0021-843x.86.4.379\n10.1001/jamainternmed.2014.190\n10.1177/0020764020927051\n10.1177/0020764020927051\n10.3389/fpsyg.2020.02065\n10.1186/s40359-021-00526-5\n10.1002/brb3.1745\n10.1016/j.euroneuro.2021.05.011\n10.1016/S2215-0366(21)00074-2\n10.1007/s11126-020-09796-5\n10.1177/0706743720986786\n10.1016/j.socscimed.2020.113356\n10.3389/fpsyt.2021.635832\n10.3389/fpsyg.2020.577684\n10.1017/S0033291720004067\n10.1186/s12992-020-00589-w\n10.1371/journal.pone.0243264\n10.1111/pcn.13004\n10.1177/13591053211012759\n10.1017/S0033291721001665\n10.3390/ijerph19031154\n10.3390/ijerph17144924\n10.1186/s12889-021-10643-w\n10.1186/s12889-020-09322-z\n10.3389/fpsyg.2021.626547\n10.1002/brb3.1837\n10.1016/S2215-0366(21)00084-5\n10.1002/acp.3770\n10.1080/08870446.2019.1673894\n10.1136/bmjopen-2020-044945\n10.7189/jogh.11.05009\n10.1177/0020764020934508\n10.1080/13814788.2021.1954154\n10.1016/j.jpsychires.2021.05.024\n10.3390/ijerph17051729\n10.1016/j.bbi.2020.04.028\n10.3399/bjgp20X713021\n10.1097/PSY.0000000000000922\n10.4269/ajtmh.20-0800\n10.3205/000281"}
{"title": "A Computational Approach in the Diagnostic Process of COVID-19: The Missing Link between the Laboratory and Emergency Department.", "abstract": "The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is responsible for the COVID-19 pandemic and so it is crucial the right evaluation of viral infection. According to the Centers for Disease Control and Prevention (CDC), the Real-Time Reverse Transcription PCR (RT-PCR) in respiratory samples is the gold standard for confirming the disease. However, it has practical limitations as time-consuming procedures and a high rate of false-negative results. We aim to assess the accuracy of COVID-19 classifiers based on Arificial Intelligence (AI) and statistical classification methods adapted on blood tests and other information routinely collected at the Emergency Departments (EDs).\nPatients admitted to the ED of Careggi Hospital from April 7th-30th 2020 with pre-specified features of suspected COVID-19 were enrolled. Physicians prospectively dichotomized them as COVID-19 likely/unlikely case, based on clinical features and bedside imaging support. Considering the limits of each method to identify a case of COVID-19, further evaluation was performed after an independent clinical review of 30-day follow-up data. Using this as a gold standard, several classifiers were implemented: Logistic Regression (LR), Quadratic Discriminant Analysis (QDA), Random Forest (RF), Support Vector Machine (SVM), Neural Networks (NN), K-nearest neighbor (K-NN), Naive Bayes (NB).\nMost of the classifiers show a ROC >0.80 on both internal and external validation samples but the best results are obtained applying RF, LR and NN. The performance from the external validation sustains the proof of concept to use such mathematical models fast, robust and efficient for a first identification of COVID-19 positive patients. These tools may constitute both a bedside support while waiting for RT-PCR results, and a tool to point to a deeper investigation, by identifying which patients are more likely to develop into positive cases within 7 days.\nConsidering the obtained results and with a rapidly changing virus, we believe that data processing automated procedures may provide a valid support to the physicians facing the decision to classify a patient as a COVID-19 case or not.", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2023-03-04", "authors": ["LuisaLanzilao", "AntonellaMariniello", "BiancaPolenzani", "AlessandraAldinucci", "PeimanNazerian", "AlessioProta", "StefanoGrifoni", "BarbaraTonietti", "ChiaraNeri", "LiviaTurco", "AlessandraFanelli", "AmedeoAmedei", "ElenaStanghellini"], "doi": "10.31083/j.fbl2802031"}
{"title": "Robust framework for COVID-19 identication from a multicenter dataset of chest CT scans.", "abstract": "The main objective of this study is to develop a robust deep learning-based framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and Normal cases based on volumetric chest CT scans, which are acquired in different imaging centers using different scanners and technical settings. We demonstrated that while our proposed model is trained on a relatively small dataset acquired from only one imaging center using a specific scanning protocol, it performs well on heterogeneous test sets obtained by multiple scanners using different technical parameters. We also showed that the model can be updated via an unsupervised approach to cope with the data shift between the train and test sets and enhance the robustness of the model upon receiving a new external dataset from a different center. More specifically, we extracted the subset of the test images for which the model generated a confident prediction and used the extracted subset along with the training set to retrain and update the benchmark model (the model trained on the initial train set). Finally, we adopted an ensemble architecture to aggregate the predictions from multiple versions of the model. For initial training and development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76 Normal cases was used, which contained volumetric CT scans acquired from one imaging center using a single scanning protocol and standard radiation dose. To evaluate the model, we collected four different test sets retrospectively to investigate the effects of the shifts in the data characteristics on the model's performance. Among the test cases, there were CT scans with similar characteristics as the train set as well as noisy low-dose and ultra-low-dose CT scans. In addition, some test CT scans were obtained from patients with a history of cardiovascular diseases or surgeries. This dataset is referred to as the \"SPGC-COVID\" dataset. The entire test dataset used in this study contains 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental results indicate that our proposed framework performs well on all test sets achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI: [76.50-99.19]), Normal sensitivity of 98.04% (95%CI: [89.55-99.95]) while the confidence intervals are obtained using the significance level of 0.05. The obtained AUC values (One class vs Others) are 0.993 (95%CI: [0.977-1]), 0.989 (95%CI: [0.962-1]), and 0.990 (95%CI: [0.971-1]) for COVID-19, CAP, and Normal classes, respectively. The experimental results also demonstrate the capability of the proposed unsupervised enhancement approach in improving the performance and robustness of the model when being evaluated on varied external test sets.", "journal": "PloS one", "date": "2023-03-03", "authors": ["SadafKhademi", "ShahinHeidarian", "ParnianAfshar", "NastaranEnshaei", "FarnooshNaderkhani", "Moezedin JavadRafiee", "AnastasiaOikonomou", "AkbarShafiee", "FaranakBabaki Fard", "Konstantinos NPlataniotis", "ArashMohammadi"], "doi": "10.1371/journal.pone.0282121\n10.1148/radiol.2020200432\n10.1109/MSP.2021.3090674\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2019190928\n10.1038/srep34921\n10.1016/j.numecd.2020.04.013\n10.3389/frai.2021.598932\n10.1016/j.imu.2022.100945\n10.1038/s41597-021-00900-3\n10.1038/s41598-022-08796-8\n10.1007/s00330-010-1990-5\n10.1016/j.patcog.2021.107942\n10.1109/LSP.2020.3034858\n10.1007/s42058-020-00034-2\n10.1002/widm.1353\n10.1016/j.media.2021.102062\n10.1109/ACCESS.2021.3084358\n10.2307/2685469\n10.1007/BF02295996\n10.1109/TMI.2020.3009029\n10.1109/TMI.2020.2971258\n10.1186/s13244-021-01096-1\n10.1016/j.chest.2021.04.004\n10.25259/JCIS_138_2020\n10.7189/jogh.10.010347\n10.1016/j.jhin.2020.03.001\n10.3389/fonc.2020.556334\n10.1016/j.chest.2020.04.003\n10.1016/j.tmaid.2020.101627\n10.1007/s15010-020-01467-8\n10.1007/s00330-020-06809-6"}
{"title": "Deep Learning Solution for Quantification of Fluorescence Particles on a Membrane.", "abstract": "The detection and quantification of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) virus particles in ambient waters using a membrane-based in-gel loop-mediated isothermal amplification (mgLAMP) method can play an important role in large-scale environmental surveillance for early warning of potential outbreaks. However, counting particles or cells in fluorescence microscopy is an expensive, time-consuming, and tedious task that only highly trained technicians and researchers can perform. Although such objects are generally easy to identify, manually annotating cells is occasionally prone to fatigue errors and arbitrariness due to the operator's interpretation of borderline cases. In this research, we proposed a method to detect and quantify multiscale and shape variant SARS-CoV-2 fluorescent cells generated using a portable (", "journal": "Sensors (Basel, Switzerland)", "date": "2023-03-01", "authors": ["Abdellah ZakariaSellam", "AzeddineBenlamoudi", "Cl\u00e9ment AntoineCid", "LeopoldDobelle", "AminaSlama", "YassinEl Hillali", "AbdelmalikTaleb-Ahmed"], "doi": "10.3390/s23041794\n10.1038/s41586-021-04188-6\n10.1021/acs.est.0c02388\n10.1038/s41587-020-0684-z\n10.1016/j.watres.2020.116404\n10.1016/j.envint.2020.105689\n10.1016/j.scitotenv.2021.149618\n10.1128/jcm.02446-21\n10.1021/acs.est.1c04623\n10.1109/MSP.2012.2204190\n10.1109/TSMCB.2012.2228639\n10.1016/0031-3203(95)00067-4\n10.1109/TBME.2009.2035102\n10.3390/s22103760\n10.1016/j.neunet.2014.09.003\n10.1155/2018/7068349\n10.1109/ICCV.2015.169\n10.1109/MSP.2009.934181\n10.1007/978-3-319-10578-9_23\n10.1109/TPAMI.2016.2577031\n10.1109/TPAMI.2019.2956516"}
{"title": "MonkeyNet: A robust deep convolutional neural network for monkeypox disease detection and classification.", "abstract": "The monkeypox virus poses a new pandemic threat while we are still recovering from COVID-19. Despite the fact that monkeypox is not as lethal and contagious as COVID-19, new patient cases are recorded every day. If preparations are not made, a global pandemic is likely. Deep learning (DL) techniques are now showing promise in medical imaging for figuring out what diseases a person has. The monkeypox virus-infected human skin and the region of the skin can be used to diagnose the monkeypox early because an image has been used to learn more about the disease. But there is still no reliable Monkeypox database that is available to the public that can be used to train and test DL models. As a result, it is essential to collect images of monkeypox patients. The \"MSID\" dataset, short form of \"Monkeypox Skin Images Dataset\", which was developed for this research, is free to use and can be downloaded from the Mendeley Data database by anyone who wants to use it. DL models can be built and used with more confidence using the images in this dataset. These images come from a variety of open-source and online sources and can be used for research purposes without any restrictions. Furthermore, we proposed and evaluated a modified DenseNet-201 deep learning-based CNN model named MonkeyNet. Using the original and augmented datasets, this study suggested a deep convolutional neural network that was able to correctly identify monkeypox disease with an accuracy of 93.19% and 98.91% respectively. This implementation also shows the Grad-CAM which indicates the level of the model's effectiveness and identifies the infected regions in each class image, which will help the clinicians. The proposed model will also help doctors make accurate early diagnoses of monkeypox disease and protect against the spread of the disease.", "journal": "Neural networks : the official journal of the International Neural Network Society", "date": "2023-02-28", "authors": ["DiponkorBala", "Md ShamimHossain", "Mohammad AlamgirHossain", "Md IbrahimAbdullah", "Md MizanurRahman", "BalachandranManavalan", "NaijieGu", "Mohammad SIslam", "ZhangjinHuang"], "doi": "10.1016/j.neunet.2023.02.022\n10.17632/r9bfpnvyxr.3"}
{"title": "Conv-CapsNet: capsule based network for COVID-19 detection through X-Ray scans.", "abstract": "Coronavirus, a virus that spread worldwide rapidly and was eventually declared a pandemic. The rapid spread made it essential to detect Coronavirus infected people to control the further spread. Recent studies show that radiological images such as X-Rays and CT scans provide essential information in detecting infection using deep learning models. This paper proposes a shallow architecture based on Capsule Networks with convolutional layers to detect COVID-19 infected persons. The proposed method combines the ability of the capsule network to understand spatial information with convolutional layers for efficient feature extraction. Due to the model's shallow architecture, it has 23", "journal": "Multimedia tools and applications", "date": "2023-02-28", "authors": ["PulkitSharma", "RhythmArya", "RichaVerma", "BinduVerma"], "doi": "10.1007/s11042-023-14353-w\n10.1016/j.patrec.2020.09.010\n10.1007/s13246-020-00865-4\n10.1007/s42979-020-00383-w\n10.1016/j.cmpb.2022.106833\n10.1109/TGRS.2021.3090410\n10.1016/j.chemosphere.2021.132569\n10.1007/s42979-021-00881-5\n10.1016/j.eswa.2020.113909\n10.1109/ACCESS.2020.3010287\n10.1007/s11547-020-01232-9\n10.1016/j.imu.2020.100297\n10.1016/j.chaos.2020.110495\n10.1016/j.imu.2020.100412\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00335-4\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2020.106744\n10.1007/s10140-020-01808-y\n10.1016/j.slast.2021.10.011\n10.1007/s42979-020-00216-w\n10.1016/j.chaos.2020.110245\n10.1016/j.imu.2020.100360\n10.1007/s42979-021-00774-7\n10.1016/j.imu.2020.100505\n10.1016/j.aej.2021.01.011\n10.1016/j.chaos.2020.110122\n10.1016/j.mehy.2020.109761\n10.1109/JSTSP.2019.2902305\n10.1007/s13244-018-0639-9\n10.1038/s41746-020-00372-6"}
{"title": "Detection of COVID-19 Case from Chest CT Images Using Deformable Deep Convolutional Neural Network.", "abstract": "The infectious coronavirus disease (COVID-19) has become a great threat to global human health. Timely and rapid detection of COVID-19 cases is very crucial to control its spreading through isolation measures as well as for proper treatment. Though the real-time reverse transcription-polymerase chain reaction (RT-PCR) test is a widely used technique for COVID-19 infection, recent researches suggest chest computed tomography (CT)-based screening as an effective substitute in cases of time and availability limitations of RT-PCR. In consequence, deep learning-based COVID-19 detection from chest CT images is gaining momentum. Furthermore, visual analysis of data has enhanced the opportunities of maximizing the prediction performance in this big data and deep learning realm. In this article, we have proposed two separate deformable deep networks converting from the conventional convolutional neural network (CNN) and the state-of-the-art ResNet-50, to detect COVID-19 cases from chest CT images. The impact of the deformable concept has been observed through performance comparative analysis among the designed deformable and normal models, and it is found that the deformable models show better prediction results than their normal form. Furthermore, the proposed deformable ResNet-50 model shows better performance than the proposed deformable CNN model. The gradient class activation mapping (Grad-CAM) technique has been used to visualize and check the targeted regions' localization effort at the final convolutional layer and has been found excellent. Total 2481 chest CT images have been used to evaluate the performance of the proposed models with a train-valid-test data splitting ratio of 80\u2009:\u200910\u2009:\u200910 in random fashion. The proposed deformable ResNet-50 model achieved training accuracy of 99.5% and test accuracy of 97.6% with specificity of 98.5% and sensitivity of 96.5% which are satisfactory compared with related works. The comprehensive discussion demonstrates that the proposed deformable ResNet-50 model-based COVID-19 detection technique can be useful for clinical applications.", "journal": "Journal of healthcare engineering", "date": "2023-02-28", "authors": ["MdFoysal", "A B M AowladHossain", "AbdulsalamYassine", "M ShamimHossain"], "doi": "10.1155/2023/4301745\n10.1016/j.ijid.2020.02.050\n10.1007/s10489-020-01943-6\n10.1016/j.patcog.2020.107700\n10.1007/s11042-020-09894-3\n10.1007/s00521-020-05437-x\n10.1007/s00330-020-07044-9\n10.1016/j.compbiomed.2020.104037\n10.1007/s10096-020-03901-z\n10.3389/fmed.2020.608525\n10.1109/tmi.2020.2994908\n10.1109/access.2020.3018498\n10.1007/s11548-020-02286-w\n10.3390/app11157004\n10.1109/INCET51464.2021.9456387\n10.1109/jbhi.2020.3019505\n10.1007/s42979-021-00782-7\n10.1007/s00330-021-07715-1\n10.1109/CITS49457.2020.9232574\n10.1016/j.inffus.2021.02.013\n10.1159/000509223\n10.1109/mnet.011.2000458\n10.1016/j.scs.2020.102582\n10.1155/2022/2950699\n10.1109/tnse.2020.3026637\n10.1109/jiot.2020.3033129\n10.1109/jiot.2020.3047662\n10.1109/jiot.2020.3013710\n10.1109/jiot.2017.2772959\n10.1016/j.eswa.2019.112821\n10.1148/radiol.2020200642\n10.1007/s11547-020-01237-4\n10.1007/s11263-019-01228-7"}
{"title": "A convolutional neural network with pixel-wise sparse graph reasoning for COVID-19 lesion segmentation in CT images.", "abstract": "The COVID-19 pandemic has extremely threatened human health, and automated algorithms are needed to segment infected regions in the lung using computed tomography (CT). Although several deep convolutional neural networks (DCNNs) have proposed for this purpose, their performance on this task is suppressed due to the limited local receptive field and deficient global reasoning ability. To address these issues, we propose a segmentation network with a novel pixel-wise sparse graph reasoning (PSGR) module for the segmentation of COVID-19 infected regions in CT images. The PSGR module, which is inserted between the encoder and decoder of the network, can improve the modeling of global contextual information. In the PSGR module, a graph is first constructed by projecting each pixel on a node based on the features produced by the encoder. Then, we convert the graph into a sparsely-connected one by keeping K strongest connections to each uncertainly segmented pixel. Finally, the global reasoning is performed on the sparsely-connected graph. Our segmentation network was evaluated on three publicly available datasets and compared with a variety of widely-used segmentation models. Our results demonstrate that (1) the proposed PSGR module can capture the long-range dependencies effectively and (2) the segmentation model equipped with this PSGR module can accurately segment COVID-19 infected regions in CT images and outperform all other competing models.", "journal": "Computers in biology and medicine", "date": "2023-02-27", "authors": ["HaozheJia", "HaotengTang", "GuixiangMa", "WeidongCai", "HengHuang", "LiangZhan", "YongXia"], "doi": "10.1016/j.compbiomed.2023.106698\n10.1007/978-3-030-58520-4\u02d91\n10.5281/zenodo.3757476"}
{"title": "Artificial Intelligence-Assisted Chest X-ray for the Diagnosis of COVID-19: A Systematic Review and Meta-Analysis.", "abstract": "Because it is an accessible and routine image test, medical personnel commonly use a chest X-ray for COVID-19 infections. Artificial intelligence (AI) is now widely applied to improve the precision of routine image tests. Hence, we investigated the clinical merit of the chest X-ray to detect COVID-19 when assisted by AI. We used PubMed, Cochrane Library, MedRxiv, ArXiv, and Embase to search for relevant research published between 1 January 2020 and 30 May 2022. We collected essays that dissected AI-based measures used for patients diagnosed with COVID-19 and excluded research lacking measurements using relevant parameters (i.e., sensitivity, specificity, and area under curve). Two independent researchers summarized the information, and discords were eliminated by consensus. A random effects model was used to calculate the pooled sensitivities and specificities. The sensitivity of the included research studies was enhanced by eliminating research with possible heterogeneity. A summary receiver operating characteristic curve (SROC) was generated to investigate the diagnostic value for detecting COVID-19 patients. Nine studies were recruited in this analysis, including 39,603 subjects. The pooled sensitivity and specificity were estimated as 0.9472 (", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-02-26", "authors": ["I-ShiangTzeng", "Po-ChunHsieh", "Wen-LinSu", "Tsung-HanHsieh", "Sheng-ChangChang"], "doi": "10.3390/diagnostics13040584\n10.1038/s41579-020-00459-7\n10.1001/jamainternmed.2020.0994\n10.1136/bmj.n693\n10.1017/S0950268820001399\n10.1038/s41598-019-45256-2\n10.1136/thoraxjnl-2021-218337\n10.3390/app11062751\n10.1038/s41591-022-01689-3\n10.1007/s11239-021-02447-x\n10.1016/S2213-2600(20)30304-0\n10.1056/NEJMoa2002032\n10.1002/jmv.25976\n10.1093/cvr/cvaa096\n10.1148/radiol.2020200463\n10.1007/s00117-020-00749-4\n10.12968/hmed.2020.0231\n10.1148/radiol.2020200343\n10.1148/radiol.2020201160\n10.1007/s12559-021-09955-1\n10.1109/TMI.2014.2377694\n10.1038/s41598-017-15617-w\n10.1016/j.clinimag.2020.04.001\n10.1016/j.ejrad.2020.109272\n10.1155/2020/9756518\n10.1186/1471-2288-3-25\n10.12788/fp.0045\n10.1145/3466690\n10.1007/s00330-021-08050-1\n10.1007/s42979-021-00690-w\n10.7717/peerj.10309\n10.1016/j.imu.2020.100405\n10.1007/s42600-020-00091-7\n10.2196/19569\n10.1136/bmj.323.7305.157\n10.1111/febs.15375\n10.1016/S2589-7500(20)30186-2\n10.1053/j.semnuclmed.2020.09.001\n10.1038/s42256-021-00307-0\n10.1101/2020.03.12.20027185\n10.1007/s10479-021-04006-2\n10.1109/ACCESS.2021.3085418\n10.1109/TCBB.2021.3066331\n10.1007/s10916-021-01747-2\n10.1371/journal.pone.0067863\n10.1109/JBHI.2020.3037127\n10.1155/2021/7788491\n10.1016/j.bspc.2021.102622\n10.5152/dir.2020.20205\n10.1148/radiol.2020201491\n10.1016/j.cell.2020.04.045\n10.1038/s41467-020-17971-2\n10.1007/s11547-020-01195-x\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1016/j.ejrad.2020.109041\n10.1038/s41591-020-0931-3\n10.1007/s11547-020-01197-9\n10.1109/TMI.2020.2995965\n10.1007/s10096-020-03901-z\n10.1016/j.ejrad.2020.109402\n10.1016/j.media.2021.102216\n10.3390/diagnostics12040869"}
{"title": "Multi-scale Triplet Hashing for Medical Image Retrieval.", "abstract": "For medical image retrieval task, deep hashing algorithms are widely applied in large-scale datasets for auxiliary diagnosis due to the retrieval efficiency advantage of hash codes. Most of which focus on features learning, whilst neglecting the discriminate area of medical images and hierarchical similarity for deep features and hash codes. In this paper, we tackle these dilemmas with a new Multi-scale Triplet Hashing (MTH) algorithm, which can leverage multi-scale information, convolutional self-attention and hierarchical similarity to learn effective hash codes simultaneously. The MTH algorithm first designs multi-scale DenseBlock module to learn multi-scale information of medical images. Meanwhile, a convolutional self-attention mechanism is developed to perform information interaction of the channel domain, which can capture the discriminate area of medical images effectively. On top of the two paths, a novel loss function is proposed to not only conserve the category-level information of deep features and the semantic information of hash codes in the learning process, but also capture the hierarchical similarity for deep features and hash codes. Extensive experiments on the Curated X-ray Dataset, Skin Cancer MNIST Dataset and COVID-19 Radiography Dataset illustrate that the MTH algorithm can further enhance the effect of medical retrieval compared to other state-of-the-art medical image retrieval algorithms.", "journal": "Computers in biology and medicine", "date": "2023-02-25", "authors": ["YaxiongChen", "YiboTang", "JinghaoHuang", "ShengwuXiong"], "doi": "10.1016/j.compbiomed.2023.106633"}
{"title": "Deep learning attention-guided radiomics for COVID-19 chest radiograph classification.", "abstract": "Accurate assessment of coronavirus disease 2019 (COVID-19) lung involvement through chest radiograph plays an important role in effective management of the infection. This study aims to develop a two-step feature merging method to integrate image features from deep learning and radiomics to differentiate COVID-19, non-COVID-19 pneumonia and normal chest radiographs (CXR).\nIn this study, a deformable convolutional neural network (deformable CNN) was developed and used as a feature extractor to obtain 1,024-dimensional deep learning latent representation (DLR) features. Then 1,069-dimensional radiomics features were extracted from the region of interest (ROI) guided by deformable CNN's attention. The two feature sets were concatenated to generate a merged feature set for classification. For comparative experiments, the same process has been applied to the DLR-only feature set for verifying the effectiveness of feature concatenation.\nUsing the merged feature set resulted in an overall average accuracy of 91.0% for three-class classification, representing a statistically significant improvement of 0.6% compared to the DLR-only classification. The recall and precision of classification into the COVID-19 class were 0.926 and 0.976, respectively. The feature merging method was shown to significantly improve the classification performance as compared to using only deep learning features, regardless of choice of classifier (P value <0.0001). Three classes' F1-score were 0.892, 0.890, and 0.950 correspondingly (i.e., normal, non-COVID-19 pneumonia, COVID-19).\nA two-step COVID-19 classification framework integrating information from both DLR and radiomics features (guided by deep learning attention mechanism) has been developed. The proposed feature merging method has been shown to improve the performance of chest radiograph classification as compared to the case of using only deep learning features.", "journal": "Quantitative imaging in medicine and surgery", "date": "2023-02-24", "authors": ["DongrongYang", "GeRen", "RuiyanNi", "Yu-HuaHuang", "Ngo Fung DanielLam", "HongfeiSun", "Shiu Bun NelsonWan", "Man Fung EstherWong", "King KwongChan", "Hoi Ching HaileyTsang", "LuXu", "Tak ChiuWu", "Feng-Ming SpringKong", "Y\u00ec Xi\u00e1ng JW\u00e1ng", "JingQin", "Lawrence Wing ChiChan", "MichaelYing", "JingCai"], "doi": "10.21037/qims-22-531\n10.1186/s13244-021-01096-1\n10.1148/radiol.2021204522\n10.1148/radiol.2020203173\n10.21037/qims-20-771\n10.1007/s13755-021-00146-8\n10.1111/exsy.12749\n10.1080/00325481.2021.2021741\n10.1016/j.radi.2022.03.011\n10.1016/j.eswa.2022.117410\n10.21037/qims-21-791\n10.1016/j.asoc.2021.108190\n10.1016/j.bspc.2021.103182\n10.1007/s10489-020-01829-7\n10.1016/j.compbiomed.2020.104181\n10.3390/diagnostics12020267\n10.1016/j.chaos.2020.110495\n10.1016/j.bspc.2021.103286\n10.3389/fonc.2019.01050\n10.1088/1361-6560/aae56a\n10.1177/20552076221092543\n10.1016/j.ejrad.2021.109673\n10.1109/TNNLS.2021.3119071\n10.1016/j.compbiomed.2021.104304\n10.1007/s10278-021-00421-w\n10.3233/XST-200831\n10.3390/diagnostics11101812\n10.1016/j.compbiomed.2021.104665\n10.1002/mp.15582\n10.1109/CVPR.2017.243\n10.1109/CVPR.2017.243\n10.1109/WACV.2018.00097\n10.1109/WACV.2018.00097\n10.1016/j.compbiomed.2021.104319\n10.1016/j.compbiomed.2021.104319\n10.1109/CiSt49399.2021.9357250\n10.1109/CiSt49399.2021.9357250\n10.1109/ICCV.2017.89\n10.1109/ICCV.2017.89\n10.1007/s11263-015-0816-y\n10.1158/0008-5472.CAN-17-0339\n10.1038/srep11044\n10.1038/srep46349\n10.1002/mp.13891\n10.1007/s13278-021-00731-5\n10.21037/qims-20-1230"}
{"title": "SecureFed: federated learning empowered medical imaging technique to analyze lung abnormalities in chest X-rays.", "abstract": "Machine learning is an effective and accurate technique to diagnose COVID-19 infections using image data, and chest X-Ray (CXR) is no exception. Considering privacy issues, machine learning scientists end up receiving less medical imaging data. Federated Learning (FL) is a privacy-preserving distributed machine learning paradigm that generates an unbiased global model that follows local model (from clients) without exposing their personal data. In the\u00a0case of heterogeneous data among clients, vanilla or default FL mechanism still introduces an insecure method for updating models. Therefore, we proposed SecureFed-a secure aggregation method-which ensures fairness and robustness. In our experiments, we employed COVID-19 CXR dataset (of size 2100 positive cases) and compared it\u00a0with the existing FL frameworks such as FedAvg, FedMGDA+, and FedRAD. In our comparison, we primarily considered robustness (accuracy) and fairness (consistency). As the SecureFed produced consistently better results, it is generic enough to be considered for multimodal data.", "journal": "International journal of machine learning and cybernetics", "date": "2023-02-24", "authors": ["AaishaMakkar", "K CSantosh"], "doi": "10.1007/s13042-023-01789-7\n10.1016/j.patrec.2020.09.010\n10.1021/acsnano.0c04421\n10.1016/j.patrec.2019.11.013\n10.1007/s10916-020-01668-6\n10.3390/app10020559\n10.1109/ACCESS.2020.3010287\n10.3390/sym12040651\n10.1016/j.ins.2022.01.062\n10.1109/MCE.2020.3048926\n10.1007/s10489-020-01943-6\n10.1109/MCI.2018.2866730\n10.1007/s10916-020-01562-1\n10.1007/s10916-020-01645-z\n10.1007/s10916-021-01747-2\n10.1080/21642583.2022.2045645\n10.1109/JIOT.2021.3056185\n10.1109/ACCESS.2019.2903587"}
{"title": "A multiple instance learning approach for detecting COVID-19 in peripheral blood smears.", "abstract": "A wide variety of diseases are commonly diagnosed via the visual examination of cell morphology within a peripheral blood smear. For certain diseases, such as COVID-19, morphological impact across the multitude of blood cell types is still poorly understood. In this paper, we present a multiple instance learning-based approach to aggregate high-resolution morphological information across many blood cells and cell types to automatically diagnose disease at a per-patient level. We integrated image and diagnostic information from across 236 patients to demonstrate not only that there is a significant link between blood and a patient's COVID-19 infection status, but also that novel machine learning approaches offer a powerful and scalable means to analyze peripheral blood smears. Our results both backup and enhance hematological findings relating blood cell morphology to COVID-19, and offer a high diagnostic efficacy; with a 79% accuracy and a ROC-AUC of 0.90.", "journal": "PLOS digital health", "date": "2023-02-23", "authors": ["Colin LCooke", "KanghyunKim", "ShiqiXu", "AmeyChaware", "XingYao", "XiYang", "JadeeNeff", "PatriciaPittman", "ChadMcCall", "CarolynGlass", "Xiaoyin SaraJiang", "RoarkeHorstmeyer"], "doi": "10.1371/journal.pdig.0000078\n10.3343/alm.2013.33.1.1\n10.1136/jcp.2005.035402\n10.1177/1533033818802789\n10.1038/s41598-019-49942-z\n10.1371/journal.pcbi.1005746\n10.1371/journal.pone.0241955\n10.1007/s12015-020-09987-4\n10.2450/2020.0242-20\n10.1159/000510914\n10.1136/bcr-2020-236117\n10.1093/ajcp/aqaa108\n10.4269/ajtmh.20-1536\n10.1016/j.cca.2020.03.022\n10.1016/j.mehy.2020.110371\n10.3390/cells9102206\n10.3389/fimmu.2020.02063\n10.1182/blood.2020007008\n10.1016/j.intimp.2020.107233\n10.1038/2141341a0\n10.1111/bjh.16690\n10.1364/OL.426152\n10.1001/jamainternmed.2018.3763"}
{"title": "Diagnosis of COVID-19 from Multimodal Imaging Data Using Optimized Deep Learning Techniques.", "abstract": "COVID-19 had a global impact, claiming many lives and disrupting healthcare systems even in many developed countries. Various mutations of the severe acute respiratory syndrome coronavirus-2, continue to be an impediment to early detection of this disease, which is vital for social well-being. Deep learning paradigm has been widely applied to investigate multimodal medical image data such as chest X-rays and CT scan images to aid in early detection and decision making about disease containment and treatment. Any method for reliable and accurate screening of COVID-19 infection would be beneficial for rapid detection as well as reducing direct virus exposure in healthcare professionals. Convolutional neural networks (CNN) have previously proven to be quite successful in the classification of medical images. A CNN is used in this study to suggest a deep learning classification method for detecting COVID-19 from chest X-ray images and CT scans. Samples from the Kaggle repository were collected to analyse model performance. Deep learning-based CNN models such as VGG-19, ResNet-50, Inception v3 and Xception models are optimized and compared by evaluating their accuracy after pre-processing the data. Because X-ray is a less expensive process than CT scan, chest X-ray images are considered to have a significant impact on COVID-19 screening. According to this work, chest X-rays outperform CT scans in terms of detection accuracy. The fine-tuned VGG-19 model detected COVID-19 with high accuracy-up to 94.17% for chest X-rays and 93% for CT scans. This work thereby concludes that VGG-19 was found to be the best suited model to detect COVID-19 and chest X-rays yield better accuracy than CT scans for the model.", "journal": "SN computer science", "date": "2023-02-23", "authors": ["S EzhilMukhi", "R ThanujaVarshini", "S Eliza FemiSherley"], "doi": "10.1007/s42979-022-01653-5\n10.1007/s42600-021-00151-6\n10.1016/j.bspc.2020.102365\n10.1007/s10489-020-01902-1\n10.3390/math8060890\n10.3390/jcm9051547\n10.1101/2020.05.05.20085902\n10.1101/2020.03.19.20038950\n10.32604/csse.2022.021438\n10.1016/j.imu.2022.101059\n10.1109/JBHI.2021.3132157"}
{"title": "A qualitative analysis of radiography students' reflective essays regarding their experience of clinical placement during the COVID-19 pandemic.", "abstract": "The COVID-19 pandemic significantly impacted healthcare services and clinical placement for healthcare students. There is a paucity of qualitative research into radiography students' experiences of clinical placement during the pandemic.\nStudents in stages three and four of a 4-year BSc Radiography degree in Ireland wrote reflective essays regarding their experience of clinical placement during the COVID-19 healthcare crisis. Permission was granted by 108 radiography students and recent graduates for their reflections to be analysed as part of this study. A thematic approach to data analysis was used, allowing themes to emerge from the reflective essays. Two researchers independently coded each reflective essay using the Braun and Clarke model.\nFour themes were highlighted; 1) Challenges associated with undertaking clinical placement during the pandemic, such as reduced patient throughput and PPE-related communication barriers; 2) Benefits of clinical placement during the pandemic, in terms of personal and professional development and completing degree requirements to graduate without delay; 3) Emotional impact and 4) Supporting students in clinical practice. Students recognised their resilience and felt proud of their contribution during this healthcare crisis but feared transmitting COVID-19 to family. Educational and emotional support provided by tutors, clinical staff and the university was deemed essential by students during this placement.\nDespite the pressure hospitals were under during the pandemic, students had positive clinical placement experiences and perceived these experiences to have contributed to their professional and personal growth.\nThis study supports the argument for clinical placements to continue throughout healthcare crisis periods, albeit with additional learning and emotional support in place. Clinical placement experiences during the pandemic prompted a deep sense of pride amongst radiography students in their profession and contributed to the development of professional identity.", "journal": "Radiography (London, England : 1995)", "date": "2023-02-23", "authors": ["MO'Connor", "ALunney", "DKearney", "SMurphy"], "doi": "10.1016/j.radi.2023.01.022"}
{"title": "Hybrid feature engineering of medical data via variational autoencoders with triplet loss: a COVID-19 prognosis study.", "abstract": "Medical machine learning frameworks have received much attention in recent years. The recent COVID-19 pandemic was also accompanied by a surge in proposed machine learning algorithms for tasks such as diagnosis and mortality prognosis. Machine learning frameworks can be helpful medical assistants by extracting data patterns that are otherwise hard to detect by humans. Efficient feature engineering and dimensionality reduction are major challenges in most medical machine learning frameworks. Autoencoders are novel unsupervised tools that can perform data-driven dimensionality reduction with minimum prior assumptions. This study, in a novel approach, investigated the predictive power of latent representations obtained from a hybrid autoencoder (HAE) framework combining variational autoencoder (VAE) characteristics with mean squared error (MSE) and triplet loss for forecasting COVID-19 patients with high mortality risk in a retrospective framework. Electronic laboratory and clinical data of 1474 patients were used in the study. Logistic regression with elastic net regularization (EN) and random forest (RF) models were used as final classifiers. Moreover, we also investigated the contribution of utilized features towards latent representations via mutual information analysis. HAE Latent representations model achieved decent performance with an area under ROC curve of 0.921 (\u00b10.027) and 0.910 (\u00b10.036) with EN and RF predictors, respectively, over the hold-out data in comparison with the raw (AUC EN: 0.913 (\u00b10.022); RF: 0.903 (\u00b10.020)) models. The study aims to provide an interpretable feature engineering framework for the medical environment with the potential to integrate imaging data for efficient feature engineering in rapid triage and other clinical predictive models.", "journal": "Scientific reports", "date": "2023-02-23", "authors": ["MahdiMahdavi", "HadiChoubdar", "ZahraRostami", "BehnazNiroomand", "Alexandra TLevine", "AlirezaFatemi", "EhsanBolhasani", "Abdol-HosseinVahabie", "Stephen GLomber", "YaserMerrikhi"], "doi": "10.1038/s41598-023-29334-0\n10.1016/j.ijantimicag.2020.105955\n10.1371/journal.pone.0252384\n10.1371/journal.pmed.1003992\n10.1080/1744666X.2021.1908886\n10.1080/17476348.2022.2049760\n10.1007/s11010-021-04217-y\n10.1142/S1469026820500029\n10.1002/mpr.329\n10.1213/ANE.0000000000005247\n10.1016/j.jbi.2021.103763\n10.1109/ACCESS.2018.2789428\n10.1111/j.1467-9868.2005.00503.x\n10.1186/1471-2105-15-8\n10.1016/j.inffus.2017.12.007\n10.1007/s10462-011-9225-y\n10.1016/j.procs.2020.01.079\n10.1038/s41598-021-93543-8\n10.3390/ijerph19116763\n10.1073/pnas.1906831117\n10.3390/s20041176\n10.3390/app112110377\n10.3389/frai.2020.507973\n10.1002/nop2.1069\n10.1007/s00330-020-06801-0\n10.1038/s41591-020-0931-3\n10.1007/s10654-020-00678-5\n10.3389/fmed.2020.00301\n10.1016/j.ebiom.2020.102925\n10.1038/s41577-020-0407-1\n10.3390/cells9061383\n10.12998/wjcc.v8.i22.5535\n10.1038/s41392-020-00243-2"}
{"title": "Artificial intelligence based approach for categorization of COVID-19 ECG images in presence of other cardiovascular disorders.", "abstract": "Coronavirus disease (COVID-19) is a class of SARS-CoV-2 virus which is initially identified in the later half of the year 2019 and then evolved as a pandemic. If it is not identified in the early stage then the infection and mortality rates increase with time. A timely and reliable approach for COVID-19 identification has become important in order to prevent the disease from spreading rapidly. In recent times, many methods have been suggested for the detection of COVID-19 disease have various flaws, to increase diagnosis performance, fresh investigations are required. In this article, automatically diagnosing COVID-19 using ECG images and deep learning approaches like as Visual Geometry Group (VGG) and AlexNet architectures have been proposed. The proposed method is able to classify between COVID-19, myocardial infarction, normal sinus rhythm, and other abnormal heart beats using Lead-II ECG image only. The efficacy of the technique proposed is validated by using a publicly available ECG image database. We have achieved an accuracy of 77.42% using Alexnet model and 75% accuracy with the help of VGG19 model.", "journal": "Biomedical physics & engineering express", "date": "2023-02-23", "authors": ["M KrishnaChaitanya", "Lakhan DevSharma", "JagdeepRahul", "DikshaSharma", "AmarjitRoy"], "doi": "10.1088/2057-1976/acbd53"}
{"title": "Infrared image method for possible COVID-19 detection through febrile and subfebrile people screening.", "abstract": "This study proposed an infrared image-based method for febrile and subfebrile people screening to comply with the society need for alternative, quick response, and effective methods for COVID-19 contagious people screening. The methodology consisted of: (i) Developing a method based on facial infrared imaging for possible COVID-19 early detection in people with and without fever (subfebrile state); (ii) Using 1206 emergency room (ER) patients to develop an algorithm for general application of the method, and (iii) Testing the method and algorithm effectiveness in 2558 cases (RT-qPCR tested for COVID-19) from 227,261 workers evaluations in five different countries. Artificial intelligence was used through a convolutional neural network (CNN) to develop the algorithm that took facial infrared images as input and classified the tested individuals in three groups: fever (high risk), subfebrile (medium risk), and no fever (low risk). The results showed that suspicious and confirmed COVID-19 (+) cases characterized by temperatures below the 37.5\u00a0\u00b0C fever threshold were identified. Also, average forehead and eye temperatures greater than 37.5\u00a0\u00b0C were not enough to detect fever similarly to the proposed CNN algorithm. Most RT-qPCR confirmed COVID-19 (+) cases found in the 2558 cases sample (17 cases/89.5%) belonged to the CNN selected subfebrile group. The COVID-19 (+) main risk factor was to be in the subfebrile group, in comparison to age, diabetes, high blood pressure, smoking and others. In sum, the proposed method was shown to be a potentially important new tool for COVID-19 (+) people screening for air travel and public places in general.", "journal": "Journal of thermal biology", "date": "2023-02-17", "authors": ["Marcos LealBrioschi", "CarlosDalmaso Neto", "Marcos deToledo", "Eduardo BorbaNeves", "Jos\u00e9 Viriato CoelhoVargas", "Manoel JacobsenTeixeira"], "doi": "10.1016/j.jtherbio.2022.103444\n10.1073/pnas.94.6.2681\n10.1056/NEJMoa2008457\n10.1056/NEJM197201062860109\n10.1001/jama.2020.2565\n10.1212/01.wnl.0000275537.71623.8e\n10.1016/S0099-1767(99)70063-2\n10.1056/NEJMc2000231\n10.1016/S0079-6123(06)62001-3\n10.1016/j.pharmthera.2005.10.013\n10.2310/7060.2004.19102\n10.1016/S0929-6646(09)60017-6\n10.1111/j.1365-2702.2010.03565.x\n10.1016/B978-0-444-64074-1.00029-X\n10.1177/101053950501700107\n10.1371/journal.pone.0201562\n10.2741/1341\n10.1002/rmv.2141\n10.1016/j.jtherbio.2020.102616\n10.1109/EMBC.2018.8513513\n10.1016/j.patrec.2005.10.010\n10.25126/jitecs.20172235\n10.1371/journal.pone.0203302\n10.1111/j.1532-5415.2005.00500.x\n10.1186/1743-422X-7-240\n10.1007/s10278-019-00227-x\n10.1086/659404\n10.1016/S0140-6736(20)30183-5\n10.1016/0163-7258(85)90085-3\n10.1056/NEJMoa2001316\n10.1086/502351\n10.2139/ssrn.3548761\n10.1111/eci.13474\n10.1016/j.autrev.2020.102537\n10.3390/ijerph16234638\n10.1179/146532805X72412\n10.1201/b12938-25\n10.1118/1.1819532\n10.1109/MEMB.2008.931018\n10.1080/03091900500225136\n10.1142/S0219519405001370\n10.1016/j.mvr.2004.05.003\n10.1109/MEMB.2006.1636353\n10.3201/eid1611.100703\n10.1186/1471-2334-11-111\n10.1038/s41598-020-73777-8\n10.2807/1560-7917.ES.2020.25.5.2000080\n10.1142/S0219519413500450\n10.1201/b12938\n10.26717/bjstr.2018.04.0001102\n10.1111/j.1749-6632.1998.tb08317.x\n10.1097/PEC.0b013e3182854465\n10.1016/S1473-3099(20)30086-4\n10.1007/978-981-10-3147-2_19\n10.1016/j.neuron.2018.02.022\n10.1016/j.ymeth.2010.01.005\n10.1111/j.1365-2214.2011.01264.x\n10.1016/j.matcom.2020.04.031\n10.1002/9780470377963\n10.1088/1361-6579/ab2af6\n10.1016/s2213-2600(20)30461-6\n10.21203/rs.3.rs-23651/v2\n10.1016/S2213-2600(20)30079-5\n10.1117/1.jbo.25.9.097002"}
{"title": "Deep Learning With Chest Radiographs for Making Prognoses in Patients With COVID-19: Retrospective Cohort Study.", "abstract": "An artificial intelligence (AI) model using chest radiography (CXR) may provide good performance in making prognoses for COVID-19.\nWe aimed to develop and validate a prediction model using CXR based on an AI model and clinical variables to predict clinical outcomes in patients with COVID-19.\nThis retrospective longitudinal study included patients hospitalized for COVID-19 at multiple COVID-19 medical centers between February 2020 and October 2020. Patients at Boramae Medical Center were randomly classified into training, validation, and internal testing sets (at a ratio of 8:1:1, respectively). An AI model using initial CXR images as input, a logistic regression model using clinical information, and a combined model using the output of the AI model (as CXR score) and clinical information were developed and trained to predict hospital length of stay (LOS) \u22642 weeks, need for oxygen supplementation, and acute respiratory distress syndrome (ARDS). The models were externally validated in the Korean Imaging Cohort of COVID-19 data set for discrimination and calibration.\nThe AI model using CXR and the logistic regression model using clinical variables were suboptimal to predict hospital LOS \u22642 weeks or the need for oxygen supplementation but performed acceptably in the prediction of ARDS (AI model area under the curve [AUC] 0.782, 95% CI 0.720-0.845; logistic regression model AUC 0.878, 95% CI 0.838-0.919). The combined model performed better in predicting the need for oxygen supplementation (AUC 0.704, 95% CI 0.646-0.762) and ARDS (AUC 0.890, 95% CI 0.853-0.928) compared to the CXR score alone. Both the AI and combined models showed good calibration for predicting ARDS (P=.079 and P=.859).\nThe combined prediction model, comprising the CXR score and clinical information, was externally validated as having acceptable performance in predicting severe illness and excellent performance in predicting ARDS in patients with COVID-19.", "journal": "Journal of medical Internet research", "date": "2023-02-17", "authors": ["Hyun WooLee", "Hyun JunYang", "HyungjinKim", "Ue-HwanKim", "Dong HyunKim", "Soon HoYoon", "Soo-YounHam", "Bo DaNam", "Kum JuChae", "DabeeLee", "Jin YoungYoo", "So HyeonBak", "Jin YoungKim", "Jin HwanKim", "Ki BeomKim", "Jung ImJung", "Jae-KwangLim", "Jong EunLee", "Myung JinChung", "Young KyungLee", "Young SeonKim", "Sang MinLee", "WoocheolKwon", "Chang MinPark", "Yun-HyeonKim", "Yeon JooJeong", "Kwang NamJin", "Jin MoGoo"], "doi": "10.2196/42717\n10.1136/thoraxjnl-2020-216425\n10.1136/bmj.m1328\n10.3389/fmed.2021.704256\n10.2196/30157\n10.21037/atm.2020.02.71\n10.1183/13993003.04188-2020\n10.1002/emp2.12205\n10.1038/s41746-021-00546-w\n10.1038/s41746-021-00546-w\n10.1186/s12879-022-07617-7\n10.1186/s12879-022-07617-7\n10.1016/S2589-7500(21)00039-X\n10.1136/bmj.g7594\n10.3346/jkms.2020.35.e413\n10.1097/JTO.0b013e3181ec173d\n10.6339/jds.2005.03(3).206\n10.1001/jamanetworkopen.2019.0204\n10.1016/j.jbi.2017.10.008\n10.1007/s11547-020-01232-9\n10.1007/s00330-020-07504-2\n10.1371/journal.pone.0245518\n10.2214/AJR.20.24801\n10.7759/cureus.9448\n10.1038/s41598-021-86853-4\n10.1038/s41598-021-86853-4\n10.3904/kjim.2020.329\n10.1038/s41467-020-18786-x\n10.1038/s41467-020-18786-x\n10.1056/NEJMcp2009575\n10.1186/s13613-020-00650-2\n10.7861/clinmed.2020-0214\n10.4046/trd.2021.0009\n10.1080/17476348.2020.1804365\n10.1016/S2213-2600(21)00105-3\n10.1056/NEJMoa2021436\n10.1056/NEJMoa2007764"}
{"title": "LDANet: Automatic lung parenchyma segmentation from CT images.", "abstract": "Automatic segmentation of the lung parenchyma from computed tomography (CT) images is helpful for the subsequent diagnosis and treatment of patients. In this paper, based on a deep learning algorithm, a lung dense attention network (LDANet) is proposed with two mechanisms: residual spatial attention (RSA) and gated channel attention (GCA). RSA is utilized to weight the spatial information of the lung parenchyma and suppress feature activation in irrelevant regions, while the weights of each channel are adaptively calibrated using GCA to implicitly predict potential key features. Then, a dual attention guidance module (DAGM) is designed to maximize the integration of the advantages of both mechanisms. In addition, LDANet introduces a lightweight dense block (LDB) that reuses feature information and a positioned transpose block (PTB) that realizes accurate positioning and gradually restores the image resolution until the predicted segmentation map is generated. Experiments are conducted on two public datasets, LIDC-IDRI and COVID-19 CT Segmentation, on which LDANet achieves Dice similarity coefficient values of 0.98430 and 0.98319, respectively, outperforming a state-of-the-art lung segmentation model. Additionally, the effectiveness of the main components of LDANet is demonstrated through ablation experiments.", "journal": "Computers in biology and medicine", "date": "2023-02-16", "authors": ["YingChen", "LongfengFeng", "ChengZheng", "TaohuiZhou", "LanLiu", "PengfeiLiu", "YiChen"], "doi": "10.1016/j.compbiomed.2023.106659"}
{"title": "Home alone: A population neuroscience investigation of brain morphology substrates.", "abstract": "As a social species, ready exchange with peers is a pivotal asset - our \"social capital\". Yet, single-person households have come to pervade metropolitan cities worldwide, with unknown consequences in the long run. Here, we systematically explore the morphological manifestations associated with singular living in \u223c40,000 UK Biobank participants. The uncovered population-level signature spotlights the highly associative default mode network, in addition to findings such as in the amygdala central, cortical and corticoamygdaloid nuclei groups, as well as the hippocampal fimbria and dentate gyrus. Both positive effects, equating to greater gray matter volume associated with living alone, and negative effects, which can be interpreted as greater gray matter associations with not living alone, were found across the cortex and subcortical structures Sex-stratified analyses revealed male-specific neural substrates, including somatomotor, saliency and visual systems, while female-specific neural substrates centered on the dorsomedial prefrontal cortex. In line with our demographic profiling results, the discovered neural pattern of living alone is potentially linked to alcohol and tobacco consumption, anxiety, sleep quality as well as daily TV watching. The persistent trend for solitary living will require new answers from public-health decision makers. SIGNIFICANCE STATEMENT: Living alone has profound consequences for mental and physical health. Despite this, there has been a rapid increase in single-person households worldwide, with the long-term consequences yet unknown. In the largest study of its kind, we investigate how the objective lack of everyday social interaction, through living alone, manifests in the brain. Our population neuroscience approach uncovered a gray matter signature that converged on the 'default network', alongside targeted subcortical, sex and demographic profiling analyses. The human urge for social relationships is highlighted by the evolving COVID-19 pandemic. Better understanding of how social isolation relates to the brain will influence health and social policy decision-making of pandemic planning, as well as social interventions in light of global shifts in houseful structures.", "journal": "NeuroImage", "date": "2023-02-14", "authors": ["MaryAnnNoonan", "ChrisZajner", "DaniloBzdok"], "doi": "10.1016/j.neuroimage.2023.119936"}
{"title": "Classifying COVID-19 Patients From Chest X-ray Images Using Hybrid Machine Learning Techniques: Development and Evaluation.", "abstract": "The COVID-19 pandemic has raised global concern, with moderate to severe cases displaying lung inflammation and respiratory failure. Chest x-ray (CXR) imaging is crucial for diagnosis and is usually interpreted by experienced medical specialists. Machine learning has been applied with acceptable accuracy, but computational efficiency has received less attention.\nWe introduced a novel hybrid machine learning model to accurately classify COVID-19, non-COVID-19, and healthy patients from CXR images with reduced computational time and promising results. Our proposed model was thoroughly evaluated and compared with existing models.\nA retrospective study was conducted to analyze 5 public data sets containing 4200 CXR images using machine learning techniques including decision trees, support vector machines, and neural networks. The images were preprocessed to undergo image segmentation, enhancement, and feature extraction. The best performing machine learning technique was selected and combined into a multilayer hybrid classification model for COVID-19 (MLHC-COVID-19). The model consisted of 2 layers. The first layer was designed to differentiate healthy individuals from infected patients, while the second layer aimed to classify COVID-19 and non-COVID-19 patients.\nThe MLHC-COVID-19 model was trained and evaluated on unseen COVID-19 CXR images, achieving reasonably high accuracy and F measures of 0.962 and 0.962, respectively. These results show the effectiveness of the MLHC-COVID-19 in classifying COVID-19 CXR images, with improved accuracy and a reduction in interpretation time. The model was also embedded into a web-based MLHC-COVID-19 computer-aided diagnosis system, which was made publicly available.\nThe study found that the MLHC-COVID-19 model effectively differentiated CXR images of COVID-19 patients from those of healthy and non-COVID-19 individuals. It outperformed other state-of-the-art deep learning techniques and showed promising results. These results suggest that the MLHC-COVID-19 model could have been instrumental in early detection and diagnosis of COVID-19 patients, thus playing a significant role in controlling and managing the pandemic. Although the pandemic has slowed down, this model can be adapted and utilized for future similar situations. The model was also integrated into a publicly accessible web-based computer-aided diagnosis system.", "journal": "JMIR formative research", "date": "2023-02-14", "authors": ["ThanakornPhumkuea", "ThakerngWongsirichot", "KasikritDamkliang", "AsmaNavasakulpong"], "doi": "10.2196/42324\n10.1016/j.clim.2020.108427\n10.1038/s41564-020-0695-z\n10.1007/s12098-020-03263-6\n10.1056/NEJMoa2001316\n10.1016/j.jmii.2020.05.001\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1016/j.jaci.2020.04.029\n10.1001/jama.2020.2783\n10.1016/j.acra.2020.04.016\n10.1002/jmv.26830\n10.22037/aaem.v9i1.993\n10.1001/jama.2020.3786\n10.1016/j.ijid.2020.03.017\n10.1056/nejmoa2030340\n10.1016/j.patrec.2020.09.010\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-76550-z\n10.1109/access.2020.3010287\n10.1038/s41746-020-00372-6\n10.1038/s41746-020-00372-6\n10.1371/journal.pone.0250688\n10.1371/journal.pone.0250688\n10.3390/a14060183\n10.1016/j.cmpb.2020.105581\n10.1016/j.mehy.2020.109761\n10.1016/j.media.2020.101794\n10.1007/s13755-020-00135-3\n10.1371/journal.pone.0256630\n10.1371/journal.pone.0256630\n10.1371/journal.pone.0247839\n10.1371/journal.pone.0247839\n10.1016/j.eswa.2020.114054\n10.1371/journal.pone.0242535\n10.1371/journal.pone.0242535\n10.1155/2021/8890226\n10.1155/2021/8890226\n10.1371/journal.pone.0029740\n10.1371/journal.pone.0029740\n10.1117/1.3115362\n10.26671/ijirg.2019.6.8.101\n10.1007/s10916-019-1376-4\n10.7763/IJIMT.2013.V4.426\n10.4015/S1016237218500412\n10.1016/j.procs.2017.08.021\n10.38094/JASTT20165\n10.1016/j.compedu.2019.04.001\n10.1016/j.procs.2019.02.085\n10.14569/IJACSA.2016.070203\n10.1007/s11227-018-2469-4\n10.1155/2018/9385947\n10.1109/MOCAST.2019.8741677\n10.5121/ijdkp.2015.5201\n10.1016/j.knosys.2011.06.013\n10.3389/fninf.2014.00014\n10.1613/jair.953\n10.1016/j.ins.2019.10.048"}
{"title": "Real-time intelligent classification of COVID-19 and thrombosis via massive image-based analysis of platelet aggregates.", "abstract": "Microvascular thrombosis is a typical symptom of COVID-19 and shows similarities to thrombosis. Using a microfluidic imaging flow cytometer, we measured the blood of 181 COVID-19 samples and 101 non-COVID-19 thrombosis samples, resulting in a total of 6.3 million bright-field images. We trained a convolutional neural network to distinguish single platelets, platelet aggregates, and white blood cells and performed classical image analysis for each subpopulation individually. Based on derived single-cell features for each population, we trained machine learning models for classification between COVID-19 and non-COVID-19 thrombosis, resulting in a patient testing accuracy of 75%. This result indicates that platelet formation differs between COVID-19 and non-COVID-19 thrombosis. All analysis steps were optimized for efficiency and implemented in an easy-to-use plugin for the image viewer napari, allowing the entire analysis to be performed within seconds on mid-range computers, which could be used for real-time diagnosis.", "journal": "Cytometry. Part A : the journal of the International Society for Analytical Cytology", "date": "2023-02-12", "authors": ["ChenqiZhang", "MaikHerbig", "YuqiZhou", "MasakoNishikawa", "MohammadShifat-E-Rabbi", "HiroshiKanno", "RuoxiYang", "YumaIbayashi", "Ting-HuiXiao", "Gustavo KRohde", "MasatakaSato", "SatoshiKodera", "MasaoDaimon", "YutakaYatomi", "KeisukeGoda"], "doi": "10.1002/cyto.a.24721"}
{"title": "CNN-RNN Network Integration for the Diagnosis of COVID-19 Using Chest X-ray and CT Images.", "abstract": "The 2019 coronavirus disease (COVID-19) has rapidly spread across the globe. It is crucial to identify positive cases as rapidly as humanely possible to provide appropriate treatment for patients and prevent the pandemic from spreading further. Both chest X-ray and computed tomography (CT) images are capable of accurately diagnosing COVID-19. To distinguish lung illnesses (i.e., COVID-19 and pneumonia) from normal cases using chest X-ray and CT images, we combined convolutional neural network (CNN) and recurrent neural network (RNN) models by replacing the fully connected layers of CNN with a version of RNN. In this framework, the attributes of CNNs were utilized to extract features and those of RNNs to calculate dependencies and classification base on extracted features. CNN models VGG19, ResNet152V2, and DenseNet121 were combined with long short-term memory (LSTM) and gated recurrent unit (GRU) RNN models, which are convenient to develop because these networks are all available as features on many platforms. The proposed method is evaluated using a large dataset totaling 16,210 X-ray and CT images (5252 COVID-19 images, 6154 pneumonia images, and 4804 normal images) were taken from several databases, which had various image sizes, brightness levels, and viewing angles. Their image quality was enhanced via normalization, gamma correction, and contrast-limited adaptive histogram equalization. The ResNet152V2 with GRU model achieved the best architecture with an accuracy of 93.37%, an F1 score of 93.54%, a precision of 93.73%, and a recall of 93.47%. From the experimental results, the proposed method is highly effective in distinguishing lung diseases. Furthermore, both CT and X-ray images can be used as input for classification, allowing for the rapid and easy detection of COVID-19.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-02-12", "authors": ["IsoonKanjanasurat", "KasiTenghongsakul", "BoonchanaPurahong", "AttasitLasakul"], "doi": "10.3390/s23031356\n10.1515/labmed-2020-0135\n10.1016/j.bios.2020.112830\n10.1016/j.eswa.2022.117275\n10.1007/s42452-021-04427-5\n10.3390/app12157953\n10.1109/ICEAST55249.2022.9826319\n10.1109/TMI.2020.3040950\n10.1016/j.imu.2020.100412\n10.1016/j.compbiomed.2021.104319\n10.1016/j.chemolab.2022.104695\n10.1016/j.ejrad.2020.109041\n10.1016/j.eng.2020.04.010\n10.1007/s10489-020-01831-z\n10.1016/j.asoc.2021.107918\n10.48550/arXiv.2006.11988\n10.1109/ACCESS.2020.3010287\n10.1016/j.cell.2020.04.045\n10.1016/j.cell.2018.02.010\n10.1109/ICACCI.2014.6968381\n10.1007/BF03178082\n10.48550/arXiv.1409.1556\n10.1145/3065386\n10.1007/978-3-319-46493-0_38\n10.1109/CVPR.2017.243\n10.1162/neco.1997.9.8.1735\n10.48550/arXiv.1406.1078\n10.48550/arxiv.1207.0580\n10.1021/ci0342472\n10.1140/epjs/s11734-022-00647-x\n10.1016/j.compbiomed.2020.103792\n10.1080/07391102.2020.1767212\n10.1016/j.imu.2020.100360\n10.48550/arXiv.2201.09952\n10.1159/000521658\n10.1145/3551647\n10.1051/matecconf/201927702001"}
{"title": "COVID-19 Classification on Chest X-ray Images Using Deep Learning Methods.", "abstract": "Since December 2019, the coronavirus disease has significantly affected millions of people. Given the effect this disease has on the pulmonary systems of humans, there is a need for chest radiographic imaging (CXR) for monitoring the disease and preventing further deaths. Several studies have been shown that Deep Learning models can achieve promising results for COVID-19 diagnosis towards the CXR perspective. In this study, five deep learning models were analyzed and evaluated with the aim of identifying COVID-19 from chest X-ray images. The scope of this study is to highlight the significance and potential of individual deep learning models in COVID-19 CXR images. More specifically, we utilized the ResNet50, ResNet101, DenseNet121, DenseNet169 and InceptionV3 using Transfer Learning. All models were trained and validated on the largest publicly available repository for COVID-19 CXR images. Furthermore, they were evaluated on unknown data that was not used for training or validation, authenticating their performance and clarifying their usage in a medical scenario. All models achieved satisfactory performance where ResNet101 was the superior model achieving 96% in Precision, Recall and Accuracy, respectively. Our outcomes show the potential of deep learning models on COVID-19 medical offering a promising way for the deeper understanding of COVID-19.", "journal": "International journal of environmental research and public health", "date": "2023-02-12", "authors": ["MariosConstantinou", "ThemisExarchos", "Aristidis GVrahatis", "PanagiotisVlamos"], "doi": "10.3390/ijerph20032035\n10.15167/2421-4248/jpmh2020.61.3.1530\n10.1080/14737159.2020.1757437\n10.1101/2022.02.11.22270873\n10.1148/radiol.2020200642\n10.2214/AJR.20.23034\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMra072149\n10.1109/RBME.2020.2987975\n10.1109/TIM.2021.3128703\n10.1109/TMI.2022.3219286\n10.1016/j.apacoust.2020.107279\n10.1016/j.measurement.2018.05.033\n10.21608/mjeer.2019.76962\n10.1038/nature21056\n10.1038/s41598-019-48995-4\n10.1109/ACCESS.2020.3010287\n10.1007/s00330-021-08050-1\n10.1016/j.compbiomed.2020.103805\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.2994762\n10.1016/j.compbiomed.2021.105002\n10.1007/s11042-022-12156-z\n10.1016/j.media.2020.101797\n10.1109/TMI.2013.2284099\n10.1109/TMI.2013.2290491\n10.1007/s13755-021-00146-8\n10.1088/1361-6560/ac34b2"}
{"title": "Interpretable Differential Diagnosis of Non-COVID Viral Pneumonia, Lung Opacity and COVID-19 Using Tuned Transfer Learning and Explainable AI.", "abstract": "The coronavirus epidemic has spread to virtually every country on the globe, inflicting enormous health, financial, and emotional devastation, as well as the collapse of healthcare systems in some countries. Any automated COVID detection system that allows for fast detection of the COVID-19 infection might be highly beneficial to the healthcare service and people around the world. Molecular or antigen testing along with radiology X-ray imaging is now utilized in clinics to diagnose COVID-19. Nonetheless, due to a spike in coronavirus and hospital doctors' overwhelming workload, developing an AI-based auto-COVID detection system with high accuracy has become imperative. On X-ray images, the diagnosis of COVID-19, non-COVID-19 non-COVID viral pneumonia, and other lung opacity can be challenging. This research utilized artificial intelligence (AI) to deliver high-accuracy automated COVID-19 detection from normal chest X-ray images. Further, this study extended to differentiate COVID-19 from normal, lung opacity and non-COVID viral pneumonia images. We have employed three distinct pre-trained models that are Xception, VGG19, and ResNet50 on a benchmark dataset of 21,165 X-ray images. Initially, we formulated the COVID-19 detection problem as a binary classification problem to classify COVID-19 from normal X-ray images and gained 97.5%, 97.5%, and 93.3% accuracy for Xception, VGG19, and ResNet50 respectively. Later we focused on developing an efficient model for multi-class classification and gained an accuracy of 75% for ResNet50, 92% for VGG19, and finally 93% for Xception. Although Xception and VGG19's performances were identical, Xception proved to be more efficient with its higher precision, recall, and f-1 scores. Finally, we have employed Explainable AI on each of our utilized model which adds interpretability to our study. Furthermore, we have conducted a comprehensive comparison of the model's explanations and the study revealed that Xception is more precise in indicating the actual features that are responsible for a model's predictions.This addition of explainable AI will benefit the medical professionals greatly as they will get to visualize how a model makes its prediction and won't have to trust our developed machine-learning models blindly.", "journal": "Healthcare (Basel, Switzerland)", "date": "2023-02-12", "authors": ["Md NazmulIslam", "Md Golam RabiulAlam", "Tasnim SakibApon", "Md ZiaUddin", "NasserAllheeib", "AlaaMenshawi", "Mohammad MehediHassan"], "doi": "10.3390/healthcare11030410\n10.1038/s41579-018-0118-9\n10.1001/jama.2020.3786\n10.1183/13993003.04188-2020\n10.1148/radiol.2020200823\n10.1016/j.media.2021.102216\n10.1016/j.media.2021.102225\n10.1016/j.media.2021.102205\n10.1038/s41598-020-76550-z\n10.1016/j.cmpb.2020.105581\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110071\n10.3390/app10134640\n10.1007/s10044-021-00984-y\n10.1109/ACCESS.2020.2994762\n10.3390/sym12040651\n10.1109/ACCESS.2020.3044858\n10.1109/TMI.2020.3040950\n10.1080/07391102.2020.1767212\n10.1109/ACCESS.2020.3010287\n10.1007/s11042-022-12156-z\n10.1007/s11042-022-12156-z\n10.1109/CVPR.2016.90\n10.1109/CVPR.2009.5206848\n10.1109/CVPR.2017.195\n10.1093/comjnl/14.4.407\n10.1007/s11263-019-01228-7"}
{"title": "[Performance in prognostic capacity and efficiency of the Thoracic Care Suite GE AI tool applied to chest radiography of patients with COVID-19 pneumonia].", "abstract": "Rapid progression of COVID-19 pneumonia may put patients at risk of requiring ventilatory support, such as non-invasive mechanical ventilation or endotracheal intubation. Implementing tools that detect COVID-19 pneumonia can improve the patient's healthcare. We aim to evaluate the efficacy and efficiency of the artificial intelligence (AI) tool GE Healthcare's Thoracic Care Suite (featuring Lunit INSIGHT CXR, TCS) to predict the ventilatory support need based on pneumonic progression of COVID-19 on consecutive chest X-rays.\nOutpatients with confirmed SARS-CoV-2 infection, with chest X-ray (CXR) findings probable or indeterminate for COVID-19 pneumonia, who required a second CXR due to unfavorable clinical course, were collected. The number of affected lung fields for the two CXRs was assessed using the AI tool.\nOne hundred fourteen patients (57.4 \u00b1 14.2 years, 65 -57%- men) were retrospectively collected. Fifteen (13.2%) required ventilatory support. Progression of pneumonic extension \u2265 0.5 lung fields per day compared to pneumonia onset, detected using the TCS tool, increased the risk of requiring ventilatory support by 4-fold. Analyzing the AI output required 26 seconds of radiological time.\nApplying the AI tool, Thoracic Care Suite, to CXR of patients with COVID-19 pneumonia allows us to anticipate ventilatory support requirements requiring less than half a minute.\nLa r\u00e1pida progresi\u00f3n de la neumon\u00eda COVID-19 puede implicar la necesidad de recurrir a sistemas de respiraci\u00f3n asistida, como la ventilaci\u00f3n mec\u00e1nica no invasiva o la intubaci\u00f3n endotraqueal. La introducci\u00f3n de herramientas que detecten la neumon\u00eda COVID-19 puede mejorar la atenci\u00f3n sanitaria de los pacientes. Nuestro objetivo es evaluar la eficacia y la eficiencia de la herramienta de inteligencia artificial (IA) Thoracic Care Suite de GE Healthcare (que incorpora Lunit Insight CXR) para predecir la necesidad de recurrir a la respiraci\u00f3n asistida en funci\u00f3n de la progresi\u00f3n de la neumon\u00eda en la COVID-19 en radiograf\u00edas tor\u00e1cicas consecutivas.\nSe incluy\u00f3 a pacientes ambulatorios con infecci\u00f3n por SARS-CoV-2 confirmada, con hallazgos probables o indeterminados de neumon\u00eda COVID-19 en la radiograf\u00eda tor\u00e1cica (RXT) y que necesitaron una segunda RXT debido a la evoluci\u00f3n cl\u00ednica desfavorable. En las 2\u00a0RXT se evaluaron el n\u00famero de campos pulmonares afectados mediante la herramienta de IA.\nSe incluy\u00f3 a 114 pacientes (57,4\u00a0\u00b114,2 a\u00f1os; 65 de ellos varones, el 57%) de forma retrospectiva; 15 pacientes (el 13,2%) precisaron respiraci\u00f3n asistida. La progresi\u00f3n de la diseminaci\u00f3n neum\u00f3nica \u22650,5 campos pulmonares al d\u00eda en comparaci\u00f3n con el inicio de la neumon\u00eda, detectada mediante la herramienta TCS, cuadruplic\u00f3 el riesgo de precisar respiraci\u00f3n asistida. El an\u00e1lisis de los resultados de IA precis\u00f3 26 segundos.\nAplicar la herramienta de IA, Thoracic Care Suite, a la RXT de pacientes con neumon\u00eda COVID-19 nos permite predecir la necesidad de recurrir a la respiraci\u00f3n asistida en menos de medio minuto.", "journal": "Radiologia", "date": "2023-02-07", "authors": ["Juana Mar\u00edaPlasencia-Mart\u00ednez", "RafaelP\u00e9rez-Costa", "M\u00f3nicaBallesta-Ruiz", "Jos\u00e9Mar\u00eda Garc\u00eda-Santos"], "doi": "10.1016/j.rx.2022.11.012\n10.3389/fpubh.2021.647315\n10.1186/s12871-020-01095-7\n10.1186/s13244-020-00954-8\n10.1016/j.jacr.2020.02.008\n10.1007/s00330-021-08418-3\n10.1007/s11547-020-01200-3\n10.1067/j.cpradiol.2021.03.017\n10.1016/j.eswa.2021.115695\n10.1007/s11547-020-01232-9\n10.1136/bmj.j4683\n10.1016/j.ejrad.2021.109583\n10.1136/bmj.m1328\n10.1136/bmj.m3339\n10.1016/j.rx.2021.09.011\n10.1007/s00330-021-08049-8\n10.1016/j.chest.2020.04.003"}
{"title": "A novel approach for detection of COVID-19 and Pneumonia using only binary classification from chest CT-scans.", "abstract": "The novel Coronavirus, Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) spread all over the world, causing a dramatic shift in circumstances that resulted in a massive pandemic, affecting the world's well-being and stability. It is an RNA virus that can infect both humans as well as animals. Diagnosis of the virus as soon as possible could contain and avoid a serious COVID-19 outbreak. Current pharmaceutical techniques and diagnostic methods tests such as Reverse Transcription-Polymerase Chain Reaction (RT-PCR) and Serology tests are time-consuming, expensive, and require a well-equipped laboratory for analysis, making them restrictive and inaccessible to everyone. Deep Learning has grown in popularity in recent years, and it now plays a crucial role in Image Classification, which also involves Medical Imaging. Using chest CT scans, this study explores the problem statement automation of differentiating COVID-19 contaminated individuals from healthy individuals. Convolutional Neural Networks (CNNs) can be trained to detect patterns in computed tomography scans (CT scans). Hence, different CNN models were used in the current study to identify variations in chest CT scans, with accuracies ranging from 91% to 98%. The Multiclass Classification method is used to build these architectures. This study also proposes a new approach for classifying CT images that use two binary classifications combined to work together, achieving 98.38% accuracy. All of these architectures' performances are compared using different classification metrics.", "journal": "Neuroscience informatics", "date": "2023-02-07", "authors": ["SanskarHasija", "PeddaputhaAkash", "MagantiBhargav Hemanth", "AnkitKumar", "SanjeevSharma"], "doi": "10.1016/j.neuri.2022.100069\n10.1016/j.compbiomed.2020.103795\n10.1016/j.chaos.2020.110120\n10.1109/ACCESS.2020.3010287\n10.3389/fmed.2020.608525\n10.3389/fmed.2020.608525\n10.1109/ACCESS.2020.3001973\n10.1148/radiol.2017162326\n10.1101/2020.04.12.20062661\n10.1007/s00330-020-06731-x\n10.1016/j.chaos.2020.109944\n10.1007/s11263-019-01228-7\n10.1101/2020.07.11.20151332\n10.1016/j.compbiomed.2020.103805\n10.1016/j.asoc.2020.106897\n10.1148/ryct.2020200067\n10.1016/j.eng.2020.04.010\n10.1007/s13244-018-0639-9"}
{"title": "Classification of COVID-19 from community-acquired pneumonia: Boosting the performance with capsule network and maximum intensity projection image of CT scans.", "abstract": "The coronavirus disease 2019 (COVID-19) and community-acquired pneumonia (CAP) present a high degree of similarity in chest computed tomography (CT) images. Therefore, a procedure for accurately and automatically distinguishing between them is crucial.\nA deep learning method for distinguishing COVID-19 from CAP is developed using maximum intensity projection (MIP) images from CT scans. LinkNet is employed for lung segmentation of chest CT images. MIP images are produced by superposing the maximum gray of intrapulmonary CT values. The MIP images are input into a capsule network for patient-level pred iction and diagnosis of COVID-19. The network is trained using 333 CT scans (168 COVID-19/165 CAP) and validated on three external datasets containing 3581 CT scans (2110 COVID-19/1471 CAP).\nLinkNet achieves the highest Dice coefficient of 0.983 for lung segmentation. For the classification of COVID-19 and CAP, the capsule network with the DenseNet-121 feature extractor outperforms ResNet-50 and Inception-V3, achieving an accuracy of 0.970 on the training dataset. Without MIP or the capsule network, the accuracy decreases to 0.857 and 0.818, respectively. Accuracy scores of 0.961, 0.997, and 0.949 are achieved on the external validation datasets. The proposed method has higher or comparable sensitivity compared with ten state-of-the-art methods.\nThe proposed method illustrates the feasibility of applying MIP images from CT scans to distinguish COVID-19 from CAP using capsule networks. MIP images provide conspicuous benefits when exploiting deep learning to detect COVID-19 lesions from CT scans and the capsule network improves COVID-19 diagnosis.", "journal": "Computers in biology and medicine", "date": "2023-02-05", "authors": ["YananWu", "QianqianQi", "ShouliangQi", "LimingYang", "HanlinWang", "HuiYu", "JianpengLi", "GangWang", "PingZhang", "ZhenyuLiang", "RongchangChen"], "doi": "10.1016/j.compbiomed.2023.106567"}
{"title": "A Review of Deep Learning Applications in Lung Ultrasound Imaging of COVID-19 Patients.", "abstract": "The massive and continuous spread of COVID-19 has motivated researchers around the world to intensely explore, understand, and develop new techniques for diagnosis and treatment. Although lung ultrasound imaging is a less established approach when compared to other medical imaging modalities such as X-ray and CT, multiple studies have demonstrated its promise to diagnose COVID-19 patients. At the same time, many deep learning models have been built to improve the diagnostic efficiency of medical imaging. The integration of these initially parallel efforts has led multiple researchers to report deep learning applications in medical imaging of COVID-19 patients, most of which demonstrate the outstanding potential of deep learning to aid in the diagnosis of COVID-19. This invited review is focused on deep learning applications in lung ultrasound imaging of COVID-19 and provides a comprehensive overview of ultrasound systems utilized for data acquisition, associated datasets, deep learning models, and comparative performance.", "journal": "BME frontiers", "date": "2023-01-31", "authors": ["LingyiZhao", "Muyinatu ALediju Bell"], "doi": "10.34133/2022/9780173"}
{"title": "Lightweight ResGRU: a deep learning-based prediction of SARS-CoV-2 (COVID-19) and its severity classification using multimodal chest radiography images.", "abstract": "The new COVID-19 emerged in a town in China named Wuhan in December 2019, and since then, this deadly virus has infected 324 million people worldwide and caused 5.53 million deaths by January 2022. Because of the rapid spread of this pandemic, different countries are facing the problem of a shortage of resources, such as medical test kits and ventilators, as the number of cases increased uncontrollably. Therefore, developing a readily available, low-priced, and automated approach for COVID-19 identification is the need of the hour. The proposed study uses chest radiography images (CRIs) such as X-rays and computed tomography (CTs) to detect chest infections, as these modalities contain important information about chest infections. This research introduces a novel hybrid deep learning model named ", "journal": "Neural computing & applications", "date": "2023-01-31", "authors": ["MugheesAhmad", "Usama IjazBajwa", "YasarMehmood", "Muhammad WaqasAnwar"], "doi": "10.1007/s00521-023-08200-0\n10.1016/S0140-6736(20)30183-5\n10.1038/s41586-020-2008-3\n10.1007/978-3-030-60188-1_2\n10.1016/j.chaos.2020.110495\n10.1016/j.chaos.2021.110713\n10.1016/S1473-3099(20)30134-1\n10.1016/j.bea.2021.100003\n10.2217/fmb-2020-0098\n10.1016/S0140-6736(20)30154-9\n10.1148/ryct.2020200028\n10.1148/radiol.2020201473\n10.1148/ryct.2020200213\n10.3389/fnins.2021.601109\n10.1002/widm.1312\n10.1145/3065386\n10.1109/CVPR.2016.90\n10.1051/matecconf/201927702001\n10.1016/j.compbiomed.2020.103795\n10.1016/j.imu.2020.100405\n10.1016/j.ejrad.2020.109402\n10.1007/s00330-021-07715-1\n10.1016/j.eswa.2020.114054\n10.1007/s11042-021-11388-9\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100412\n10.1016/j.asoc.2021.107160\n10.1007/s10489-020-01888-w\n10.1007/s42600-021-00151-6\n10.1016/j.ijleo.2021.166405\n10.1038/s41568-020-00327-9\n10.1016/j.compmedimag.2019.05.001\n10.1038/nature14539\n10.1080/07391102.2020.1767212\n10.1016/j.bspc.2021.102490\n10.1016/j.patcog.2021.108255"}
{"title": "COVID-19 lung infection segmentation from chest CT images based on CAPA-ResUNet.", "abstract": "Coronavirus disease 2019 (COVID-19) epidemic has devastating effects on personal health around the world. It is significant to achieve accurate segmentation of pulmonary infection regions, which is an early indicator of disease. To solve this problem, a deep learning model, namely, the content-aware pre-activated residual UNet (CAPA-ResUNet), was proposed for segmenting COVID-19 lesions from CT slices. In this network, the pre-activated residual block was used for down-sampling to solve the problems of complex foreground and large fluctuations of distribution in datasets during training and to avoid gradient disappearance. The area loss function based on the false segmentation regions was proposed to solve the problem of fuzzy boundary of the lesion area. This model was evaluated by the public dataset (COVID-19 Lung CT Lesion Segmentation Challenge-2020) and compared its performance with those of classical models. Our method gains an advantage over other models in multiple metrics. Such as the Dice coefficient, specificity (Spe), and intersection over union (IoU), our CAPA-ResUNet obtained 0.775 points, 0.972 points, and 0.646 points, respectively. The Dice coefficient of our model was 2.51% higher than Content-aware residual UNet (CARes-UNet). The code is available at https://github.com/malu108/LungInfectionSeg.", "journal": "International journal of imaging systems and technology", "date": "2023-01-31", "authors": ["LuMa", "ShuniSong", "LitingGuo", "WenjunTan", "LishengXu"], "doi": "10.1002/ima.22819"}
{"title": "Longitudinal changes in global structural brain connectivity and cognitive performance in former hospitalized COVID-19 survivors: an exploratory study.", "abstract": "Long-term sequelae of COVID-19 can result in reduced functionality of the central nervous system and substandard quality of life. Gaining insight into the recovery trajectory of admitted COVID-19 patients on their cognitive performance and global structural brain connectivity may allow a better understanding of the diseases' relevance.\nTo\u00a0assess whole-brain structural connectivity in former non-intensive-care unit (ICU)- and ICU-admitted COVID-19 survivors over 2 months following hospital discharge and correlate structural connectivity measures to cognitive performance.\nParticipants underwent Magnetic Resonance Imaging brain scans and a cognitive test battery after hospital discharge to evaluate structural connectivity and cognitive performance. Multilevel models were constructed for each graph measure and cognitive test, assessing the groups' influence, time since discharge, and interactions. Linear regression models estimated whether the graph measurements affected cognitive measures and whether they differed between ICU and non-ICU patients.\nSix former ICU and six non-ICU patients completed the study. Across the various graph measures, the characteristic path length decreased over time (\u03b2\u2009=\u20090.97, p\u2009=\u20090.006). We detected no group-level effects (\u03b2\u2009=\u20091.07, p\u2009=\u20090.442) nor interaction effects (\u03b2\u2009=\u20091.02, p\u2009=\u20090.220). Cognitive performance improved for both non-ICU and ICU COVID-19 survivors on four out of seven cognitive tests 2 months later (p\u2009<\u20090.05).\nAdverse effects of COVID-19 on brain functioning and structure abate over time. These results should be supported by future research including larger sample sizes, matched control groups of healthy non-infected individuals, and more extended follow-up periods.", "journal": "Experimental brain research", "date": "2023-01-29", "authors": ["BTassignon", "ARadwan", "JBlommaert", "LStas", "S DAllard", "FDe Ridder", "EDe Waele", "L CBulnes", "NHoornaert", "PLacor", "ELathouwers", "RMertens", "MNaeyaert", "HRaeymaekers", "LSeyler", "A MVan Binst", "LVan Imschoot", "LVan Liedekerke", "JVan Schependom", "PVan Schuerbeek", "MVandekerckhove", "RMeeusen", "SSunaert", "GNagels", "JDe Mey", "KDe Pauw"], "doi": "10.1007/s00221-023-06545-5\n10.1016/j.neuroimage.2010.09.025\n10.1177/19714009211029177\n10.1093/sleep/34.5.581\n10.3357/AMHP.4343.2015\n10.1016/j.neuroimage.2018.09.073\n10.18637/jss.v067.i01\n10.1016/0028-3932(95)00035-2\n10.2967/jnumed.121.262128\n10.1016/j.bbi.2020.06.008\n10.1002/alz.12255\n10.1111/ene.14775\n10.1016/j.neuroimage.2006.01.021\n10.1038/s41586-022-04569-5\n10.1503/cmaj.1095958\n10.1016/j.neuroimage.2012.01.021\n10.3233/JAD-200581\n10.1212/WNL.0000000000010979\n10.1016/j.jns.2021.117486\n10.1038/s41564-020-0695-z\n10.1378/chest.130.3.869\n10.1093/brain/awab009\n10.1093/brain/awab435\n10.1016/j.cell.2020.08.028\n10.1016/j.nicl.2015.05.003\n10.1016/j.neuroimage.2011.09.015\n10.1016/j.media.2013.02.006\n10.1016/j.ebiom.2021.103512\n10.1186/s12879-022-07062-6\n10.1212/WNL.0000000000010112\n10.1148/radiol.2020202222\n10.18637/jss.v082.i13\n10.1007/s00330-020-06761-5\n10.1037/1076-898X.8.2.75\n10.1007/s00415-020-10313-8\n10.1016/j.bbi.2020.05.037\n10.1016/j.eclinm.2020.100484\n10.1111/ene.14444\n10.9778/cmajo.20210023\n10.1001/jamaneurol.2020.1127\n10.1006/nimg.1995.1012\n10.1016/j.cmi.2020.11.005\n10.1111/nyas.12807\n10.1016/j.clineuro.2020.105921\n10.1016/j.yebeh.2013.11.019\n10.1007/s11571-017-9461-1\n10.1007/s00406-021-01346-9\n10.1016/j.neuroimage.2008.05.046\n10.1016/j.mri.2019.05.008\n10.1002/brb3.518\n10.1016/j.neuroimage.2012.06.005\n10.1016/j.neuroimage.2012.11.049\n10.1016/j.neuroimage.2015.06.092\n10.1084/jem.20202135\n10.1001/jama.2016.0701\n10.1016/j.neuroimage.2007.02.016\n10.1162/NECO_a_00914\n10.1007/s10072-020-04575-3\n10.1038/s41398-021-01426-3\n10.1007/s11571-019-09562-9"}
{"title": "Artificial Intelligence in Paediatric Tuberculosis.", "abstract": "Tuberculosis (TB) continues to be a leading cause of death in children despite global efforts focused on early diagnosis and interventions to limit the spread of the disease. This challenge has been made more complex in the context of the coronavirus pandemic, which has disrupted the \"End TB Strategy\" and framework set out by the World Health Organization (WHO). Since the inception of artificial intelligence (AI) more than 60\u00a0years ago, the interest in AI has risen and more recently we have seen the emergence of multiple real-world applications, many of which relate to medical imaging. Nonetheless, real-world AI applications and clinical studies are limited in the niche area of paediatric imaging. This review article will focus on how AI, or more specifically deep learning, can be applied to TB diagnosis and management in children. We describe how deep learning can be utilised in chest imaging to provide computer-assisted diagnosis to augment workflow and screening efforts. We also review examples of recent AI applications for TB screening in resource constrained environments and we explore some of the challenges and the future directions of AI in paediatric TB.", "journal": "Pediatric radiology", "date": "2023-01-28", "authors": ["JaishreeNaidoo", "Susan ChengShelmerdine", "Carlos F Ugas-Charcape", "Arhanjit SinghSodhi"], "doi": "10.1007/s00247-023-05606-9\n10.7754/Clin.Lab.2015.150509\n10.21037/jtd-21-1342\n10.1093/cid/ciac011\n10.1155/2014/291841\n10.1097/INF.0000000000000792\n10.1136/adc.2004.062315\n10.5588/ijtld.15.0201\n10.5588/ijtld.18.0122\n10.1007/s00247-017-3866-1\n10.1007/s00247-020-04625-0\n10.1164/rccm.202202-0259OC\n10.1093/cid/ciab708\n10.1038/s41598-021-03265-0\n10.1007/s00330-020-07024-z\n10.1155/2021/5359084\n10.1016/S2589-7500(20)30221-1\n10.1148/radiol.2017162326\n10.1007/s00330-020-07219-4\n10.1148/radiol.2021210063\n10.1038/s41598-021-93967-2\n10.3389/fmolb.2022.874475\n10.1016/S2589-7500(21)00116-3\n10.1038/s41598-019-51503-3\n10.1038/s41746-020-0273-z\n10.1093/cid/ciab639\n10.1007/s00259-021-05432-x\n10.3389/frai.2022.827299\n10.1007/s00330-021-08365-z\n10.1016/j.clinimag.2022.04.009\n10.21037/qims-21-676\n10.1148/radiol.2018181422\n10.1038/s41591-018-0307-0\n10.1371/journal.pone.0212094\n10.1007/s00247-021-05146-0\n10.1093/cid/ciy967\n10.1371/journal.pone.0221339\n10.5588/ijtld.17.0520\n10.1038/s41746-021-00393-9\n10.3389/fdata.2022.850383\n10.1007/s00247-019-04593-0\n10.1038/s41598-020-73831-5\n10.1371/journal.pone.0206410\n10.1097/INF.0000000000001872\n10.1002/ppul.24230\n10.1002/ppul.24500\n10.1007/s00247-017-3895-9\n10.1038/srep12215\n10.1038/s41598-020-62148-y"}
{"title": "CT imaging of HIV-associated pulmonary disorders in COVID-19 pandemic.", "abstract": "Opportunistic infections in people living with human immunodeficiency virus (HIV) are readily detected with thoracic computed tomography (CT), but differential diagnosis remains a challenge. The global COVID-19 pandemic further exacerbates the issue, with SARS-CoV-2 having overlapping CT findings with infections common in HIV patients and complicating prior epidemiological data. We present a pictorial review of CT findings associated with COVID-19-mimicking opportunistic infections that can be encountered in HIV patients. PubMed database was searched for the complete list of relevant conditions, and a Venn diagram was constructed to highlight overlapping entities. The diagram showed five major disease groups: viral pneumonia, fungal pneumonia, bacterial pneumonia, sarcoidosis, and lung cancer. As these pathologies possess a wide range of features, the findings were grouped as \u201ctypical\u201d and \u201cother\u201d for easier comprehension with provided relevant epidemiological data and discrepancies observed in available literature. The review highlights the importance of a specific approach to differential diagnosis in immunocompromised patients compared to immunocompetent hosts and the utility of follow-up scans.", "journal": "Clinical imaging", "date": "2023-01-28", "authors": ["Liya RAbuladze", "Ivan ABlokhin", "Anna PGonchar", "Maria MSuchilova", "Anton VVladzymyrskyy", "Victor AGombolevskiy", "Eleonora ABalanyuk", "Oksana GNi", "Dmitry VTroshchansky", "Roman VReshetnikov"], "doi": "10.1016/j.clinimag.2023.01.006\n10.1098/rstb.2010.0031\n10.1182/blood.V68.1.281.281\n10.1371/journal.pone.0112237\n10.1148/rg.2018180149\n10.21037/jtd.2019.06.22\n10.3201/eid1010.030985\n10.1177/014107680109400907\n10.1183/09031936.00200210\n10.3389/fmed.2022.829624\n10.1080/22221751.2021.1957402\n10.3390/microorganisms10010178\n10.15585/mmwr.mm6937a6\n10.15585/mmwr.mm7129a1\n10.1007/s00330-003-2044-z\n10.1007/s10140-021-01919-0\n10.1186/s13244-020-00933-z\n10.1007/s10461-020-02983-2\n10.1097/RTI.0000000000000554\n10.1002/rmv.655\n10.1097/00002030-199703110-00009\n10.1097/QAD.0000000000002484\n10.1055/s-0036-1578802\n10.1371/journal.pone.0256452\n10.1016/S0002-9343(97)00015-6\n10.1098/rstb.2015.0468\n10.1097/QAD.0000000000002498\n10.1038/s41598-021-92605-1\n10.2147/HIV.S53910\n10.4172/2161-0517.1000113\n10.7860/JCDR/2017/24219.9277\n10.1126/scitranslmed.3004404\n10.4236/jbm.2020.89002\n10.1016/j.pt.2021.07.010\n10.1148/rg.2017160110\n10.1016/j.ejrad.2006.04.017\n10.1186/s12879-020-05217-x\n10.1183/13993003.01369-2016\n10.1164/rccm.200804-617OC\n10.1097/MCP.0b013e3283375825\n10.1080/14787210.2018.1495560\n10.1097/QAI.0b013e3181eef4f7\n10.1016/j.rcl.2005.10.009\n10.1007/s11604-011-0574-x\n10.1089/108729101750301906\n10.2169/internalmedicine.51.7326\n10.1183/13993003.congress-2021.PA1985\n10.1093/ofid/ofaa441\n10.1378/chest.119.3.978\n10.1148/radiology.218.1.r01ja25242\n10.1097/MCP.0000000000000800\n10.1016/j.crad.2010.03.004\n10.1067/j.cpradiol.2020.10.013\n10.1148/rg.306105512\n10.1097/QAD.0b013e328352d1ad\n10.1097/COH.0000000000000326\n10.1097/QAD.0000000000000943\n10.1148/radiol.2017161659\n10.1016/j.rcl.2018.01.004\n10.1378/chest.12-2355\n10.1513/AnnalsATS.201607-568BC\n10.2214/AJR.10.7262\n10.1016/j.lungcan.2003.07.001\n10.1097/JTO.0b013e31821038ab\n10.3389/fonc.2021.616149\n10.21037/jtd.2019.02.91\n10.21569/2222-7415-2020-10-1-252-256\n10.1016/j.chest.2016.10.010\n10.3109/03009731003695624\n10.1183/16000617.0019-2016\n10.3390/v14050972\n10.3390/jcm11072017\n10.2147/IDR.S335711\n10.1002/rcr2.725\n10.1016/j.jmii.2020.06.007"}
{"title": "Artificial intelligence for differentiating COVID-19 from other viral pneumonias on CT: comparative analysis of different models based on quantitative and radiomic approaches.", "abstract": "To develop a pipeline for automatic extraction of quantitative metrics and radiomic features from lung computed tomography (CT) and develop artificial intelligence (AI) models supporting differential diagnosis between coronavirus disease 2019 (COVID-19) and other viral pneumonia (non-COVID-19).\nChest CT of 1,031 patients (811 for model building; 220 as independent validation set (IVS) with positive swab for severe acute respiratory syndrome coronavirus-2 (647 COVID-19) or other respiratory viruses (384 non-COVID-19) were segmented automatically. A Gaussian model, based on the HU histogram distribution describing well-aerated and ill portions, was optimised to calculate quantitative metrics (QM, n = 20) in both lungs (2L) and four geometrical subdivisions (GS) (upper front, lower front, upper dorsal, lower dorsal; n = 80). Radiomic features (RF) of first (RF1, n = 18) and second (RF2, n = 120) order were extracted from 2L using PyRadiomics tool. Extracted metrics were used to develop four multilayer-perceptron classifiers, built with different combinations of QM and RF: Model1 (RF1-2L); Model2 (QM-2L, QM-GS); Model3 (RF1-2L, RF2-2L); Model4 (RF1-2L, QM-2L, GS-2L, RF2-2L).\nThe classifiers showed accuracy from 0.71 to 0.80 and area under the receiving operating characteristic curve (AUC) from 0.77 to 0.87 in differentiating COVID-19 versus non-COVID-19 pneumonia. Best results were associated with Model3 (AUC 0.867 \u00b1 0.008) and Model4 (AUC 0.870 \u00b1 0.011. For the IVS, the AUC values were 0.834 \u00b1 0.008 for Model3 and 0.828 \u00b1 0.011 for Model4.\nFour AI-based models for classifying patients as COVID-19 or non-COVID-19 viral pneumonia showed good diagnostic performances that could support clinical decisions.", "journal": "European radiology experimental", "date": "2023-01-24", "authors": ["GiuliaZorzi", "LucaBerta", "FrancescoRizzetto", "CristinaDe Mattia", "Marco Maria JacopoFelisi", "StefanoCarrazza", "SilviaNerini Molteni", "ChiaraVismara", "FrancescoScaglione", "AngeloVanzulli", "AlbertoTorresin", "Paola EnricaColombo"], "doi": "10.1186/s41747-022-00317-6\n10.1016/S0140-6736(21)00947-8\n10.1056/NEJMoa2035389\n10.1148/radiol.2020201365\n10.1016/j.diii.2020.03.014\n10.2214/AJR.20.22976\n10.1148/rg.2020200097\n10.1016/j.ejmp.2021.01.004\n10.1038/nrclinonc.2017.141\n10.1016/j.cmpb.2020.105608\n10.7150/thno.46428\n10.1016/j.ejro.2020.100271\n10.1016/j.ejmp.2021.04.022\n10.1007/s00330-020-07269-8\n10.1148/radiol.2020201491\n10.1016/S2589-7500(20)30199-0\n10.21037/atm-20-5328\n10.1007/s00259-020-05075-4\n10.1148/radiol.2020200823\n10.1016/j.ejrad.2021.110028\n10.3390/diagnostics12040869\n10.1186/s41747-020-00173-2\n10.1016/j.acra.2015.01.008\n10.1148/radiol.2020201433\n10.1097/RCT.0b013e31818da65c\n10.1158/0008-5472.CAN-17-0339\n10.1111/j.2517-6161.1996.tb02080.x\n10.1016/j.ejmp.2021.06.001\n10.7717/peerj-cs.564\n10.1038/s41467-020-17971-2\n10.1016/j.ejrad.2021.109552\n10.1007/s11547-021-01370-8\n10.1109/TSMC.1973.4309314\n10.1093/annonc/mdx034\n10.1038/s41598-019-56989-5\n10.21037/tcr.2016.07.11\n10.1016/j.ejrad.2021.109650\n10.1007/s00330-022-08685-8"}
{"title": "Towards precision medicine: Omics approach for COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic had a devastating impact on human society. Beginning with genome surveillance of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the development of omics technologies brought a clearer understanding of the complex SARS-CoV-2 and COVID-19. Here, we reviewed how omics, including genomics, proteomics, single-cell multi-omics, and clinical phenomics, play roles in answering biological and clinical questions about COVID-19. Large-scale sequencing and advanced analysis methods facilitate COVID-19 discovery from virus evolution and severity risk prediction to potential treatment identification. Omics would indicate precise and globalized prevention and medicine for the COVID-19 pandemic under the utilization of big data capability and phenotypes refinement. Furthermore, decoding the evolution rule of SARS-CoV-2 by deep learning models is promising to forecast new variants and achieve more precise data to predict future pandemics and prevent them on time.", "journal": "Biosafety and health", "date": "2023-01-24", "authors": ["XiaopingCen", "FengaoWang", "XinheHuang", "DragomirkaJovic", "FredDubee", "HuanmingYang", "YixueLi"], "doi": "10.1016/j.bsheal.2023.01.002\n10.1073/pnas.0408290102\n10.1038/s41588-022-01033-y\n10.1126/scitranslmed.abk3445\n10.1126/science.abd7331\n10.1126/science.abm1208\n10.1016/j.gpb.2022.01.001\n10.3389/fimmu.2021.622176\n10.1038/s41588-021-00854-7\n10.1038/s41421-021-00318-6\n10.1056/nejmoa2020283\n10.1038/s41586-021-03767-x\n10.1038/s41588-021-00996-8\n10.1038/s41588-021-00955-3\n10.1038/s41588-021-00986-w\n10.1038/s41588-022-01042-x\n10.1038/s41588-021-01006-7\n10.1038/s41586-022-04826-7\n10.1038/s43587-021-00067-x\n10.1016/j.cell.2021.01.004\n10.1016/j.immuni.2020.10.008\n10.1038/s41586-021-03493-4\n10.1038/s41467-021-27716-4\n10.1038/s41467-021-24482-1\n10.1016/j.cell.2022.01.012\n10.1126/scitranslmed.abj7521\n10.1038/s41586-020-2588-y\n10.7554/eLife.62522\n10.1084/jem.20210582\n10.1016/j.immuni.2020.11.017\n10.1016/j.cell.2020.10.037\n10.1038/s41586-021-03570-8\n10.1038/s42255-021-00425-4\n10.1038/s41591-021-01329-2\n10.1038/s41746-021-00399-3\n10.1038/s41467-020-17971-2\n10.1038/s41467-020-17280-8\n10.1038/s41551-020-00633-5\n10.1016/j.xcrm.2022.100580\n10.1080/10408363.2020.1851167\n10.1002/mco2.90\n10.1038/s41587-021-01131-y\n10.1016/j.xinn.2022.100289\n10.1055/s-0040-1712549\n10.1093/cid/ciab754\n10.1080/14760584.2021.1976153\n10.1002/jmv.27524\n10.1038/s41392-022-01105-9\n10.1038/s41586-021-04188-6\n10.2807/1560-7917.ES.2021.26.24.2100509\n10.46234/ccdcw2021.255\n10.1016/j.gpb.2020.09.001\n10.1093/nar/gkw1065\n10.1093/ve/veab064\n10.1126/science.abe3261\n10.1126/science.abc0523\n10.1126/science.abb9263\n10.1038/s41467-022-31511-0\n10.1038/s41467-020-18314-x\n10.1126/scitranslmed.abn7979\n10.1126/science.abp8337\n10.1038/s41467-020-19345-0\n10.1038/s41591-020-0997-y\n10.1038/s41591-020-1000-7\n10.1126/science.abq5358\n10.1038/s41576-022-00483-8\n10.1016/S1473-3099(21)00170-5\n10.1016/S2468-2667(21)00055-4\n10.1016/j.cell.2020.11.020\n10.7554/eLife.65365\n10.1038/s41576-021-00408-x\n10.1038/s41586-021-04352-y\n10.1038/s41586-021-03792-w\n10.1089/omi.2021.0182\n10.1021/acs.jproteome.1c00475\n10.1039/d0cb00163e\n10.1038/s41746-021-00431-6\n10.1038/s42256-021-00307-0\n10.1038/s42256-021-00377-0\n10.1038/s41591-022-01843-x\n10.1089/jwh.2021.0411\n10.1001/jamanetworkopen.2021.47053\n10.1016/S0140-6736(22)00941-2\n10.1038/s41591-022-01840-0\n10.1001/jamapsychiatry.2022.2640\n10.1016/j.cell.2022.01.014\n10.1016/j.immuni.2022.01.017\n10.1126/sciimmunol.abk1741\n10.1038/s41591-022-01837-9\n10.3389/fimmu.2022.838132\n10.1126/sciimmunol.abm7996\n10.1136/bmj.o407\n10.1038/s41586-020-2355-0\n10.1016/j.jval.2021.10.007\n10.1038/d41586-022-03181-x"}
{"title": "Development of a Fast Fourier Transform-based Analytical Method for COVID-19 Diagnosis from Chest X-Ray Images Using GNU Octave.", "abstract": "Many artificial intelligence-based computational procedures are developed to diagnose COVID-19 infection from chest X-ray (CXR) images, as diagnosis by CXR imaging is less time consuming and economically cheap compared to other detection procedures. Due to unavailability of skilled computer professionals and high computer architectural resource, majority of the employed methods are difficult to implement in rural and poor economic settings. Majority of such reports are devoid of codes and ignores related diseases (pneumonia). The absence of codes makes limitation in applying them widely. Hence, validation testing followed by evidence-based medical practice is difficult. The present work was aimed to develop a simple method that requires a less computational expertise and minimal level of computer resource, but with statistical inference.\nA Fast Fourier Transform-based (FFT) method was developed with GNU Octave, a free and open-source platform. This was employed to the images of CXR for further analysis. For statistical inference, two variables, i.e., the highest peak and number of peaks in the FFT distribution plot were considered.\nThe comparison of mean values among different groups (normal, COVID-19, viral, and bacterial pneumonia [BP]) showed statistical significance, especially when compared to normal, except between viral and BP groups.\nParametric statistical inference from our result showed high level of significance (", "journal": "Journal of medical physics", "date": "2023-01-24", "authors": ["DurjoyMajumder"], "doi": "10.4103/jmp.jmp_26_22"}
{"title": "DMFL_Net: A Federated Learning-Based Framework for the Classification of COVID-19 from Multiple Chest Diseases Using X-rays.", "abstract": "Coronavirus Disease 2019 (COVID-19) is still a threat to global health and safety, and it is anticipated that deep learning (DL) will be the most effective way of detecting COVID-19 and other chest diseases such as lung cancer (LC), tuberculosis (TB), pneumothorax (PneuTh), and pneumonia (Pneu). However, data sharing across hospitals is hampered by patients' right to privacy, leading to unexpected results from deep neural network (DNN) models. Federated learning (FL) is a game-changing concept since it allows clients to train models together without sharing their source data with anybody else. Few studies, however, focus on improving the model's accuracy and stability, whereas most existing FL-based COVID-19 detection techniques aim to maximize secondary objectives such as latency, energy usage, and privacy. In this work, we design a novel model named decision-making-based federated learning network (DMFL_Net) for medical diagnostic image analysis to distinguish COVID-19 from four distinct chest disorders including LC, TB, PneuTh, and Pneu. The DMFL_Net model that has been suggested gathers data from a variety of hospitals, constructs the model using the DenseNet-169, and produces accurate predictions from information that is kept secure and only released to authorized individuals. Extensive experiments were carried out with chest X-rays (CXR), and the performance of the proposed model was compared with two transfer learning (TL) models, i.e., VGG-19 and VGG-16 in terms of accuracy (ACC), precision (PRE), recall (REC), specificity (SPF), and F1-measure. Additionally, the DMFL_Net model is also compared with the default FL configurations. The proposed DMFL_Net + DenseNet-169 model achieves an accuracy of 98.45% and outperforms other approaches in classifying COVID-19 from four chest diseases and successfully protects the privacy of the data among diverse clients.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-01-22", "authors": ["HassaanMalik", "AhmadNaeem", "Rizwan AliNaqvi", "Woong-KeeLoh"], "doi": "10.3390/s23020743\n10.3390/electronics11172714\n10.1109/OJCS.2022.3206407\n10.3390/life12070958\n10.1002/int.22777\n10.1109/TCBB.2022.3184319\n10.1109/ACCESS.2020.3037474\n10.1038/s41746-020-00323-1\n10.1109/bigdata50022.2020.9377873\n10.3389/fpubh.2022.892499\n10.1109/JIOT.2021.3056185\n10.1109/JIOT.2021.3120998\n10.1145/3501296\n10.1016/j.asoc.2021.107330\n10.1038/s41591-021-01506-3\n10.2196/24207\n10.1007/978-3-030-11723-8_9\n10.1109/JIOT.2019.2956615\n10.1109/isbi.2019.8759317\n10.1145/3528580.3532845\n10.1109/JBHI.2022.3143576\n10.1109/icic53490.2021.9693006\n10.1007/s00530-021-00878-3\n10.1371/journal.pone.0266462\n10.18280/ijdne.170106\n10.1007/978-981-16-7618-5_13\n10.14419/ijet.v9i3.30655\n10.1007/s11042-022-13843-7\n10.1007/s10796-022-10307-z\n10.3233/shti220697\n10.1109/JIOT.2022.3144450\n10.1016/j.compbiomed.2022.105233\n10.1109/TMM.2018.2889934\n10.1109/JIOT.2019.2920987\n10.1109/ACCESS.2018.2885997\n10.1016/j.asoc.2020.106859\n10.1016/j.knosys.2021.106775\n10.32604/cmc.2022.020344\n10.1016/j.dib.2020.106520\n10.1111/exsy.13173\n10.1109/ACCESS.2021.3102399\n10.1145/3431804\n10.32604/cmc.2021.013191\n10.1007/s11042-022-13499-3\n10.3390/diagnostics11091735\n10.1001/jamanetworkopen.2019.1095\n10.1016/j.cell.2018.02.010\n10.2214/ajr.174.1.1740071\n10.1016/j.fcij.2017.12.001\n10.1109/ACCESS.2020.3031384\n10.1038/s41598-020-76282-0\n10.26599/TST.2021.9010026\n10.1109/ubmk52708.2021.9558913\n10.1109/JBHI.2020.3005160\n10.1016/j.bspc.2021.102588\n10.1016/j.patrec.2020.09.010\n10.1111/exsy.12759\n10.1146/annurev-bioeng-110220-012203\n10.2174/1573405617666210414101941\n10.1007/s42600-021-00135-6\n10.3390/s22155652\n10.1016/j.jpha.2021.12.006\n10.1007/s00530-021-00826-1\n10.3390/jpm12020275\n10.1002/wcms.1597\n10.1109/ACCESS.2020.3001507\n10.1007/s40747-022-00866-8"}
{"title": "Novel Comparative Study for the Detection of COVID-19 Using CT Scan and Chest X-ray Images.", "abstract": "The number of coronavirus disease (COVID-19) cases is constantly rising as the pandemic continues, with new variants constantly emerging. Therefore, to prevent the virus from spreading, coronavirus cases must be diagnosed as soon as possible. The COVID-19 pandemic has had a devastating impact on people's health and the economy worldwide. For COVID-19 detection, reverse transcription-polymerase chain reaction testing is the benchmark. However, this test takes a long time and necessitates a lot of laboratory resources. A new trend is emerging to address these limitations regarding the use of machine learning and deep learning techniques for automatic analysis, as these can attain high diagnosis results, especially by using medical imaging techniques. However, a key question arises whether a chest computed tomography scan or chest X-ray can be used for COVID-19 detection. A total of 17,599 images were examined in this work to develop the models used to classify the occurrence of COVID-19 infection, while four different classifiers were studied. These are the convolutional neural network (proposed architecture (named, SCovNet) and Resnet18), support vector machine, and logistic regression. Out of all four models, the proposed SCoVNet architecture reached the best performance with an accuracy of almost 99% and 98% on chest computed tomography scan images and chest X-ray images, respectively.", "journal": "International journal of environmental research and public health", "date": "2023-01-22", "authors": ["AhatshamHayat", "PreetyBaglat", "F\u00e1bioMendon\u00e7a", "Sheikh ShanawazMostafa", "FernandoMorgado-Dias"], "doi": "10.3390/ijerph20021268\n10.1016/j.jds.2020.02.002\n10.1016/j.ajem.2020.03.036\n10.1007/s11042-021-10714-5\n10.1093/aje/kwab093\n10.1038/s41597-021-00900-3\n10.1038/s41598-020-76282-0\n10.1007/s11547-019-00990-5\n10.1007/s11547-020-01135-9\n10.1007/s11547-020-01277-w\n10.1007/s12559-020-09773-x\n10.1109/TNNLS.2018.2790388\n10.1016/j.jksuci.2020.03.013\n10.1016/j.chaos.2020.110190\n10.1016/S2589-7500(21)00039-X\n10.1007/s00500-020-05424-3\n10.1016/j.bbe.2021.05.013\n10.1155/2021/6658058\n10.1016/j.chaos.2020.109944\n10.3389/frai.2021.694875\n10.1016/j.cmpb.2020.105581\n10.1101/2020.03.26.20044610\n10.3390/app11083414\n10.3390/healthcare10020343\n10.17632/8h65ywd2jr.3\n10.1155/2021/5587188\n10.1038/s41598-021-86735-9\n10.1109/72.788646\n10.1088/1742-6596/1748/4/042054\n10.1007/s11263-019-01228-7"}
{"title": "Research on the Application of Artificial Intelligence in Public Health Management: Leveraging Artificial Intelligence to Improve COVID-19 CT Image Diagnosis.", "abstract": "Since the start of 2020, the outbreak of the Coronavirus disease (COVID-19) has been a global public health emergency, and it has caused unprecedented economic and social disaster. In order to improve the diagnosis efficiency of COVID-19 patients, a number of researchers have conducted extensive studies on applying artificial intelligence techniques to the analysis of COVID-19-related medical images. The automatic segmentation of lesions from computed tomography (CT) images using deep learning provides an important basis for the quantification and diagnosis of COVID-19 cases. For a deep learning-based CT diagnostic method, a few of accurate pixel-level labels are essential for the training process of a model. However, the translucent ground-glass area of the lesion usually leads to mislabeling while performing the manual labeling operation, which weakens the accuracy of the model. In this work, we propose a method for correcting rough labels; that is, to hierarchize these rough labels into precise ones by performing an analysis on the pixel distribution of the infected and normal areas in the lung. The proposed method corrects the incorrectly labeled pixels and enables the deep learning model to learn the infected degree of each infected pixel, with which an aiding system (named DLShelper) for COVID-19 CT image diagnosis using the hierarchical labels is also proposed. The DLShelper targets lesion segmentation from CT images, as well as the severity grading. The DLShelper assists medical staff in efficient diagnosis by providing rich auxiliary diagnostic information (including the severity grade, the proportions of the lesion and the visualization of the lesion area). A comprehensive experiment based on a public COVID-19 CT image dataset is also conducted, and the experimental results show that the DLShelper significantly improves the accuracy of segmentation for the lesion areas and also achieves a promising accuracy for the severity grading task.", "journal": "International journal of environmental research and public health", "date": "2023-01-22", "authors": ["TianchengHe", "HongLiu", "ZhihaoZhang", "ChaoLi", "YoumeiZhou"], "doi": "10.3390/ijerph20021158\n10.3390/diagnostics10110901\n10.1007/s11548-021-02466-2\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.06.025\n10.1109/RBME.2020.2987975\n10.1016/j.cell.2020.04.045\n10.1109/TIP.2021.3058783\n10.1609/aaai.v35i6.16617\n10.1109/TMI.2020.2996645\n10.1016/j.media.2018.11.010\n10.1109/TMI.2020.3000314\n10.1007/s00521-022-07709-0\n10.1109/TMI.2017.2775636\n10.1016/j.jvcir.2016.11.019\n10.1186/s12938-015-0014-8\n10.1109/TMI.2012.2196285\n10.3389/fpubh.2022.1015876\n10.1002/mp.12273\n10.1016/j.cmpb.2019.06.005\n10.1007/s10278-019-00254-8\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2644615\n10.1613/jair.953"}
{"title": "COVID-19 Detection Mechanism in Vehicles Using a Deep Extreme Machine Learning Approach.", "abstract": "COVID-19 is a rapidly spreading pandemic, and early detection is important to halting the spread of infection. Recently, the outbreak of this virus has severely affected people around the world with increasing death rates. The increased death rates are because of its spreading nature among people, mainly through physical interactions. Therefore, it is very important to control the spreading of the virus and detect people's symptoms during the initial stages so proper preventive measures can be taken in good time. In response to COVID-19, revolutionary automation such as deep learning, machine learning, image processing, and medical images such as chest radiography (CXR) and computed tomography (CT) have been developed in this environment. Currently, the coronavirus is identified via an RT-PCR test. Alternative solutions are required due to the lengthy moratorium period and the large number of false-negative estimations. To prevent the spreading of the virus, we propose the Vehicle-based COVID-19 Detection System to reveal the related symptoms of a person in the vehicles. Moreover, deep extreme machine learning is applied. The proposed system uses headaches, flu, fever, cough, chest pain, shortness of breath, tiredness, nasal congestion, diarrhea, breathing difficulty, and pneumonia. The symptoms are considered parameters to reveal the presence of COVID-19 in a person. Our proposed approach in Vehicles will make it easier for governments to perform COVID-19 tests timely in cities. Due to the ambiguous nature of symptoms in humans, we utilize fuzzy modeling for simulation. The suggested COVID-19 detection model achieved an accuracy of more than 90%.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-22", "authors": ["AreejFatima", "TariqShahzad", "SagheerAbbas", "AbdurRehman", "YousafSaeed", "MeshalAlharbi", "Muhammad AdnanKhan", "KhmaiesOuahada"], "doi": "10.3390/diagnostics13020270\n10.1056/NEJMoa2001017\n10.46234/ccdcw2020.017\n10.1016/S0140-6736(20)30154-9\n10.1016/S0140-6736(20)30183-5\n10.1017/ice.2020.61\n10.9781/ijimai.2020.02.002\n10.3390/jcm9030674\n10.1136/jim-2020-001491\n10.3390/diagnostics12040846\n10.1007/s10140-021-01937-y\n10.3390/diagnostics12071617\n10.9781/ijimai.2018.04.003\n10.1198/004017005000000058\n10.3978/j.issn.2072-1439.2015.04.61\n10.1088/1742-6596/892/1/012016\n10.15837/ijccc.2010.3.2481\n10.1016/j.artmed.2016.12.003\n10.1080/00401706.1980.10486139\n10.3390/app12094493\n10.1007/s00330-021-07715-1\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.2976452\n10.3233/AIS-200554\n10.32604/cmc.2020.011155\n10.1007/s13042-011-0019-y"}
{"title": "Automated Pneumonia Based Lung Diseases Classification with Robust Technique Based on a Customized Deep Learning Approach.", "abstract": "Many people have been affected by infectious lung diseases (ILD). With the outbreak of the COVID-19 disease in the last few years, many people have waited for weeks to recover in the intensive care wards of hospitals. Therefore, early diagnosis of ILD is of great importance to reduce the occupancy rates of health institutions and the treatment time of patients. Many artificial intelligence-based studies have been carried out in detecting and classifying diseases from medical images using imaging applications. The most important goal of these studies was to increase classification performance and model reliability. In this approach, a powerful algorithm based on a new customized deep learning model (ACL model), which trained synchronously with the attention and LSTM model with CNN models, was proposed to classify healthy, COVID-19 and Pneumonia. The important stains and traces in the chest X-ray (CX-R) image were emphasized with the marker-controlled watershed (MCW) segmentation algorithm. The ACL model was trained for different training-test ratios (90-10%, 80-20%, and 70-30%). For 90-10%, 80-20%, and 70-30% training-test ratios, accuracy scores were 100%, 96%, and 96%, respectively. The best performance results were obtained compared to the existing methods. In addition, the contribution of the strategies utilized in the proposed model to classification performance was analyzed in detail. Deep learning-based applications can be used as a useful decision support tool for physicians in the early diagnosis of ILD diseases. However, for the reliability of these applications, it is necessary to undertake verification with many datasets.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-22", "authors": ["YamanAkbulut"], "doi": "10.3390/diagnostics13020260\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200463\n10.1148/radiol.2020200432\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1016/j.chemolab.2020.104054\n10.1016/j.aiia.2020.10.002\n10.1001/jama.2018.19323\n10.1002/emmm.201100182\n10.1016/j.aiia.2020.09.002\n10.1016/j.patrec.2020.09.010\n10.33889/IJMEMS.2020.5.4.052\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3016780\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110071\n10.3892/etm.2020.8797\n10.1016/j.asoc.2021.107160\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2022.108610\n10.1007/s00354-021-00152-0\n10.14358/PERS.70.3.351\n10.1109/JSTARS.2018.2830410\n10.14569/IJACSA.2017.080853\n10.3390/jpm12010055\n10.1016/j.bspc.2022.103625\n10.1016/j.bspc.2020.102194\n10.1016/j.bbe.2021.07.004\n10.3390/jpm11121276\n10.1016/j.apacoust.2021.108260\n10.1016/j.mehy.2020.109761\n10.1016/j.asoc.2020.106580\n10.1007/s10489-020-01888-w\n10.1016/j.compbiomed.2020.103805"}
{"title": "Deep Learning for Detecting COVID-19 Using Medical Images.", "abstract": "The global spread of COVID-19 (also known as SARS-CoV-2) is a major international public health crisis [...].", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-01-22", "authors": ["JiaLiu", "JingQi", "WeiChen", "YiWu", "YongjianNian"], "doi": "10.3390/bioengineering10010019\n10.3390/bioengineering8070098\n10.3390/bioengineering8040049\n10.1016/j.media.2020.101794\n10.1007/s00500-020-05424-3\n10.1038/s41598-020-76550-z\n10.1016/j.ins.2020.09.041\n10.1016/j.neucom.2022.01.055\n10.1109/TNNLS.2021.3114747\n10.1016/j.media.2021.102299\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2022.105233\n10.1109/TMI.2020.3040950\n10.1016/j.compbiomed.2022.105732\n10.1109/TNNLS.2022.3201198\n10.1016/j.irbm.2020.05.003\n10.1109/JBHI.2020.3023246\n10.1109/JBHI.2020.3030853\n10.1109/TCYB.2020.3042837\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2994908\n10.1016/j.media.2021.102105\n10.1016/j.media.2020.101913\n10.1109/TMI.2020.2996256\n10.1109/TMI.2020.2995965\n10.3390/bioengineering8020026"}
{"title": "Classification of Pulmonary Damage Stages Caused by COVID-19 Disease from CT Scans via Transfer Learning.", "abstract": "The COVID-19 pandemic has produced social and economic changes that are still affecting our lives. The coronavirus is proinflammatory, it is replicating, and it is quickly spreading. The most affected organ is the lung, and the evolution of the disease can degenerate very rapidly from the early phase, also known as mild to moderate and even severe stages, where the percentage of recovered patients is very low. Therefore, a fast and automatic method to detect the disease stages for patients who underwent a computer tomography investigation can improve the clinical protocol. Transfer learning is used do tackle this issue, mainly by decreasing the computational time. The dataset is composed of images from public databases from 118 patients and new data from 55 patients collected during the COVID-19 spread in Romania in the spring of 2020. Even if the disease detection by the computerized tomography scans was studied using deep learning algorithms, to our knowledge, there are no studies related to the multiclass classification of the images into pulmonary damage stages. This could be helpful for physicians to automatically establish the disease severity and decide on the proper treatment for patients and any special surveillance, if needed. An evaluation study was completed by considering six different pre-trained CNNs. The results are encouraging, assuring an accuracy of around 87%. The clinical impact is still huge, even if the disease spread and severity are currently diminished.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2023-01-22", "authors": ["Irina AndraTache", "DimitriosGlotsos", "Silviu MarcelStanciu"], "doi": "10.3390/bioengineering10010006\n10.1016/j.jacr.2020.02.008\n10.1371/journal.pone.0235844\n10.1148/ryct.2020200028\n10.1148/radiol.2020200230\n10.1007/s10916-020-01562-1\n10.1016/j.patcog.2022.108538\n10.1016/j.chaos.2020.109944\n10.1142/S0218348X20501145\n10.1007/s10489-020-01826-w\n10.1016/j.bspc.2021.102588\n10.1007/s10916-021-01707-w\n10.1016/j.bbe.2021.04.006\n10.1016/j.chaos.2020.110153\n10.3390/e22050517\n10.1016/j.bspc.2022.104250\n10.3389/fonc.2020.01560\n10.1056/NEJMoa2001316\n10.1186/s43055-020-00236-9\n10.1148/radiol.2020200463\n10.2214/AJR.20.22976\n10.1016/j.jormas.2019.06.002\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.bspc.2021.103326\n10.1186/s40537-019-0235-y\n10.1186/s40537-021-00428-8"}
{"title": "Long-term respiratory follow-up of ICU hospitalized COVID-19 patients: Prospective cohort study.", "abstract": "Coronavirus disease (COVID-19) survivors exhibit multisystemic alterations after hospitalization. Little is known about long-term imaging and pulmonary function of hospitalized patients intensive care unit (ICU) who survive COVID-19. We aimed to investigate long-term consequences of COVID-19 on the respiratory system of patients discharged from hospital ICU and identify risk factors associated with chest computed tomography (CT) lesion severity.\nA prospective cohort study of COVID-19 patients admitted to a tertiary hospital ICU in Brazil (March-August/2020), and followed-up six-twelve months after hospital admission. Initial assessment included: modified Medical Research Council dyspnea scale, SpO2 evaluation, forced vital capacity, and chest X-Ray. Patients with alterations in at least one of these examinations were eligible for CT and pulmonary function tests (PFTs) approximately 16 months after hospital admission. Primary outcome: CT lesion severity (fibrotic-like or non-fibrotic-like). Baseline clinical variables were used to build a machine learning model (ML) to predict the severity of CT lesion.\nIn total, 326 patients (72%) were eligible for CT and PFTs. COVID-19 CT lesions were identified in 81.8% of patients, and half of them showed mild restrictive lung impairment and impaired lung diffusion capacity. Patients with COVID-19 CT findings were stratified into two categories of lesion severity: non-fibrotic-like (50.8%-ground-glass opacities/reticulations) and fibrotic-like (49.2%-traction bronchiectasis/architectural distortion). No association between CT feature severity and altered lung diffusion or functional restrictive/obstructive patterns was found. The ML detected that male sex, ICU and invasive mechanic ventilation (IMV) period, tracheostomy and vasoactive drug need during hospitalization were predictors of CT lesion severity(sensitivity,0.78\u00b10.02;specificity,0.79\u00b10.01;F1-score,0.78\u00b10.02;positive predictive rate,0.78\u00b10.02; accuracy,0.78\u00b10.02; and area under the curve,0.83\u00b10.01).\nICU hospitalization due to COVID-19 led to respiratory system alterations six-twelve months after hospital admission. Male sex and critical disease acute phase, characterized by a longer ICU and IMV period, and need for tracheostomy and vasoactive drugs, were risk factors for severe CT lesions six-twelve months after hospital admission.", "journal": "PloS one", "date": "2023-01-21", "authors": ["Carlos RobertoRibeiro Carvalho", "Celina AlmeidaLamas", "Rodrigo CarusoChate", "Jo\u00e3o MarcosSalge", "Marcio Valente YamadaSawamura", "Andr\u00e9 L Pde Albuquerque", "CarlosToufen Junior", "Daniel MarioLima", "Michelle LouvaesGarcia", "Paula GobiScudeller", "Cesar HigaNomura", "Marco AntonioGutierrez", "Bruno GuedesBaldi", "NoneNone"], "doi": "10.1371/journal.pone.0280567\n10.1186/s12931-020-01429-6\n10.1513/AnnalsATS.202004-324OC\n10.1016/j.cmi.2020.09.023\n10.1038/s41591-021-01283-z\n10.3390/ijerph18084350\n10.1080/17476348.2021.1916472\n10.1001/jamanetworkopen.2020.36142\n10.1038/s41591-022-01840-0\n10.1136/bmjopen-2021-059110\n10.1016/S0140-6736(20)32656-8\n10.1186/s13054-021-03465-0\n10.1038/s41413-020-0084-5\n10.1097/01.rti.0000213581.14225.f1\n10.1590/s1807-59322011000600002\n10.1056/NEJMoa022450\n10.1186/s13613-021-00882-w\n10.1136/bmjopen-2021-051706\n10.1148/radiol.2021203153\n10.1164/ajrccm/144.5.1202\n10.1590/s0100-879x1999000600006\n10.1590/s1806-37132007000400008\n10.1590/s0100-879x1999000600008\n10.1186/s13613-018-0469-4\n10.1038/s41598-021-01215-4\n10.1016/j.rmed.2021.106435\n10.1136/thx.2004.030205\n10.1136/thoraxjnl-2021-218275\n10.1007/s15010-022-01755-5\n10.1148/radiol.2021210972\n10.1148/radiol.211670\n10.1016/S2213-2600(21)00174-0\n10.1016/j.siny.2017.06.003\n10.1016/j.chest.2018.07.016\n10.4103/atm.atm_103_22\n10.1016/j.rmed.2021.106602\n10.1007/s11739-020-02614-7"}
{"title": "Social Media Devices' Influence on User Neck Pain during the COVID-19 Pandemic: Collaborating Vertebral-GLCM Extracted Features with a Decision Tree.", "abstract": "The prevalence of neck pain, a chronic musculoskeletal disease, has significantly increased due to the uncontrollable use of social media (SM) devices. The use of SM devices by younger generations increased enormously during the COVID-19 pandemic, being-in some cases-the only possibility for maintaining interpersonal, social, and friendship relationships. This study aimed to predict the occurrence of neck pain and its correlation with the intensive use of SM devices. It is based on nine quantitative parameters extracted from the retrospective X-ray images. The three parameters related to angle_1 (i.e., the angle between the global horizontal and the vector pointing from C7 vertebra to the occipito-cervical joint), angle_2 (i.e., the angle between the global horizontal and the vector pointing from C1 vertebra to the occipito-cervical joint), and the area between them were measured from the shape of the neck vertebrae, while the rest of the parameters were extracted from the images using the gray-level co-occurrence matrix (GLCM). In addition, the users' ages and the duration of the SM usage (H.mean) were also considered. The decision tree (DT) machine-learning algorithm was employed to predict the abnormal cases (painful subjects) against the normal ones (no pain). The results showed that angle_1, area, and the image contrast significantly increased statistically with the time of SM-device usage, precisely in the range of 2 to 9 h. The DT showed a promising result demonstrated by classification accuracy and F1-scores of 94% and 0.95, respectively. Our findings confirmed that the objectively detected parameters, which elucidate the negative impacts of SM-device usage on neck pain, can be predicted by DT machine learning.", "journal": "Journal of imaging", "date": "2023-01-21", "authors": ["BassamAl-Naami", "Bashar E ABadr", "Yahia ZRawash", "Hamza AbuOwida", "RobertoDe Fazio", "PaoloVisconti"], "doi": "10.3390/jimaging9010014\n10.1016/S0140-6736(15)60692-4\n10.1080/23808985.2021.1976070\n10.1590/bjpt-rbf.2014.0149\n10.3109/15360288.2012.678473\n10.1631/FITEE.2100085\n10.21307/ijssis-2021-003\n10.3390/children7090148\n10.1007/s41347-020-00134-x\n10.1097/ACM.0000000000002811\n10.1080/00140139.2013.820844\n10.1166/jmihi.2011.1034\n10.9734/jpri/2021/v33i50B33441\n10.17762/pae.v57i9.560\n10.3390/ijerph18168680\n10.1016/j.apmr.2020.10.019\n10.1177/21925682211041618\n10.3389/fneur.2020.573095\n10.47108/jidhealth.Vol3.IssSpecial1.65\n10.47626/1679-4435-2021-812\n10.13075/mp.5893.01189\n10.1038/s41598-021-01967-z\n10.3390/healthcare9111526\n10.1186/s12889-021-11144-6\n10.1145/1414471.1414519\n10.1109/MECBME.2011.5752156\n10.1109/TIM.2009.2022102\n10.1518/001872000779656598\n10.1037/1076-898X.5.1.35\n10.1518/001872001775992480\n10.1080/0014013031000090107\n10.1080/001401398186586\n10.1080/00140139.2011.576777\n10.1177/003335490712200511\n10.1109/ICPR.2010.764\n10.1109/CCDC.2019.8832978\n10.1016/j.jvcir.2017.03.003\n10.1140/epje/i2017-11587-3\n10.1097/SCS.0000000000002022\n10.1007/s00586-021-07019-4\n10.1097/MEG.0b013e3282202bb8\n10.1007/BF00116251\n10.1007/BF00993309\n10.1002/j.1538-7305.1948.tb01338.x\n10.48550/arXiv.cs/0102027\n10.1371/journal.pone.0217231\n10.3390/ijerph18073507\n10.24171/j.phrp.2018.9.6.04\n10.1109/CCNC.2013.6488520\n10.1016/j.chbr.2021.100108\n10.1155/2018/4518269\n10.1080/00140139.2015.1035762\n10.1016/j.ejpain.2006.02.006\n10.1046/j.1526-4610.2002.02178.x\n10.1109/COMPSAC48688.2020.0-176\n10.3390/ijerph18041565\n10.1080/00140139.2014.967311\n10.1080/00140130701711000\n10.1080/00140139.2011.568634\n10.1080/10447318.2015.1064639\n10.3390/electronics11060938"}
{"title": "A Survey on Deep Learning in COVID-19 Diagnosis.", "abstract": "According to the World Health Organization statistics, as of 25 October 2022, there have been 625,248,843 confirmed cases of COVID-19, including 65,622,281 deaths worldwide. The spread and severity of COVID-19 are alarming. The economy and life of countries worldwide have been greatly affected. The rapid and accurate diagnosis of COVID-19 directly affects the spread of the virus and the degree of harm. Currently, the classification of chest X-ray or CT images based on artificial intelligence is an important method for COVID-19 diagnosis. It can assist doctors in making judgments and reduce the misdiagnosis rate. The convolutional neural network (CNN) is very popular in computer vision applications, such as applied to biological image segmentation, traffic sign recognition, face recognition, and other fields. It is one of the most widely used machine learning methods. This paper mainly introduces the latest deep learning methods and techniques for diagnosing COVID-19 using chest X-ray or CT images based on the convolutional neural network. It reviews the technology of CNN at various stages, such as rectified linear units, batch normalization, data augmentation, dropout, and so on. Several well-performing network architectures are explained in detail, such as AlexNet, ResNet, DenseNet, VGG, GoogleNet, etc. We analyzed and discussed the existing CNN automatic COVID-19 diagnosis systems from sensitivity, accuracy, precision, specificity, and F1 score. The systems use chest X-ray or CT images as datasets. Overall, CNN has essential value in COVID-19 diagnosis. All of them have good performance in the existing experiments. If expanding the datasets, adding GPU acceleration and data preprocessing techniques, and expanding the types of medical images, the performance of CNN will be further improved. This paper wishes to make contributions to future research.", "journal": "Journal of imaging", "date": "2023-01-21", "authors": ["XueHan", "ZuojinHu", "ShuihuaWang", "YudongZhang"], "doi": "10.3390/jimaging9010001\n10.1016/j.procbio.2020.08.016\n10.1056/NEJMoa2002032\n10.1016/j.asoc.2020.106580\n10.1016/j.bios.2020.112349\n10.1016/j.bios.2020.112437\n10.1021/acsnano.0c02439\n10.1016/j.cartre.2020.100011\n10.1148/radiol.2020201237\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)30154-9\n10.1186/s12938-020-00831-x\n10.1016/j.neucom.2020.05.078\n10.1007/s10462-021-09985-z\n10.1016/j.aej.2021.07.007\n10.1016/j.eswa.2017.08.006\n10.1186/s41747-018-0061-6\n10.1142/S0129065718500582\n10.1049/cit2.12042\n10.3348/kjr.2017.18.4.570\n10.1038/s41746-022-00592-y\n10.20517/ais.2021.15\n10.1049/cit2.12060\n10.1038/nature14539\n10.1049/cit2.12059\n10.1002/ett.4080\n10.1016/j.job.2022.03.003\n10.1016/j.bspc.2021.103165\n10.1109/TIP.2005.852470\n10.1016/j.neunet.2012.02.023\n10.1007/s00521-021-06762-5\n10.1134/S1054661822020110\n10.1145/3507902\n10.3390/su14031447\n10.1155/2022/1830010\n10.1016/j.media.2021.102311\n10.1016/j.compbiomed.2022.105244\n10.1007/s42600-020-00120-5\n10.1016/j.displa.2022.102150\n10.1259/0007-1285-46-552-1016\n10.1007/s11604-020-01010-7\n10.1016/j.crad.2020.04.001\n10.1148/radiol.2020200463\n10.1148/radiol.2020201160\n10.1016/j.jacr.2018.09.012\n10.21037/atm.2017.07.20\n10.1177/0846537120924606\n10.1109/TMI.2020.2993291\n10.3390/jimaging8010002\n10.1109/TIP.2017.2713099\n10.1016/j.patcog.2017.10.013\n10.1007/s13369-020-04758-2\n10.1145/3065386\n10.1016/j.neunet.2015.07.007\n10.1016/j.cmpb.2019.05.004\n10.1016/j.knosys.2020.106396\n10.1016/j.swevo.2021.100863\n10.3390/app12178643\n10.1016/j.powtec.2022.117409\n10.1016/j.ymssp.2017.06.022\n10.32604/cmc.2022.020140\n10.3389/fpubh.2021.726144\n10.1016/j.patrec.2021.02.005\n10.1016/j.patrec.2020.04.018\n10.1016/j.jksuci.2021.05.001\n10.1016/j.neucom.2020.06.117\n10.1111/nph.16830\n10.1016/j.neucom.2018.03.080\n10.1007/s12145-019-00383-2\n10.1155/2021/6633755\n10.1109/JSEN.2020.3025855\n10.1016/j.jksuci.2021.07.005\n10.1186/s40537-019-0197-0\n10.1016/j.compbiomed.2021.104375\n10.1016/j.bspc.2021.103326\n10.1364/OL.390026\n10.1016/j.inffus.2021.07.001\n10.1364/BOE.10.006145\n10.1097/JU.0000000000000852.020\n10.3389/frobt.2019.00144\n10.1007/s11042-017-5243-3\n10.1109/TCSVT.2019.2935128\n10.1167/16.12.326\n10.1109/ACCESS.2017.2696121\n10.1109/5.726791\n10.1109/72.279181\n10.1186/s40537-016-0043-6\n10.1109/TKDE.2009.191\n10.1023/A:1007379606734\n10.1613/jair.1872\n10.1016/S0378-3758(00)00115-4\n10.21037/jtd-21-747\n10.3390/sym14071310\n10.1016/j.asoc.2020.106912\n10.1007/s11390-020-0679-8\n10.1016/j.eng.2020.04.010\n10.1016/j.ejrad.2020.109041\n10.1016/j.bspc.2021.102588\n10.1007/s00521-020-05437-x\n10.1371/journal.pone.0259179\n10.1007/s42979-021-00782-7\n10.1109/TCYB.2020.3042837\n10.1007/s10140-020-01886-y\n10.1007/s13755-021-00140-0\n10.1109/JSEN.2021.3062442\n10.3844/jcssp.2020.620.625\n10.1007/s00138-020-01128-8\n10.1038/s41598-020-74164-z\n10.1016/j.bspc.2021.102987\n10.1109/TLA.2021.9451239\n10.1155/2021/8829829\n10.1007/s10044-021-00984-y\n10.1109/ACCESS.2020.3010287\n10.1007/s10489-020-02055-x\n10.1088/1361-6501/ac8ca4\n10.1016/j.bspc.2021.102814\n10.1101/2022.03.13.22272311\n10.32628/IJSRST207614\n10.1016/j.ijmedinf.2020.104284"}
{"title": "Is the generalizability of a developed artificial intelligence algorithm for COVID-19 on chest CT sufficient for clinical use? Results from the International Consortium for COVID-19 Imaging AI (ICOVAI).", "abstract": "Only few published artificial intelligence (AI) studies for COVID-19 imaging have been externally validated. Assessing the generalizability of developed models is essential, especially when considering clinical implementation. We report the development of the International Consortium for COVID-19 Imaging AI (ICOVAI) model and perform independent external validation.\nThe ICOVAI model was developed using multicenter data (n = 1286 CT scans) to quantify disease extent and assess COVID-19 likelihood using the COVID-19 Reporting and Data System (CO-RADS). A ResUNet model was modified to automatically delineate lung contours and infectious lung opacities on CT scans, after which a random forest predicted the CO-RADS score. After internal testing, the model was externally validated on a multicenter dataset (n = 400) by independent researchers. CO-RADS classification performance was calculated using linearly weighted Cohen's kappa and segmentation performance using Dice Similarity Coefficient (DSC).\nRegarding internal versus external testing, segmentation performance of lung contours was equally excellent (DSC = 0.97 vs. DSC = 0.97, p = 0.97). Lung opacities segmentation performance was adequate internally (DSC = 0.76), but significantly worse on external validation (DSC = 0.59, p < 0.0001). For CO-RADS classification, agreement with radiologists on the internal set was substantial (kappa = 0.78), but significantly lower on the external set (kappa = 0.62, p < 0.0001).\nIn this multicenter study, a model developed for CO-RADS score prediction and quantification of COVID-19 disease extent was found to have a significant reduction in performance on independent external validation versus internal testing. The limited reproducibility of the model restricted its potential for clinical use. The study demonstrates the importance of independent external validation of AI models.\n\u2022 The ICOVAI model for prediction of CO-RADS and quantification of disease extent on chest CT of COVID-19 patients was developed using a large sample of multicenter data. \u2022 There was substantial performance on internal testing; however, performance was significantly reduced on external validation, performed by independent researchers. The limited generalizability of the model restricts its potential for clinical use. \u2022 Results of AI models for COVID-19 imaging on internal tests may not generalize well to external data, demonstrating the importance of independent external validation.", "journal": "European radiology", "date": "2023-01-19", "authors": ["LaurensTopff", "Kevin B WGroot Lipman", "FredericGuffens", "RianneWittenberg", "AnnemariekeBartels-Rutten", "Gerbenvan Veenendaal", "MircoHess", "KayLamerigts", "JorisWakkie", "ErikRanschaert", "StefanoTrebeschi", "Jacob JVisser", "Regina G HBeets-Tan", "NoneNone"], "doi": "10.1007/s00330-022-09303-3\n10.1109/RBME.2020.2987975\n10.1007/s00330-020-07033-y\n10.1148/ryct.2020200047\n10.2214/AJR.20.24044\n10.1007/s00330-020-07013-2\n10.1148/radiol.2020201473\n10.1016/j.chest.2020.11.026\n10.1186/s13244-021-00998-4\n10.1148/ryct.2020200492\n10.1038/s42256-021-00307-0\n10.1093/ckj/sfaa188\n10.1148/radiol.2020202439\n10.1148/radiol.2020201491\n10.1148/radiol.2020200905\n10.1038/s41467-020-18685-1\n10.1016/S2589-7500(20)30199-0\n10.1007/s00330-020-07042-x\n10.1007/s00330-020-07156-2\n10.1038/s41598-022-06854-9\n10.1016/j.asoc.2020.106897"}
{"title": "Pandemic disease detection through wireless communication using infrared image based on deep learning.", "abstract": "Rapid diagnosis to test diseases, such as COVID-19, is a significant issue. It is a routine virus test in a reverse transcriptase-polymerase chain reaction. However, a test like this takes longer to complete because it follows the serial testing method, and there is a high chance of a false-negative ratio (FNR). Moreover, there arises a deficiency of R.T.-PCR test kits. Therefore, alternative procedures for a quick and accurate diagnosis of patients are urgently needed to deal with these pandemics. The infrared image is self-sufficient for detecting these diseases by measuring the temperature at the initial stage. C.T. scans and other pathological tests are valuable aspects of evaluating a patient with a suspected pandemic infection. However, a patient's radiological findings may not be identified initially. Therefore, we have included an Artificial Intelligence (A.I.) algorithm-based Machine Intelligence (MI) system in this proposal to combine C.T. scan findings with all other tests, symptoms, and history to quickly diagnose a patient with a positive symptom of current and future pandemic diseases. Initially, the system will collect information by an infrared camera of the patient's facial regions to measure temperature, keep it as a record, and complete further actions. We divided the face into eight classes and twelve regions for temperature measurement. A database named patient-info-mask is maintained. While collecting sample data, we incorporate a wireless network using a cloudlets server to make processing more accessible with minimal infrastructure. The system will use deep learning approaches. We propose convolution neural networks (CNN) to cross-verify the collected data. For better results, we incorporated tenfold cross-verification into the synthesis method. As a result, our new way of estimating became more accurate and efficient. We achieved 3.29% greater accuracy by incorporating the \"decision tree level synthesis method\" and \"ten-folded-validation method\". It proves the robustness of our proposed method.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2023-01-19", "authors": ["MohammedAlhameed", "FatheJeribi", "Bushra Mohamed ElaminElnaim", "Mohammad AlamgirHossain", "Mohammed EltahirAbdelhag"], "doi": "10.3934/mbe.2023050"}
{"title": "Automated grading of chest x-ray images for viral pneumonia with convolutional neural networks ensemble and region of interest localization.", "abstract": "Following its initial identification on December 31, 2019, COVID-19 quickly spread around the world as a pandemic claiming more than six million lives. An early diagnosis with appropriate intervention can help prevent deaths and serious illness as the distinguishing symptoms that set COVID-19 apart from pneumonia and influenza frequently don't show up until after the patient has already suffered significant damage. A chest X-ray (CXR), one of many imaging modalities that are useful for detection and one of the most used, offers a non-invasive method of detection. The CXR image analysis can also reveal additional disorders, such as pneumonia, which show up as anomalies in the lungs. Thus these CXRs can be used for automated grading aiding the doctors in making a better diagnosis. In order to classify a CXR image into the Negative for Pneumonia, Typical, Indeterminate, and Atypical, we used the publicly available CXR image competition dataset SIIM-FISABIO-RSNA COVID-19 from Kaggle. The suggested architecture employed an ensemble of EfficientNetv2-L for classification, which was trained via transfer learning from the initialised weights of ImageNet21K on various subsets of data (Code for the proposed methodology is available at: https://github.com/asadkhan1221/siim-covid19.git). To identify and localise opacities, an ensemble of YOLO was combined using Weighted Boxes Fusion (WBF). Significant generalisability gains were made possible by the suggested technique's addition of classification auxiliary heads to the CNN backbone. The suggested method improved further by utilising test time augmentation for both classifiers and localizers. The results for Mean Average Precision score show that the proposed deep learning model achieves 0.617 and 0.609 on public and private sets respectively and these are comparable to other techniques for the Kaggle dataset.", "journal": "PloS one", "date": "2023-01-18", "authors": ["AsadKhan", "Muhammad UsmanAkram", "SajidNazir"], "doi": "10.1371/journal.pone.0280352\n10.1007/s13246-020-00865-4\n10.1186/s40537-020-00392-9\n10.1016/j.compbiomed.2021.104771\n10.1038/s41598-020-76550-z\n10.1007/s10489-020-01888-w\n10.1111/exsy.12759\n10.1007/s10462-020-09825-6\n10.1007/s13244-018-0639-9\n10.1016/j.bspc.2021.102764\n10.1016/j.asoc.2020.106691\n10.1016/j.compbiomed.2020.103792\n10.1007/s13755-021-00146-8\n10.1007/s00521-020-05636-6\n10.1109/ICCV.2017.74\n10.1016/j.compbiomed.2021.104356\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2021.104304\n10.1007/978-3-030-88163-4_33\n10.1016/j.neucom.2020.07.144\n10.1007/s10044-021-00984-y\n10.1080/0952813X.2021.1908431\n10.1016/j.bspc.2022.103677\n10.1080/07391102.2020.1767212\n10.1016/j.compbiomed.2021.104348\n10.1016/j.compbiomed.2022.105604\n10.1016/j.compbiomed.2021.104375\n10.1002/ima.22627\n10.3390/app11062884\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.eswa.2020.114054\n10.1049/ipr2.12153\n10.1016/j.compbiomed.2021.105002\n10.1016/j.media.2021.102299\n10.1155/2021/8890226\n10.1016/j.media.2021.102046\n10.1136/bmjqs-2018-008370"}
{"title": "Carotid Vessel-Wall-Volume Ultrasound Measurement via a UNet++ Ensemble Algorithm Trained on Small Data Sets.", "abstract": "Vessel wall volume (VWV) is a 3-D ultrasound measurement for the assessment of therapy in patients with carotid atherosclerosis. Deep learning can be used to segment the media-adventitia boundary (MAB) and lumen-intima boundary (LIB) and to quantify VWV automatically; however, it typically requires large training data sets with expert manual segmentation, which are difficult to obtain. In this study, a UNet++ ensemble approach was developed for automated VWV measurement, trained on five small data sets (n\u00a0=\u00a030 participants) and tested on 100 participants with clinically diagnosed coronary artery disease enrolled in a multicenter CAIN trial. The Dice similarity coefficient (DSC), average symmetric surface distance (ASSD), Pearson correlation coefficient (r), Bland-Altman plots and coefficient of variation (CoV) were used to evaluate algorithm segmentation accuracy, agreement and reproducibility. The UNet++ ensemble yielded DSCs of 91.07%-91.56% and 87.53%-89.44% and ASSDs of 0.10-0.11 mm and 0.33-0.39 mm for the MAB and LIB, respectively; the algorithm VWV measurements were correlated (r\u00a0=\u00a00.763-0.795, p < 0.001) with manual segmentations, and the CoV for VWV was 8.89%. In addition, the UNet++ ensemble trained on 30 participants achieved a performance similar to that of U-Net and Voxel-FCN trained on 150 participants. These results suggest that our approach could provide accurate and reproducible carotid VWV measurements using relatively small training data sets, supporting deep learning applications for monitoring atherosclerosis progression in research and clinical trials.", "journal": "Ultrasound in medicine & biology", "date": "2023-01-16", "authors": ["RanZhou", "FuminGuo", "M RezaAzarpazhooh", "J DavidSpence", "HaitaoGan", "MingyueDing", "AaronFenster"], "doi": "10.1016/j.ultrasmedbio.2022.12.005"}
{"title": "ACSN: Attention capsule sampling network for diagnosing COVID-19 based on chest CT scans.", "abstract": "Automated diagnostic techniques based on computed tomography (CT) scans of the chest for the coronavirus disease (COVID-19) help physicians detect suspected cases rapidly and precisely, which is critical in providing timely medical treatment and preventing the spread of epidemic outbreaks. Existing capsule networks have played a significant role in automatic COVID-19 detection systems based on small datasets. However, extracting key slices is difficult because CT scans typically show many scattered lesion sections. In addition, existing max pooling sampling methods cannot effectively fuse the features from multiple regions. Therefore, in this study, we propose an attention capsule sampling network (ACSN) to detect COVID-19 based on chest CT scans. A key slices enhancement method is used to obtain critical information from a large number of slices by applying attention enhancement to key slices. Then, the lost active and background features are retained by integrating two types of sampling. The results of experiments on an open dataset of 35,000 slices show that the proposed ACSN achieve high performance compared with state-of-the-art models and exhibits 96.3% accuracy, 98.8% sensitivity, 93.8% specificity, and 98.3% area under the receiver operating characteristic curve.", "journal": "Computers in biology and medicine", "date": "2023-01-15", "authors": ["CuihongWen", "ShaowuLiu", "ShuaiLiu", "Ali AsgharHeidari", "MohammadHijji", "CarmenZarco", "KhanMuhammad"], "doi": "10.1016/j.compbiomed.2022.106338\n10.21203/rs.3.rs-32511/v1\n10.1109/ACCESS.2021.3067311\n10.3390/s22176709\n10.1109/TII.2021.3056386\n10.1109/CVPR.2016.90\n10.48550/arXiv.2010.16041"}
{"title": "Utilisation of deep learning for COVID-19 diagnosis.", "abstract": "The COVID-19 pandemic that began in 2019 has resulted in millions of deaths worldwide. Over this period, the economic and healthcare consequences of COVID-19 infection in survivors of acute COVID-19 infection have become apparent. During the course of the pandemic, computer analysis of medical images and data have been widely used by the medical research community. In particular, deep-learning methods, which are artificial intelligence (AI)-based approaches, have been frequently employed. This paper provides a review of deep-learning-based AI techniques for COVID-19 diagnosis using chest radiography and computed tomography. Thirty papers published from February 2020 to March 2022 that used two-dimensional (2D)/three-dimensional (3D) deep convolutional neural networks combined with transfer learning for COVID-19 detection were reviewed. The review describes how deep-learning methods detect COVID-19, and several limitations of the proposed methods are highlighted.", "journal": "Clinical radiology", "date": "2023-01-14", "authors": ["SAslani", "JJacob"], "doi": "10.1016/j.crad.2022.11.006\n10.1101/2020.02.11.20021493\n10.5555/3305890.3305954\n10.1109/ICCVW.2019.00052"}
{"title": "Coronavirus covid-19 detection by means of explainable deep learning.", "abstract": "The coronavirus is caused by the infection of the SARS-CoV-2 virus: it represents a complex and new condition, considering that until the end of December 2019 this virus was totally unknown to the international scientific community. The clinical management of patients with the coronavirus disease has undergone an evolution over the months, thanks to the increasing knowledge of the virus, symptoms and efficacy of the various therapies. Currently, however, there is no specific therapy for SARS-CoV-2 virus, know also as Coronavirus disease 19, and treatment is based on the symptoms of the patient taking into account the overall clinical picture. Furthermore, the test to identify whether a patient is affected by the virus is generally performed on sputum and the result is generally available within a few hours or days. Researches previously found that the biomedical imaging analysis is able to show signs of pneumonia. For this reason in this paper, with the aim of providing a fully automatic and faster diagnosis, we design and implement a method adopting deep learning for the novel coronavirus disease detection, starting from computed tomography medical images. The proposed approach is aimed to detect whether a computed tomography medical images is related to an healthy patient, to a patient with a pulmonary disease or to a patient affected with Coronavirus disease 19. In case the patient is marked by the proposed method as affected by the Coronavirus disease 19, the areas symptomatic of the Coronavirus disease 19 infection are automatically highlighted in the computed tomography medical images. We perform an experimental analysis to empirically demonstrate the effectiveness of the proposed approach, by considering medical images belonging from different institutions, with an average time for Coronavirus disease 19 detection of approximately 8.9 s and an accuracy equal to 0.95.", "journal": "Scientific reports", "date": "2023-01-11", "authors": ["FrancescoMercaldo", "Maria PaolaBelfiore", "AlfonsoReginelli", "LucaBrunese", "AntonellaSantone"], "doi": "10.1038/s41598-023-27697-y\n10.1016/j.procs.2020.09.258\n10.1038/s41577-020-00434-6\n10.1016/j.cmpb.2020.105608\n10.1038/d41573-020-00073-5\n10.1056/NEJMoa2001316\n10.1053/j.gastro.2020.02.054\n10.3390/biology9050097\n10.1002/jmv.25748\n10.1056/NEJMp030078\n10.1038/nm1024\n10.3201/eid2009.140378\n10.3390/jcm9020523\n10.1001/jama.2020.4344\n10.1016/S0140-6736(20)30185-9\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200905\n10.1016/j.ejrad.2020.108961\n10.1111/irv.12734\n10.1148/radiol.2020200432\n10.1016/S1470-2045(19)30739-9\n10.1097/RLI.0b013e318074fd81\n10.2967/jnumed.117.189704\n10.1007/s00330-007-0613-2\n10.1007/978-3-031-01821-3\n10.1109/5.726791\n10.1038/s41598-020-76282-0\n10.1007/s00330-021-07715-1\n10.1016/j.eng.2020.04.010\n10.1007/s13246-020-00865-4\n10.1109/TCBB.2021.3065361\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103795\n10.1016/j.bbe.2015.12.005\n10.1016/j.measurement.2020.108116\n10.1016/j.ifacol.2021.10.282\n10.1016/j.cose.2021.102198\n10.1109/ACCESS.2019.2961754"}
{"title": "Artificial intelligence-assisted multistrategy image enhancement of chest X-rays for COVID-19 classification.", "abstract": "The coronavirus disease 2019 (COVID-19) led to a dramatic increase in the number of cases of patients with pneumonia worldwide. In this study, we aimed to develop an AI-assisted multistrategy image enhancement technique for chest X-ray (CXR) images to improve the accuracy of COVID-19 classification.\nOur new classification strategy consisted of 3 parts. First, the improved U-Net model with a variational encoder segmented the lung region in the CXR images processed by histogram equalization. Second, the residual net (ResNet) model with multidilated-rate convolution layers was used to suppress the bone signals in the 217 lung-only CXR images. A total of 80% of the available data were allocated for training and validation. The other 20% of the remaining data were used for testing. The enhanced CXR images containing only soft tissue information were obtained. Third, the neural network model with a residual cascade was used for the super-resolution reconstruction of low-resolution bone-suppressed CXR images. The training and testing data consisted of 1,200 and 100 CXR images, respectively. To evaluate the new strategy, improved visual geometry group (VGG)-16 and ResNet-18 models were used for the COVID-19 classification task of 2,767 CXR images. The accuracy of the multistrategy enhanced CXR images was verified through comparative experiments with various enhancement images. In terms of quantitative verification, 8-fold cross-validation was performed on the bone suppression model. In terms of evaluating the COVID-19 classification, the CXR images obtained by the improved method were used to train 2 classification models.\nCompared with other methods, the CXR images obtained based on the proposed model had better performance in the metrics of peak signal-to-noise ratio and root mean square error. The super-resolution CXR images of bone suppression obtained based on the neural network model were also anatomically close to the real CXR images. Compared with the initial CXR images, the classification accuracy rates of the internal and external testing data on the VGG-16 model increased by 5.09% and 12.81%, respectively, while the values increased by 3.51% and 18.20%, respectively, for the ResNet-18 model. The numerical results were better than those of the single-enhancement, double-enhancement, and no-enhancement CXR images.\nThe multistrategy enhanced CXR images can help to classify COVID-19 more accurately than the other existing methods.", "journal": "Quantitative imaging in medicine and surgery", "date": "2023-01-10", "authors": ["HongfeiSun", "GeRen", "XinzhiTeng", "LimingSong", "KangLi", "JianhuaYang", "XiaofeiHu", "YuefuZhan", "Shiu Bun NelsonWan", "Man Fung EstherWong", "King KwongChan", "Hoi Ching HaileyTsang", "LuXu", "Tak ChiuWu", "Feng-Ming SpringKong", "Yi Xiang JWang", "JingQin", "Wing Chi LawrenceChan", "MichaelYing", "JingCai"], "doi": "10.21037/qims-22-610\n10.1016/j.ijantimicag.2020.105924\n10.1016/j.arr.2020.101205\n10.1155/2022/5681574\n10.1155/2021/2158184\n10.3233/XST-200784\n10.1016/j.compbiomed.2020.103792\n10.1016/j.bspc.2020.102365\n10.1109/TMI.2020.2993291\n10.1007/s10489-020-01867-1\n10.1016/j.cmpb.2020.105581\n10.1002/mp.12831\n10.1002/mp.13891\n10.3348/kjr.2021.0146\n10.1259/bjr.20201384\n10.1109/ISBI.2019.8759510\n10.1109/ISBI.2019.8759510\n10.2147/IJGM.S325609\n10.3390/jcm10143100\n10.1109/TMI.2021.3134270\n10.1016/j.media.2022.102369\n10.21037/qims-21-791\n10.1109/TMI.2013.2284016\n10.1088/0031-9155/61/6/2283\n10.1109/TMI.2006.871549\n10.1016/j.media.2006.06.002\n10.1007/978-3-540-89208-3_116\n10.1007/978-3-540-89208-3_116\n10.1109/TMI.2013.2274212\n10.1016/j.compmedimag.2021.102008\n10.21037/qims-20-1230\n10.1109/TMI.2020.2986242\n10.1016/j.cmpb.2022.106627\n10.1109/TNNLS.2021.3114747\n10.1016/j.compbiomed.2022.105213\n10.3390/diagnostics11050840\n10.1371/journal.pone.0265691\n10.3390/jcm11113013\n10.3788/AOS201535.0110001\n10.3788/AOS201737.0318011\n10.4304/jcp.9.8.1959-1966\n10.1007/s10489-020-02123-2\n10.3390/diagnostics12030741\n10.3390/mi12111418\n10.1007/s11042-020-09773-x\n10.1007/s42979-021-00690-w\n10.1016/j.compbiomed.2020.104139\n10.1016/j.compbiomed.2020.103869\n10.3390/s21175813\n10.1016/j.chaos.2020.110495\n10.1142/S0218001421510046\n10.1016/j.compbiomed.2021.104425\n10.3390/app10124282\n10.1109/TPAMI.2018.2856256\n10.2214/ajr.174.1.1740071\n10.1007/978-3-642-13039-7_90\n10.1007/978-3-642-13039-7_90\n10.1016/j.compbiomed.2021.105002\n10.1148/radiol.2021203957\n10.3390/diagnostics12030717\n10.1007/s11042-021-10907-y\n10.1007/s42979-021-00695-5\n10.1002/mp.13468\n10.1007/s12652-020-01701-z\n10.1007/s10489-020-02055-x\n10.1038/s41598-021-03287-8"}
{"title": "LDDNet: A Deep Learning Framework for the Diagnosis of Infectious Lung Diseases.", "abstract": "This paper proposes a new deep learning (DL) framework for the analysis of lung diseases, including COVID-19 and pneumonia, from chest CT scans and X-ray (CXR) images. This framework is termed optimized DenseNet201 for lung diseases (LDDNet). The proposed LDDNet was developed using additional layers of 2D global average pooling, dense and dropout layers, and batch normalization to the base DenseNet201 model. There are 1024 Relu-activated dense layers and 256 dense layers using the sigmoid activation method. The hyper-parameters of the model, including the learning rate, batch size, epochs, and dropout rate, were tuned for the model. Next, three datasets of lung diseases were formed from separate open-access sources. One was a CT scan dataset containing 1043 images. Two X-ray datasets comprising images of COVID-19-affected lungs, pneumonia-affected lungs, and healthy lungs exist, with one being an imbalanced dataset with 5935 images and the other being a balanced dataset with 5002 images. The performance of each model was analyzed using the Adam, Nadam, and SGD optimizers. The best results have been obtained for both the CT scan and CXR datasets using the Nadam optimizer. For the CT scan images, LDDNet showed a COVID-19-positive classification accuracy of 99.36%, a 100% precision recall of 98%, and an F1 score of 99%. For the X-ray dataset of 5935 images, LDDNet provides a 99.55% accuracy, 73% recall, 100% precision, and 85% F1 score using the Nadam optimizer in detecting COVID-19-affected patients. For the balanced X-ray dataset, LDDNet provides a 97.07% classification accuracy. For a given set of parameters, the performance results of LDDNet are better than the existing algorithms of ResNet152V2 and XceptionNet.", "journal": "Sensors (Basel, Switzerland)", "date": "2023-01-09", "authors": ["PrajoyPodder", "Sanchita RaniDas", "M Rubaiyat HossainMondal", "SubratoBharati", "AzraMaliha", "Md JunayedHasan", "FarzinPiltan"], "doi": "10.3390/s23010480\n10.1016/j.scitotenv.2020.138762\n10.1007/s10489-020-01826-w\n10.1101/2020.04.22.056283\n10.1007/s10489-021-02393-4\n10.1016/j.bea.2021.100003\n10.1148/radiol.2020200527\n10.1148/radiol.2020200823\n10.1016/j.ejrad.2020.108961\n10.1155/2021/5527923\n10.1016/j.compbiomed.2020.103795\n10.3390/s21020369\n10.1016/j.cmpb.2019.06.005\n10.1109/RBME.2020.2990959\n10.1007/s42979-021-00785-4\n10.1007/s10489-020-01943-6\n10.1016/j.compbiomed.2021.104575\n10.3233/HIS-210008\n10.1101/2020.04.24.20078584\n10.2174/1573405617666210713113439\n10.1016/j.bspc.2021.102588\n10.1109/ACCESS.2020.3010287\n10.20944/preprints202003.0300.v1\n10.1007/s10140-020-01886-y\n10.1101/2020.03.12.20027185\n10.1007/s00330-021-07715-1\n10.1371/journal.pone.0259179\n10.1016/j.compbiomed.2022.105213\n10.3390/s22020669\n10.1016/j.compbiomed.2022.105418\n10.3390/info11090419\n10.1038/s41597-021-00900-3\n10.5281/zenodo.3757476\n10.3390/app10217639\n10.1016/j.patrec.2021.08.035\n10.1109/JBHI.2022.3177854"}
{"title": "Habitat Imaging Biomarkers for Diagnosis and Prognosis in Cancer Patients Infected with COVID-19.", "abstract": "Cancer patients have worse outcomes from the COVID-19 infection and greater need for ventilator support and elevated mortality rates than the general population. However, previous artificial intelligence (AI) studies focused on patients without cancer to develop diagnosis and severity prediction models. Little is known about how the AI models perform in cancer patients. In this study, we aim to develop a computational framework for COVID-19 diagnosis and severity prediction particularly in a cancer population and further compare it head-to-head to a general population.\nWe have enrolled multi-center international cohorts with 531 CT scans from 502 general patients and 420 CT scans from 414 cancer patients. In particular, the habitat imaging pipeline was developed to quantify the complex infection patterns by partitioning the whole lung regions into phenotypically different subregions. Subsequently, various machine learning models nested with feature selection were built for COVID-19 detection and severity prediction.\nThese models showed almost perfect performance in COVID-19 infection diagnosis and predicting its severity during cross validation. Our analysis revealed that models built separately on the cancer population performed significantly better than those built on the general population and locked to test on the cancer population. This may be because of the significant difference among the habitat features across the two different cohorts.\nTaken together, our habitat imaging analysis as a proof-of-concept study has highlighted the unique radiologic features of cancer patients and demonstrated effectiveness of CT-based machine learning model in informing COVID-19 management in the cancer population.", "journal": "Cancers", "date": "2023-01-09", "authors": ["MuhammadAminu", "DivyaYadav", "LingzhiHong", "EllianaYoung", "PaulEdelkamp", "MaliazurinaSaad", "MortezaSalehjahromi", "PingjunChen", "Sheeba JSujit", "Melissa MChen", "BradleySabloff", "GregoryGladish", "Patricia Mde Groot", "Myrna C BGodoy", "TinaCascone", "Natalie IVokes", "JianjunZhang", "Kristy KBrock", "NavalDaver", "Scott EWoodman", "Hussein ATawbi", "AjaySheshadri", "J JackLee", "DavidJaffray", "NoneD Code Team", "Carol CWu", "CarolineChung", "JiaWu"], "doi": "10.3390/cancers15010275\n10.1111/eci.13706\n10.7326/M20-1495\n10.1016/j.mayocp.2020.04.004\n10.1148/radiol.2020203173\n10.1097/RLI.0000000000000672\n10.1016/j.tmaid.2020.101623\n10.1093/brain/awaa240\n10.1016/j.jinf.2020.03.004\n10.1001/jamaoncol.2021.4083\n10.1038/s41746-020-00372-6\n10.1016/j.imu.2020.100378\n10.1515/cclm-2020-1294\n10.1016/S2589-7500(21)00272-7\n10.1016/S2589-7500(20)30274-0\n10.1038/s41591-020-0931-3\n10.1515/cclm-2020-0593\n10.1038/s41746-021-00453-0\n10.21037/tlcr-20-892\n10.3390/tomography8010041\n10.1016/j.jtho.2020.05.001\n10.1111/cas.14882\n10.1186/s12879-021-06038-2\n10.1002/cncr.31630\n10.1126/science.1256930\n10.1148/radiol.2018172462\n10.7937/tcia.bbag-2923\n10.1007/s10278-013-9622-7\n10.7937/91ah-v663\n10.1148/radiol.2021203957\n10.1186/s41747-020-00173-2\n10.1016/j.ijrobp.2016.03.018\n10.2967/jnumed.119.230037\n10.1109/TPAMI.2012.120\n10.1148/ryct.2020200152\n10.1016/j.cell.2020.04.045\n10.1001/jama.2020.2648\n10.1016/j.annonc.2020.03.296\n10.1016/j.patcog.2021.108071\n10.1038/s42256-021-00421-z\n10.1007/s00330-020-07032-z\n10.1007/s00259-020-05075-4\n10.1148/ryct.2020200322\n10.1016/j.asoc.2021.107323\n10.1038/s41467-020-18685-1\n10.1016/j.aej.2021.03.052\n10.1109/TCBB.2021.3065361\n10.7150/thno.50565\n10.36227/techrxiv.14100890.v1"}
{"title": "A Holistic Approach to Identify and Classify COVID-19 from Chest Radiographs, ECG, and CT-Scan Images Using ShuffleNet Convolutional Neural Network.", "abstract": "Early and precise COVID-19 identification and analysis are pivotal in reducing the spread of COVID-19. Medical imaging techniques, such as chest X-ray or chest radiographs, computed tomography (CT) scan, and electrocardiogram (ECG) trace images are the most widely known for early discovery and analysis of the coronavirus disease (COVID-19). Deep learning (DL) frameworks for identifying COVID-19 positive patients in the literature are limited to one data format, either ECG or chest radiograph images. Moreover, using several data types to recover abnormal patterns caused by COVID-19 could potentially provide more information and restrict the spread of the virus. This study presents an effective COVID-19 detection and classification approach using the Shufflenet CNN by employing three types of images, i.e., chest radiograph, CT-scan, and ECG-trace images. For this purpose, we performed extensive classification experiments with the proposed approach using each type of image. With the chest radiograph dataset, we performed three classification experiments at different levels of granularity, i.e., binary, three-class, and four-class classifications. In addition, we performed a binary classification experiment with the proposed approach by classifying CT-scan images into COVID-positive and normal. Finally, utilizing the ECG-trace images, we conducted three experiments at different levels of granularity, i.e., binary, three-class, and five-class classifications. We evaluated the proposed approach with the baseline COVID-19 Radiography Database, SARS-CoV-2 CT-scan, and ECG images dataset of cardiac and COVID-19 patients. The average accuracy of 99.98% for COVID-19 detection in the three-class classification scheme using chest radiographs, optimal accuracy of 100% for COVID-19 detection using CT scans, and average accuracy of 99.37% for five-class classification scheme using ECG trace images have proved the efficacy of our proposed method over the contemporary methods. The optimal accuracy of 100% for COVID-19 detection using CT scans and the accuracy gain of 1.54% (in the case of five-class classification using ECG trace images) from the previous approach, which utilized ECG images for the first time, has a major contribution to improving the COVID-19 prediction rate in early stages. Experimental findings demonstrate that the proposed framework outperforms contemporary models. For example, the proposed approach outperforms state-of-the-art DL approaches, such as Squeezenet, Alexnet, and Darknet19, by achieving the accuracy of 99.98 (proposed method), 98.29, 98.50, and 99.67, respectively.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-09", "authors": ["NaeemUllah", "Javed AliKhan", "ShakerEl-Sappagh", "NoraEl-Rashidy", "Mohammad SohailKhan"], "doi": "10.3390/diagnostics13010162\n10.3390/app12126269\n10.1038/s41368-020-0075-9\n10.3389/fmed.2022.1005920\n10.1016/j.radi.2020.10.013\n10.1016/j.patcog.2021.108255\n10.1007/s00521-021-06737-6\n10.1016/j.compbiomed.2022.105350\n10.1016/j.cmpb.2022.106731\n10.1101/2020.03.30.20047787\n10.1016/j.cma.2022.114570\n10.1016/j.cma.2020.113609\n10.1016/j.cie.2021.107250\n10.1016/j.eswa.2021.116158\n10.1109/ACCESS.2022.3147821\n10.1007/s13246-020-00888-x\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1007/s40846-020-00529-4\n10.1016/j.mehy.2020.109761\n10.18576/amis/100122\n10.12785/amis/080617\n10.1016/j.jksuci.2021.12.017\n10.1155/2012/205391\n10.1038/s41591-020-0931-3\n10.1007/s00330-021-07715-1\n10.1016/j.patcog.2021.108135\n10.3390/s21175702\n10.1155/2021/3366057\n10.1007/s13755-021-00169-1\n10.1016/j.jrras.2022.02.002\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1101/2020.04.24.20078584\n10.1016/j.dib.2021.106762\n10.1145/3065386\n10.3390/technologies10020037\n10.1016/j.eswa.2021.116377\n10.1155/2022/4130674\n10.1155/2022/6486570\n10.3390/app12115645\n10.3390/s22197575\n10.3390/electronics11071146\n10.1109/ACCESS.2022.3189676\n10.1109/ACCESS.2019.2909969\n10.1109/ACCESS.2019.2904800\n10.3390/s22051747\n10.1007/s00521-022-08007-5\n10.1007/s00500-022-07420-1\n10.1007/s00521-021-06631-1"}
{"title": "An Efficient Deep Learning Method for Detection of COVID-19 Infection Using Chest X-ray Images.", "abstract": "The research community has recently shown significant interest in designing automated systems to detect coronavirus disease 2019 (COVID-19) using deep learning approaches and chest radiography images. However, state-of-the-art deep learning techniques, especially convolutional neural networks (CNNs), demand more learnable parameters and memory. Therefore, they may not be suitable for real-time diagnosis. Thus, the design of a lightweight CNN model for fast and accurate COVID-19 detection is an urgent need. In this paper, a lightweight CNN model called LW-CORONet is proposed that comprises a sequence of convolution, rectified linear unit (ReLU), and pooling layers followed by two fully connected layers. The proposed model facilitates extracting meaningful features from the chest X-ray (CXR) images with only five learnable layers. The proposed model is evaluated using two larger CXR datasets (Dataset-1: 2250 images and Dataset-2: 15,999 images) and the classification accuracy obtained are 98.67% and 99.00% on Dataset-1 and 95.67% and 96.25% on Dataset-2 for multi-class and binary classification cases, respectively. The results are compared with four contemporary pre-trained CNN models as well as state-of-the-art models. The effect of several hyperparameters: different optimization techniques, batch size, and learning rate have also been investigated. The proposed model demands fewer parameters and requires less memory space. Hence, it is effective for COVID-19 detection and can be utilized as a supplementary tool to assist radiologists in their diagnosis.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2023-01-09", "authors": ["Soumya RanjanNayak", "Deepak RanjanNayak", "UtkarshSinha", "VaibhavArora", "Ram BilasPachori"], "doi": "10.3390/diagnostics13010131\n10.1016/S0140-6736(20)30183-5\n10.32604/cmc.2020.010691\n10.1148/radiol.2020200432\n10.1109/RBME.2020.2990959\n10.1148/radiol.2020200527\n10.1148/radiol.2020200230\n10.1109/JBHI.2022.3196489\n10.1148/radiol.2020200343\n10.1109/RBME.2020.2987975\n10.3390/app10020559\n10.1148/radiol.2017162326\n10.1371/journal.pmed.1002686\n10.1016/j.compbiomed.2020.103792\n10.1007/s10044-021-00984-y\n10.1016/j.mehy.2020.109761\n10.1016/j.imu.2020.100360\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103805\n10.1016/j.chaos.2020.110122\n10.1109/TMI.2020.2996256\n10.1109/JSEN.2020.3025855\n10.1016/j.inffus.2020.11.005\n10.1016/j.compbiomed.2021.104454\n10.1145/3551647\n10.1016/j.bspc.2021.103182\n10.1016/j.compbiomed.2022.106331\n10.1016/j.bspc.2020.102365\n10.1007/s11042-022-12156-z\n10.1109/ACCESS.2019.2950228\n10.1007/BF03178082\n10.1016/j.neucom.2017.12.030\n10.1109/TMI.2016.2528162\n10.1186/s40537-019-0197-0\n10.1016/j.patrec.2020.04.018\n10.1007/s12652-020-02612-9\n10.1016/j.compmedimag.2019.05.001\n10.1016/j.media.2017.07.005"}
{"title": "The 2000HIV study: Design, multi-omics methods and participant characteristics.", "abstract": "Even during long-term combination antiretroviral therapy (cART), people living with HIV (PLHIV) have a dysregulated immune system, characterized by persistent immune activation, accelerated immune ageing and increased risk of non-AIDS comorbidities. A multi-omics approach is applied to a large cohort of PLHIV to understand pathways underlying these dysregulations in order to identify new biomarkers and novel genetically validated therapeutic drugs targets.\nThe 2000HIV study is a prospective longitudinal cohort study of PLHIV on cART. In addition, untreated HIV spontaneous controllers were recruited. In-depth multi-omics characterization will be performed, including genomics, epigenomics, transcriptomics, proteomics, metabolomics and metagenomics, functional immunological assays and extensive immunophenotyping. Furthermore, the latent viral reservoir will be assessed through cell associated HIV-1 RNA and DNA, and full-length individual proviral sequencing on a subset. Clinical measurements include an ECG, carotid intima-media thickness and plaque measurement, hepatic steatosis and fibrosis measurement as well as psychological symptoms and recreational drug questionnaires. Additionally, considering the developing pandemic, COVID-19 history and vaccination was recorded. Participants return for a two-year follow-up visit. The 2000HIV study consists of a discovery and validation cohort collected at separate sites to immediately validate any finding in an independent cohort.\nOverall, 1895 PLHIV from four sites were included for analysis, 1559 in the discovery and 336 in the validation cohort. The study population was representative of a Western European HIV population, including 288 (15.2%) \nThe 2000HIV study established a cohort of 1895 PLHIV that employs multi-omics to discover new biological pathways and biomarkers to unravel non-AIDS comorbidities, extreme phenotypes and the latent viral reservoir that impact the health of PLHIV. The ultimate goal is to contribute to a more personalized approach to the best standard of care and a potential cure for PLHIV.", "journal": "Frontiers in immunology", "date": "2023-01-07", "authors": ["Wilhelm A J WVos", "Albert LGroenendijk", "Marc J TBlaauw", "Louise Evan Eekeren", "AdrianaNavas", "Maartje C PCleophas", "NadiraVadaq", "VasilikiMatzaraki", "J\u00e9ssica CDos Santos", "Elise M GMeeder", "JaneriFr\u00f6berg", "GertWeijers", "YueZhang", "JingyuanFu", "RobTer Horst", "ChristophBock", "RainerKnoll", "Anna CAschenbrenner", "JoachimSchultze", "LinosVanderkerckhove", "TalentHwandih", "Elizabeth RWonderlich", "Sai VVemula", "Mikevan der Kolk", "Sterre C Pde Vet", "Willem LBlok", "KeesBrinkman", "CasperRokx", "Arnt F ASchellekens", "Quirijnde Mast", "Leo A BJoosten", "Marvin A HBerrevoets", "Janneke EStalenhoef", "AnneliesVerbon", "Janvan Lunzen", "Mihai GNetea", "Andre J A Mvan der Ven"], "doi": "10.3389/fimmu.2022.982746\n10.3390/v11030200\n10.1016/S2352-3018(18)30039-0\n10.1371/journal.pbio.1002050\n10.3389/fgene.2017.00084\n10.1016/j.cell.2016.10.017\n10.1016/j.cell.2016.10.018\n10.1016/j.cell.2016.10.020\n10.1016/j.celrep.2016.10.053\n10.1016/j.cell.2016.10.040\n10.1161/ATVBAHA.120.314508\n10.1210/clinem/dgaa356\n10.1002/eji.201948390\n10.1016/j.metabol.2021.154795\n10.1111/dom.14172\n10.1172/jci.insight.145928\n10.1038/s41598-021-85775-5\n10.3389/fimmu.2021.661990\n10.1038/s41590-021-00867-8\n10.1172/JCI133935\n10.1136/annrheumdis-2019-216233\n10.1038/s41591-021-01590-5\n10.1016/j.ultrasmedbio.2015.11.004\n10.1136/bmjopen-2018-022516\n10.1161/CIRCULATIONAHA.107.699579\n10.1016/j.jelectrocard.2014.07.022\n10.1016/j.amjcard.2013.09.005\n10.1016/s0022-3999(01)00296-3\n10.1002/1097-4679(199511)51:6<768\n10.1111/j.1360-0443.2009.02889.x\n10.1089/bio.2015.0104\n10.1515/cclm-2022-0094\n10.2450/2015.0007-15\n10.1371/journal.pntd.0007183\n10.1371/journal.pntd.0009187\n10.7554/eLife.63195\n10.1038/s41467-021-25949-x\n10.1097/QAI.0000000000000842\n10.1177/20499361221075454\n10.2174/1874613601408010058\n10.1093/cid/civ171\n10.1038/s41467-019-10884-9\n10.1371/journal.pmed.1003991\n10.2165/00003495-200666060-00004\n10.1161/CIRCULATIONAHA.117.033369"}
{"title": "Development and validation of a deep learning model to diagnose COVID-19 using time-series heart rate values before the onset of symptoms.", "abstract": "One of the effective ways to minimize the spread of COVID-19 infection is to diagnose it as early as possible before the onset of symptoms. In addition, if the infection can be simply diagnosed using a smartwatch, the effectiveness of preventing the spread will be greatly increased. In this study, we aimed to develop a deep learning model to diagnose COVID-19 before the onset of symptoms using heart rate (HR) data obtained from a smartwatch. In the deep learning model for the diagnosis, we proposed a transformer model that learns HR variability patterns in presymptom by tracking relationships in sequential HR data. In the cross-validation (CV) results from the COVID-19 unvaccinated patients, our proposed deep learning model exhibited high accuracy metrics: sensitivity of 84.38%, specificity of 85.25%, accuracy of 84.85%, balanced accuracy of 84.81%, and area under the receiver operating characteristics\u00a0(AUROC) of 0.8778. Furthermore, we validated our model using external multiple datasets including healthy subjects, COVID-19 patients, as well as vaccinated patients. In the external healthy subject group, our model also achieved high specificity of 77.80%. In the external COVID-19 unvaccinated patient group, our model also provided similar accuracy metrics to those from the CV: balanced accuracy of 87.23% and AUROC of 0.8897. In the COVID-19 vaccinated patients, the balanced accuracy and AUROC dropped by 66.67% and 0.8072, respectively. The first finding in this study is that our proposed deep learning model can simply and accurately diagnose COVID-19 patients using HRs obtained from a smartwatch before the onset of symptoms. The second finding is that the model trained from unvaccinated patients may provide less accurate diagnosis performance compared with the vaccinated patients. The last finding is that the model trained in a certain period of time may provide degraded diagnosis performances as the virus continues to mutate.", "journal": "Journal of medical virology", "date": "2023-01-06", "authors": ["HeewonChung", "HoonKo", "HooseokLee", "Dong KeonYon", "Won HeeLee", "Tae-SeongKim", "Kyung WonKim", "JinseokLee"], "doi": "10.1002/jmv.28462"}
{"title": "Diagnostic performance of artificial intelligence algorithms for detection of pulmonary involvement by COVID-19 based on portable radiography.", "abstract": "To evaluate the diagnostic performance of different artificial intelligence (AI) algorithms for the identification of pulmonary involvement by SARS-CoV-2 based on portable chest radiography (RX).\nProspective observational study that included patients admitted for suspected COVID-19 infection in a university hospital between July and November 2020. The reference standard of pulmonary involvement by SARS-CoV-2 comprised a positive PCR test and low-tract respiratory symptoms.\n493 patients were included, 140 (28%) with positive PCR and 32 (7%) with SARS-CoV-2 pneumonia. The AI-B algorithm had the best diagnostic performance (areas under the ROC curve AI-B 0.73, vs. AI-A 0.51, vs. AI-C 0.57). Using a detection threshold greater than 55%, AI-B had greater diagnostic performance than the specialist [(area under the curve of 0.68 (95% CI 0.64-0.72), vs. 0.54 (95% CI 0.49-0.59)].\nAI algorithms based on portable RX enabled a diagnostic performance comparable to human assessment for the detection of SARS-CoV-2 lung involvement.\nEvaluar el rendimiento diagn\u00f3stico de diferentes algoritmos de inteligencia artificial (IA) para la identificaci\u00f3n de compromiso pulmonar por SARS-CoV-2 basados en radiograf\u00eda (Rx) de t\u00f3rax port\u00e1til.\nEstudio observacional prospectivo que incluy\u00f3 pacientes ingresados por sospecha de infecci\u00f3n por COVID-19 en un hospital universitario entre julio y noviembre de 2020. El patr\u00f3n de referencia de compromiso pulmonar por SARS-CoV-2 comprendi\u00f3 una PCR positiva y s\u00edntomas respiratorios bajos.\nSe incluyeron 493 pacientes, 140 (28%) con PCR positiva y 32 (7%) con neumon\u00eda por SARS-CoV-2. El algoritmo AI-B tuvo el mejor rendimiento diagn\u00f3stico (\u00e1reas bajo la curva ROC AI-B 0,73 vs. AI-A 0,51 vs. AI-C 0,57). Utilizando un umbral de detecci\u00f3n superior al 55%. AI-B present\u00f3 mayor precisi\u00f3n que el especialista (\u00e1rea bajo la curva de 0,68 [IC 95%: 0,64\u20130,72] vs. 0,54 [IC 95%: 0,49\u20130,59]).\nLos algoritmos de IA basados en Rx port\u00e1tiles permiten una precisi\u00f3n diagn\u00f3stica comparable a la humana para la detecci\u00f3n de compromiso pulmonar por SARS-CoV-2.", "journal": "Medicina clinica (English ed.)", "date": "2023-01-05", "authors": ["Ricardo LuisCobe\u00f1as", "Mar\u00edade Vedia", "JuanFlorez", "DanielaJaramillo", "LucianaFerrari", "RicardoRe"], "doi": "10.1016/j.medcle.2022.04.020\n10.2196/19104\n10.1109/RBME.2020.2987975\n10.7717/peerj-cs.551\n10.1186/s41747-020-00195-w\n10.1148/radiol.2020201874\n10.1016/j.jiph.2020.06.028"}
{"title": "Identification of Asymptomatic COVID-19 Patients on Chest CT Images Using Transformer-Based or Convolutional Neural Network-Based Deep Learning Models.", "abstract": "Novel coronavirus disease 2019 (COVID-19) has rapidly spread throughout the world; however, it is difficult for clinicians to make early diagnoses. This study is to evaluate the feasibility of using deep learning (DL) models to identify asymptomatic COVID-19 patients based on chest CT images. In this retrospective study, six DL models (Xception, NASNet, ResNet, EfficientNet, ViT, and Swin), based on convolutional neural networks (CNNs) or transformer architectures, were trained to identify asymptomatic patients with COVID-19 on chest CT images. Data from Yangzhou were randomly split into a training set (n\u2009=\u20092140) and an internal-validation set (n\u2009=\u2009360). Data from Suzhou was the external-test set (n\u2009=\u2009200). Model performance was assessed by the metrics accuracy, recall, and specificity and was compared with the assessments of two radiologists. A total of 2700 chest CT images were collected in this study. In the validation dataset, the Swin model achieved the highest accuracy of 0.994, followed by the EfficientNet model (0.954). The recall and the precision of the Swin model were 0.989 and 1.000, respectively. In the test dataset, the Swin model was still the best and achieved the highest accuracy (0.980). All the DL models performed remarkably better than the two experts. Last, the time on the test set diagnosis spent by two experts-42\u00a0min, 17\u00a0s (junior); and 29\u00a0min, 43\u00a0s (senior)-was significantly higher than those of the DL models (all below 2\u00a0min). This study evaluated the feasibility of multiple DL models in distinguishing asymptomatic patients with COVID-19 from healthy subjects on chest CT images. It found that a transformer-based model, the Swin model, performed best.", "journal": "Journal of digital imaging", "date": "2023-01-04", "authors": ["MinyueYin", "XiaolongLiang", "ZilanWang", "YijiaZhou", "YuHe", "YuhanXue", "JingwenGao", "JiaxiLin", "ChenyanYu", "LuLiu", "XiaolinLiu", "ChaoXu", "JinzhouZhu"], "doi": "10.1007/s10278-022-00754-0\n10.1056/NEJMoa2002032\n10.1007/s00330-020-06886-7\n10.1186/s12911-021-01521-x\n10.1016/j.compbiomed.2020.103805\n10.1016/s0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1148/radiol.2020200642\n10.1148/radiol.2020200230\n10.1016/s0140-6736(20)30183-5\n10.1148/radiol.2020200463\n10.1016/j.chest.2020.04.003\n10.1001/jama.2020.12839\n10.1038/s41598-020-74164-z\n10.1148/radiol.2020200823\n10.1016/s1473-3099(14)70846-1\n10.1001/jama.2020.2565\n10.1016/j.ijid.2020.06.052\n10.1038/s41467-020-17971-2\n10.1007/s00330-020-07042-x\n10.1109/tmi.2020.3040950\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-020-76550-z\n10.3389/fimmu.2021.732756\n10.1002/smll.202002169\n10.1016/j.talanta.2020.121726\n10.1148/radiol.2020200490\n10.1148/radiol.2020201365\n10.1016/j.annonc.2020.04.003\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200343\n10.1126/science.abb3221\n10.1007/s00330-021-07715-1\n10.1109/rbme.2020.2987975\n10.1155/2021/5185938\n10.1148/radiol.2020201491\n10.1109/tmi.2016.2528162"}
{"title": "COVID-19 Diagnosis on Chest Radiograph Using Artificial Intelligence.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has disrupted the world since 2019, causing significant morbidity and mortality in developed and developing countries alike. Although substantial resources have been diverted to developing diagnostic, preventative, and treatment measures, disparities in the availability and efficacy of these tools vary across countries. We seek to assess the ability of commercial artificial intelligence (AI) technology to diagnose COVID-19 by analyzing chest radiographs.\nChest radiographs taken from symptomatic patients within two days of polymerase chain reaction (PCR) tests were assessed for COVID-19 infection by board-certified radiologists and commercially available AI software. Sixty\u00a0patients with negative and 60 with positive COVID reverse transcription-polymerase chain reaction (RT-PCR) tests were chosen. Results were compared against results of the PCR test for accuracy and statistically analyzed by receiver operating characteristic (ROC) curves along with area under the curve (AUC) values.\nA total of 120 chest radiographs (60 positive and 60 negative RT-PCR tests) radiographs were analyzed. The AI software performed significantly better than chance (p = 0.001) and did not differ significantly from the radiologist ROC curve (p = 0.78).\nCommercially available AI software was not inferior compared with trained radiologists in accurately identifying COVID-19 cases by analyzing radiographs. While RT-PCR testing remains the standard, current advances in AI help correctly analyze chest radiographs to diagnose COVID-19 infection.", "journal": "Cureus", "date": "2022-12-30", "authors": ["DhirajBaruah", "LouisRunge", "Richard HJones", "Heather RCollins", "Ismail MKabakus", "Morgan PMcBee"], "doi": "10.7759/cureus.31897"}
{"title": "Covid-19 Diagnosis by WE-SAJ.", "abstract": "With a global COVID-19 pandemic, the number of confirmed patients increases rapidly, leaving the world with very few medical resources. Therefore, the fast diagnosis and monitoring of COVID-19 are one of the world's most critical challenges today. Artificial intelligence-based CT image classification models can quickly and accurately distinguish infected patients from healthy populations. Our research proposes a deep learning model (WE-SAJ) using wavelet entropy for feature extraction, two-layer FNNs for classification and the adaptive Jaya algorithm as a training algorithm. It achieves superior performance compared to the Jaya-based model. The model has a sensitivity of 85.47\u00b11.84, specificity of 87.23\u00b11.67 precision of 87.03\u00b11.34, an accuracy of 86.35\u00b10.70, and F1 score of 86.23\u00b10.77, Matthews correlation coefficient of 72.75\u00b11.38, and Fowlkes-Mallows Index of 86.24\u00b10.76. Our experiments demonstrate the potential of artificial intelligence techniques for COVID-19 diagnosis and the effectiveness of the Self-adaptive Jaya algorithm compared to the Jaya algorithm for medical image classification tasks.", "journal": "Systems science & control engineering", "date": "2022-12-27", "authors": ["WeiWang", "XinZhang", "Shui-HuaWang", "Yu-DongZhang"], "doi": "10.1080/21642583.2022.2045645"}
{"title": "Digital Health Applications to Establish a Remote Diagnosis of Orthopedic Knee Disorders: Scoping Review.", "abstract": "Knee pain is highly prevalent worldwide, and this number is expected to rise in the future. The COVID-19 outbreak, in combination with the aging population, rising health care costs, and the need to make health care more accessible worldwide, has led to an increasing demand for digital health care applications to deliver care for patients with musculoskeletal conditions. Digital health and other forms of telemedicine can add value in optimizing health care for patients and health care providers. This might reduce health care costs and make health care more accessible while maintaining a high level of quality. Although expectations are high, there is currently no overview comparing digital health applications with face-to-face contact in clinical trials to establish a primary knee diagnosis in orthopedic surgery.\nThis study aimed to investigate the currently available digital health and telemedicine applications to establish a primary knee diagnosis in orthopedic surgery in the general population in comparison with imaging or face-to-face contact between patients and physicians.\nA scoping review was conducted using the PubMed and Embase databases according to the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) statement. The inclusion criteria were studies reporting methods to determine a primary knee diagnosis in orthopedic surgery using digital health or telemedicine. On April 28 and 29, 2021, searches were conducted in PubMed (MEDLINE) and Embase. Data charting was conducted using a predefined form and included details on general study information, study population, type of application, comparator, analyses, and key findings. A risk-of-bias analysis was not deemed relevant considering the scoping review design of the study.\nAfter screening 5639 articles, 7 (0.12%) were included. In total, 2 categories to determine a primary diagnosis were identified: screening studies (4/7, 57%) and decision support studies (3/7, 43%). There was great heterogeneity in the included studies in algorithms used, disorders, input parameters, and outcome measurements. No more than 25 knee disorders were included in the studies. The included studies showed a relatively high sensitivity (67%-91%). The accuracy of the different studies was generally lower, with a specificity of 27% to 48% for decision support studies and 73% to 96% for screening studies.\nThis scoping review shows that there are a limited number of available applications to establish a remote diagnosis of knee disorders in orthopedic surgery. To date, there is limited evidence that digital health applications can assist patients or orthopedic surgeons in establishing the primary diagnosis of knee disorders. Future research should aim to integrate multiple sources of information and a standardized study design with close collaboration among clinicians, data scientists, data managers, lawyers, and service users to create reliable and secure databases.", "journal": "Journal of medical Internet research", "date": "2022-12-26", "authors": ["Sander Cvan Eijck", "Daan MJanssen", "Maria Cvan der Steen", "Eugenie J L GDelvaux", "Johannes G EHendriks", "Rob P AJanssen"], "doi": "10.2196/40504\n10.1109/JBHI.2016.2636665\n10.1177/1357633X15572201\n10.1016/j.gaitpost.2017.06.019\n10.1007/s00132-018-3604-x\n10.1016/j.arth.2019.05.055\n10.1080/17453674.2021.1884408\n10.1080/17453674.2021.1884408\n10.3928/01477447-20210819-09\n10.1097/CORR.0000000000001494\n10.1016/j.dsx.2020.06.007\n10.1136/annrheumdis-2013-204680\n10.1186/s12891-019-2510-7\n10.1186/s12891-019-2510-7\n10.1093/rheumatology/keu409\n10.1016/S0140-6736(20)30925-9\n10.7326/M18-0850?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.7326/M18-0850\n10.1186/s13643-016-0384-4\n10.1186/s13643-016-0384-4\n10.1177/0363546514541654\n10.1177/2325967116630286?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub0pubmed\n10.1177/2325967116630286\n10.1055/s-0038-1656547\n10.3390/ijerph16071281\n10.1016/j.joca.2012.08.017\n10.1136/ard.2007.075952\n10.3399/bjgp15X686089\n10.1136/ard.62.10.957\n10.1093/ije/dyt228\n10.1186/s12891-017-1439-y\n10.1186/s12891-017-1439-y\n10.1007/s00264-009-0760-y\n10.1002/hec.3933\n10.1001/jamasurg.2019.4917\n10.1136/bmj.m127\n10.1007/s43681-020-00010-7\n10.1186/1748-7161-9-10\n10.1186/1748-7161-9-10\n10.2196/14172\n10.1016/j.medengphy.2017.02.004\n10.1002/pmrj.12393\n10.1016/j.diii.2019.02.007\n10.1186/s12984-021-00841-3\n10.1186/s12984-021-00841-3\n10.1038/s41746-018-0066-9\n10.1038/s41746-018-0066-9\n10.1001/jama.2016.17216\n10.1111/joim.12822\n10.1111/joim.12822\n10.1056/NEJMoa1901183\n10.3389/fbioe.2018.00075\n10.1136/ebmental-2020-300180\n10.1007/s00520-020-05539-1\n10.1016/j.hlpt.2018.04.003\n10.1016/j.hlpt.2018.04.003\n10.1371/journal.pone.0243043\n10.1371/journal.pone.0243043"}
{"title": "Detection of COVID-19 in X-ray Images Using Densely Connected Squeeze Convolutional Neural Network (DCSCNN): Focusing on Interpretability and Explainability of the Black Box Model.", "abstract": "The novel coronavirus (COVID-19), which emerged as a pandemic, has engulfed so many lives and affected millions of people across the world since December 2019. Although this disease is under control nowadays, yet it is still affecting people in many countries. The traditional way of diagnosis is time taking, less efficient, and has a low rate of detection of this disease. Therefore, there is a need for an automatic system that expedites the diagnosis process while retaining its performance and accuracy. Artificial intelligence (AI) technologies such as machine learning (ML) and deep learning (DL) potentially provide powerful solutions to address this problem. In this study, a state-of-the-art CNN model densely connected squeeze convolutional neural network (DCSCNN) has been developed for the classification of X-ray images of COVID-19, pneumonia, normal, and lung opacity patients. Data were collected from different sources. We applied different preprocessing techniques to enhance the quality of images so that our model could learn accurately and give optimal performance. Moreover, the attention regions and decisions of the AI model were visualized using the Grad-CAM and LIME methods. The DCSCNN combines the strength of the Dense and Squeeze networks. In our experiment, seven kinds of classification have been performed, in which six are binary classifications (COVID vs. normal, COVID vs. lung opacity, lung opacity vs. normal, COVID vs. pneumonia, pneumonia vs. lung opacity, pneumonia vs. normal) and one is multiclass classification (COVID vs. pneumonia vs. lung opacity vs. normal). The main contributions of this paper are as follows. First, the development of the DCSNN model which is capable of performing binary classification as well as multiclass classification with excellent classification accuracy. Second, to ensure trust, transparency, and explainability of the model, we applied two popular Explainable AI techniques (XAI). i.e., Grad-CAM and LIME. These techniques helped to address the black-box nature of the model while improving the trust, transparency, and explainability of the model. Our proposed DCSCNN model achieved an accuracy of 98.8% for the classification of COVID-19 vs normal, followed by COVID-19 vs. lung opacity: 98.2%, lung opacity vs. normal: 97.2%, COVID-19 vs. pneumonia: 96.4%, pneumonia vs. lung opacity: 95.8%, pneumonia vs. normal: 97.4%, and lastly for multiclass classification of all the four classes i.e., COVID vs. pneumonia vs. lung opacity vs. normal: 94.7%, respectively. The DCSCNN model provides excellent classification performance consequently, helping doctors to diagnose diseases quickly and efficiently.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-12-24", "authors": ["SikandarAli", "AliHussain", "SubrataBhattacharjee", "AliAthar", "NoneAbdullah", "Hee-CheolKim"], "doi": "10.3390/s22249983\n10.1016/S0140-6736(66)92364-6\n10.1016/j.celrep.2020.108175\n10.1159/000149390\n10.1001/jama.2020.1097\n10.1016/S0140-6736(20)30251-8\n10.1016/S0140-6736(21)02758-6\n10.15585/mmwr.mm7050e1\n10.2174/0929867328666210521164809\n10.1111/cbdd.13761\n10.1002/ped4.12178\n10.1093/cid/ciaa799\n10.1148/radiol.2020200642\n10.1001/jama.2020.3786\n10.3390/diagnostics10030165\n10.1001/jama.2020.1585\n10.1148/radiol.2020200432\n10.1016/j.measurement.2019.05.076\n10.54112/bcsrj.v2020i1.31\n10.1007/s12195-020-00629-w\n10.1007/s10044-021-00984-y\n10.1007/s00330-021-07715-1\n10.20944/preprints202003.0300.v1\n10.1016/j.chaos.2020.110120\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.104037\n10.1016/j.bbe.2020.08.008\n10.3390/life11101092\n10.3390/diagnostics11050829\n10.3390/jpm12060988\n10.1109/JBHI.2022.3168604\n10.32604/cmc.2020.013249\n10.1016/j.inffus.2021.07.016\n10.3892/etm.2020.8797\n10.1016/j.compbiomed.2022.105244\n10.1002/ima.22706\n10.1038/s41598-020-76550-z\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2021.104335\n10.1016/j.intimp.2020.106705\n10.1007/s10489-020-01829-7\n10.1007/s42600-020-00112-5\n10.1016/j.mri.2014.03.010\n10.1109/42.996338\n10.1016/j.chaos.2020.110190\n10.1016/j.compbiomed.2020.104041"}
{"title": "Image Translation by Ad CycleGAN for COVID-19 X-Ray Images: A New Approach for Controllable GAN.", "abstract": "We propose a new generative model named adaptive cycle-consistent generative adversarial network, or Ad CycleGAN to perform image translation between normal and COVID-19 positive chest X-ray images. An independent pre-trained criterion is added to the conventional Cycle GAN architecture to exert adaptive control on image translation. The performance of Ad CycleGAN is compared with the Cycle GAN without the external criterion. The quality of the synthetic images is evaluated by quantitative metrics including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQI), visual information fidelity (VIF), Frechet Inception Distance (FID), and translation accuracy. The experimental results indicate that the synthetic images generated either by the Cycle GAN or by the Ad CycleGAN have lower MSE and RMSE, and higher scores in PSNR, UIQI, and VIF in homogenous image translation (i.e., Y \u2192 Y) compared to the heterogenous image translation process (i.e., X \u2192 Y). The synthetic images by Ad CycleGAN through the heterogeneous image translation have significantly higher FID score compared to Cycle GAN (p < 0.01). The image translation accuracy of Ad CycleGAN is higher than that of Cycle GAN when normal images are converted to COVID-19 positive images (p < 0.01). Therefore, we conclude that the Ad CycleGAN with the independent criterion can improve the accuracy of GAN image translation. The new architecture has more control on image synthesis and can help address the common class imbalance issue in machine learning methods and artificial intelligence applications with medical images.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-12-24", "authors": ["ZhaohuiLiang", "Jimmy XiangjiHuang", "SameerAntani"], "doi": "10.3390/s22249628\n10.1016/S0140-6736(20)30211-7\n10.1002/14651858.CD013652\n10.1109/TMI.2020.3008025\n10.1016/j.sysarc.2020.101830\n10.1016/j.media.2020.101794\n10.1371/journal.pone.0243963\n10.48550/arXiv.1406.2661\n10.1109/CVPR.2017.632\n10.1109/ICCV.2017.244\n10.1016/j.media.2018.12.002\n10.1109/TIP.2019.2922854\n10.1109/iccv.2019.00028\n10.1088/2057-1976/ab6e1f\n10.3340/jkns.2019.0084\n10.21037/atm-21-4056\n10.1007/978-3-319-66179-7_48\n10.1002/mp.13047\n10.1088/1361-6560/aba5e9\n10.3390/s22124640\n10.48550/arXiv.1701.07875\n10.48550/arXiv.1505.04597\n10.48550/arXiv.1807.08536\n10.1109/TIP.2003.819861\n10.1109/97.995823\n10.48550/arXiv.1706.08500\n10.1016/j.imu.2021.100779\n10.1016/j.eswa.2021.115681\n10.1007/s12530-020-09346-1\n10.1109/TNNLS.2018.2875194"}
{"title": "The Capacity of Artificial Intelligence in COVID-19 Response: A Review in Context of COVID-19 Screening and Diagnosis.", "abstract": "Artificial intelligence (AI) has been shown to solve several issues affecting COVID-19 diagnosis. This systematic review research explores the impact of AI in early COVID-19 screening, detection, and diagnosis. A comprehensive survey of AI in the COVID-19 literature, mainly in the context of screening and diagnosis, was observed by applying the preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. Data sources for the years 2020, 2021, and 2022 were retrieved from google scholar, web of science, Scopus, and PubMed, with target keywords relating to AI in COVID-19 screening and diagnosis. After a comprehensive review of these studies, the results found that AI contributed immensely to improving COVID-19 screening and diagnosis. Some proposed AI models were shown to have comparable (sometimes even better) clinical decision outcomes, compared to experienced radiologists in the screening/diagnosing of COVID-19. Additionally, AI has the capacity to reduce physician work burdens and fatigue and reduce the problems of several false positives, associated with the RT-PCR test (with lower sensitivity of 60-70%) and medical imaging analysis. Even though AI was found to be timesaving and cost-effective, with less clinical errors, it works optimally under the supervision of a physician or other specialists.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-12-24", "authors": ["Dilber UzunOzsahin", "Nuhu AbdulhaqqIsa", "BernaUzun"], "doi": "10.3390/diagnostics12122943\n10.3390/diagnostics12081880\n10.1016/S0140-6736(20)30154-9\n10.1186/s12916-021-01954-1\n10.1007/s42399-020-00655-9\n10.21037/jmai-20-48\n10.1148/radiol.2020200432\n10.1016/j.compbiomed.2020.103792\n10.5858/arpa.2020-0901-SA\n10.3389/frai.2021.652669\n10.1007/s13369-022-06841-2\n10.1016/j.dsx.2020.04.012\n10.1155/2020/9756518\n10.1016/j.amsu.2021.102489\n10.1002/ima.22697\n10.1007/s12195-020-00629-w\n10.1016/j.chaos.2020.110059\n10.31083/j.rcm.2020.04.236\n10.2217/fvl-2020-0130\n10.3390/diagnostics11071155\n10.3390/diagnostics12081853\n10.1155/2021/5527271\n10.3390/diagnostics12040869\n10.1016/j.jacbts.2016.11.010\n10.1007/s10916-020-1536-6\n10.1155/2020/1560250\n10.1007/s00354-022-00172-4\n10.1146/annurev-bioeng-071516-044442\n10.1136/bmj.m1328\n10.2196/preprints.19526\n10.47102/annals-acadmedsg.202057\n10.1101/2020.03.30.20047308\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1371/journal.pone.0213653\n10.1038/s41598-022-07890-1\n10.1093/jtm/taaa008\n10.1016/S1473-3099(20)30297-8\n10.1038/s41591-020-0931-3\n10.1016/S2213-2600(20)30076-X\n10.1186/s12968-019-0551-6\n10.1148/radiol.2020201491\n10.1109/RBME.2020.2987975\n10.1088/1361-6560/abbf9e\n10.1613/jair.1.12162\n10.1148/radiol.2020200230\n10.1148/radiol.2020200241\n10.1109/ACCESS.2020.3001973\n10.1145/3465398\n10.1007/s00500-020-05424-3\n10.1155/2021/5527923\n10.1016/j.bspc.2021.102490\n10.3390/app12094694\n10.32604/csse.2022.022158\n10.11591/ijece.v12i4.pp3655-3664\n10.1016/j.ibmed.2022.100049\n10.1016/j.eswa.2022.117410\n10.3390/s22020669\n10.1080/07391102.2021.1875049\n10.1016/j.neucom.2022.01.055\n10.1016/j.bspc.2020.102365\n10.1038/s41598-022-11990-3\n10.1016/j.bspc.2022.103778\n10.1186/s41747-018-0061-6\n10.1016/j.neunet.2014.09.003\n10.1016/j.chaos.2020.109944\n10.1016/j.matpr.2021.11.512\n10.1016/j.ssci.2020.105034\n10.1109/ACCESS.2020.3025010\n10.1007/s00264-020-04609-7\n10.1038/s41591-020-0824-5\n10.1109/TMI.2020.2996256\n10.1186/s12938-020-00809-9\n10.1007/s10489-020-01826-w\n10.1007/s10096-020-03901-z"}
{"title": "Diagnostic Performance in Differentiating COVID-19 from Other Viral Pneumonias on CT Imaging: Multi-Reader Analysis Compared with an Artificial Intelligence-Based Model.", "abstract": "Growing evidence suggests that artificial intelligence tools could help radiologists in differentiating COVID-19 pneumonia from other types of viral (non-COVID-19) pneumonia. To test this hypothesis, an R-AI classifier capable of discriminating between COVID-19 and non-COVID-19 pneumonia was developed using CT chest scans of 1031 patients with positive swab for SARS-CoV-2 (", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-12-23", "authors": ["FrancescoRizzetto", "LucaBerta", "GiuliaZorzi", "AntoninoCincotta", "FrancescaTravaglini", "DianaArtioli", "SilviaNerini Molteni", "ChiaraVismara", "FrancescoScaglione", "AlbertoTorresin", "Paola EnricaColombo", "Luca AlessandroCarbonaro", "AngeloVanzulli"], "doi": "10.3390/tomography8060235\n10.1016/j.ejrad.2021.109650\n10.1007/s00330-020-06827-4\n10.1148/radiol.2020202504\n10.2214/AJR.21.25640\n10.1148/rg.2018170048\n10.1148/radiol.2020200823\n10.1148/radiol.2020201473\n10.1007/s00330-022-08576-y\n10.1007/s00330-020-07273-y\n10.1148/ryai.2020200053\n10.1016/S2589-7500(20)30199-0\n10.21037/atm-20-5328\n10.1007/s00259-020-05075-4\n10.1148/radiol.2020201491\n10.3390/diagnostics12040869\n10.1016/j.ejrad.2021.110028\n10.1080/01621459.1927.10502953\n10.1080/03610918.2018.1490428\n10.1080/00031305.2016.1141708\n10.1007/s10826-019-01536-z\n10.1016/j.ejmp.2021.06.001\n10.1007/s10140-021-01967-6\n10.1186/s12890-020-1170-6\n10.1016/j.resmer.2021.100824\n10.1007/s11547-021-01370-8\n10.1016/j.ejrad.2021.109552\n10.1148/radiol.2020201365"}
{"title": "Interactive framework for Covid-19 detection and segmentation with feedback facility for dynamically improved accuracy and trust.", "abstract": "Due to the severity and speed of spread of the ongoing Covid-19 pandemic, fast but accurate diagnosis of Covid-19 patients has become a crucial task. Achievements in this respect might enlighten future efforts for the containment of other possible pandemics. Researchers from various fields have been trying to provide novel ideas for models or systems to identify Covid-19 patients from different medical and non-medical data. AI-based researchers have also been trying to contribute to this area by mostly providing novel approaches of automated systems using convolutional neural network (CNN) and deep neural network (DNN) for Covid-19 detection and diagnosis. Due to the efficiency of deep learning (DL) and transfer learning (TL) models in classification and segmentation tasks, most of the recent AI-based researches proposed various DL and TL models for Covid-19 detection and infected region segmentation from chest medical images like X-rays or CT images. This paper describes a web-based application framework for Covid-19 lung infection detection and segmentation. The proposed framework is characterized by a feedback mechanism for self learning and tuning. It uses variations of three popular DL models, namely Mask R-CNN, U-Net, and U-Net++. The models were trained, evaluated and tested using CT images of Covid patients which were collected from two different sources. The web application provide a simple user friendly interface to process the CT images from various resources using the chosen models, thresholds and other parameters to generate the decisions on detection and segmentation. The models achieve high performance scores for Dice similarity, Jaccard similarity, accuracy, loss, and precision values. The U-Net model outperformed the other models with more than 98% accuracy.", "journal": "PloS one", "date": "2022-12-23", "authors": ["KashfiaSailunaz", "DenizBestepe", "Tansel\u00d6zyer", "JonRokne", "RedaAlhajj"], "doi": "10.1371/journal.pone.0278487\n10.3390/v12040372\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020203173\n10.3389/fpubh.2022.1046296\n10.1148/rg.2020200159\n10.1155/2021/2560388\n10.32604/cmc.2023.032064\n10.3390/s21062215\n10.3390/ijerph18031117\n10.1016/j.compbiomed.2022.105350\n10.3389/fcvm.2021.638011\n10.1155/2020/9756518\n10.1109/ACCESS.2021.3054484\n10.1016/j.dsx.2020.05.008\n10.1109/RBME.2020.2987975\n10.1016/j.jiph.2020.06.028\n10.3390/jcm10091961\n10.1007/s10489-020-02102-7\n10.1016/j.chaos.2020.110059\n10.3389/fpubh.2022.948205\n10.1016/j.neucom.2021.03.034\n10.1016/j.bea.2022.100041\n10.1007/s13369-021-05958-0\n10.1016/j.asoc.2021.107522\n10.1016/j.patcog.2020.107747\n10.1038/s41598-022-06854-9\n10.1007/s10489-021-02731-6\n10.3390/electronics11152296\n10.1007/s11063-022-10785-x\n10.1007/s13755-021-00146-8\n10.1038/s41598-020-76282-0\n10.1016/j.cmpbup.2021.100007\n10.1002/mp.15231\n10.3390/electronics11010130\n10.7717/peerj-cs.349\n10.1016/j.patcog.2021.107828\n10.3390/su13031224\n10.1016/j.cell.2020.04.045\n10.3389/fmed.2020.608525\n10.1016/j.compbiomed.2021.104319\n10.1186/s41747-020-00173-2\n10.5194/isprs-archives-XLIII-B3-2020-1507-2020\n10.1021/acs.jcim.8b00671\n10.1145/3352020.3352029\n10.3390/brainsci10070427\n10.3390/app10051897\n10.1016/j.neunet.2015.07.007\n10.1016/j.neucom.2022.01.014\n10.1016/j.media.2021.102035\n10.1186/s12880-015-0068-x"}
{"title": "Disease Recognition in X-ray Images with Doctor Consultation-Inspired Model.", "abstract": "The application of chest X-ray imaging for early disease screening is attracting interest from the computer vision and deep learning community. To date, various deep learning models have been applied in X-ray image analysis. However, models perform inconsistently depending on the dataset. In this paper, we consider each individual model as a medical doctor. We then propose a doctor consultation-inspired method that fuses multiple models. In particular, we consider both early and late fusion mechanisms for consultation. The early fusion mechanism combines the deep learned features from multiple models, whereas the late fusion method combines the confidence scores of all individual models. Experiments on two X-ray imaging datasets demonstrate the superiority of the proposed method relative to baseline. The experimental results also show that early consultation consistently outperforms the late consultation mechanism in both benchmark datasets. In particular, the early doctor consultation-inspired model outperforms all individual models by a large margin, i.e., 3.03 and 1.86 in terms of accuracy in the UIT COVID-19 and chest X-ray datasets, respectively.", "journal": "Journal of imaging", "date": "2022-12-23", "authors": ["Kim AnhPhung", "Thuan TrongNguyen", "NileshkumarWangad", "SamahBaraheem", "Nguyen DVo", "KhangNguyen"], "doi": "10.3390/jimaging8120323\n10.1148/radiol.2020200642\n10.1016/j.cca.2020.03.009\n10.1145/3065386\n10.7861/futurehosp.6-2-94\n10.3390/info13080360\n10.1038/s41551-018-0305-z\n10.1016/j.jacr.2019.05.047\n10.1016/j.imu.2020.100405\n10.20944/preprints202003.0300.v1\n10.1016/j.bbe.2020.08.005\n10.1038/s41591-020-0931-3\n10.1097/RTI.0000000000000512\n10.1016/j.chemolab.2020.104054\n10.1109/TPAMI.2007.1110\n10.1016/j.compbiomed.2020.103795\n10.1109/TPAMI.2020.2983686\n10.1109/ACCESS.2020.2994762\n10.1109/TMI.2020.2993291\n10.3390/s21217116\n10.3390/ijerph17186933\n10.1016/j.artmed.2021.102156\n10.1016/j.compbiomed.2022.105383\n10.1007/s10489-020-01831-z\n10.3390/jimaging7020012\n10.1109/TPAMI.2017.2723009"}
{"title": "Improving COVID-19 CT classification of CNNs by learning parameter-efficient representation.", "abstract": "The COVID-19 pandemic continues to spread rapidly over the world and causes a tremendous crisis in global human health and the economy. Its early detection and diagnosis are crucial for controlling the further spread. Many deep learning-based methods have been proposed to assist clinicians in automatic COVID-19 diagnosis based on computed tomography imaging. However, challenges still remain, including low data diversity in existing datasets, and unsatisfied detection resulting from insufficient accuracy and sensitivity of deep learning models. To enhance the data diversity, we design augmentation techniques of incremental levels and apply them to the largest open-access benchmark dataset, COVIDx CT-2A. Meanwhile, similarity regularization (SR) derived from contrastive learning is proposed in this study to enable CNNs to learn more parameter-efficient representations, thus improve the accuracy and sensitivity of CNNs. The results on seven commonly used CNNs demonstrate that CNN performance can be improved stably through applying the designed augmentation and SR techniques. In particular, DenseNet121 with SR achieves an average test accuracy of 99.44% in three trials for three-category classification, including normal, non-COVID-19 pneumonia, and COVID-19 pneumonia. The achieved precision, sensitivity, and specificity for the COVID-19 pneumonia category are 98.40%, 99.59%, and 99.50%, respectively. These statistics suggest that our method has surpassed the existing state-of-the-art methods on the COVIDx CT-2A dataset. Source code is available at https://github.com/YujiaKCL/COVID-CT-Similarity-Regularization.", "journal": "Computers in biology and medicine", "date": "2022-12-22", "authors": ["YujiaXu", "Hak-KeungLam", "GuangyuJia", "JianJiang", "JunkaiLiao", "XinqiBao"], "doi": "10.1016/j.compbiomed.2022.106417\n10.1136/bmj.n597\n10.3389/fmed.2020.608525\n10.5281/zenodo.4414861"}
{"title": "Virus Detection and Identification in Minutes Using Single-Particle Imaging and Deep Learning.", "abstract": "The increasing frequency and magnitude of viral outbreaks in recent decades, epitomized by the COVID-19 pandemic, has resulted in an urgent need for rapid and sensitive diagnostic methods. Here, we present a methodology for virus detection and identification that uses a convolutional neural network to distinguish between microscopy images of fluorescently labeled intact particles of different viruses. Our assay achieves labeling, imaging, and virus identification in less than 5 min and does not require any lysis, purification, or amplification steps. The trained neural network was able to differentiate SARS-CoV-2 from negative clinical samples, as well as from other common respiratory pathogens such as influenza and seasonal human coronaviruses. We were also able to differentiate closely related strains of influenza, as well as SARS-CoV-2 variants. Additional and novel pathogens can easily be incorporated into the test through software updates, offering the potential to rapidly utilize the technology in future infectious disease outbreaks or pandemics. Single-particle imaging combined with deep learning therefore offers a promising alternative to traditional viral diagnostic and genomic sequencing methods and has the potential for significant impact.", "journal": "ACS nano", "date": "2022-12-22", "authors": ["NicolasShiaelis", "AlexanderTometzki", "LeonPeto", "AndrewMcMahon", "ChristofHepp", "EricaBickerton", "CyrilFavard", "DelphineMuriaux", "MoniqueAndersson", "SarahOakley", "AliVaughan", "Philippa CMatthews", "NicoleStoesser", "Derrick WCrook", "Achillefs NKapanidis", "Nicole CRobb"], "doi": "10.1021/acsnano.2c10159\n10.1021/acsnano.0c02624\n10.1111/1751-7915.13586\n10.1007/s12250-020-00218-1\n10.1371/journal.pone.0234682\n10.1101/2020.02.26.20028373\n10.1039/D0AN01835J\n10.1021/acscentsci.0c01288\n10.1002/14651858.CD013705.pub2\n10.1038/s41598-019-52759-5\n10.1162/neco_a_00990\n10.1038/nature14539\n10.7554/eLife.40183\n10.1007/s12560-018-9335-7\n10.1093/cid/ciaa1382\n10.1016/S1473-3099(20)30113-4\n10.7150/ijbs.45018\n10.1111/j.1365-2672.2010.04663.x\n10.1002/elps.202000121\n10.1016/0166-0934(91)90012-O\n10.1038/s41598-021-91371-4\n10.1128/JVI.75.24.12359-12369.2001\n10.2807/1560-7917.ES.2021.26.3.2100008\n10.1136/bmj.308.6943.1552"}
{"title": "Deep features to detect pulmonary abnormalities in chest X-rays due to infectious diseaseX: Covid-19, pneumonia, and tuberculosis.", "abstract": "Chest X-ray (CXR) imaging is a low-cost, easy-to-use imaging alternative that can be used to diagnose/screen pulmonary abnormalities due to infectious diseaseX: Covid-19, Pneumonia and Tuberculosis (TB). Not limited to binary decisions (with respect to healthy cases) that are reported in the state-of-the-art literature, we also consider non-healthy CXR screening using a lightweight deep neural network (DNN) with a reduced number of epochs and parameters. On three diverse publicly accessible and fully categorized datasets, for non-healthy versus healthy CXR screening, the proposed DNN produced the following accuracies: 99.87% on Covid-19 versus healthy, 99.55% on Pneumonia versus healthy, and 99.76% on TB versus healthy datasets. On the other hand, when considering non-healthy CXR screening, we received the following accuracies: 98.89% on Covid-19 versus Pneumonia, 98.99% on Covid-19 versus TB, and 100% on Pneumonia versus TB. To further precisely analyze how well the proposed DNN worked, we considered well-known DNNs such as ResNet50, ResNet152V2, MobileNetV2, and InceptionV3. Our results are comparable with the current state-of-the-art, and as the proposed CNN is light, it could potentially be used for mass screening in resource-constraint regions.", "journal": "Information sciences", "date": "2022-12-20", "authors": ["Md KawsherMahbub", "MilonBiswas", "LoveleenGaur", "FayadhAlenezi", "K CSantosh"], "doi": "10.1016/j.ins.2022.01.062"}
{"title": "LWSNet - a novel deep-learning architecture to segregate Covid-19 and pneumonia from x-ray imagery.", "abstract": "Automatic detection of lung diseases using AI-based tools became very much necessary to handle the huge number of cases occurring across the globe and support the doctors. This paper proposed a novel deep learning architecture named LWSNet (Light Weight Stacking Network) to separate Covid-19, cold pneumonia, and normal chest x-ray images. This framework is based on single, double, triple, and quadruple stack mechanisms to address the above-mentioned tri-class problem. In this framework, a truncated version of standard deep learning models and a lightweight CNN model was considered to conviniently deploy in resource-constraint devices. An evaluation was conducted on three publicly available datasets alongwith their combination. We received 97.28%, 96.50%, 97.41%, and 98.54% highest classification accuracies using quadruple stack. On further investigation, we found, using LWSNet, the average accuracy got improved from individual model to quadruple model by 2.31%, 2.55%, 2.88%, and 2.26% on four respective datasets.", "journal": "Multimedia tools and applications", "date": "2022-12-20", "authors": ["AsifuzzamanLasker", "MridulGhosh", "Sk MdObaidullah", "ChandanChakraborty", "KaushikRoy"], "doi": "10.1007/s11042-022-14247-3\n10.1111/exsy.12749\n10.1145/3431804\n10.1148/radiol.2020200642\n10.1007/s10489-021-02199-4\n10.1007/s13246-020-00865-4\n10.3390/ijerph19042013\n10.7717/peerj-cs.551\n10.1007/s11042-021-11103-8\n10.1007/s00371-021-02094-6\n10.1038/s41598-018-33214-3\n10.1016/S0140-6736(20)30183-5\n10.1016/j.imu.2020.100412\n10.1007/s10489-020-01902-1\n10.3390/su14116785\n10.1016/j.compbiomed.2020.104181\n10.1038/nature14539\n10.1016/j.neucom.2016.12.038\n10.1016/j.compbiomed.2022.105213\n10.31661/jbpe.v0i0.2008-1153\n10.1007/s10489-020-01943-6\n10.1016/j.bbe.2021.06.011\n10.1109/JBHI.2021.3051470\n10.1109/TNNLS.2021.3054746\n10.1016/j.compbiomed.2021.104319\n10.1016/j.imu.2020.100505\n10.1007/s11548-020-02286-w\n10.1016/j.bspc.2021.102622\n10.1142/S0218001421510046\n10.1109/TII.2021.3057683\n10.1007/s11042-021-11807-x\n10.1049/iet-ipr.2020.1127\n10.1016/S0893-6080(05)80023-1\n10.1002/jmv.26741\n10.1371/journal.pone.0236621"}
{"title": "Multi-objective automatic analysis of lung ultrasound data from COVID-19 patients by means of deep learning and decision trees.", "abstract": "COVID-19 raised the need for automatic medical diagnosis, to increase the physicians' efficiency in managing the pandemic. Among all the techniques for evaluating the status of the lungs of a patient with COVID-19, lung ultrasound (LUS) offers several advantages: portability, cost-effectiveness, safety. Several works approached the automatic detection of LUS imaging patterns related COVID-19 by using deep neural networks (DNNs). However, the decision processes based on DNNs are not fully explainable, which generally results in a lack of trust from physicians. This, in turn, slows down the adoption of such systems. In this work, we use two previously built DNNs as feature extractors at the frame level, and automatically synthesize, by means of an evolutionary algorithm, a decision tree (DT) that aggregates in an interpretable way the predictions made by the DNNs, returning the severity of the patients' conditions according to a LUS score of prognostic value. Our results show that our approach performs comparably or better than previously reported aggregation techniques based on an empiric combination of frame-level predictions made by DNNs. Furthermore, when we analyze the evolved DTs, we discover properties about the DNNs used as feature extractors. We make our data publicly available for further development and reproducibility.", "journal": "Applied soft computing", "date": "2022-12-20", "authors": ["Leonardo LucioCustode", "FedericoMento", "FrancescoTursi", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone", "LibertarioDemi", "GiovanniIacca"], "doi": "10.1016/j.asoc.2022.109926\n10.1002/jum.15284\n10.1002/jum.15285\n10.1148/radiol.2020200847\n10.1016/j.ejro.2020.100231\n10.1016/j.jamda.2020.05.050\n10.4269/ajtmh.20-0280\n10.1186/s13054-020-02876-9\n10.1007/s00134-020-05996-6\n10.1007/s00134-020-06058-7\n10.1121/10.0002183\n10.1016/j.ultrasmedbio.2020.07.018\n10.1109/TUFFC.2020.3012289\n10.1121/10.0001797\n10.1121/10.0001797\n10.1007/s40477-017-0244-7\n10.1016/j.ultrasmedbio.2017.01.011\n10.1186/s12931-020-01338-8\n10.1109/TMI.2020.2994459\n10.1109/TUFFC.2020.3005512\n10.1016/j.media.2021.101975\n10.1109/TMI.2021.3117246\n10.1002/jum.15548\n10.1007/BFb0055930\n10.1007/BFb0055930"}
{"title": "An NLP tool for data extraction from electronic health records: COVID-19 mortalities and comorbidities.", "abstract": "The high infection rate, severe symptoms, and evolving aspects of the COVID-19 pandemic provide challenges for a variety of medical systems around the world. Automatic information retrieval from unstructured text is greatly aided by Natural Language Processing (NLP), the primary approach taken in this field. This study addresses COVID-19 mortality data from the intensive care unit (ICU) in Kuwait during the first 18 months of the pandemic. A key goal is to extract and classify the primary and intermediate causes of death from electronic health records (EHRs) in a timely way. In addition, comorbid conditions or concurrent diseases were retrieved and analyzed in relation to a variety of causes of mortality.\nAn NLP system using the Python programming language is constructed to automate the process of extracting primary and secondary causes of death, as well as comorbidities. The system is capable of handling inaccurate and messy data, this includes inadequate formats, spelling mistakes and mispositioned information. A machine learning decision trees method is used to classify the causes of death.\nFor 54.8% of the 1691 ICU patients we studied, septic shock or sepsis-related multiorgan failure was the leading cause of mortality. About three-quarters of patients die from acute respiratory distress syndrome (ARDS), a common intermediate cause of death. An arrhythmia (AF) disorder was determined to be the strongest predictor of intermediate cause of death, whether caused by ARDS or other causes.\nWe created an NLP system to automate the extraction of causes of death and comorbidities from EHRs. Our method processes messy and erroneous data and classifies the primary and intermediate causes of death of COVID-19 patients. We advocate arranging the EHR with well-defined sections and menu-driven options to reduce incorrect forms.", "journal": "Frontiers in public health", "date": "2022-12-20", "authors": ["Sana SBuHamra", "Abdullah NAlmutairi", "Abdullah KBuhamrah", "Sabah HAlmadani", "Yusuf AAlibrahim"], "doi": "10.3389/fpubh.2022.1070870\n10.4258/hir.2019.25.1.1\n10.1093/jamia/ocw082\n10.2196/12239\n10.4338/ACI-2013-10-RA-0080\n10.1038/nn.4493\n10.1101/2020.03.16.20036723\n10.1038/s41746-020-00372-6\n10.2196/21801\n10.1080/07853890.2020.1868564\n10.1080/13467581.2019.1696203\n10.1016/J.PROCS.2018.10.313\n10.1007/s00134-020-05991-x\n10.1002/jmv.25689\n10.1159/000513047\n10.3389/fmed.2020.00348\n10.3346/jkms.2020.35.e328\n10.1017/S0950268820001405\n10.1016/S0140-6736(20)30566-3\n10.1001/jama.2020.5394\n10.1097/CCE.0000000000000419\n10.1136/bmjdrc-2020-001343\n10.1007/s00392-020-01626-9\n10.1016/j.ijid.2020.03.017\n10.1001/jama.2020.4326\n10.1016/j.tmaid.2020.101623\n10.1001/jama.286.14.1754\n10.5694/mja2.50674\n10.1186/s13054-020-03240-7\n10.1371/journal.pone.0242768\n10.1038/s41598-021-82862-5"}
{"title": "Feature fusion based VGGFusionNet model to detect COVID-19 patients utilizing computed tomography scan images.", "abstract": "COVID-19 is one of the most life-threatening and dangerous diseases caused by the novel Coronavirus, which has already afflicted a larger human community worldwide. This pandemic disease recovery is possible if detected in the early stage. We proposed an automated deep learning approach from Computed Tomography (CT) scan images to detect COVID-19 positive patients by following a four-phase paradigm for COVID-19 detection: preprocess the CT scan images; remove noise from test image by using anisotropic diffusion techniques; make a different segment for the preprocessed images; and train and test COVID-19 detection using Convolutional Neural Network (CNN) models. This study employed well-known pre-trained models, including AlexNet, ResNet50, VGG16 and VGG19 to evaluate experiments. 80% of images are used to train the network in the detection process, while the remaining 20% are used to test it. The result of the experiment evaluation confirmed that the VGG19 pre-trained CNN model achieved better accuracy (98.06%). We used 4861 real-life COVID-19 CT images for experiment purposes, including 3068 positive and 1793 negative images. These images were acquired from a hospital in Sao Paulo, Brazil and two other different data sources. Our proposed method revealed very high accuracy and, therefore, can be used as an assistant to help professionals detect COVID-19 patients accurately.", "journal": "Scientific reports", "date": "2022-12-17", "authors": ["Khandaker Mohammad MohiUddin", "Samrat KumarDey", "Hafiz Md HasanBabu", "RafidMostafiz", "ShahadatUddin", "WatsharaShoombuatong", "Mohammad AliMoni"], "doi": "10.1038/s41598-022-25539-x\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2001017\n10.1002/jmv.25743\n10.1016/j.ijsu.2020.02.034\n10.1016/S1473-3099(20)30120-1\n10.3389/fpubh.2020.00154\n10.1148/radiol.2020200432\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1016/j.ejrad.2020.108961\n10.1016/S1473-3099(20)30134-1\n10.2214/AJR.20.22954\n10.1038/nature21056\n10.1016/j.patrec.2020.03.011\n10.1016/j.compmedimag.2019.101673\n10.1109/JSEN.2019.2959617\n10.1016/j.ejrad.2020.109041\n10.1371/journal.pone.0250952\n10.1038/s41467-020-18685-1\n10.1016/j.compbiomed.2020.103795\n10.1038/s41598-021-83424-5\n10.1109/TCBB.2021.3065361\n10.3390/e22050517\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.2995965\n10.1148/radiol.2020200905\n10.3390/bdcc3020027\n10.1007/s12065-020-00550-1\n10.1109/ICMA.2013.6618111\n10.1148/radiol.2020200230\n10.1148/radiol.2020200241\n10.1016/j.eng.2020.04.010\n10.1183/13993003.00775-2020\n10.1016/j.imu.2020.100405\n10.1088/1361-6560/abe838"}
{"title": "AI support for accurate and fast radiological diagnosis of COVID-19: an international multicenter, multivendor CT study.", "abstract": "Differentiation between COVID-19 and community-acquired pneumonia (CAP) in computed tomography (CT) is a task that can be performed by human radiologists and artificial intelligence (AI). The present study aims to (1) develop an AI algorithm for differentiating COVID-19 from CAP and (2) evaluate its performance. (3) Evaluate the benefit of using the AI result as assistance for radiological diagnosis and the impact on relevant parameters such as accuracy of the diagnosis, diagnostic time, and confidence.\nWe included n = 1591 multicenter, multivendor chest CT scans and divided them into AI training and validation datasets to develop an AI algorithm (n = 991 CT scans; n = 462 COVID-19, and n = 529 CAP) from three centers in China. An independent Chinese and German test dataset of n = 600 CT scans from six centers (COVID-19 / CAP; n = 300 each) was used to test the performance of eight blinded radiologists and the AI algorithm. A subtest dataset (180 CT scans; n = 90 each) was used to evaluate the radiologists' performance without and with AI assistance to quantify changes in diagnostic accuracy, reporting time, and diagnostic confidence.\nThe diagnostic accuracy of the AI algorithm in the Chinese-German test dataset was 76.5%. Without AI assistance, the eight radiologists' diagnostic accuracy was 79.1% and increased with AI assistance to 81.5%, going along with significantly shorter decision times and higher confidence scores.\nThis large multicenter study demonstrates that AI assistance in CT-based differentiation of COVID-19 and CAP increases radiological performance with higher accuracy and specificity, faster diagnostic time, and improved diagnostic confidence.\n\u2022 AI can help radiologists to get higher diagnostic accuracy, make faster decisions, and improve diagnostic confidence. \u2022 The China-German multicenter study demonstrates the advantages of a human-machine interaction using AI in clinical radiology for diagnostic differentiation between COVID-19 and CAP in CT scans.", "journal": "European radiology", "date": "2022-12-17", "authors": ["FanyangMeng", "JonathanKottlors", "RahilShahzad", "HaifengLiu", "PhilippFervers", "YinhuaJin", "MiriamRinneburger", "DouLe", "MathildaWeisthoff", "WenyunLiu", "MengzheNi", "YeSun", "LiyingAn", "XiaochenHuai", "DorottyaM\u00f3r\u00e9", "AthanasiosGiannakis", "IsabelKaltenborn", "AndreasBucher", "DavidMaintz", "LeiZhang", "FrankThiele", "MingyangLi", "MichaelPerkuhn", "HuimaoZhang", "ThorstenPersigehl"], "doi": "10.1007/s00330-022-09335-9\n10.1148/radiol.2020203173\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1038/s41568-018-0016-5\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.7150/thno.46465\n10.1007/s00330-020-07044-9\n10.1038/s41591-020-0931-3\n10.1038/s41467-020-17971-2\n10.1183/13993003.00775-2020\n10.1183/13993003.01104-2020\n10.1038/s41467-020-18685-1\n10.1016/j.media.2020.101860\n10.1038/s41746-020-00369-1\n10.1148/radiol.2015141579\n10.1186/s12931-021-01670-7\n10.1186/s40779-020-0233-6\n10.1148/radiol.2020200370\n10.1109/TPAMI.2016.2644615\n10.1148/radiol.2020200823\n10.1080/22221751.2020.1750307\n10.1148/radiol.2020201473\n10.1097/RTI.0000000000000524\n10.1016/j.ejrad.2021.110002"}
{"title": "Diagnosis of COVID-19 Disease in Chest CT-Scan Images Based on Combination of Low-Level Texture Analysis and MobileNetV2 Features.", "abstract": "Since two years ago, the COVID-19 virus has spread strongly in the world and has killed more than 6 million people directly and has affected the lives of more than 500 million people. Early diagnosis of the virus can help to break the chain of transmission and reduce the death rate. In most cases, the virus spreads in the infected person's chest. Therefore, the analysis of a chest CT scan is one of the most efficient methods for diagnosing a patient. Until now, various methods have been presented to diagnose COVID-19 disease in chest CT-scan images. Most recent studies have proposed deep learning-based methods. But handcrafted features provide acceptable results in some studies too. In this paper, an innovative approach is proposed based on the combination of low-level and deep features. First of all, local neighborhood difference patterns are performed to extract handcrafted texture features. Next, deep features are extracted using MobileNetV2. Finally, a two-level decision-making algorithm is performed to improve the detection rate especially when the proposed decisions based on the two different feature set are not the same. The proposed approach is evaluated on a collected dataset of chest CT scan images from June 1, 2021, to December 20, 2021, of 238 cases in two groups of patient and healthy in different COVID-19 variants. The results show that the combination of texture and deep features can provide better performance than using each feature set separately. Results demonstrate that the proposed approach provides higher accuracy in comparison with some state-of-the-art methods in this scope.", "journal": "Computational intelligence and neuroscience", "date": "2022-12-13", "authors": ["AzitaYazdani", "ShervanFekri-Ershad", "SaeedJelvay"], "doi": "10.1155/2022/1658615\n10.1007/s10278-021-00445-2\n10.2196/27468\n10.1001/jama.2020.3786\n10.1016/j.ejrad.2020.108961\n10.1109/jbhi.2021.3074893\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1109/rbme.2020.2987975\n10.1007/s00330-021-07715-1\n10.1148/radiol.2021203957\n10.1007/s00330-021-08409-4\n10.1155/2022/2564022\n10.1101/2020.04.13.20063941\n10.3390/s21020455\n10.1016/j.cmpb.2020.105581\n10.1007/s10140-020-01886-y\n10.1007/s10044-021-00984-y\n10.3390/app12104825\n10.1016/j.cmpb.2020.105532\n10.1016/j.ins.2020.09.041\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.3390/app11199023\n10.3390/math10142472\n10.3390/jpm12020309\n10.1109/CVPR40276.2018\n10.1007/3-540-45054-8_27\n10.1007/s13369-013-0725-8\n10.1109/tpami.2002.1017623\n10.1109/tip.2010.2042645\n10.1007/s11042-020-10321-w\n10.1007/s11042-017-4834-3\n10.11591/ijece.v11i1.pp844-850"}
{"title": "Automatic diagnosis of COVID-19 with MCA-inspired TQWT-based classification of chest X-ray images.", "abstract": "In this era of Coronavirus disease 2019 (COVID-19), an accurate method of diagnosis with less diagnosis time and cost can effectively help in controlling the disease spread with the new variants taking birth from time to time. In order to achieve this, a two-dimensional (2D) tunable Q-wavelet transform (TQWT) based on a memristive crossbar array (MCA) is introduced in this work for the decomposition of chest X-ray images of two different datasets. TQWT has resulted in promising values of peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) at the optimum values of its parameters namely quality factor (Q) of 4, and oversampling rate (r) of 3 and at a decomposition level (J) of 2. The MCA-based model is used to process decomposed images for further classification with efficient storage. These images have been further used for the classification of COVID-19 and non-COVID-19 images using ResNet50 and AlexNet convolutional neural network (CNN) models. The average accuracy values achieved for the processed chest X-ray images classification in the small and large datasets are 98.82% and 94.64%, respectively which are higher than the reported conventional methods based on different models of deep learning techniques. The average accuracy of detection of COVID-19 via the proposed method of image classification has also been achieved with less complexity, energy, power, and area consumption along with lower cost estimation as compared to CMOS-based technology.", "journal": "Computers in biology and medicine", "date": "2022-12-13", "authors": ["KumariJyoti", "SaiSushma", "SaurabhYadav", "PawanKumar", "Ram BilasPachori", "ShaibalMukherjee"], "doi": "10.1016/j.compbiomed.2022.106331\n10.1007/s13755-020-00135-3"}
{"title": "US-Net: A lightweight network for simultaneous speckle suppression and texture enhancement in ultrasound images.", "abstract": "Numerous traditional filtering approaches and deep learning-based methods have been proposed to improve the quality of ultrasound (US) image data. However, their results tend to suffer from over-smoothing and loss of texture and fine details. Moreover, they perform poorly on images with different degradation levels and mainly focus on speckle reduction, even though texture and fine detail enhancement are of crucial importance in clinical diagnosis.\nWe propose an end-to-end framework termed US-Net for simultaneous speckle suppression and texture enhancement in US images. The architecture of US-Net is inspired by U-Net, whereby a feature refinement attention block (FRAB) is introduced to enable an effective learning of multi-level and multi-contextual representative features. Specifically, FRAB aims to emphasize high-frequency image information, which helps boost the restoration and preservation of fine-grained and textural details. Furthermore, our proposed US-Net is trained essentially with real US image data, whereby real US images embedded with simulated multi-level speckle noise are used as an auxiliary training set.\nExtensive quantitative and qualitative experiments indicate that although trained with only one US image data type, our proposed US-Net is capable of restoring images acquired from different body parts and scanning settings with different degradation levels, while exhibiting favorable performance against state-of-the-art image enhancement approaches. Furthermore, utilizing our proposed US-Net as a pre-processing stage for COVID-19 diagnosis results in a gain of 3.6% in diagnostic accuracy.\nThe proposed framework can help improve the accuracy of ultrasound diagnosis.", "journal": "Computers in biology and medicine", "date": "2022-12-10", "authors": ["PatriceMonkam", "WenkaiLu", "SongbaiJin", "WenjunShan", "JingWu", "XiangZhou", "BoTang", "HuaZhao", "HongminZhang", "XinDing", "HuanChen", "LongxiangSu"], "doi": "10.1016/j.compbiomed.2022.106385"}
{"title": "[Chest imaging-based artificial intelligence in the diagnosis of coronavirus disease 2019 and prospects for future research].", "abstract": "Artificial intelligence (AI) has been applied increasingly in the medical field during the past 5 years. Within respiratory medicine, chest imaging AI is one of the relevant hotspots, commonly trained to identify pulmonary nodules/lung tumors, tuberculosis, pneumonia, interstitial lung disease, chronic obstructive pulmonary disease, pulmonary embolism and other pathologies. Due to the non-specific clinical manifestations and the low detection rate of pathogens, precise diagnosis and treatment of pneumonia remain challengeable. Since the outbreak of coronavirus disease 2019 (COVID-19), chest imaging AI has demonstrated its clinical value in accurate diagnosis and quantitative measurements of COVID-19. Moreover, an AI system can assist the clinicians to identify the high-risk COVID-19 patients who warrant close monitoring and timely intervention. However, there are still some limitations in the existing studies, such as small sample size, lack of multi-modal assessment of the AI model, and rough classification of pneumonia. Therefore, some suggestions for future research were put forward in this paper. Most of all, more attention should be paid to the collection of high-quality datasets, standardization of image annotation, technology innovation, algorithm optimization and model verification. Besides, the application of imaging AI on other types of pneumonia including viral pneumonia, bacterial pneumonia and pneumomycosis deserves further study. In conclusion, chest imaging AI is expected to play a vital role in decision-making for pneumonia in the future.\n\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\u8d8a\u6765\u8d8a\u591a\uff0c\u5f71\u50cfAI\u662f\u6700\u53d7\u5173\u6ce8\u7684\u70ed\u70b9\u4e4b\u4e00\u3002\u9274\u4e8e\u4e34\u5e8a\u8868\u73b0\u7f3a\u4e4f\u7279\u5f02\u6027\u3001\u75c5\u539f\u68c0\u6d4b\u7387\u4f4e\u7b49\u56e0\u7d20\uff0c\u80ba\u708e\u7684\u7cbe\u51c6\u8bca\u7597\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u80ba\u708e\uff08\u7b80\u79f0\u65b0\u51a0\u80ba\u708e\uff09\u75ab\u60c5\u66b4\u53d1\u4ee5\u6765\uff0c\u80f8\u90e8\u5f71\u50cfAI\u5c55\u793a\u4e86\u5176\u5728\u65b0\u51a0\u80ba\u708e\u5feb\u901f\u8bc6\u522b\u3001\u75c5\u7076\u5b9a\u91cf\u5206\u6790\u3001\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u53ca\u9884\u540e\u8bc4\u4f30\u7b49\u65b9\u9762\u7684\u4ef7\u503c\uff0c\u4f46\u4ecd\u5b58\u5728\u4e00\u4e9b\u4e0d\u8db3\uff0c\u5982\u7814\u7a76\u6837\u672c\u91cf\u5c0f\uff0c\u6a21\u578b\u7f3a\u4e4f\u591a\u6a21\u5f0f\u8bc4\u4f30\uff0c\u80ba\u708e\u5206\u7c7b\u6b20\u7cbe\u7ec6\u7b49\u3002\u672c\u6587\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5bf9\u5f71\u50cfAI\u8f85\u52a9\u80ba\u708e\u8bca\u65ad\u7684\u4eca\u540e\u7814\u7a76\u63d0\u51fa\u4e00\u4e9b\u5efa\u8bae\uff0c\u5f3a\u8c03\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u91c7\u96c6\u3001\u5f71\u50cf\u6570\u636e\u6807\u6ce8\u7684\u6807\u51c6\u5316\u3001\u6280\u672f\u521b\u65b0\u3001\u7b97\u6cd5\u4f18\u5316\u548cAI\u6a21\u578b\u7684\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u91cd\u89c6AI\u5728\u5176\u4ed6\u7c7b\u578b\u80ba\u708e\u4e2d\u7684\u7814\u7a76\uff0c\u671f\u5f85\u5f71\u50cfAI\u4e3a\u80ba\u708e\u7684\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u66f4\u591a\u53c2\u8003\u3002.", "journal": "Zhonghua jie he he hu xi za zhi = Zhonghua jiehe he huxi zazhi = Chinese journal of tuberculosis and respiratory diseases", "date": "2022-12-09", "authors": ["YLi", "S YLiu", "J PZheng"], "doi": "10.3760/cma.j.cn112147-20220519-00429"}
{"title": "Prediction of oxygen requirement in patients with COVID-19 using a pre-trained chest radiograph xAI model: efficient development of auditable risk prediction models via a fine-tuning approach.", "abstract": "Risk prediction requires comprehensive integration of clinical information and concurrent radiological findings. We present an upgraded chest radiograph (CXR) explainable artificial intelligence (xAI) model, which was trained on 241,723 well-annotated CXRs obtained prior to the onset of the COVID-19 pandemic. Mean area under the receiver operating characteristic curve (AUROC) for detection of 20 radiographic features was 0.955 (95% CI 0.938-0.955) on PA view and 0.909 (95% CI 0.890-0.925) on AP view. Coexistent and correlated radiographic findings are displayed in an interpretation table, and calibrated classifier confidence is displayed on an AI scoreboard. Retrieval of similar feature patches and comparable CXRs from a Model-Derived Atlas provides justification for model predictions. To demonstrate the feasibility of a fine-tuning approach for efficient and scalable development of xAI risk prediction models, we applied our CXR xAI model, in combination with clinical information, to predict oxygen requirement in COVID-19 patients. Prediction accuracy for high flow oxygen (HFO) and mechanical ventilation (MV) was 0.953 and 0.934 at 24\u00a0h and 0.932 and 0.836 at 72\u00a0h from the time of emergency department (ED) admission, respectively. Our CXR xAI model is auditable and captures key pathophysiological manifestations of cardiorespiratory diseases and cardiothoracic comorbidities. This model can be efficiently and broadly applied via a fine-tuning approach to provide fully automated risk and outcome predictions in various clinical scenarios in real-world practice.", "journal": "Scientific reports", "date": "2022-12-09", "authors": ["JoowonChung", "DoyunKim", "JongmunChoi", "SehyoYune", "KyungdooSong", "SeonkyoungKim", "MichelleChua", "Marc DSucci", "JohnConklin", "Maria G FigueiroLongo", "Jeanne BAckman", "MilenaPetranovic", "Michael HLev", "SynhoDo"], "doi": "10.1038/s41598-022-24721-5\n10.1109/access.2020.3034032\n10.1016/s2589-7500(21)00039-x\n10.1136/bmjresp-2021-001045\n10.1007/s00330-020-07269-8\n10.1038/s41746-021-00453-0\n10.1038/s41467-022-29437-8\n10.1016/j.clinimag.2020.11.004\n10.1186/s12890-020-01286-5\n10.1056/NEJMcp2009575\n10.1016/s2213-2600(20)30304-0\n10.1007/s42058-021-00078-y\n10.1001/jamanetworkopen.2019.1095\n10.1001/jamanetworkopen.2020.22779\n10.1148/radiol.2018180237\n10.1371/journal.pmed.1002686\n10.1007/s00330-019-06532-x\n10.1148/radiol.2020201874\n10.1148/radiol.2021204164\n10.1148/radiol.2281020126\n10.3390/diagnostics11112114\n10.1016/s2589-7500(21)00208-9\n10.1017/cem.2017.14\n10.1148/rg.266065168\n10.30953/bhty.v4.176"}
{"title": "Cov-caldas: A new COVID-19 chest X-Ray dataset from state of Caldas-Colombia.", "abstract": "The emergence of COVID-19 as a global pandemic forced researchers worldwide in various disciplines to investigate and propose efficient strategies and/or technologies to prevent COVID-19 from further spreading. One of the main challenges to be overcome is the fast and efficient detection of COVID-19 using deep learning approaches and medical images such as Chest Computed Tomography (CT) and Chest X-ray images. In order to contribute to this challenge, a new dataset was collected in collaboration with \"S.E.S Hospital Universitario de Caldas\" ( https://hospitaldecaldas.com/ ) from Colombia and organized following the Medical Imaging Data Structure (MIDS) format. The dataset contains 7,307 chest X-ray images divided into 3,077 and 4,230 COVID-19 positive and negative images. Images were subjected to a selection and anonymization process to allow the scientific community to use them freely. Finally, different convolutional neural networks were used to perform technical validation. This dataset contributes to the scientific community by tackling significant limitations regarding data quality and availability for the detection of COVID-19.", "journal": "Scientific data", "date": "2022-12-09", "authors": ["Jes\u00fas AlejandroAlzate-Grisales", "AlejandroMora-Rubio", "Harold BrayanArteaga-Arteaga", "Mario AlejandroBravo-Ortiz", "DanielArias-Garz\u00f3n", "Luis HumbertoL\u00f3pez-Murillo", "EstebanMercado-Ruiz", "Juan PabloVilla-Pulgarin", "OscarCardona-Morales", "SimonOrozco-Arias", "FelipeBuitrago-Carmona", "Maria JosePalancares-Sosa", "FernandaMart\u00ednez-Rodr\u00edguez", "Sonia HContreras-Ortiz", "Jose ManuelSaborit-Torres", "Joaquim \u00c1ngelMontell Serrano", "Mar\u00eda M\u00f3nicaRamirez-S\u00e1nchez", "Mario AlfonsoSierra-Gaber", "OscarJaramillo-Robledo", "Mariade la Iglesia-Vay\u00e1", "ReinelTabares-Soto"], "doi": "10.1038/s41597-022-01576-z\n10.1001/jama.2020.1585\n10.1109/ACCESS.2020.3028390\n10.1109/JAS.2020.1003393\n10.1001/archinternmed.2009.427.Radiation\n10.1038/s41597-020-00741-6\n10.1016/J.MEDIA.2021.102046\n10.1016/j.media.2020.101797\n10.1016/j.cell.2018.02.010\n10.1136/ADC.83.1.82\n10.6084/m9.figshare.c.5833484.v1\n10.1016/J.MLWA.2021.100138\n10.1007/s11263-015-0816-y"}
{"title": "MRI Assessment of Cerebral Blood Flow in Nonhospitalized Adults Who Self-Isolated Due to COVID-19.", "abstract": "Neurological symptoms associated with coronavirus disease 2019 (COVID-19), such as fatigue and smell/taste changes, persist beyond infection. However, little is known of brain physiology in the post-COVID-19 timeframe.\nTo determine whether adults who experienced flu-like symptoms due to COVID-19 would exhibit cerebral blood flow (CBF) alterations in the weeks/months beyond infection, relative to controls who experienced flu-like symptoms but tested negative for COVID-19.\nProspective observational.\nA total of 39 adults who previously self-isolated at home due to COVID-19 (41.9\u2009\u00b1\u200912.6\u2009years of age, 59% female, 116.5\u2009\u00b1\u200962.2\u2009days since positive diagnosis) and 11 controls who experienced flu-like symptoms but had a negative COVID-19 diagnosis (41.5\u2009\u00b1\u200913.4\u2009years of age, 55% female, 112.1\u2009\u00b1\u200959.5 since negative diagnosis).\nA 3.0\u2009T; T1-weighted magnetization-prepared rapid gradient and echo-planar turbo gradient-spin echo arterial spin labeling sequences.\nArterial spin labeling was used to estimate CBF. A self-reported questionnaire assessed symptoms, including ongoing fatigue. CBF was compared between COVID-19 and control groups and between those with (n\u00a0=\u00a011) and without self-reported ongoing fatigue (n\u00a0=\u00a028) within the COVID-19 group.\nBetween-group and within-group comparisons of CBF were performed in a voxel-wise manner, controlling for age and sex, at a family-wise error rate of 0.05.\nRelative to controls, the COVID-19 group exhibited significantly decreased CBF in subcortical regions including the thalamus, orbitofrontal cortex, and basal ganglia (maximum cluster size\u00a0=\u00a06012 voxels and maximum t-statistic\u00a0=\u00a05.21). Within the COVID-19 group, significant CBF differences in occipital and parietal regions were observed between those with and without self-reported on-going fatigue.\nThese cross-sectional data revealed regional CBF decreases in the COVID-19 group, suggesting the relevance of brain physiology in the post-COVID-19 timeframe. This research may help elucidate the heterogeneous symptoms of the post-COVID-19 condition.\n2.\nStage 3.", "journal": "Journal of magnetic resonance imaging : JMRI", "date": "2022-12-07", "authors": ["William S HKim", "XiangJi", "EugenieRoudaia", "J JeanChen", "AsafGilboa", "AllisonSekuler", "FuqiangGao", "ZhongminLin", "AravinthanJegatheesan", "MarioMasellis", "MagedGoubran", "Jennifer SRabin", "BenjaminLam", "IvyCheng", "RobertFowler", "ChrisHeyn", "Sandra EBlack", "Simon JGraham", "Bradley JMacIntosh"], "doi": "10.1002/jmri.28555"}
{"title": "Harris hawks optimization for COVID-19 diagnosis based on multi-threshold image segmentation.", "abstract": "Digital image processing techniques and algorithms have become a great tool to support medical experts in identifying, studying, diagnosing certain diseases. Image segmentation methods are of the most widely used techniques in this area simplifying image representation and analysis. During the last few decades, many approaches have been proposed for image segmentation, among which multilevel thresholding methods have shown better results than most other methods. Traditional statistical approaches such as the Otsu and the Kapur methods are the standard benchmark algorithms for automatic image thresholding. Such algorithms provide optimal results, yet they suffer from high computational costs when multilevel thresholding is required, which is considered as an optimization matter. In this work, the Harris hawks optimization technique is combined with Otsu's method to effectively reduce the required computational cost while maintaining optimal outcomes. The proposed approach is tested on a publicly available imaging datasets, including chest images with clinical and genomic correlates, and represents a rural COVID-19-positive (COVID-19-AR) population. According to various performance measures, the proposed approach can achieve a substantial decrease in the computational cost and the time to converge while maintaining a level of quality highly competitive with the Otsu method for the same threshold values.", "journal": "Neural computing & applications", "date": "2022-12-07", "authors": ["Mohammad HashemRyalat", "OsamaDorgham", "SaraTedmori", "ZainabAl-Rahamneh", "NijadAl-Najdawi", "SeyedaliMirjalili"], "doi": "10.1007/s00521-022-08078-4\n10.1109/ACCESS.2020.2990893\n10.1007/s11042-019-7515-6\n10.1166/jmihi.2016.1600\n10.1016/j.jksuci.2018.04.007\n10.1016/j.eswa.2019.113113\n10.1016/j.eswa.2019.112999\n10.1016/j.eswa.2019.113018\n10.1016/j.cmpb.2009.07.006\n10.1038/s41597-020-00741-6\n10.1016/j.imu.2020.100375\n10.1007/s11042-020-10147-6\n10.1109/TMI.2020.3002417\n10.1016/j.eswa.2019.01.075\n10.1016/j.eswa.2019.01.047\n10.1007/s11548-012-0783-5\n10.1117/1.JMI.6.2.020901\n10.1093/jcde/qwab082\n10.1093/jamia/ocy098\n10.1016/j.future.2019.02.028\n10.1109/JBHI.2017.2725903\n10.1016/j.compbiomed.2013.10.028\n10.1016/0734-189X(85)90125-2\n10.1016/j.compbiomed.2012.12.004\n10.1109/TPAMI.2021.3132068\n10.1109/ACCESS.2019.2891673\n10.1016/j.compbiomed.2012.09.003\n10.1016/j.media.2010.02.004\n10.1016/j.advengsoft.2017.07.002\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.advengsoft.2013.12.007\n10.1016/j.asoc.2019.04.002\n10.1007/s10044-017-0653-4\n10.1007/s00500-017-2794-1\n10.1109/TSMC.1979.4310076\n10.1007/s40998-019-00251-1\n10.3390/diagnostics9010029\n10.1118/1.2948349\n10.1016/j.advengsoft.2017.01.004\n10.3348/kjr.2008.9.1.1\n10.1007/s11045-018-0603-3\n10.1118/1.3633941\n10.1016/j.eswa.2019.07.037\n10.1016/j.asoc.2019.105522\n10.1016/j.compbiomed.2018.10.033"}
{"title": "A novel intelligent radiomic analysis of perfusion SPECT/CT images to optimize pulmonary embolism diagnosis in COVID-19 patients.", "abstract": "COVID-19 infection, especially in cases with pneumonia, is associated with a high rate of pulmonary embolism (PE). In patients with contraindications for CT pulmonary angiography (CTPA) or non-diagnostic CTPA, perfusion single-photon emission computed tomography/computed tomography (Q-SPECT/CT) is a diagnostic alternative. The goal of this study is to develop a radiomic diagnostic system to detect PE based only on the analysis of Q-SPECT/CT scans.\nThis radiomic diagnostic system is based on a local analysis of Q-SPECT/CT volumes that includes both CT and Q-SPECT values for each volume point. We present a combined approach that uses radiomic features extracted from each scan as input into a fully connected classification neural network that optimizes a weighted cross-entropy loss trained to discriminate between three different types of image patterns (pixel sample level): healthy lungs (control group), PE and pneumonia. Four types of models using different configuration of parameters were tested.\nThe proposed radiomic diagnostic system was trained on 20 patients (4,927 sets of samples of three types of image patterns) and validated in a group of 39 patients (4,410 sets of samples of three types of image patterns). In the training group, COVID-19 infection corresponded to 45% of the cases and 51.28% in the test group. In the test group, the best model for determining different types of image patterns with PE presented a sensitivity, specificity, positive predictive value and negative predictive value of 75.1%, 98.2%, 88.9% and 95.4%, respectively. The best model for detecting pneumonia presented a sensitivity, specificity, positive predictive value and negative predictive value of 94.1%, 93.6%, 85.2% and 97.6%, respectively. The area under the curve (AUC) was 0.92 for PE and 0.91 for pneumonia. When the results obtained at the pixel sample level are aggregated into regions of interest, the sensitivity of the PE increases to 85%, and all metrics improve for pneumonia.\nThis radiomic diagnostic system was able to identify the different lung imaging patterns and is a first step toward a comprehensive intelligent radiomic system to optimize the diagnosis of PE by Q-SPECT/CT.\nArtificial intelligence applied to Q-SPECT/CT is a diagnostic option in patients with contraindications to CTPA or a non-diagnostic test in times of COVID-19.", "journal": "EJNMMI physics", "date": "2022-12-06", "authors": ["SoniaBaeza", "DeboraGil", "IgnasiGarcia-Oliv\u00e9", "MaiteSalcedo-Pujantell", "JordiDeport\u00f3s", "CarlesSanchez", "GuillermoTorres", "GloriaMoragas", "AntoniRosell"], "doi": "10.1186/s40658-022-00510-x\n10.1016/j.ejrad.2020.109336\n10.1055/s-0040-1710019\n10.1007/s00259-019-04450-0\n10.1111/1754-9485.12471\n10.2967/jnumed.120.245571\n10.1016/j.ajpath.2020.08.009\n10.1016/j.autrev.2020.102537\n10.1016/S2352-3026(20)30145-9\n10.1056/NEJMoa2015432\n10.1007/s11239-021-02394-7\n10.1111/jth.14888\n10.1016/j.thromres.2020.04.013\n10.1148/radiol.2020201955\n10.1148/radiol.2020203557\n10.1007/s11547-020-01328-2\n10.1148/radiol.2020201544\n10.1080/00325481.2021.1920723\n10.1007/s00259-020-05043-y\n10.1007/s00259-020-04837-4\n10.1007/s00259-020-04851-6\n10.1097/MNM.0000000000001246\n10.1378/chest.13-2090\n10.1159/000439543\n10.1148/radiol.2020200905\n10.1016/S2589-7500(20)30199-0\n10.1148/radiol.2020201491\n10.1148/radiol.2020204226\n10.1016/S1361-8415(01)80004-9\n10.1158/0008-5472.CAN-17-0339\n10.1148/radiol.2020202944\n10.1007/s12553-021-00520-2\n10.1007/s00146-020-00978-0\n10.1016/j.inffus.2021.04.008\n10.1155/2017/3762651"}
{"title": "Application of Machine Learning and Deep Learning Techniques for COVID-19 Screening Using Radiological Imaging: A Comprehensive Review.", "abstract": "Lung, being one of the most important organs in human body, is often affected by various SARS diseases, among which COVID-19 has been found to be the most fatal disease in recent times. In fact, SARS-COVID 19 led to pandemic that spreads fast among the community causing respiratory problems. Under such situation, radiological imaging-based screening [mostly chest X-ray and computer tomography (CT) modalities] has been performed for rapid screening of the disease as it is a non-invasive approach. Due to scarcity of physician/chest specialist/expert doctors, technology-enabled disease screening techniques have been developed by several researchers with the help of artificial intelligence and machine learning (AI/ML). It can be remarkably observed that the researchers have introduced several AI/ML/DL (deep learning) algorithms for computer-assisted detection of COVID-19 using chest X-ray and CT images. In this paper, a comprehensive review has been conducted to summarize the works related to applications of AI/ML/DL for diagnostic prediction of COVID-19, mainly using X-ray and CT images. Following the PRISMA guidelines, total 265 articles have been selected out of 1715 published articles till the third quarter of 2021. Furthermore, this review summarizes and compares varieties of ML/DL techniques, various datasets, and their results using X-ray and CT imaging. A detailed discussion has been made on the novelty of the published works, along with advantages and limitations.", "journal": "SN computer science", "date": "2022-12-06", "authors": ["AsifuzzamanLasker", "Sk MdObaidullah", "ChandanChakraborty", "KaushikRoy"], "doi": "10.1007/s42979-022-01464-8\n10.1109/TMI.2020.3001810\n10.1016/j.asoc.2020.106859\n10.1007/s13246-020-00934-8\n10.3348/kjr.2020.0536\n10.1056/nejmoa2001017\n10.3389/frai.2020.00065\n10.1016/j.cmpb.2020.105581\n10.1109/ICOASE51841.2020.9436542\n10.1016/j.compbiomed.2021.104868\n10.1016/j.sysarc.2020.101830\n10.3390/healthcare8010046\n10.1007/s42979-020-00383-w\n10.1007/s11042-021-10907-y\n10.1038/s41746-020-00372-6\n10.3389/fimmu.2020.01441\n10.3390/jpm11010028\n10.1007/s11042-020-10340-7\n10.26355/eurrev_202008_22510\n10.1016/j.acra.2020.09.004\n10.1186/s12911-020-01316-6\n10.1038/s41467-020-18684-2\n10.1016/j.ijsu.2010.02.007\n10.1016/j.compbiomed.2021.104605\n10.1016/j.patrec.2019.11.013\n10.1016/j.patcog.2012.10.005\n10.1016/j.eswa.2021.115152\n10.3390/e23020204\n10.1016/j.media.2020.101836\n10.1007/s10278-019-00227-x\n10.1016/j.compbiomed.2020.104037\n10.1002/mp.14676\n10.1016/j.cmpbup.2021.100007\n10.1109/TIP.2021.3058783\n10.1080/01431160600746456\n10.1109/ACCESS.2021.3054484\n10.1016/j.compbiomed.2021.104319\n10.1007/s13246-020-00865-4\n10.1016/j.chaos.2020.109944\n10.1007/s10489-020-01902-1\n10.3233/XST-200720\n10.1016/j.ijmedinf.2020.104284\n10.31661/jbpe.v0i0.2008-1153\n10.1371/journal.pone.0242535\n10.1097/RLI.0000000000000748\n10.14358/PERS.80.2.000\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200230\n10.2214/AJR.20.22954\n10.1016/j.media.2016.10.004\n10.1016/j.chaos.2020.110153\n10.1007/s00330-021-07715-1\n10.1016/j.compbiomed.2021.104588\n10.1038/s41598-021-83237-6\n10.1002/mp.14609\n10.1007/s00259-020-05075-4\n10.1007/s00259-020-04953-1\n10.34171/mjiri.34.174\n10.1007/s12539-021-00420-z\n10.1007/s42399-020-00643-z\n10.7759/cureus.10378\n10.1097/MD.0000000000023167\n10.1148/rg.2020200149\n10.1056/nejmp2000929\n10.1148/radiol.2462070712\n10.1148/radiol.2020200370\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020202791\n10.2214/AJR.20.23513\n10.1016/j.ultrasmedbio.2020.07.018\n10.1016/j.irbm.2020.07.001\n10.1007/s10489-020-01826-w\n10.16984/saufenbilder.459659\n10.1007/s12539-020-00393-5\n10.1109/ACCESS.2020.3016780\n10.1007/s42979-021-00605-9\n10.1007/s11042-021-10714-5\n10.1016/j.compbiomed.2021.104304\n10.1371/journal.pone.0235187\n10.3390/v12070769\n10.1186/s12938-020-00831-x\n10.1002/ima.22564\n10.1007/s00138-020-01101-5\n10.1016/j.cmpb.2020.105532\n10.1016/j.imu.2021.100621\n10.1038/s41598-021-88807-2\n10.1007/s11042-020-09894-3\n10.1016/j.bspc.2021.102622\n10.1016/j.chemolab.2020.104054\n10.1186/s12879-021-05839-9\n10.1186/s12938-020-00809-9\n10.1016/j.imu.2020.100505\n10.1007/s12539-020-00403-6\n10.1088/1742-6596/1933/1/012040\n10.1007/s00330-021-07957-z\n10.1007/s10489-020-01943-6\n10.1038/s41598-020-76282-0\n10.1038/s41598-020-80261-w\n10.32604/cmc.2021.016264\n10.1007/s11548-020-02305-w\n10.1109/ACCESS.2021.3061058\n10.1016/j.bbe.2020.08.008\n10.3233/SHTI210223\n10.1007/s11548-020-02286-w\n10.1049/iet-ipr.2020.1127\n10.1007/s11517-020-02299-2\n10.1007/s10489-021-02199-4\n10.1016/j.compbiomed.2020.103869\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110495\n10.1038/s41598-020-76550-z\n10.1016/j.imu.2021.100620\n10.7717/peerj-cs.551\n10.1109/TII.2021.3057683\n10.2196/27468\n10.1007/s10140-020-01886-y\n10.47611/jsrhs.v9i2.1246\n10.1016/j.media.2020.101913\n10.1016/j.patrec.2020.09.010\n10.1016/j.neucom.2021.03.034\n10.1109/TNNLS.2021.3070467\n10.1109/EIConCIT50028.2021.9431887\n10.1016/j.iot.2021.100377\n10.2196/19569\n10.1109/ACCESS.2021.3083516\n10.9781/ijimai.2020.04.003\n10.1007/s10489-020-01867-1\n10.29194/njes.23040408\n10.7717/PEERJ-CS.345\n10.1145/3431804\n10.3389/fmed.2021.629134\n10.1148/radiol.2020201491\n10.32604/cmc.2021.014956\n10.1177/2472630320958376\n10.1016/j.irbm.2021.01.004\n10.1109/ICCC51575.2020.9344870\n10.1007/s13246-020-00888-x\n10.1007/s00330-020-07225-6\n10.1109/TMI.2020.2994908\n10.32604/cmc.2021.013228\n10.7717/PEERJ-CS.364\n10.1016/j.inffus.2020.10.004\n10.1016/j.ejrad.2020.109041\n10.2196/25535\n10.1016/j.bspc.2020.102365\n10.3233/XST-200715\n10.1109/TNNLS.2021.3054746\n10.1016/j.matpr.2021.01.820\n10.1038/s41598-020-80936-4\n10.1109/TMI.2020.3000314\n10.31763/sitech.v1i2.202\n10.1016/j.compbiomed.2020.104181\n10.1007/s12559-020-09775-9\n10.1109/TMI.2020.2996645\n10.1038/s41551-021-00704-1\n10.1007/s10489-020-02019-1\n10.1007/s00146-020-00978-0\n10.1136/bmj.m1808\n10.1016/j.bspc.2021.102490\n10.1016/j.inffus.2020.11.005\n10.7717/peerj.10309\n10.7150/ijbs.58855\n10.1109/ACCESS.2020.3003810\n10.1007/s11263-019-01228-7"}
{"title": "PulDi-COVID: Chronic obstructive pulmonary (lung) diseases with COVID-19 classification using ensemble deep convolutional neural network from chest X-ray images to minimize severity and mortality rates.", "abstract": "In the current COVID-19 outbreak, efficient testing of COVID-19 individuals has proven vital to limiting and arresting the disease's accelerated spread globally. It has been observed that the severity and mortality ratio of COVID-19 affected patients is at greater risk because of chronic pulmonary diseases. This study looks at radiographic examinations exploiting chest X-ray images (CXI), which have become one of the utmost feasible assessment approaches for pulmonary disorders, including COVID-19. Deep Learning(DL) remains an excellent image classification method and framework; research has been conducted to predict pulmonary diseases with COVID-19 instances by developing DL classifiers with nine class CXI. However, a few claim to have strong prediction results; because of noisy and small data, their recommended DL strategies may suffer from significant deviation and generality failures.\nTherefore, a unique CNN model(PulDi-COVID) for detecting nine diseases (atelectasis, bacterial-pneumonia, cardiomegaly, covid19, effusion, infiltration, no-finding, pneumothorax, viral-Pneumonia) using CXI has been proposed using the SSE algorithm. Several transfer-learning models: VGG16, ResNet50, VGG19, DenseNet201, MobileNetV2, NASNetMobile, ResNet152V2, DenseNet169 are trained on CXI of chronic lung diseases and COVID-19 instances. Given that the proposed thirteen SSE ensemble models solved DL's constraints by making predictions with different classifiers rather than a single, we present PulDi-COVID, an ensemble DL model that combines DL with ensemble learning. The PulDi-COVID framework is created by incorporating various snapshots of DL\u00a0models, which have spearheaded chronic lung diseases with COVID-19 cases identification process with a deep neural network produced CXI by applying a suggested SSE method.\u00a0That is familiar with the idea of various DL perceptions on different classes.\nPulDi-COVID findings were compared to thirteen existing studies for nine-class classification using COVID-19. Test results reveal that PulDi-COVID offers impressive outcomes for chronic diseases with COVID-19 identification with a 99.70% accuracy, 98.68% precision, 98.67% recall, 98.67% F1 score, lowest 12 CXIs zero-one loss, 99.24% AUC-ROC score, and lowest 1.33% error rate. Overall test results are superior to the existing Convolutional Neural Network(CNN). To the\u00a0best of our knowledge, the observed results for nine-class classification are significantly superior to the state-of-the-art approaches employed for COVID-19 detection. Furthermore, the CXI\u00a0that we used\u00a0to assess our algorithm is one of the larger datasets for COVID detection with pulmonary diseases.\nThe empirical findings of our suggested approach PulDi-COVID\u00a0show that it outperforms previously developed methods. The suggested SSE method with PulDi-COVID can effectively fulfill the COVID-19 speedy detection needs with different lung diseases for physicians to minimize patient severity and mortality.", "journal": "Biomedical signal processing and control", "date": "2022-12-06", "authors": ["Yogesh HBhosale", "K SridharPatnaik"], "doi": "10.1016/j.bspc.2022.104445\n10.3389/fmed.2021.588013\n10.1109/TII.2021.3057683"}
{"title": "Rapid diagnosis of Covid-19 infections by a progressively growing GAN and CNN optimisation.", "abstract": "Covid-19 infections are spreading around the globe since December 2019. Several diagnostic methods were developed based on biological investigations and the success of each method depends on the accuracy of identifying Covid infections. However, access to diagnostic tools can be limited, depending on geographic region and the diagnosis duration plays an important role in treating Covid-19. Since the virus causes pneumonia, its presence can also be detected using medical imaging by Radiologists. Hospitals with X-ray capabilities are widely distributed all over the world, so a method for diagnosing Covid-19 from chest X-rays would present itself. Studies have shown promising results in automatically detecting Covid-19 from medical images using supervised Artificial neural network (ANN) algorithms. The major drawback of supervised learning algorithms is that they require huge amounts of data to train. Also, the radiology equipment is not computationally efficient for deep neural networks. Therefore, we aim to develop a Generative Adversarial Network (GAN) based image augmentation to optimize the performance of custom, light, Convolutional networks used for the classification of Chest X-rays (CXR).\nA Progressively Growing Generative Adversarial Network (PGGAN) is used to generate synthetic and augmented data to supplement the dataset. We propose two novel CNN architectures to perform the Multi-class classification of Covid-19, healthy and pneumonia affected Chest X-rays. Comparisons have been drawn to the state of the art models and transfer learning methods to evaluate the superiority of the networks. All the models are trained using enhanced and augmented X-ray images and are compared based on classification metrics.\nThe proposed models had extremely high classification metrics with proposed Architectures having test accuracy of 98.78% and 99.2% respectively while having 40% lesser training parameters than their state of the art counterpart.\nIn the present study, a method based on artificial intelligence is proposed, leading to a rapid diagnostic tool for Covid infections based on Generative Adversarial Network (GAN) and Convolutional Neural Networks (CNN). The benefit will be a high accuracy of detection with up to 99% hit rate, a rapid diagnosis, and an accessible Covid identification method by chest X-ray images.", "journal": "Computer methods and programs in biomedicine", "date": "2022-12-05", "authors": ["RutwikGulakala", "BerndMarkert", "MarcusStoffel"], "doi": "10.1016/j.cmpb.2022.107262"}
{"title": "COVID-DSNet: A novel deep convolutional neural network for detection of coronavirus (SARS-CoV-2) cases from CT and Chest X-Ray images.", "abstract": "COVID-19 (SARS-CoV-2), which causes acute respiratory syndrome, is a contagious and deadly disease that has devastating effects on society and human life. COVID-19 can cause serious complications, especially in patients with pre-existing chronic health problems such as diabetes, hypertension, lung cancer, weakened immune systems, and the elderly. The most critical step in the fight against COVID-19 is the rapid diagnosis of infected patients. Computed Tomography (CT), chest X-ray (CXR), and RT-PCR diagnostic kits are frequently used to diagnose the disease. However, due to difficulties such as the inadequacy of RT-PCR test kits and false negative (FN) results in the early stages of the disease, the time-consuming examination of medical images obtained from CT and CXR imaging techniques by specialists/doctors, and the increasing workload on specialists, it is challenging to detect COVID-19. Therefore, researchers have suggested searching for new methods in COVID- 19 detection. In analysis studies with CT and CXR radiography images, it was determined that COVID-19-infected patients experienced abnormalities related to COVID-19. The anomalies observed here are the primary motivation for artificial intelligence researchers to develop COVID-19 detection applications with deep convolutional neural networks. Here, convolutional neural network-based deep learning algorithms from artificial intelligence technologies with high discrimination capabilities can be considered as an alternative approach in the disease detection process. This study proposes a deep convolutional neural network, COVID-DSNet, to diagnose typical pneumonia (bacterial, viral) and COVID-19 diseases from CT, CXR, hybrid CT\u00a0+\u00a0CXR images. In the multi-classification study with the CT dataset, 97.60\u00a0% accuracy and 97.60\u00a0% sensitivity values were obtained from the COVID-DSNet model, and 100\u00a0%, 96.30\u00a0%, and 96.58\u00a0% sensitivity values were obtained in the detection of typical, common pneumonia and COVID-19, respectively. The proposed model is an economical, practical deep learning network that data scientists can benefit from and develop. Although it is not a definitive solution in disease diagnosis, it may help experts as it produces successful results in detecting pneumonia and COVID-19.", "journal": "Artificial intelligence in medicine", "date": "2022-12-04", "authors": ["Hatice CatalReis", "VeyselTurk"], "doi": "10.1016/j.artmed.2022.102427\n10.1016/j.asoc.2020.106859\n10.1016/j.virs.2022.09.003\n10.1016/j.mtbio.2022.100265\n10.1111/1348-0421.12945\n10.1002/jmv.27132\n10.1038/s41579-021-00573-0\n10.1001/jama.2022.14711\n10.1038/s41565-022-01177-2\n10.1038/s41580-021-00418-x\n10.3390/ijms23031716\n10.1038/s41580-021-00432-z\n10.1016/j.media.2021.102096\n10.3390/cancers13020162\n10.3389/fimmu.2021.660632\n10.1007/s12559-020-09787-5\n10.3389/fphar.2021.664349\n10.3390/cells10030587\n10.1007/s11033-021-06358-1\n10.1001/jama.2021.13084\n10.1016/j.arbres.2021.06.003\n10.1038/s41598-021-96755-0\n10.1016/S0140-6736(22)00009-5\n10.1016/j.cmpb.2019.105162\n10.1148/ryct.2021200564\n10.1016/j.ejrad.2020.109147\n10.1186/s12890-021-01450-5\n10.1007/s12559-020-09779-5\n10.1093/bib/bbab412\n10.3390/v14020322\n10.1007/s00521-020-05410-8\n10.1016/j.nupar.2022.01.003\n10.1016/j.biochi.2022.01.015\n10.1109/ACCESS.2020.3010287\n10.1016/j.ijid.2020.10.069\n10.3390/biomedicines10020242\n10.1016/bs.acr.2020.10.001\n10.1084/jem.20202489\n10.17305/bjbms.2021.6340\n10.1007/s11154-021-09707-4\n10.1016/j.jacbts.2021.10.011\n10.1001/jamacardio.2020.3557\n10.1016/j.chom.2020.05.008\n10.1126/science.369.6500.125\n10.1093/cid/ciaa644\n10.1016/j.asoc.2022.109207\n10.1038/s41392-022-00884-5\n10.1038/s41579-020-00462-y\n10.1136/bmj.n597\n10.3390/diagnostics12020467\n10.1007/s10311-021-01369-7\n10.1056/NEJMc2119236\n10.1001/jama.2021.24315\n10.1056/NEJMoa2108891\n10.1126/science.abn7591\n10.1016/j.eclinm.2021.100861\n10.1016/j.compbiomed.2021.104742\n10.1016/j.bspc.2021.103415\n10.3390/diagnostics11111972\n10.1007/s00521-020-05636-6\n10.1007/s42979-021-00695-5\n10.1038/s41598-021-84630-x\n10.1038/s41598-021-03889-2\n10.3390/s22010372\n10.1038/s41598-022-06264-x\n10.1016/j.compbiomed.2022.105810\n10.1016/j.chaos.2020.110245\n10.1016/j.bspc.2022.103977\n10.1007/s10489-020-01943-6\n10.1007/s00500-021-06579-3\n10.1007/s11042-020-10165-4\n10.1007/s12559-021-09955-1\n10.3390/s22031211\n10.1007/s10522-021-09946-7\n10.1007/s11042-021-11319-8\n10.1038/s41598-020-76550-z\n10.48550/arXiv.2003.11597\n10.1016/j.patcog.2021.108255\n10.1007/s10489-020-01829-7\n10.1148/radiol.2020200905\n10.1016/j.patrec.2021.08.018\n10.1155/2022/6185013\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.1162/neco.1997.9.8.1735\n10.3390/s21030832\n10.1016/j.chaos.2020.110153\n10.1016/j.compbiomed.2021.104319\n10.17632/rscbjbr9sj.2\n10.1609/aaai.v31i1.11231\n10.1109/CVPR.2016.308\n10.48550/arXiv.1704.04861\n10.1109/CVPR.2018.00907\n10.48550/arXiv.1412.6980\n10.1016/j.eswa.2020.113909\n10.1016/j.compbiomed.2021.105134\n10.1016/j.compbiomed.2022.105213\n10.1016/j.knosys.2021.106849\n10.1038/s41586-022-04569-5\n10.1007/s42452-019-1903-4"}
{"title": "Classification and visual explanation for COVID-19 pneumonia from CT images using triple learning.", "abstract": "This study presents a novel framework for classifying and visualizing pneumonia induced by COVID-19 from CT images. Although many image classification methods using deep learning have been proposed, in the case of medical image fields, standard classification methods are unable to be used in some cases because the medical images that belong to the same category vary depending on the progression of the symptoms and the size of the inflamed area. In addition, it is essential that the models used be transparent and explainable, allowing health care providers to trust the models and avoid mistakes. In this study, we propose a classification method using contrastive learning and an attention mechanism. Contrastive learning is able to close the distance for images of the same category and generate a better feature space for classification. An attention mechanism is able to emphasize an important area in the image and visualize the location related to classification. Through experiments conducted on two-types of classification using a three-fold cross validation, we confirmed that the classification accuracy was significantly improved; in addition, a detailed visual explanation was achieved comparison with conventional methods.", "journal": "Scientific reports", "date": "2022-12-03", "authors": ["SotaKato", "MasahiroOda", "KensakuMori", "AkinobuShimizu", "YoshitoOtake", "MasahiroHashimoto", "ToshiakiAkashi", "KazuhiroHotta"], "doi": "10.1038/s41598-022-24936-6\n10.1148/radiol.2020200905\n10.1016/j.ejrad.2020.109041\n10.1109/ACCESS.2020.3005510\n10.1016/j.asoc.2020.106885\n10.1109/TCBB.2021.3065361\n10.1016/j.compbiomed.2020.104037\n10.3390/diagnostics11050893\n10.1016/j.patcog.2021.107826\n10.1016/j.patcog.2021.107848\n10.1016/j.media.2021.102105\n10.1007/s11263-021-01559-4\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2995965"}
{"title": "A lightweight network for COVID-19 detection in X-ray images.", "abstract": "The Novel Coronavirus 2019 (COVID-19) is a global pandemic which has a devastating impact. Due to its quick transmission, a prominent challenge in confronting this pandemic is the rapid diagnosis. Currently, the commonly-used diagnosis is the specific molecular tests aided with the medical imaging modalities such as chest X-ray (CXR). However, with the large demand, the diagnoses of CXR are time-consuming and laborious. Deep learning is promising for automatically diagnosing COVID-19 to ease the burden on medical systems. At present, the most applied neural networks are large, which hardly satisfy the rapid yet inexpensive requirements of COVID-19 detection. To reduce huge computation and memory demands, in this paper, we focus on implementing lightweight networks for COVID-19 detection in CXR. Concretely, we first augment data based on clinical visual features of CXR from expertise. Then, according to the fact that all the input data are CXR, we design a targeted four-layer network with either 11\u00a0\u00d7\u00a011 or 3\u00a0\u00d7\u00a03 kernels to recognize regional features and detail features. A pruning criterion based on the weights importance is also proposed to further prune the network. Experiments on a public COVID-19 dataset validate the effectiveness and efficiency of the proposed method.", "journal": "Methods (San Diego, Calif.)", "date": "2022-12-03", "authors": ["YongShi", "AndaTang", "YangXiao", "LingfengNiu"], "doi": "10.1016/j.ymeth.2022.11.004"}
{"title": "Radiomorphological signs and clinical severity of SARS-CoV-2 lineage B.1.1.7.", "abstract": "We aimed to assess the differences in the severity and chest-CT radiomorphological signs of SARS-CoV-2 B.1.1.7 and non-B.1.1.7 variants.\nWe collected clinical data of consecutive patients with laboratory-confirmed COVID-19 and chest-CT imaging who were admitted to the Emergency Department between September 1- November 13, 2020 (non-B.1.1.7 cases) and March 1-March 18, 2021 (B.1.1.7 cases). We also examined the differences in the severity and radiomorphological features associated with COVID-19 pneumonia. Total pneumonia burden (%), mean attenuation of ground-glass opacities and consolidation were quantified using deep-learning research software.\nThe final population comprised 500 B.1.1.7 and 500 non-B.1.1.7 cases. Patients with B.1.1.7 infection were younger (58.5 \u00b1 15.6 vs 64.8 \u00b1 17.3; \nDespite B.1.1.7 patients were younger and had fewer comorbidities, they experienced more severe disease than non-B.1.1.7 patients, however, the risk of death was the same between the two groups.\nOur study provides data on deep-learning based quantitative lung lesion burden and clinical outcomes of patients infected by B.1.1.7 VOC. Our findings might serve as a model for later investigations, as new variants are emerging across the globe.", "journal": "BJR open", "date": "2022-12-02", "authors": ["JuditSimon", "KajetanGrodecki", "SebastianCadet", "AdityaKillekar", "PiotrSlomka", "Samuel JamesZara", "EmeseZsarn\u00f3czay", "ChiaraNardocci", "NorbertNagy", "KatalinKrist\u00f3f", "BarnaV\u00e1s\u00e1rhelyi", "VeronikaM\u00fcller", "B\u00e9laMerkely", "DaminiDey", "P\u00e1lMaurovich-Horvat"], "doi": "10.1259/bjro.20220016\n10.1126/science.abg3055\n10.1016/S1473-3099(21)00170-5\n10.1136/bmj.n579\n10.2807/1560-7917.ES.2021.26.11.2100256\n10.1007/s00330-020-06748-2\n10.1148/radiol.2020200843\n10.2214/AJR.20.22954\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1016/S1473-3099(20)30483-7\n10.1148/ryct.2020200389\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200432\n10.1136/bmj.m1464\n10.1016/S1473-3099(20)30086-4\n10.1556/1647.2020.00002\n10.1038/s41379-020-0536-x\n10.1016/S0140-6736(20)30566-3\n10.1038/s41598-020-76550-z\n10.1007/s00330-021-07715-1\n10.1183/13993003.00775-2020\n10.1016/j.compbiomed.2020.103795"}
{"title": "3D CT-Inclusive Deep-Learning Model to Predict Mortality, ICU Admittance, and Intubation in COVID-19 Patients.", "abstract": "Chest CT is a useful initial exam in patients with coronavirus disease 2019 (COVID-19) for assessing lung damage. AI-powered predictive models could be useful to better allocate resources in the midst of the pandemic. Our aim was to build a deep-learning (DL) model for COVID-19 outcome prediction inclusive of 3D chest CT images acquired at hospital admission. This retrospective multicentric study included 1051 patients (mean age 69, SD\u2009=\u200915) who presented to the emergency department of three different institutions between 20th March 2020 and 20th January 2021 with COVID-19 confirmed by real-time reverse transcriptase polymerase chain reaction (RT-PCR). Chest CT at hospital admission were evaluated by a 3D residual neural network algorithm. Training, internal validation, and external validation groups included 608, 153, and 290 patients, respectively. Images, clinical, and laboratory data were fed into different customizations of a dense neural network to choose the best performing architecture for the prediction of mortality, intubation, and intensive care unit (ICU) admission. The AI model tested on CT and clinical features displayed accuracy, sensitivity, specificity, and ROC-AUC, respectively, of 91.7%, 90.5%, 92.4%, and 95% for the prediction of patient's mortality; 91.3%, 91.5%, 89.8%, and 95% for intubation; and 89.6%, 90.2%, 86.5%, and 94% for ICU admission (internal validation) in the testing cohort. The performance was lower in the validation cohort for mortality (71.7%, 55.6%, 74.8%, 72%), intubation (72.6%, 74.7%, 45.7%, 64%), and ICU admission (74.7%, 77%, 46%, 70%) prediction. The addition of the available laboratory data led to an increase in sensitivity for patient's mortality (66%) and specificity for intubation and ICU admission (50%, 52%, respectively), while the other metrics maintained similar performance results. We present a deep-learning model to predict mortality, ICU admittance, and intubation in COVID-19 patients. KEY POINTS: \u2022 3D CT-based deep learning model predicted the internal validation set with high accuracy, sensibility and specificity (>\u200990%) mortality, ICU admittance, and intubation in COVID-19 patients. \u2022 The model slightly increased prediction results when laboratory data were added to the analysis, despite data imbalance. However, the model accuracy dropped when CT images were not considered in the analysis, implying an important role of CT in predicting outcomes.", "journal": "Journal of digital imaging", "date": "2022-12-01", "authors": ["AlbertoDi Napoli", "EmanuelaTagliente", "LucaPasquini", "EnricaCipriano", "FilomenaPietrantonio", "PiermariaOrtis", "SimonaCurti", "AlessandroBoellis", "TeseoStefanini", "AntonioBernardini", "ChiaraAngeletti", "Sofia ChiatamoneRanieri", "PaolaFranchi", "Ioan PaulVoicu", "CarloCapotondi", "AntonioNapolitano"], "doi": "10.1007/s10278-022-00734-4\n10.1148/radiol.2015151169\n10.1111/jon.12903\n10.3390/jpm11040290\n10.3389/fonc.2021.601425\n10.1001/jama.2020.5046\n10.3390/JPM11090893\n10.1148/radiol.2020200432\n10.1016/j.ejrad.2020.108961\n10.1016/j.jacr.2020.03.006\n10.1016/j.diii.2020.06.001\n10.1038/s41467-020-18786-x\n10.1007/s00330-020-07033-y\n10.1148/radiol.2020200905\n10.1038/s41598-021-96755-0\n10.1038/s41598-022-07890-1\n10.1038/s41598-022-13298-8\n10.1038/s41598-022-05532-0\n10.5281/zenodo.4295521\n10.1016/j.artmed.2020.101987\n10.3389/fpubh.2017.00307\n10.1186/S13244-020-00887-2\n10.1148/radiol.2020191145\n10.1016/J.CMPB.2021.106288\n10.1016/j.chest.2020.04.003\n10.2214/AJR.20.22976\n10.1007/s00330-020-06817-6\n10.3348/kjr.2020.0293\n10.1038/s41598-020-76141-y\n10.1007/s11548-020-02299-5\n10.1038/s41551-020-00633-5\n10.1007/s00705-021-05012-2\n10.7554/eLife.60519\n10.1148/radiol.2020201433\n10.1007/s00330-020-07013-2\n10.1002/jmv.27301"}
{"title": "Automatic detection of Covid-19 from chest X-ray and lung computed tomography images using deep neural networks and transfer learning.", "abstract": "The world has been undergoing the most ever unprecedented circumstances caused by the coronavirus pandemic, which is having a devastating global effect in different aspects of life. Since there are not effective antiviral treatments for Covid-19 yet, it is crucial to early detect and monitor the progression of the disease, thereby helping to reduce mortality. While different measures are being used to combat the virus, medical imaging techniques have been examined to support doctors in diagnosing the disease. In this paper, we present a practical solution for the detection of Covid-19 from chest X-ray (CXR) and lung computed tomography (LCT) images, exploiting cutting-edge Machine Learning techniques. As the main classification engine, we make use of EfficientNet and MixNet, two recently developed families of deep neural networks. Furthermore, to make the training more effective and efficient, we apply three transfer learning algorithms. The ultimate aim is to build a reliable expert system to detect Covid-19 from different sources of images, making it be a multi-purpose AI diagnosing system. We validated our proposed approach using four real-world datasets. The first two are CXR datasets consist of 15,000 and 17,905 images, respectively. The other two are LCT datasets with 2,482 and 411,528 images, respectively. The five-fold cross-validation methodology was used to evaluate the approach, where the dataset is split into five parts, and accordingly the evaluation is conducted in five rounds. By each evaluation, four parts are combined to form the training data, and the remaining one is used for testing. We obtained an encouraging prediction performance for all the considered datasets. In all the configurations, the obtained accuracy is always larger than 95.0%. Compared to various existing studies, our approach yields a substantial performance gain. Moreover, such an improvement is statistically significant.", "journal": "Applied soft computing", "date": "2022-12-01", "authors": ["Linh TDuong", "Phuong TNguyen", "LudovicoIovino", "MicheleFlammini"], "doi": "10.1016/j.asoc.2022.109851\n10.1109/TITS.2021.3053373\n10.1016/j.eswa.2021.115519\n10.1016/j.eswa.2017.12.020\n10.1038/s41591-020-0931-3\n10.1007/s11263-015-0816-y\n10.1016/j.compag.2018.02.016\n10.1186/s40537-016-0043-6\n10.1007/11564096_40\n10.3390/rs9090907\n10.1016/j.compag.2020.105326\n10.1101/2020.04.24.20078584\n10.1016/j.cell.2020.04.045\n10.1016/j.asoc.2020.106691\n10.1016/j.tube.2022.102234\n10.1016/j.asoc.2021.107323\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.06.08.20125963\n10.1109/CVPR.2016.90\n10.1007/s11042-021-10783-6\n10.1038/s41598-021-99015-3\n10.1007/978-1-4612-4380-9_16\n10.1101/2020.06.08.20121541\n10.36227/techrxiv.12328061\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020201491\n10.1148/radiol.2020201491\n10.1101/2020.08.13.20173997"}
{"title": "Mentoring within the medical radiation sciences - Establishing a national program.", "abstract": "The aim of this study was to compare the accuracy and performance of 12 pre-trained deep learning models for classifying covid-19 and normal chest X-ray images from Kaggle.\na desktop computer with an Intel CPU i9-10900 2.80GHz and NVIDIA GPU GeForce RTX2070 SUPER, Anaconda3 software with 12 pre-trained models including VGG16, VGG19, DenseNet121, DenseNet169, DenseNet201, RestNet50V2, RestNet101V2, RestNet152V2, InceptionRestnetV2, InceptionV3, XceptionV1 and MobileNetV2, covid-19 and normal chest X-ray from Kaggle website.\nthe images were divided into three sets of train, test, and validation sets using a ratio of 70:20:10, respectively. The performance was recorded for each pre-train model with hyperparameters of epoch, batch size, and learning rate as 16, 16 and 0.0001 respectively. The prediction results of each model were recorded and compared.\nfrom the results of all 12 pre-trained deep learning model, five models that have highest validation accuracy were DenseNet169, DenseNet201, InceptionV3, DenseNet121 and InceptionRestNetV2, respectively.\nThe top-5 highest accuracy models for classifying the COVID-19 were DenseNet169, DenseNet201, InceptionV3, DenseNet121 and InceptionRestnetV2 with accuracies of 95.4%, 95.07%, 94.73%, 94.51% and 93.61% respectively.", "journal": "Journal of medical imaging and radiation sciences", "date": "2022-11-29", "authors": ["AllieTonks", "FranziskaJerjen"], "doi": "10.1016/j.jmir.2022.10.190"}
{"title": "The optimal use of colon capsule endoscopes in clinical practice.", "abstract": "Colon capsule endoscopy (CCE) has been available for nearly two decades but has grappled with being an equal diagnostic alternative to optical colonoscopy (OC). Due to the COVID-19 pandemic, CCE has gained more foothold in clinical practice. In this cutting-edge review, we aim to present the existing knowledge on the pros and cons of CCE and discuss whether the modality is ready for a larger roll-out in clinical settings. We have included clinical trials and reviews with the most significant impact on the current position of CCE in clinical practice and discuss the challenges that persist and how they could be addressed to make CCE a more sustainable imaging modality with an adenoma detection rate equal to OC and a low re-investigation rate by a proper preselection of suitable populations. CCE is embedded with a very low risk of severe complications and can be performed in the patient's home as a pain-free procedure. The diagnostic accuracy is found to be equal to OC. However, a significant drawback is low completion rates eliciting a high re-investigation rate. Furthermore, the bowel preparation before CCE is extensive due to the high demand for clean mucosa. CCE is currently not suitable for large-scale implementation in clinical practice mainly due to high re-investigation rates. By a better preselection before CCE and the implantation of artificial intelligence for picture and video analysis, CCE could be the alternative to OC needed to move away from in-hospital services and relieve long-waiting lists for OC.", "journal": "Therapeutic advances in chronic disease", "date": "2022-11-29", "authors": ["ThomasBj\u00f8rsum-Meyer", "AnastasiosKoulaouzidis", "GunnarBaatrup"], "doi": "10.1177/20406223221137501"}
{"title": "Optimal Ensemble learning model for COVID-19 detection using chest X-ray images.", "abstract": "COVID-19 pandemic is the main outbreak in the world, which has shown a bad impact on people's lives in more than 150 countries. The major steps in fighting COVID-19 are identifying the affected patients as early as possible and locating them with special care. Images from radiology and radiography are among the most effective tools for determining a patient's ailment. Recent studies have shown detailed abnormalities of affected patients with COVID-19 in the chest radiograms. The purpose of this work is to present a COVID-19 detection system with three\u00a0key steps: \"(i) preprocessing, (ii) Feature extraction, (iii) Classification.\" Originally, the input image is given to the preprocessing step as its input, extracting the deep features and texture features from the preprocessed image. Particularly, it extracts the deep features by inceptionv3. Then, the features like proposed Local Vector Patterns (LVP) and Local Binary Pattern (LBP) are extracted from the preprocessed image. Moreover, the extracted features are subjected to the proposed ensemble model based classification phase, including Support Vector Machine (SVM), Convolutional Neural Network (CNN), Optimized Neural Network (NN), and Random Forest (RF). A novel Self Adaptive Kill Herd Optimization (SAKHO) approach is used to properly tune the weight of NN to improve classification accuracy and precision. The performance of the proposed method is then compared to the performance of the conventional approaches using a variety of metrics, including recall, FNR, MCC, FDR, Thread score, FPR, precision, FOR, accuracy, specificity, NPV, FMS, and sensitivity, accordingly.", "journal": "Biomedical signal processing and control", "date": "2022-11-29", "authors": ["SBalasubramaniam", "KSatheesh Kumar"], "doi": "10.1016/j.bspc.2022.104392\n10.1109/TBDATA.2020.3035935\n10.1109/ACCESS.2020.3033762\n10.3233/HIS-120161\n10.1504/IJCSE.2013.053087\n10.1007/s40747-020-00216-6\n10.1109/ICIP.2014.7025047"}
{"title": "Diagnostic performance of attenuated total reflection Fourier-transform infrared spectroscopy for detecting COVID-19 from routine nasopharyngeal swab samples.", "abstract": "Attenuated total reflection Fourier-transform infrared (ATR-FTIR) spectroscopy coupled with machine learning-based partial least squares discriminant analysis (PLS-DA) was applied to study if severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) could be detected from nasopharyngeal swab samples originally collected for polymerase chain reaction (PCR) analysis. Our retrospective study included 558 positive and 558 negative samples collected from Northern Finland. Overall, we found moderate diagnostic performance for ATR-FTIR when PCR analysis was used as the gold standard: the average area under the receiver operating characteristics curve (AUROC) was 0.67-0.68 (min. 0.65, max. 0.69) with 20, 10 and 5\u00a0k-fold cross validations. Mean accuracy, sensitivity and specificity was 0.62-0.63 (min. 0.60, max. 0.65), 0.61 (min. 0.58, max. 0.65) and 0.64 (min. 0.59, max. 0.67) with 20, 10 and 5\u00a0k-fold cross validations. As a conclusion, our study with relatively large sample set clearly indicate that measured ATR-FTIR spectrum contains specific information for SARS-CoV-2 infection (P\u2009<\u20090.001 for AUROC in label permutation test). However, the diagnostic performance of ATR-FTIR remained only moderate, potentially due to low concentration of viral particles in the transport medium. Further studies are needed before ATR-FTIR can be recommended for fast screening of SARS-CoV-2 from nasopharyngeal swab samples.", "journal": "Scientific reports", "date": "2022-11-28", "authors": ["Helin\u00e4Heino", "LassiRieppo", "TuijaM\u00e4nnist\u00f6", "Mikko JSillanp\u00e4\u00e4", "VesaM\u00e4ntynen", "SimoSaarakkala"], "doi": "10.1038/s41598-022-24751-z\n10.1016/j.jiph.2020.03.019\n10.1038/s41579-020-00459-7\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.3233/BSI-200203\n10.1038/nprot.2014.110\n10.1155/2020/4343590\n10.1039/C7RA03361C\n10.1016/j.clispe.2020.100001\n10.1021/acs.analchem.0c04049\n10.1021/acs.analchem.1c00596\n10.1021/acs.analchem.0c04608\n10.1002/anie.202104453\n10.1038/s41598-021-93511-2\n10.1021/pr101067u\n10.1186/s12859-019-3310-7\n10.1016/j.aca.2015.02.012\n10.1039/C8AN00599K\n10.1016/j.patrec.2005.10.010\n10.1038/scientificamerican1000-82\n10.1002/jmv.27204"}
{"title": "Deep Learning-Based Computer-Aided Diagnosis (CAD): Applications for Medical Image Datasets.", "abstract": "Computer-aided diagnosis (CAD) has proved to be an effective and accurate method for diagnostic prediction over the years. This article focuses on the development of an automated CAD system with the intent to perform diagnosis as accurately as possible. Deep learning methods have been able to produce impressive results on medical image datasets. This study employs deep learning methods in conjunction with meta-heuristic algorithms and supervised machine-learning algorithms to perform an accurate diagnosis. Pre-trained convolutional neural networks (CNNs) or auto-encoder are used for feature extraction, whereas feature selection is performed using an ant colony optimization (ACO) algorithm. Ant colony optimization helps to search for the best optimal features while reducing the amount of data. Lastly, diagnosis prediction (classification) is achieved using learnable classifiers. The novel framework for the extraction and selection of features is based on deep learning, auto-encoder, and ACO. The performance of the proposed approach is evaluated using two medical image datasets: chest X-ray (CXR) and magnetic resonance imaging (MRI) for the prediction of the existence of COVID-19 and brain tumors. Accuracy is used as the main measure to compare the performance of the proposed approach with existing state-of-the-art methods. The proposed system achieves an average accuracy of 99.61% and 99.18%, outperforming all other methods in diagnosing the presence of COVID-19 and brain tumors, respectively. Based on the achieved results, it can be claimed that physicians or radiologists can confidently utilize the proposed approach for diagnosing COVID-19 patients and patients with specific brain tumors.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-11-27", "authors": ["Yezi AliKadhim", "Muhammad UmerKhan", "AlokMishra"], "doi": "10.3390/s22228999\n10.3389/fmed.2020.00027\n10.1097/00004424-196601000-00032\n10.1118/1.3013555\n10.3390/s22218326\n10.1148/83.6.1029\n10.1353/pbm.1992.0011\n10.1016/j.compmedimag.2007.02.002\n10.1002/mp.13764\n10.1007/s10489-020-02002-w\n10.1164/ajrccm.153.1.8542102\n10.12928/telkomnika.v15i4.3163\n10.1056/NEJM199105303242205\n10.1016/j.eswa.2020.113274\n10.1016/j.ijmedinf.2019.06.017\n10.1016/j.compbiomed.2019.103345\n10.1007/s10278-013-9600-0\n10.1080/21681163.2016.1138324\n10.22146/ijeis.34713\n10.1016/j.jocs.2018.12.003\n10.3390/e24070869\n10.3390/life12111709\n10.3390/s22155880\n10.1016/j.conbuildmat.2017.09.110\n10.1016/j.compeleceng.2018.07.042\n10.1186/s40537-019-0276-2\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103795\n10.1186/s41256-020-00135-6\n10.1101/2020.08.31.20175828\n10.1371/journal.pone.0157112\n10.1109/ACCESS.2019.2912200\n10.1109/MGRS.2018.2853555\n10.1109/MCI.2006.329691\n10.1016/j.tcs.2005.05.020\n10.1016/j.engappai.2014.03.007\n10.1016/j.eswa.2006.04.010\n10.1016/j.eswa.2015.07.007\n10.1016/j.eswa.2014.04.019\n10.1371/journal.pone.0140381\n10.1016/j.bspc.2019.101678\n10.3390/ijerph17124204\n10.20944/preprints202003.0300.v1\n10.1016/j.chaos.2020.110210\n10.1016/j.patrec.2020.09.010\n10.1016/j.cor.2021.105359\n10.1016/j.ins.2017.12.047\n10.1023/B:ANOR.0000039523.95673.33\n10.1016/j.imu.2020.100330\n10.1155/2021/6621540\n10.3390/sym11020157\n10.1109/ACCESS.2021.3076756\n10.1109/ACCESS.2021.3051723"}
{"title": "Dual_Pachi: Attention-based dual path framework with intermediate second order-pooling for Covid-19 detection from chest X-ray images.", "abstract": "Numerous machine learning and image processing algorithms, most recently deep learning, allow the recognition and classification of COVID-19 disease in medical images. However, feature extraction, or the semantic gap between low-level visual information collected by imaging modalities and high-level semantics, is the fundamental shortcoming of these techniques. On the other hand, several techniques focused on the first-order feature extraction of the chest X-Ray thus making the employed models less accurate and robust. This study presents Dual_Pachi: Attention Based Dual Path Framework with Intermediate Second Order-Pooling for more accurate and robust Chest X-ray feature extraction for Covid-19 detection. Dual_Pachi consists of 4 main building Blocks; Block one converts the received chest X-Ray image to CIE LAB coordinates (L & AB channels which are separated at the first three layers of a modified Inception V3 Architecture.). Block two further exploit the global features extracted from block one via a global second-order pooling while block three focuses on the low-level visual information and the high-level semantics of Chest X-ray image features using a multi-head self-attention and an MLP Layer without sacrificing performance. Finally, the fourth block is the classification block where classification is done using fully connected layers and SoftMax activation. Dual_Pachi is designed and trained in an end-to-end manner. According to the results, Dual_Pachi outperforms traditional deep learning models and other state-of-the-art approaches described in the literature with an accuracy of 0.96656 (Data_A) and 0.97867 (Data_B) for the Dual_Pachi approach and an accuracy of 0.95987 (Data_A) and 0.968 (Data_B) for the Dual_Pachi without attention block model. A Grad-CAM-based visualization is also built to highlight where the applied attention mechanism is concentrated.", "journal": "Computers in biology and medicine", "date": "2022-11-25", "authors": ["Chiagoziem CUkwuoma", "ZhiguangQin", "Victor KAgbesi", "Bernard MCobbinah", "Sophyani BYussif", "Hassan SAbubakar", "Bona DLemessa"], "doi": "10.1016/j.compbiomed.2022.106324\n10.1142/S0218339020500096\n10.7150/ijbs.45053\n10.1016/j.jare.2020.03.005\n10.1016/j.cpcardiol.2020.100618\n10.1016/j.diii.2020.03.014\n10.1016/S1473-3099(20)30190-0\n10.1002/jmv.25721\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1016/j.jcct.2011.07.001\n10.1109/prai53619.2021.9551094\n10.1007/s10278-017-9983-4\n10.3390/s20113243\n10.1016/j.compbiomed.2020.103795\n10.1007/s10278-019-00227-x\n10.1007/s13246-020-00888-x\n10.1093/cid/ciaa1383\n10.1016/j.compbiomed.2020.103869\n10.1016/j.bspc.2021.102696\n10.1016/j.eswa.2021.114576\n10.3390/diagnostics12051152\n10.1016/j.sciaf.2022.e01151\n10.1016/j.patcog.2020.107613\n10.1016/j.compbiomed.2020.103792\n10.1016/j.pdpdt.2021.102473\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.09.010\n10.1007/s00330-021-07715-1\n10.1007/s10044-021-00984-y\n10.1016/j.media.2020.101794\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1109/SSCI47803.2020.9308571\n10.1016/j.chaos.2020.109944\n10.33889/IJMEMS.2020.5.4.052\n10.1109/ICCC51575.2020.9344870\n10.1007/s42600-021-00151-6\n10.1016/j.chaos.2020.110495\n10.1016/j.compbiomed.2020.104181\n10.1007/s10916-021-01745-4\n10.1016/j.compbiomed.2021.104816\n10.1016/j.knosys.2022.108207\n10.3233/idt-210002\n10.1007/s12065-021-00679-7\n10.1016/j.eswa.2020.114054\n10.1007/s00354-021-00152-0\n10.34133/2019/9237136\n10.1109/ACCESS.2020.3010287\n10.3390/covid1010034\n10.1016/j.cmpb.2020.105581\n10.1109/JBHI.2021.3058293\n10.1016/j.asoc.2022.108867\n10.1109/JBHI.2021.3074893\n10.3390/s22031211\n10.3390/ijerph182111086\n10.1007/s00521-019-04332-4"}
{"title": "Automated Lung-Related Pneumonia and COVID-19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images.", "abstract": "According to research, classifiers and detectors are less accurate when images are blurry, have low contrast, or have other flaws which raise questions about the machine learning model's ability to recognize items effectively. The chest X-ray image has proven to be the preferred image modality for medical imaging as it contains more information about a patient. Its interpretation is quite difficult, nevertheless. The goal of this research is to construct a reliable deep-learning model capable of producing high classification accuracy on chest x-ray images for lung diseases. To enable a thorough study of the chest X-ray image, the suggested framework first derived richer features using an ensemble technique, then a global second-order pooling is applied to further derive higher global features of the images. Furthermore, the images are then separated into patches and position embedding before analyzing the patches individually via a vision transformer approach. The proposed model yielded 96.01% sensitivity, 96.20% precision, and 98.00% accuracy for the COVID-19 Radiography Dataset while achieving 97.84% accuracy, 96.76% sensitivity and 96.80% precision, for the Covid-ChestX-ray-15k dataset. The experimental findings reveal that the presented models outperform traditional deep learning models and other state-of-the-art approaches provided in the literature.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2022-11-25", "authors": ["Chiagoziem CUkwuoma", "ZhiguangQin", "Md Belal BinHeyat", "FaijanAkhtar", "AblaSmahi", "Jehoiada KJackson", "SyedFurqan Qadri", "Abdullah YMuaad", "Happy NMonday", "Grace UNneji"], "doi": "10.3390/bioengineering9110709\n10.3390/bioengineering9070305\n10.1007/s42399-020-00527-2\n10.1007/s00415-020-10067-3\n10.1007/s42399-020-00383-0\n10.1155/2022/9210947\n10.1109/ACCESS.2022.3194152\n10.1155/2022/5641727\n10.1016/j.jare.2022.08.021\n10.1186/s12890-020-01286-5\n10.1007/s11042-019-08394-3\n10.3390/bioengineering9040172\n10.1155/2022/5718501\n10.1109/ACCESS.2019.2928020\n10.3390/bios12060427\n10.1155/2022/3599246\n10.2174/1871527319666201110124954\n10.3390/app10217410\n10.2174/1389450121666201027125828\n10.1145/3465055\n10.32604/cmc.2021.014134\n10.3390/diagnostics10090649\n10.1155/2019/4180949\n10.1007/s10916-021-01745-4\n10.1016/j.compeleceng.2019.08.004\n10.1016/j.cmpb.2019.06.023\n10.3390/app10020559\n10.1016/j.measurement.2020.108046\n10.1016/j.bspc.2020.102365\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2021.110713\n10.1016/j.compbiomed.2021.104375\n10.1016/j.chaos.2021.110749\n10.1016/j.patcog.2021.108255\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1007/s11063-022-10834-5\n10.1109/ACCESS.2020.3010287\n10.3390/covid1010034\n10.1155/2022/9475162\n10.3389/fnins.2021.754058\n10.1109/ACCESS.2022.3212120\n10.1155/2022/3408501\n10.3390/app12031344\n10.31083/j.jin2101020\n10.1016/b978-0-323-99031-8.00012-0\n10.3390/diagnostics12112815\n10.1016/j.cmpb.2020.105581\n10.1109/JBHI.2021.3058293\n10.1016/j.asoc.2022.108867\n10.1109/JBHI.2021.3074893\n10.3390/s22031211\n10.18280/ts.380337\n10.18201/ijisae.2020466310\n10.3233/XST-211005\n10.1016/j.cell.2018.02.010\n10.1007/s12559-020-09787-5\n10.1016/j.chaos.2020.109944"}
{"title": "Portable, Automated and Deep-Learning-Enabled Microscopy for Smartphone-Tethered Optical Platform Towards Remote Homecare Diagnostics: A Review.", "abstract": "Globally new pandemic diseases induce urgent demands for portable diagnostic systems to prevent and control infectious diseases. Smartphone-based portable diagnostic devices are significantly efficient tools to user-friendly connect personalized health conditions and collect valuable optical information for rapid diagnosis and biomedical research through at-home screening. Deep learning algorithms for portable microscopes also help to enhance diagnostic accuracy by reducing the imaging resolution gap between benchtop and portable microscopes. This review highlighted recent progress and continued efforts in a smartphone-tethered optical platform through portable, automated, and deep-learning-enabled microscopy for personalized diagnostics and remote monitoring. In detail, the optical platforms through smartphone-based microscopes and lens-free holographic microscopy are introduced, and deep learning-based portable microscopic imaging is explained to improve the image resolution and accuracy of diagnostics. The challenges and prospects of portable optical systems with microfluidic channels and a compact microscope to screen COVID-19 in the current pandemic are also discussed. It has been believed that this review offers a novel guide for rapid diagnosis, biomedical imaging, and digital healthcare with low cost and portability.", "journal": "Small methods", "date": "2022-11-25", "authors": ["KisooKim", "Won GuLee"], "doi": "10.1002/smtd.202200979"}
{"title": "A systematic review: Chest radiography images (X-ray images) analysis and COVID-19 categorization diagnosis using artificial intelligence techniques.", "abstract": "COVID-19 pandemic created a turmoil across nations due to Severe Acute Respiratory Syndrome Corona virus-1(SARS - Co-V-2). The severity of COVID-19 symptoms is starting from cold, breathing problems, issues in respiratory system which may also lead to life threatening situations. This disease is widely contaminating and transmitted from man-to-man. The contamination is spreading when the human organs like eyes, nose, and mouth get in contact with contaminated fluids. This virus can be screened through performing a nasopharyngeal swab test which is time consuming. So the physicians are preferring the fast detection methods like chest radiography images and CT scans. At times some confusion in finding out the accurate disorder from chest radiography images can happen. To overcome this issue this study reviews several deep learning and machine learning procedures to be implemented in X-ray images of chest. This also helps the professionals to find out the other types of malfunctions happening in the chest other than COVID-19 also. This review can act as a guidance to the doctors and radiologists in identifying the COVID-19 and other types of viruses causing illness in the human anatomy and can provide aid soon.", "journal": "Network (Bristol, England)", "date": "2022-11-25", "authors": ["SaravananSuba", "MMuthulakshmi"], "doi": "10.1080/0954898X.2022.2147231"}
{"title": "Deep progressive learning achieves whole-body low-dose ", "abstract": "To validate a total-body PET-guided deep progressive learning reconstruction method (DPR) for low-dose \nList-mode data from the retrospective study (n\u2009=\u200926) were rebinned into short-duration scans and reconstructed with DPR. The standard uptake value (SUV) and tumor-to-liver ratio (TLR) in lesions and coefficient of variation (COV) in the liver in the DPR images were compared to the reference (OSEM images with full-duration data). In the prospective study, another 41 patients were injected with 1/3 of the activity based on the retrospective results. The DPR images (DPR_1/3(p)) were generated and compared with the reference (OSEM images with extended acquisition time). The SUV and COV were evaluated in three selected organs: liver, blood pool and muscle. Quantitative analyses were performed with lesion SUV and TLR, furthermore on small lesions (\u2264\u200910\u00a0mm in diameter). Additionally, a 5-point Likert scale visual analysis was performed on the following perspectives: contrast, noise and diagnostic confidence.\nIn the retrospective study, the DPR with one-third duration can maintain the image quality as the reference. In the prospective study, good agreement among the SUVs was observed in all selected organs. The quantitative results showed that there was no significant difference in COV between the DPR_1/3(p) group and the reference, while the visual analysis showed no significant differences in image contrast, noise and diagnostic confidence. The lesion SUVs and TLRs in the DPR_1/3(p) group were significantly enhanced compared with the reference, even for small lesions.\nThe proposed DPR method can reduce the administered activity of ", "journal": "EJNMMI physics", "date": "2022-11-23", "authors": ["TaisongWang", "WenliQiao", "YingWang", "JingyiWang", "YangLv", "YunDong", "ZhengQian", "YanXing", "JinhuaZhao"], "doi": "10.1186/s40658-022-00508-5\n10.1007/s00259-014-2961-x\n10.1016/S0377-1237(09)80099-3\n10.2967/jnumed.107.047787\n10.1007/s12350-016-0522-3\n10.1016/j.nuclcard.2007.04.006\n10.1136/jnnp.2003.028175\n10.2967/jnumed.117.200790\n10.1097/RLU.0000000000003075\n10.1007/s00259-020-05167-1\n10.1186/s13550-020-00695-1\n10.1007/s00259-021-05197-3.10.1007/s00259-021-05197-3\n10.1007/s00259-021-05478-x\n10.1109/TRPMS.2020.3014786\n10.1088/1361-6560/abfb17\n10.1007/s00247-006-0191-5\n10.1007/s00247-009-1404-5\n10.1259/bjr/01948454\n10.1007/s00259-020-05091-4\n10.1007/s00259-021-05304-4\n10.1007/s00259-021-05462-5\n10.1007/s00259-021-05537-3\n10.2967/jnumed.121.262038\n10.1007/s00259-021-05592-w\n10.1259/bjr.20201356\n10.1186/s13550-019-0536-3\n10.1007/s00259-017-3893-z\n10.1186/s13550-019-0565-y"}
{"title": "COVID-19 classification using chest X-ray images based on fusion-assisted deep Bayesian optimization and Grad-CAM visualization.", "abstract": "The COVID-19 virus's rapid global spread has caused millions of illnesses and deaths. As a result, it has disastrous consequences for people's lives, public health, and the global economy. Clinical studies have revealed a link between the severity of COVID-19 cases and the amount of virus present in infected people's lungs. Imaging techniques such as computed tomography (CT) and chest x-rays can detect COVID-19 (CXR). Manual inspection of these images is a difficult process, so computerized techniques are widely used. Deep convolutional neural networks (DCNNs) are a type of machine learning that is frequently used in computer vision applications, particularly in medical imaging, to detect and classify infected regions. These techniques can assist medical personnel in the detection of patients with COVID-19. In this article, a Bayesian optimized DCNN and explainable AI-based framework is proposed for the classification of COVID-19 from the chest X-ray images. The proposed method starts with a multi-filter contrast enhancement technique that increases the visibility of the infected part. Two pre-trained deep models, namely, EfficientNet-B0 and MobileNet-V2, are fine-tuned according to the target classes and then trained by employing Bayesian optimization (BO). Through BO, hyperparameters have been selected instead of static initialization. Features are extracted from the trained model and fused using a slicing-based serial fusion approach. The fused features are classified using machine learning classifiers for the final classification. Moreover, visualization is performed using a Grad-CAM that highlights the infected part in the image. Three publically available COVID-19 datasets are used for the experimental process to obtain improved accuracies of 98.8, 97.9, and 99.4%, respectively.", "journal": "Frontiers in public health", "date": "2022-11-22", "authors": ["AmeerHamza", "MuhammadAttique Khan", "Shui-HuaWang", "MajedAlhaisoni", "MeshalAlharbi", "Hany SHussein", "HammamAlshazly", "Ye JinKim", "JaehyukCha"], "doi": "10.3389/fpubh.2022.1046296\n10.3390/s21020455\n10.32604/cmc.2022.020140\n10.1111/exsy.12776\n10.1016/j.compbiomed.2022.105233\n10.1155/2022/7672196\n10.3389/fcomp.2020.00005\n10.1016/j.bbi.2020.04.081\n10.1148/radiol.2020200230\n10.7717/peerj-cs.655\n10.1109/TMI.2020.3040950\n10.1109/TMI.2020.2993291\n10.1109/TMI.2016.2528162\n10.1155/2022/7377502\n10.1016/j.media.2017.07.005\n10.1038/nature14539\n10.1148/radiol.2017162326\n10.1007/s11263-015-0816-y\n10.48550/arXiv.1602.07360\n10.3389/fpubh.2022.948205\n10.1155/2021/2560388\n10.1016/j.compbiomed.2022.105213\n10.3389/fmed.2020.00427\n10.1371/journal.pone.0242535\n10.1007/s13755-020-00119-3\n10.3390/info11090419\n10.1155/2020/8828855\n10.1155/2022/4254631\n10.1007/s00530-021-00826-1\n10.1007/s42600-020-00120-5\n10.1155/2022/1307944\n10.3390/jpm12020309\n10.3390/s21217286\n10.1016/j.ecoinf.2020.101182\n10.1007/s12530-020-09345-2\n10.1016/j.chaos.2020.110511\n10.1007/s00521-022-07052-4\n10.1016/j.asoc.2020.106580\n10.1016/j.compbiomed.2022.105244"}
{"title": "Efficient-ECGNet framework for COVID-19 classification and correlation prediction with the cardio disease through electrocardiogram medical imaging.", "abstract": "In the last 2 years, we have witnessed multiple waves of coronavirus that affected millions of people around the globe. The proper cure for COVID-19 has not been diagnosed as vaccinated people also got infected with this disease. Precise and timely detection of COVID-19 can save human lives and protect them from complicated treatment procedures. Researchers have employed several medical imaging modalities like CT-Scan and X-ray for COVID-19 detection, however, little concentration is invested in the ECG imaging analysis. ECGs are quickly available image modality in comparison to CT-Scan and X-ray, therefore, we use them for diagnosing COVID-19. Efficient and effective detection of COVID-19 from the ECG signal is a complex and time-taking task, as researchers usually convert them into numeric values before applying any method which ultimately increases the computational burden. In this work, we tried to overcome these challenges by directly employing the ECG images in a deep-learning (DL)-based approach. More specifically, we introduce an Efficient-ECGNet method that presents an improved version of the EfficientNetV2-B4 model with additional dense layers and is capable of accurately classifying the ECG images into healthy, COVID-19, myocardial infarction (MI), abnormal heartbeats (AHB), and patients with Previous History of Myocardial Infarction (PMI) classes. Moreover, we introduce a module to measure the similarity of COVID-19-affected ECG images with the rest of the diseases. To the best of our knowledge, this is the first effort to approximate the correlation of COVID-19 patients with those having any previous or current history of cardio or respiratory disease. Further, we generate the heatmaps to demonstrate the accurate key-points computation ability of our method. We have performed extensive experimentation on a publicly available dataset to show the robustness of the proposed approach and confirmed that the Efficient-ECGNet framework is reliable to classify the ECG-based COVID-19 samples.", "journal": "Frontiers in medicine", "date": "2022-11-22", "authors": ["MarriamNawaz", "TahiraNazir", "AliJaved", "Khalid MahmoodMalik", "Abdul Khader JilaniSaudagar", "Muhammad BadruddinKhan", "Mozaherul HoqueAbul Hasanat", "AbdullahAlTameem", "MohammedAlKhathami"], "doi": "10.3389/fmed.2022.1005920\n10.1016/j.ejim.2020.06.015\n10.1007/s15010-020-01401-y\n10.1016/j.jinf.2020.02.016\n10.1007/s13755-021-00169-1\n10.7717/peerj-cs.386\n10.1016/j.compeleceng.2020.106960\n10.1109/ICAI52203.2021.9445258\n10.1016/j.asoc.2020.106691\n10.1016/j.asoc.2021.107323\n10.1016/j.asoc.2021.107160\n10.1155/2021/9619079\n10.1155/2022/1575303\n10.1016/j.compbiomed.2021.104575\n10.1016/j.chaos.2020.110190\n10.1016/j.bspc.2021.102588\n10.1016/j.irbm.2021.01.004\n10.1002/jemt.23578\n10.1016/j.imu.2020.100360\n10.1007/s10489-020-01826-w\n10.1007/s00521-021-05910-1\n10.1007/s10489-020-01902-1\n10.1007/s10489-020-01943-6\n10.1007/s11356-020-10133-3\n10.21203/rs.3.rs-646890/v1\n10.1007/s00521-020-05410-8\n10.1186/s12911-021-01521-x\n10.1109/JIOT.2021.3051080\n10.1016/j.dib.2021.106762\n10.1109/CVPR.2015.7298594\n10.1016/j.inpa.2020.04.004\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.1109/CVPR.2018.00474\n10.1109/CVPR.2018.00745\n10.1109/LSP.2016.2573042\n10.1109/TPAMI.2005.165\n10.1049/cit2.12101\n10.1007/s13369-021-06182-6\n10.1007/s42979-020-0114-9\n10.1109/ICCASIT50869.2020.9368658\n10.1145/3341095"}
{"title": "Developing medical imaging AI for emerging infectious diseases.", "abstract": "Advances in artificial intelligence (AI) and computer vision hold great promise for assisting medical staff, optimizing healthcare workflow, and improving patient outcomes. The COVID-19 pandemic, which caused unprecedented stress on healthcare systems around the world, presented what seems to be a perfect opportunity for AI to demonstrate its usefulness. However, of the several hundred medical imaging AI models developed for COVID-19, very few were fit for deployment in real-world settings, and some were potentially harmful. This review aims to examine the strengths and weaknesses of prior studies and provide recommendations for different stages of building useful AI models for medical imaging, among them: needfinding, dataset curation, model development and evaluation, and post-deployment considerations. In addition, this review summarizes the lessons learned to inform the scientific community about ways to create useful medical imaging AI\u00a0in a future pandemic.\nVery few of the COVID-19 ML models were fit for deployment in real-world settings. In this Comment, Huang et al. discuss the main steps required to develop clinically useful models in the context of an emerging infectious disease.", "journal": "Nature communications", "date": "2022-11-19", "authors": ["Shih-ChengHuang", "Akshay SChaudhari", "Curtis PLanglotz", "NigamShah", "SerenaYeung", "Matthew PLungren"], "doi": "10.1038/s41467-022-34234-4\n10.1038/s42256-021-00307-0\n10.2214/AJR.21.26717\n10.1136/bmj.m1328\n10.1016/j.patter.2021.100269\n10.2196/19786\n10.1038/s41591-020-0931-3\n10.1007/s00330-021-07937-3\n10.1007/s00330-020-06829-2\n10.1109/JBHI.2021.3069169\n10.1148/ryai.2020200079\n10.1148/radiol.2020202723\n10.3348/kjr.2020.0485\n10.7150/ijms.48432\n10.1148/radiol.2020201365\n10.1001/jama.2019.10306\n10.1016/j.media.2021.102225\n10.1038/s42256-021-00338-7\n10.1016/j.media.2021.102046\n10.1148/radiol.2020204214\n10.1001/amajethics.2019.167\n10.1126/science.aax2342\n10.1136/bmj.i6\n10.1016/j.ejim.2010.12.012\n10.1016/j.jbi.2021.103826\n10.1259/bjr.73.874.11271897\n10.1016/j.jacr.2007.02.003\n10.1371/journal.pone.0250602\n10.1038/s41598-020-78888-w\n10.1038/s41746-020-00341-z\n10.1038/s41746-022-00613-w\n10.1148/radiol.2020192536\n10.1093/jamia/ocy017\n10.1038/s42256-019-0048-x\n10.1016/j.jacr.2019.05.036\n10.1016/j.acra.2020.12.026\n10.1016/j.jacr.2021.08.022\n10.1056/NEJMc2104626"}
{"title": "Blockchain and homomorphic encryption based privacy-preserving model aggregation for medical images.", "abstract": "Medical healthcare centers are envisioned as a promising paradigm to handle the massive volume of data for COVID-19 patients using artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and training models within a single organization. This practice can be considered a weakness as it leads to several privacy and security concerns related to raw data communication. To overcome this weakness and secure raw data communication, we propose a blockchain-based federated learning framework that provides a solution for collaborative data training. The proposed framework enables the coordination of multiple hospitals to train and share encrypted federated models while preserving data privacy. Blockchain ledger technology provides decentralization of federated learning models without relying on a central server. Moreover, the proposed homomorphic encryption scheme encrypts and decrypts the gradients of the model to preserve privacy. More precisely, the proposed framework: (i) train the local model by a novel capsule network for segmentation and classification of COVID-19 images, (ii) furthermore, we use the homomorphic encryption scheme to secure the local model that encrypts and decrypts the gradients, (iii) finally, the model is shared over a decentralized platform through the proposed blockchain-based federated learning algorithm. The integration of blockchain and federated learning leads to a new paradigm for medical image data sharing over the decentralized network. To validate our proposed model, we conducted comprehensive experiments and the results demonstrate the superior performance of the proposed scheme.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-11-18", "authors": ["RajeshKumar", "JayKumar", "Abdullah AmanKhan", "NoneZakria", "HubAli", "Cobbinah MBernard", "Riaz UllahKhan", "ShaoningZeng"], "doi": "10.1016/j.compmedimag.2022.102139\n10.1109/TII.2019.2942190"}
{"title": "COVID-19 Data Analytics Using Extended Convolutional Technique.", "abstract": "The healthcare system, lifestyle, industrial growth, economy, and livelihood of human beings worldwide were affected due to the triggered global pandemic by the COVID-19 virus that originated and was first reported in Wuhan city, Republic Country of China. COVID cases are difficult to predict and detect in their early stages, and their spread and mortality are uncontrollable. The reverse transcription polymerase chain reaction (RT-PCR) is still the first and foremost diagnostical methodology accepted worldwide; hence, it creates a scope of new diagnostic tools and techniques of detection approach which can produce effective and faster results compared with its predecessor. Innovational through current studies that complement the existence of the novel coronavirus (COVID-19) to findings in the thorax (chest) X-ray imaging, the projected research's method makes use of present deep learning (DL) models with the integration of various frameworks such as GoogleNet, U-Net, and ResNet50 to novel method those X-ray images and categorize patients as the corona positive (COVID\u2009+\u2009ve) or the corona negative (COVID -ve). The anticipated technique entails the pretreatment phase through dissection of the lung, getting rid of the environment which does now no longer provide applicable facts and can provide influenced consequences; then after this, the preliminary degree comes up with the category version educated below the switch mastering system; and in conclusion, consequences are evaluated and interpreted through warmth maps visualization. The proposed research method completed a detection accuracy of COVID-19 at around 99%.", "journal": "Interdisciplinary perspectives on infectious diseases", "date": "2022-11-18", "authors": ["Anand KumarGupta", "AsadiSrinivasulu", "Olutayo OyeyemiOyerinde", "GiovanniPau", "C VRavikumar"], "doi": "10.1155/2022/4578838\n10.1016/j.mlwa.2021.100138\n10.1155/2021/6621607\n10.1371/journal.pone.0262052\n10.1148/radiol.2020200642\n10.23750/abm.v91i1.9397\n10.1109/access.2020.2997311\n10.1056/NEJMoa2002032\n10.1155/2022/4838009\n10.1001/jama.2020.3786\n10.21227/4kcm-m312\n10.3390/ijerph18063056\n10.1007/s13755-021-00152-w\n10.1007/s13755-021-00158-4\n10.3390/healthcare9050522"}
{"title": "Detection of COVID-19: A Smartphone-Based Machine-Learning-Assisted ECL Immunoassay Approach with the Ability of RT-PCR CT Value Prediction.", "abstract": "The unstoppable spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has severely threatened public health over the past 2 years. The current ubiquitously accepted method for its diagnosis provides sensitive detection of the virus; however, it is relatively time-consuming and costly, not to mention the need for highly skilled personnel. There is a clear need to develop novel computer-based diagnostic tools to provide rapid, cost-efficient, and time-saving detection in places where massive traditional testing is not practical. Here, we develop an electrochemiluminescence (ECL)-based detection system whose results are quantified as reverse transcriptase polymerase chain reaction (RT-PCR) cyclic threshold (CT) values. A concentration-dependent signal is generated upon the introduction of the virus to the electrode and is recorded with a smartphone camera. The ECL images are used to train machine learning algorithms, and a model using artificial neural networks (ANNs) for 45 samples was developed. The model demonstrated more than 90% accuracy in the diagnosis of 50 unknown real samples, detecting up to a CT value of 32 and a limit of detection (LOD) of 10", "journal": "Analytical chemistry", "date": "2022-11-17", "authors": ["AliFiroozbakhtian", "MortezaHosseini", "Mahsa NaghaviSheikholeslami", "FoadSalehnia", "GuobaoXu", "HodjattallahRabbani", "EbtesamSobhanie"], "doi": "10.1021/acs.analchem.2c03502"}
{"title": "Calibrated bagging deep learning for image semantic segmentation: A case study on COVID-19 chest X-ray image.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes coronavirus disease 2019 (COVID-19). Imaging tests such as chest X-ray (CXR) and computed tomography (CT) can provide useful information to clinical staff for facilitating a diagnosis of COVID-19 in a more efficient and comprehensive manner. As a breakthrough of artificial intelligence (AI), deep learning has been applied to perform COVID-19 infection region segmentation and disease classification by analyzing CXR and CT data. However, prediction uncertainty of deep learning models for these tasks, which is very important to safety-critical applications like medical image processing, has not been comprehensively investigated. In this work, we propose a novel ensemble deep learning model through integrating bagging deep learning and model calibration to not only enhance segmentation performance, but also reduce prediction uncertainty. The proposed method has been validated on a large dataset that is associated with CXR image segmentation. Experimental results demonstrate that the proposed method can improve the segmentation performance, as well as decrease prediction uncertainty.", "journal": "PloS one", "date": "2022-11-17", "authors": ["LucyNwosu", "XiangfangLi", "LijunQian", "SeungchanKim", "XishuangDong"], "doi": "10.1371/journal.pone.0276250\n10.1002/ima.22469\n10.1145/3411760\n10.33889/IJMEMS.2020.5.4.052\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.3390/ijerph18063056\n10.1097/RTI.0000000000000532\n10.3390/s21217116\n10.1016/j.compbiomed.2021.104984\n10.1016/j.bspc.2021.103182\n10.1175/1520-0450(1967)006<0748:VOPPAB>2.0.CO;2\n10.1177/0962280213497434\n10.1136/amiajnl-2011-000291\n10.1007/BF00058655\n10.1186/s12880-020-00529-5\n10.1148/ryct.2020200082\n10.1038/nature21056"}
{"title": "Cardiovascular CT, MRI, and PET/CT in 2021: Review of Key Articles.", "abstract": "This review focuses on three key noninvasive cardiac imaging modalities-cardiac CT angiography (CTA), MRI, and PET/CT-and summarizes key publications in 2021 relevant to radiologists in clinical practice. Although this review focuses primarily on articles published in ", "journal": "Radiology", "date": "2022-11-16", "authors": ["GeorgiosTzimas", "David TRyan", "David JMurphy", "Jonathon ALeipsic", "Jonathan DDodd"], "doi": "10.1148/radiol.221181"}
{"title": "CNN Features and Optimized Generative Adversarial Network for COVID-19 Detection from Chest X-Ray Images.", "abstract": "Coronavirus is a RNA type virus, which makes various respiratory infections in both human as well as animals. In addition, it could cause pneumonia in humans. The Coronavirus affected patients has been increasing day to day, due to the wide spread of diseases. As the count of corona affected patients increases, most of the regions are facing the issue of test kit shortage. In order to resolve this issue, the deep learning approach provides a better solution for automatically detecting the COVID-19 disease. In this research, an optimized deep learning approach, named Henry gas water wave optimization-based deep generative adversarial network (HGWWO-Deep GAN) is developed. Here, the HGWWO algorithm is designed by the hybridization of Henry gas solubility optimization (HGSO) and water wave optimization (WWO) algorithm. The pre-processing method is carried out using region of interest (RoI) and median filtering in order to remove the noise from the images. Lung lobe segmentation is carried out using U-net architecture and lung region extraction is done using convolutional neural network (CNN) features. Moreover, the COVID-19 detection is done using Deep GAN trained by the HGWWO algorithm. The experimental result demonstrates that the developed model attained the optimal performance based on the testing accuracy of 0.9169, sensitivity of 0.9328, and specificity of 0.9032.", "journal": "Critical reviews in biomedical engineering", "date": "2022-11-15", "authors": ["GotlurKalpana", "A KanakaDurga", "GKaruna"], "doi": "10.1615/CritRevBiomedEng.2022042286"}
{"title": "A survey on deep learning applied to medical images: from simple artificial neural networks to generative models.", "abstract": "Deep learning techniques, in particular generative models, have taken on great importance in medical image analysis. This paper surveys fundamental deep learning concepts related to medical image generation. It provides concise overviews of studies which use some of the latest state-of-the-art models from last years applied to medical images of different injured body areas or organs that have a disease associated with (e.g., brain tumor and COVID-19 lungs pneumonia). The motivation for this study is to offer a comprehensive overview of artificial neural networks (NNs) and deep generative models in medical imaging, so more groups and authors that are not familiar with deep learning take into consideration its use in medicine works. We review the use of generative models, such as generative adversarial networks and variational autoencoders, as techniques to achieve semantic segmentation, data augmentation, and better classification algorithms, among other purposes. In addition, a collection of widely used public medical datasets containing magnetic resonance (MR) images, computed tomography (CT) scans, and common pictures is presented. Finally, we feature a summary of the current state of generative models in medical image including key features, current challenges, and future research paths.", "journal": "Neural computing & applications", "date": "2022-11-15", "authors": ["PCelard", "E LIglesias", "J MSorribes-Fdez", "RRomero", "A SearaVieira", "LBorrajo"], "doi": "10.1007/s00521-022-07953-4\n10.1016/j.artmed.2021.102164\n10.1016/j.artmed.2021.102165\n10.1145/3464423\n10.1145/3465398\n10.1007/s00521-022-07099-3\n10.1007/s00521-022-06960-9\n10.1016/j.artmed.2020.101938\n10.1016/j.neunet.2014.09.003\n10.1016/j.compmedimag.2019.04.005\n10.1245/ASO.2004.04.018\n10.3109/02841851.2010.498444\n10.7314/APJCP.2012.13.3.927\n10.1136/bjo.80.11.940\n10.1136/bjo.83.8.902\n10.1016/j.compbiomed.2005.01.006\n10.1109/10.959322\n10.1016/j.compbiomed.2021.104319\n10.1038/nature14539\n10.1016/0730-725X(93)90417-C\n10.1093/clinchem/48.10.1828\n10.1245/ASO.2004.03.007\n10.1179/016164104773026534\n10.1145/3065386\n10.1007/s00521-022-06953-8\n10.1007/BF00344251\n10.1109/42.476112\n10.1016/j.media.2017.07.005\n10.1007/978-3-319-46448-0_2\n10.1016/j.ejca.2019.04.001\n10.1371/journal.pmed.1002730\n10.1016/j.cmpb.2020.105532\n10.1016/j.media.2018.10.006\n10.1016/j.media.2019.01.012\n10.1016/j.media.2019.101557\n10.1016/j.bspc.2021.102901\n10.1007/s10278-019-00227-x\n10.1016/j.media.2020.101884\n10.1016/j.neucom.2019.02.003\n10.1109/TPAMI.2021.3059968\n10.1007/s10462-020-09854-1\n10.1016/j.compeleceng.2021.107036\n10.3390/diagnostics11020169\n10.1016/j.compbiomed.2021.104699\n10.3389/fgene.2021.639930\n10.1038/s41592-020-01008-z\n10.1155/2021/6625688\n10.1109/TMI.2020.2995508\n10.1016/j.neunet.2021.03.006\n10.1109/JBHI.2020.2986926\n10.1109/ACCESS.2019.2899108\n10.1016/j.bspc.2019.101678\n10.1016/j.bspc.2019.101641\n10.1002/mp.13927\n10.1002/mp.14006\n10.1016/j.remnie.2016.07.002\n10.1007/s00259-020-04816-9\n10.1186/s13195-021-00797-5\n10.1016/j.media.2020.101716\n10.1016/j.cmpb.2020.105568\n10.1007/s12539-020-00403-6\n10.2174/1573405616666200604163954\n10.1002/mp.15044\n10.1016/j.cmpb.2021.106018\n10.1561/2200000056\n10.1007/s11548-018-1898-0\n10.1016/j.ejmp.2021.02.013\n10.1016/j.artmed.2020.102006\n10.1007/s10278-020-00413-2\n10.1016/j.media.2020.101952\n10.1212/WNL.0b013e3181cb3e25\n10.1007/s10916-019-1475-2\n10.1016/j.compbiomed.2020.103764\n10.1371/journal.pone.0140381\n10.1016/j.compbiomed.2019.103345\n10.1109/TMI.2014.2377694\n10.1109/TMI.2018.2867350\n10.1016/j.compbiomed.2020.103774\n10.1016/j.acra.2011.09.014\n10.12913/22998624/137964\n10.1016/j.cell.2018.02.010\n10.1016/j.cmpb.2020.105581\n10.1016/j.media.2020.101794\n10.1118/1.3528204\n10.1038/s41597-021-00815-z\n10.3758/BRM.42.1.351\n10.1007/s13246-020-00865-4\n10.1016/j.artmed.2020.101880\n10.1016/j.media.2021.102327\n10.1109/TMI.2022.3147426\n10.1016/j.media.2022.102479\n10.1109/ACCESS.2022.3172975\n10.1016/j.dib.2022.108258"}
{"title": "SARS-CoV-2 Morphometry Analysis and Prediction of Real Virus Levels Based on Full Recurrent Neural Network Using TEM Images.", "abstract": "The SARS-CoV-2 virus is responsible for the rapid global spread of the COVID-19 disease. As a result, it is critical to understand and collect primary data on the virus, infection epidemiology, and treatment. Despite the speed with which the virus was detected, studies of its cell biology and architecture at the ultrastructural level are still in their infancy. Therefore, we investigated and analyzed the viral morphometry of SARS-CoV-2 to extract important key points of the virus's characteristics. Then, we proposed a prediction model to identify the real virus levels based on the optimization of a full recurrent neural network (RNN) using transmission electron microscopy (TEM) images. Consequently, identification of virus levels depends on the size of the morphometry of the area (width, height, circularity, roundness, aspect ratio, and solidity). The results of our model were an error score of training network performance 3.216 \u00d7 10", "journal": "Viruses", "date": "2022-11-12", "authors": ["Bakr AhmedTaha", "Yousif AlMashhadany", "Abdulmajeed H JAl-Jumaily", "Mohd Saiful Dzulkefly BinZan", "NorhanaArsad"], "doi": "10.3390/v14112386\n10.3390/v11010059\n10.3390/pathogens9030240\n10.1038/s41577-021-00578-z\n10.56770/jcp2021525\n10.7326/M20-1301\n10.1007/s11356-022-18849-0\n10.3390/diagnostics11061119\n10.3390/s21248362\n10.1126/scitranslmed.abc1931\n10.1007/s11801-013-3017-3\n10.1016/j.jcv.2020.104412\n10.1002/jmv.25721\n10.1021/acscentsci.0c00501\n10.1007/s11606-020-05762-w\n10.1007/s00253-022-11930-1\n10.3389/fgene.2021.569120\n10.1016/j.compbiomed.2020.103792\n10.3934/mbe.2021440\n10.3233/XST-200715\n10.1109/ACCESS.2019.2930111\n10.3390/bios11080253\n10.3390/s20143924\n10.3390/s20236764\n10.1016/j.cosrev.2009.03.005\n10.1007/s10462-017-9572-4\n10.1016/j.sbsr.2017.09.002\n10.1038/s41592-018-0261-2\n10.1016/j.cell.2018.03.040\n10.1016/j.jsb.2007.04.003\n10.1109/TMI.2020.3001810\n10.1109/ACCESS.2020.2993788\n10.1109/CVPR.2019.00864\n10.53293/jasn.2021.11016\n10.1007/s12596-022-00978-x\n10.1038/s41378-019-0069-y\n10.1007/s11082-022-03786-6\n10.1016/j.patcog.2021.107885\n10.1155/2020/4621403\n10.1038/s41377-021-00506-9\n10.3390/info12070272\n10.1016/j.trsl.2017.10.010\n10.1016/j.bbe.2014.07.003\n10.1128/JCM.01521-17\n10.1155/2022/9690940\n10.3390/w14050761\n10.1109/72.97934\n10.1162/neco.1991.3.2.246\n10.1007/s00703-012-0205-9\n10.3389/fmicb.2018.03255\n10.1038/s41598-021-82852-7\n10.1056/NEJMoa2001017\n10.1099/jgv.0.001453\n10.1038/s41467-020-15562-9\n10.21037/jtd-20-1368\n10.1126/science.abb3405\n10.1126/science.abb2762\n10.1016/j.bios.2021.112969\n10.1007/s11760-021-02045-7"}
{"title": "EVAE-Net: An Ensemble Variational Autoencoder Deep Learning Network for COVID-19 Classification Based on Chest X-ray Images.", "abstract": "The COVID-19 pandemic has had a significant impact on many lives and the economies of many countries since late December 2019. Early detection with high accuracy is essential to help break the chain of transmission. Several radiological methodologies, such as CT scan and chest X-ray, have been employed in diagnosing and monitoring COVID-19 disease. Still, these methodologies are time-consuming and require trial and error. Machine learning techniques are currently being applied by several studies to deal with COVID-19. This study exploits the latent embeddings of variational autoencoders combined with ensemble techniques to propose three effective EVAE-Net models to detect COVID-19 disease. Two encoders are trained on chest X-ray images to generate two feature maps. The feature maps are concatenated and passed to either a combined or individual reparameterization phase to generate latent embeddings by sampling from a distribution. The latent embeddings are concatenated and passed to a classification head for classification. The COVID-19 Radiography Dataset from Kaggle is the source of chest X-ray images. The performances of the three models are evaluated. The proposed model shows satisfactory performance, with the best model achieving 99.19% and 98.66% accuracy on four classes and three classes, respectively.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-11-12", "authors": ["DanielAddo", "ShijieZhou", "Jehoiada KofiJackson", "Grace UgochiNneji", "Happy NkantaMonday", "KwabenaSarpong", "Rutherford AgbeshiPatamia", "FavourEkong", "Christyn AkosuaOwusu-Agyei"], "doi": "10.3390/diagnostics12112569\n10.1148/radiol.2020200432\n10.1109/ACCESS.2020.3033762\n10.1128/JCM.01438-20\n10.1016/j.knosys.2020.106647\n10.3238/arztebl.2014.0181\n10.1007/s11548-019-01917-1\n10.1016/j.jemermed.2020.04.004\n10.1148/radiol.2020201160\n10.14245/ns.1938396.198\n10.1038/s41746-020-0273-z\n10.1109/TMI.2016.2535865\n10.1109/ISBI.2018.8363572\n10.1016/j.measurement.2019.05.076\n10.1016/j.bspc.2022.103848\n10.1016/j.bspc.2022.103595\n10.1016/j.neucom.2022.01.055\n10.1016/j.jksuci.2020.12.010\n10.1038/323533a0\n10.1561/2200000056\n10.21437/Interspeech.2016-1183\n10.1016/S0140-6736(20)30304-4\n10.1109/TCYB.2020.2990162\n10.1093/jtm/taaa080\n10.3390/healthcare8010046\n10.1038/s41467-020-17280-8\n10.1016/j.compbiomed.2020.103869\n10.3390/s21175813\n10.1148/radiol.2020200905\n10.3390/app12126269\n10.1016/j.compbiomed.2020.103792\n10.1109/CVPR.2017.690\n10.1016/j.cmpb.2020.105581\n10.1007/s12530-021-09385-2\n10.1109/TNNLS.2021.3070467\n10.3390/sym14071398\n10.1007/s11517-020-02299-2\n10.1007/s12539-020-00403-6\n10.3390/healthcare10071313\n10.1016/j.compbiomed.2022.105233\n10.1007/s13246-020-00865-4\n10.1109/CVPR.2017.243\n10.1007/s13755-021-00140-0\n10.1038/s41598-020-76550-z\n10.1109/TCBB.2021.3065361\n10.21037/atm.2020.03.132\n10.1007/s10489-020-01900-3\n10.1016/j.asoc.2020.106912\n10.1109/IJCNN.2015.7280578\n10.1109/ICCCI50826.2021.9402545\n10.1155/2021/5527923\n10.1007/978-3-030-74575-2_14\n10.1111/j.1365-2818.2010.03415.x\n10.1109/ICACCI.2014.6968381\n10.32604/cmc.2022.020698\n10.1109/TBDATA.2017.2717439\n10.14569/IJACSA.2021.0120717\n10.1155/2018/3078374\n10.3390/jimaging7050083\n10.1148/ryai.2021200218\n10.1111/srt.13145\n10.1007/s10489-020-01813-1\n10.1016/j.micpro.2020.103280\n10.1016/j.neucom.2015.08.104\n10.1016/j.cmpb.2022.106883\n10.1016/j.bbe.2021.09.004\n10.1016/j.compbiomed.2021.105134\n10.1155/2022/5329014\n10.32604/cmc.2021.018449\n10.1007/s10489-020-02002-w\n10.1145/3451357\n10.1016/j.patrec.2021.08.018\n10.1038/s41598-022-05532-0\n10.1016/j.bspc.2021.103326\n10.1016/j.compmedimag.2021.102008\n10.1016/j.bspc.2022.103677\n10.1016/j.compbiomed.2022.105340\n10.1016/j.jksuci.2021.07.005\n10.1016/j.jksuci.2022.04.006\n10.1016/j.compbiomed.2021.104319\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104834\n10.1016/j.compbiomed.2022.105244\n10.1016/j.bspc.2022.103860"}
{"title": "Generative adversarial network based data augmentation for CNN based detection of Covid-19.", "abstract": "Covid-19 has been a global concern since 2019, crippling the world economy and health. Biological diagnostic tools have since been developed to identify the virus from bodily fluids and since the virus causes pneumonia, which results in lung inflammation, the presence of the virus can also be detected using medical imaging by expert radiologists. The success of each diagnostic method is measured by the hit rate for identifying Covid infections. However, the access for people to each diagnosis tool can be limited, depending on the geographic region and, since Covid treatment denotes a race against time, the diagnosis duration plays an important role. Hospitals with X-ray opportunities are widely distributed all over the world, so a method investigating lung X-ray images for possible Covid-19 infections would offer itself. Promising results have been achieved in the literature in automatically detecting the virus using medical images like CT scans and X-rays using supervised artificial neural network algorithms. One of the major drawbacks of supervised learning models is that they require enormous amounts of data to train, and generalize on new data. In this study, we develop a Swish activated, Instance and Batch normalized Residual U-Net GAN with dense blocks and skip connections to create synthetic and augmented data for training. The proposed GAN architecture, due to the presence of instance normalization and swish activation, can deal with the randomness of luminosity, that arises due to different sources of X-ray images better than the classical architecture and generate realistic-looking synthetic data. Also, the radiology equipment is not generally computationally efficient. They cannot efficiently run state-of-the-art deep neural networks such as DenseNet and ResNet effectively. Hence, we propose a novel CNN architecture that is 40% lighter and more accurate than state-of-the-art CNN networks. Multi-class classification of the three classes of chest X-rays (CXR), ie Covid-19, healthy and Pneumonia, is performed using the proposed model which had an extremely high test accuracy of 99.2% which has not been achieved in any previous studies in the literature. Based on the mentioned criteria for developing Corona infection diagnosis, in the present study, an Artificial Intelligence based method is proposed, resulting in a rapid diagnostic tool for Covid infections based on generative adversarial and convolutional neural networks. The benefit will be a high accuracy of lung infection identification with 99% accuracy. This could lead to a support tool that helps in rapid diagnosis, and an accessible Covid identification method using CXR images.", "journal": "Scientific reports", "date": "2022-11-11", "authors": ["RutwikGulakala", "BerndMarkert", "MarcusStoffel"], "doi": "10.1038/s41598-022-23692-x\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020201160\n10.1038/s42003-020-01535-7\n10.1155/2021/3366057\n10.1515/cdbme-2020-3051\n10.1016/j.cmpb.2021.106279\n10.1016/j.medengphy.2016.10.010\n10.1016/j.cma.2020.112989\n10.1016/j.mechrescom.2021.103817\n10.1016/j.euromechsol.2006.12.002\n10.1016/j.mechmat.2005.06.001\n10.1038/s41598-021-93543-8\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100505\n10.1016/j.ijmedinf.2020.104284\n10.1148/radiol.2017162326\n10.1109/ACCESS.2020.2994762\n10.1007/s00521-022-06918-x\n10.1016/j.compbiomed.2020.103792\n10.1007/s10044-020-00950-0\n10.3390/diagnostics12020267\n10.1109/TMI.2020.2995518\n10.3390/app112210528\n10.1007/s00500-019-04602-2\n10.1016/j.imu.2021.100779\n10.1038/s41598-021-87994-2\n10.1109/TMI.2013.2290491\n10.1109/TMI.2013.2284099\n10.1186/s40537-021-00444-8\n10.1007/s13246-020-00865-4\n10.1371/journal.pone.0262052"}
{"title": "CXR-Net: A Multitask Deep Learning Network for Explainable and Accurate Diagnosis of COVID-19 Pneumonia from Chest X-ray Images.", "abstract": "Accurate and rapid detection of COVID-19 pneumonia is crucial for optimal patient treatment. Chest X-Ray (CXR) is the first-line imaging technique for COVID-19 pneumonia diagnosis as it is fast, cheap and easily accessible. Currently, many deep learning (DL) models have been proposed to detect COVID-19 pneumonia from CXR images. Unfortunately, these deep classifiers lack the transparency in interpreting findings, which may limit their applications in clinical practice. The existing explanation methods produce either too noisy or imprecise results, and hence are unsuitable for diagnostic purposes. In this work, we propose a novel explainable CXR deep neural Network (CXR-Net) for accurate COVID-19 pneumonia detection with an enhanced pixel-level visual explanation using CXR images. An Encoder-Decoder-Encoder architecture is proposed, in which an extra encoder is added after the encoder-decoder structure to ensure the model can be trained on category samples. The method has been evaluated on real world CXR datasets from both public and private sources, including healthy, bacterial pneumonia, viral pneumonia and COVID-19 pneumonia cases. The results demonstrate that the proposed method can achieve a satisfactory accuracy and provide fine-resolution activation maps for visual explanation in the lung disease detection. The Average Accuracy, Sensitivity, Specificity, PPV and F1-score of models in the COVID-19 pneumonia detection reach 0.992, 0.998, 0.985 and 0.989, respectively. Compared to current state-of-the-art visual explanation methods, the proposed method can provide more detailed, high-resolution, visual explanation for the classification results. It can be deployed in various computing environments, including cloud, CPU and GPU environments. It has a great potential to be used in clinical practice for COVID-19 pneumonia diagnosis.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-11-10", "authors": ["XinZhang", "LiangxiuHan", "TamSobeih", "LianghaoHan", "NinaDempsey", "SymeonLechareas", "AscanioTridente", "HaomingChen", "StephenWhite", "DaoqiangZhang"], "doi": "10.1109/JBHI.2022.3220813"}
{"title": "Mental health and chest CT scores mediate the relationship between COVID-19 vaccination status and seroconversion time: A cross-sectional observational study in B.1.617.2 (Delta) infection patients.", "abstract": "The coronavirus disease (COVID-19) pandemic, which has been ongoing for more than 2 years, has become one of the largest public health issues. Vaccination against severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection is one of the most important interventions to mitigate the COVID-19 pandemic. Our objective is to investigate the relationship between vaccination status and time to seroconversion.\nWe conducted a cross-sectional observational study during the SARS-CoV-2 B.1.617.2 outbreak in Jiangsu, China. Participants who infected with the B.1.617.2 variant were enrolled. Cognitive performance, quality of life, emotional state, chest computed tomography (CT) score and seroconversion time were evaluated for each participant. Statistical analyses were performed using one-way ANOVA, univariate and multivariate regression analyses, Pearson correlation, and mediation analysis.\nA total of 91 patients were included in the analysis, of whom 37.3, 25.3, and 37.3% were unvaccinated, partially vaccinated, and fully vaccinated, respectively. Quality of life was impaired in 30.7% of patients, especially for mental component summary (MCS) score. Vaccination status, subjective cognitive decline, and depression were risk factors for quality-of-life impairment. The chest CT score mediated the relationship of vaccination status with the MCS score, and the MCS score mediated the relationship of the chest CT score with time to seroconversion.\nFull immunization course with an inactivated vaccine effectively lowered the chest CT score and improved quality of life in hospitalized patients. Vaccination status could influence time to seroconversion by affecting CT score and MCS score indirectly. Our study emphasizes the importance of continuous efforts in encouraging a full vaccination course.", "journal": "Frontiers in public health", "date": "2022-11-08", "authors": ["WenZhang", "QianChen", "JinghongDai", "JiamingLu", "JieLi", "YongxiangYi", "LinqingFu", "XinLi", "JianiLiu", "JinlongLiufu", "CongLong", "BingZhang"], "doi": "10.3389/fpubh.2022.974848\n10.1016/j.landusepol.2021.105772\n10.1016/j.ijid.2022.01.030\n10.5694/mja2.51182\n10.1016/S0140-6736(21)00677-2\n10.1001/jama.2021.8565\n10.1056/NEJMoa2107715\n10.1016/S0140-6736(21)01429-X\n10.7499/j.issn.1008-8830.2101133\n10.3390/vaccines10020277\n10.1002/jmv.27458\n10.1016/j.bbi.2020.04.027\n10.3390/vaccines9121444\n10.1016/j.psychres.2020.113172\n10.46234/ccdcw2020.032\n10.1080/22221751.2021.1969291\n10.3233/JAD-160119\n10.1016/j.dadm.2015.09.004\n10.1097/00005650-199206000-00002\n10.1002/da.20837\n10.1111/j.1600-0447.1983.tb09716.x\n10.1148/radiol.2020200463\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)32656-8\n10.1146/annurev-publhealth-032315-021402\n10.2807/1560-7917.ES.2021.26.24.2100509\n10.1016/S0262-4079(21)01121-0\n10.2147/IJGM.S323316\n10.3390/biomedicines9080900\n10.1186/s12877-021-02140-x\n10.1001/jamanetworkopen.2021.28568\n10.1016/j.clinimag.2021.01.023\n10.3389/fpsyt.2021.774504\n10.1183/13993003.01217-2020"}
{"title": "Strong semantic segmentation for Covid-19 detection: Evaluating the use of deep learning models as a performant tool in radiography.", "abstract": "With the increasing number of Covid-19 cases as well as care costs, chest diseases have gained increasing interest in several communities, particularly in medical and computer vision. Clinical and analytical exams are widely recognized techniques for diagnosing and handling Covid-19 cases. However, strong detection tools can help avoid damage to chest tissues. The proposed method provides an important way to enhance the semantic segmentation process using combined potential deep learning (DL) modules to increase consistency. Based on Covid-19 CT images, this work hypothesized that a novel model for semantic segmentation might be able to extract definite graphical features of Covid-19 and afford an accurate clinical diagnosis while optimizing the classical test and saving time.\nCT images were collected considering different cases (normal chest CT, pneumonia, typical viral causes, and Covid-19 cases). The study presents an advanced DL method to deal with chest semantic segmentation issues. The approach employs a modified version of the U-net to enable and support Covid-19 detection from the studied images.\nThe validation tests demonstrated competitive results with important performance rates: Precision (90.96%\u00a0\u00b1\u00a02.5) with an F-score of (91.08%\u00a0\u00b1\u00a03.2), an accuracy of (93.37%\u00a0\u00b1\u00a01.2), a sensitivity of (96.88%\u00a0\u00b1\u00a02.8) and a specificity of (96.91%\u00a0\u00b1\u00a02.3). In addition, the visual segmentation results are very close to the Ground truth.\nThe findings of this study reveal the proof-of-principle for using cooperative components to strengthen the semantic segmentation modules for effective and truthful Covid-19 diagnosis.\nThis paper has highlighted that DL based approach, with several modules, may be contributing to provide strong support for radiographers and physicians, and that further use of DL is required to design and implement performant automated vision systems to detect chest diseases.", "journal": "Radiography (London, England : 1995)", "date": "2022-11-07", "authors": ["HAllioui", "YMourdi", "MSadgal"], "doi": "10.1016/j.radi.2022.10.010\n10.1016/j.neucom.2017.08.043\n10.3389/fnins.2018.00777\n10.1007/s13369-021-05958-0\n10.3390/technologies10050105\n10.1109/ISBI.2016.7493515\n10.5114/pjr.2022.119027\n10.1109/CISP-BMEI.2018.8633056\n10.1016/j.asoc.2021.107160\n10.48550/arXiv.2110.09619\n10.7937/K9/TCIA.2017.3r3fvz08\n10.5281/zenodo.375747\n10.1111/cgf.14521"}
{"title": "Prediction of COVID-19 patients in danger of death using radiomic features of portable chest radiographs.", "abstract": "Computer-aided diagnostic systems have been developed for the detection and differential diagnosis of coronavirus disease 2019 (COVID-19) pneumonia using imaging studies to characterise a patient's current condition. In this radiomic study, we propose a system for predicting COVID-19 patients in danger of death using portable chest X-ray images.\nIn this retrospective study, we selected 100 patients, including ten that died and 90 that recovered from the COVID-19-AR database of the Cancer Imaging Archive. Since it can be difficult to analyse portable chest X-ray images of patients with COVID-19 because bone components overlap with the abnormal patterns of this disease, we employed a bone-suppression technique during pre-processing. A total of 620 radiomic features were measured in the left and right lung regions, and four radiomic features were selected using the least absolute shrinkage and selection operator technique. We distinguished death from recovery cases using a linear discriminant analysis (LDA) and a support vector machine (SVM). The leave-one-out method was used to train and test the classifiers, and the area under the receiver-operating characteristic curve (AUC) was used to evaluate discriminative performance.\nThe AUCs for LDA and SVM were 0.756 and 0.959, respectively. The discriminative performance was improved when the bone-suppression technique was employed. When the SVM was used, the sensitivity for predicting disease severity was 90.9% (9/10), and the specificity was 95.6% (86/90).\nWe believe that the radiomic features of portable chest X-ray images can predict COVID-19 patients in danger of death.", "journal": "Journal of medical radiation sciences", "date": "2022-11-06", "authors": ["MaokoNakashima", "YoshikazuUchiyama", "HirotakeMinami", "SatoshiKasai"], "doi": "10.1002/jmrs.631\n10.1007/s12652-020-02669-6"}
{"title": "Contrastive domain adaptation with consistency match for automated pneumonia diagnosis.", "abstract": "Pneumonia can be difficult to diagnose since its symptoms are too variable, and the radiographic signs are often very similar to those seen in other illnesses such as a cold or influenza. Deep neural networks have shown promising performance in automated pneumonia diagnosis using chest X-ray radiography, allowing mass screening and early intervention to reduce the severe cases and death toll. However, they usually require many well-labelled chest X-ray images for training to achieve high diagnostic accuracy. To reduce the need for training data and annotation resources, we propose a novel method called Contrastive Domain Adaptation with Consistency Match (CDACM). It transfers the knowledge from different but relevant datasets to the unlabelled small-size target dataset and improves the semantic quality of the learnt representations. Specifically, we design a conditional domain adversarial network to exploit discriminative information conveyed in the predictions to mitigate the domain gap between the source and target datasets. Furthermore, due to the small scale of the target dataset, we construct a feature cloud for each target sample and leverage contrastive learning to extract more discriminative features. Lastly, we propose adaptive feature cloud expansion to push the decision boundary to a low-density area. Unlike most existing transfer learning methods that aim only to mitigate the domain gap, our method instead simultaneously considers the domain gap and the data deficiency problem of the target dataset. The conditional domain adaptation and the feature cloud generation of our method are learning jointly to extract discriminative features in an end-to-end manner. Besides, the adaptive feature cloud expansion improves the model's generalisation ability in the target domain. Extensive experiments on pneumonia and COVID-19 diagnosis tasks demonstrate that our method outperforms several state-of-the-art unsupervised domain adaptation approaches, which verifies the effectiveness of CDACM for automated pneumonia diagnosis using chest X-ray imaging.", "journal": "Medical image analysis", "date": "2022-11-05", "authors": ["YangqinFeng", "ZizhouWang", "XinxingXu", "YanWang", "HuazhuFu", "ShaohuaLi", "LiangliZhen", "XiaofengLei", "YingnanCui", "JordanSim Zheng Ting", "YonghanTing", "Joey TianyiZhou", "YongLiu", "RickSiow Mong Goh", "CherHeng Tan"], "doi": "10.1016/j.media.2022.102664"}
{"title": "Assessment of COVID-19 lung involvement on computed tomography by deep-learning-, threshold-, and human reader-based approaches-an international, multi-center comparative study.", "abstract": "The extent of lung involvement in coronavirus disease 2019 (COVID-19) pneumonia, quantified on computed tomography (CT), is an established biomarker for prognosis and guides clinical decision-making. The clinical standard is semi-quantitative scoring of lung involvement by an experienced reader. We aim to compare the performance of automated deep-learning- and threshold-based methods to the manual semi-quantitative lung scoring. Further, we aim to investigate an optimal threshold for quantification of involved lung in COVID pneumonia chest CT, using a multi-center dataset.\nIn total 250 patients were included, 50 consecutive patients with RT-PCR confirmed COVID-19 from our local institutional database, and another 200 patients from four international datasets (n=50 each). Lung involvement was scored semi-quantitatively by three experienced radiologists according to the established chest CT score (CCS) ranging from 0-25. Inter-rater reliability was reported by the intraclass correlation coefficient (ICC). Deep-learning-based segmentation of ground-glass and consolidation was obtained by CT Pulmo Auto Results prototype plugin on IntelliSpace Discovery (Philips Healthcare, The Netherlands). Threshold-based segmentation of involved lung was implemented using an open-source tool for whole-lung segmentation under the presence of severe pathologies (R231CovidWeb, Hofmanninger \nMedian CCS among 250 evaluated patients was 10 [6-15]. Inter-rater reliability of the CCS was excellent [ICC 0.97 (0.97-0.98)]. Best attenuation threshold for identification of involved lung was -522 HU. While the relationship of deep-learning- and threshold-based quantification was linear and strong (r\nThe manual semi-quantitative CCS underestimates the extent of COVID pneumonia in higher score ranges, which limits its clinical usefulness in cases of severe disease. Clinical implementation of fully automated methods, such as deep-learning or threshold-based approaches (best threshold in our multi-center dataset: -522 HU), might save time of trained personnel, abolish inter-reader variability, and allow for truly quantitative, linear assessment of COVID lung involvement.", "journal": "Quantitative imaging in medicine and surgery", "date": "2022-11-05", "authors": ["PhilippFervers", "FlorianFervers", "AsthaJaiswal", "MiriamRinneburger", "MathildaWeisthoff", "PhilipPollmann-Schweckhorst", "JonathanKottlors", "HeikeCarolus", "SimonLennartz", "DavidMaintz", "RahilShahzad", "ThorstenPersigehl"], "doi": "10.21037/qims-22-175\n10.1148/rg.2020200159\n10.1038/s41467-020-18786-x\n10.3389/fpubh.2021.596938\n10.1148/ryct.2020200130\n10.1371/journal.pone.0237302\n10.1186/s12931-020-01411-2\n10.1007/s00330-020-07033-y\n10.1186/s43055-021-00525-x\n10.1148/ryct.2020200441\n10.1148/radiol.2020200370\n10.1148/ryai.2020200048\n10.1007/s11760-022-02183-6\n10.1148/radiol.2020201433\n10.1007/s00330-020-07013-2\n10.1148/radiol.2021203957\n10.1186/s13104-021-05592-x\n10.3390/bioengineering8020026\n10.1007/s10278-013-9622-7\n10.1007/s00330-021-08482-9\n10.1117/12.2293528\n10.1186/s41747-020-00173-2\n10.1038/s41586-020-2649-2\n10.1038/s41598-021-01489-8\n10.1177/1536867X0800800212\n10.3758/s13423-016-1039-0\n10.1353/bsp.2020.0011\n10.21037/jtd.2017.08.17\n10.1117/1.1631315\n10.1155/2021/6697677\n10.1007/s00330-021-08435-2"}
{"title": "A novel deep learning-based method for COVID-19 pneumonia detection from CT images.", "abstract": "The sensitivity of RT-PCR in diagnosing COVID-19 is only 60-70%, and chest CT plays an indispensable role in the auxiliary diagnosis of COVID-19 pneumonia, but the results of CT imaging are highly dependent on professional radiologists.\nThis study aimed to develop a deep learning model to assist radiologists in detecting COVID-19 pneumonia.\nThe total study population was 437. The training dataset contained 26,477, 2468, and 8104 CT images of normal, CAP, and COVID-19, respectively. The validation dataset contained 14,076, 1028, and 3376 CT images of normal, CAP, and COVID-19 patients, respectively. The test set included 51 normal cases, 28 CAP patients, and 51 COVID-19 patients. We designed and trained a deep learning model to recognize normal, CAP, and COVID-19 patients based on U-Net and ResNet-50. Moreover, the diagnoses of the deep learning model were compared with different levels of radiologists.\nIn the test set, the sensitivity of the deep learning model in diagnosing normal cases, CAP, and COVID-19 patients was 98.03%, 89.28%, and 92.15%, respectively. The diagnostic accuracy of the deep learning model was 93.84%. In the validation set, the accuracy was 92.86%, which was better than that of two novice doctors (86.73% and 87.75%) and almost equal to that of two experts (94.90% and 93.88%). The AI model performed\u00a0significantly\u00a0better\u00a0than all four radiologists in terms of time consumption (35\u00a0min vs. 75\u00a0min, 93\u00a0min, 79\u00a0min, and 82\u00a0min).\nThe AI model we obtained had strong decision-making ability, which could potentially assist doctors in detecting COVID-19 pneumonia.", "journal": "BMC medical informatics and decision making", "date": "2022-11-04", "authors": ["JuLuo", "YuhaoSun", "JingshuChi", "XinLiao", "CanxiaXu"], "doi": "10.1186/s12911-022-02022-1\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1056/NEJMoa2001316\n10.1056/NEJMoa2001191\n10.1101/2020.02.11.20021493v2\n10.1148/radiol.2020200432\n10.1109/TMI.2020.2996645\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)30154-9\n10.1148/ryct.2020204002\n10.1148/radiol.2020200642\n10.1038/s41467-020-17971-2\n10.1016/j.cell.2020.08.029\n10.1016/j.media.2021.102096\n10.1007/s00354-022-00172-4\n10.18280/ts.370313\n10.1186/s41747-020-00173-2\n10.1016/j.compbiomed.2020.103792\n10.18280/ts.380117\n10.1007/s10044-021-00984-y\n10.21203/rs.3.rs-104621/v1\n10.1148/radiol.2020200905\n10.1001/jama.2020.8259\n10.7326/M20-1495\n10.1001/jama.2020.12839\n10.1148/radiol.2020200702"}
{"title": "Improved Fine-Tuning of In-Domain Transformer Model for Inferring COVID-19 Presence in Multi-Institutional Radiology Reports.", "abstract": "Building a document-level classifier for COVID-19 on radiology reports could help assist providers in their daily clinical routine, as well as create large numbers of labels for computer vision models. We have developed such a classifier by fine-tuning a BERT-like model initialized from RadBERT, its continuous pre-training on radiology reports that can be used on all radiology-related tasks. RadBERT outperforms all biomedical pre-trainings on this COVID-19 task (P<0.01) and helps our fine-tuned model achieve an 88.9 macro-averaged F1-score, when evaluated on both X-ray and CT reports. To build this model, we rely on a multi-institutional dataset re-sampled and enriched with concurrent lung diseases, helping the model to resist to distribution shifts. In addition, we explore a variety of fine-tuning and hyperparameter optimization techniques that accelerate fine-tuning convergence, stabilize performance, and improve accuracy, especially when data or computational resources are limited. Finally, we provide a set of visualization tools and explainability methods to better understand the performance of the model, and support its practical use in the clinical setting. Our approach offers a ready-to-use COVID-19 classifier and can be applied similarly to other radiology report classification tasks.", "journal": "Journal of digital imaging", "date": "2022-11-04", "authors": ["PierreChambon", "Tessa SCook", "Curtis PLanglotz"], "doi": "10.1007/s10278-022-00714-8\n10.1093/bioinformatics/btz682\n10.1145/3458754\n10.1148/ryai.210258\n10.3345/kjp.2012.55.11.403\n10.3390/info11020108"}
{"title": "Multi Level Approach for Segmentation of Interstitial Lung Disease (ILD) Patterns Classification Based on Superpixel Processing and Fusion of ", "abstract": "During the COVID-19 pandemic, huge interstitial lung disease (ILD) lung images have been captured. It is high time to develop the efficient segmentation techniques utilized to separate the anatomical structures and ILD patterns for disease and infection level identification. The effectiveness of disease classification directly depends on the accuracy of initial stages like preprocessing and segmentation. This paper proposed a hybrid segmentation algorithm designed for ILD images by taking advantage of superpixel and ", "journal": "Computational intelligence and neuroscience", "date": "2022-11-02", "authors": ["Anni UGupta", "SaritaSingh Bhadauria"], "doi": "10.1155/2022/4431817\n10.1155/2019/2045432\n10.1186/s12880-020-00529-5\n10.1186/s12967-021-02992-2\n10.1007/s11042-021-10594-9\n10.3390/ijgi9050329\n10.1016/j.imavis.2009.10.009\n10.1016/j.procs.2017.11.282\n10.1016/j.heliyon.2020.e05267\n10.1007/s11517-015-1404-6\n10.3390/jimaging7020022\n10.1016/j.bspc.2021.103113\n10.1186/s13014-022-02035-0\n10.1016/j.jneumeth.2021.109296\n10.1038/s41598-021-82085-8\n10.1016/j.procs.2016.07.370\n10.3389/fmed.2022.794126\n10.3390/a13090207\n10.1007/s00521-021-06273-3\n10.1007/s11517-021-02379-x\n10.1016/j.net.2020.03.011\n10.1109/access.2020.3012160\n10.1109/tpami.2006.233\n10.1007/s41095-021-0239-3\n10.1109/tfuzz.2015.2505328\n10.1016/j.measurement.2019.107432\n10.1016/j.patcog.2017.03.012\n10.1186/s40537-019-0276-2\n10.1088/1742-6596/1613/1/012006\n10.1088/1757-899x/342/1/012060\n10.1186/s13640-018-0309-3\n10.1109/access.2020.2988796\n10.1007/s41095-020-0177-5\n10.1371/journal.pone.0240015\n10.1007/s11063-020-10330-8\n10.14569/ijacsa.2020.0111149\n10.2214/ajr.09.2843\n10.14419/ijet.v7i2.7.10275\n10.5120/ijca2016911409\n10.1109/tfuzz.2017.2743679\n10.1109/tip.2017.2675165\n10.1109/tpami.2014.2303095\n10.1109/tip.2017.2651389\n10.1155/2021/9654059\n10.1155/2021/8922656\n10.1155/2022/8961456\n10.1016/j.compmedimag.2011.07.003"}
{"title": "Severity detection of COVID-19 infection with machine learning of clinical records and CT images.", "abstract": "Coronavirus disease 2019 (COVID-19) is a deadly viral infection spreading rapidly around the world since its outbreak in 2019. In the worst case a patient's organ may fail leading to death. Therefore, early diagnosis is crucial to provide patients with adequate and effective treatment.\nThis paper aims to build machine learning prediction models to automatically diagnose COVID-19 severity with clinical and computed tomography (CT) radiomics features.\nP-V-Net was used to segment the lung parenchyma and then radiomics was used to extract CT radiomics features from the segmented lung parenchyma regions. Over-sampling, under-sampling, and a combination of over- and under-sampling methods were used to solve the data imbalance problem. RandomForest was used to screen out the optimal number of features. Eight different machine learning classification algorithms were used to analyze the data.\nThe experimental results showed that the COVID-19 mild-severe prediction model trained with clinical and CT radiomics features had the best prediction results. The accuracy of the GBDT classifier was 0.931, the ROUAUC 0.942, and the AUCPRC 0.694, which indicated it was better than other classifiers.\nThis study can help clinicians identify patients at risk of severe COVID-19 deterioration early on and provide some treatment for these patients as soon as possible. It can also assist physicians in prognostic efficacy assessment and decision making.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2022-11-01", "authors": ["FubaoZhu", "ZelinZhu", "YijunZhang", "HanleiZhu", "ZhengyuanGao", "XiaomanLiu", "GuanbinZhou", "YanXu", "FeiShan"], "doi": "10.3233/THC-220321"}
{"title": "Application of artificial intelligence in diagnosing COVID-19 disease symptoms on chest X-rays: A systematic review.", "abstract": "This systematic review focuses on using artificial intelligence (AI) to detect COVID-19 infection with the help of X-ray images. ", "journal": "International journal of medical sciences", "date": "2022-11-01", "authors": ["JakubKufel", "KatarzynaBargie\u0142", "MaciejKo\u017alik", "\u0141ukaszCzogalik", "PiotrDudek", "AleksanderJaworski", "MaciejCebula", "KatarzynaGruszczy\u0144ska"], "doi": "10.7150/ijms.76515"}
{"title": "Towards smart diagnostic methods for COVID-19: Review of deep learning for medical imaging.", "abstract": "The infectious disease known as COVID-19 has spread dramatically all over the world since December 2019. The fast diagnosis and isolation of infected patients are key factors in slowing down the spread of this virus and better management of the pandemic. Although the CT and X-ray modalities are commonly used for the diagnosis of COVID-19, identifying COVID-19 patients from medical images is a time-consuming and error-prone task. Artificial intelligence has shown to have great potential to speed up and optimize the prognosis and diagnosis process of COVID-19. Herein, we review publications on the application of deep learning (DL) techniques for diagnostics of patients with COVID-19 using CT and X-ray chest images for a period from January 2020 to October 2021. Our review focuses solely on peer-reviewed, well-documented articles. It provides a comprehensive summary of the technical details of models developed in these articles and discusses the challenges in the smart diagnosis of COVID-19 using DL techniques. Based on these challenges, it seems that the effectiveness of the developed models in clinical use needs to be further investigated. This review provides some recommendations to help researchers develop more accurate prediction models.", "journal": "IPEM-translation", "date": "2022-11-01", "authors": ["MarjanJalali Moghaddam", "MinaGhavipour"], "doi": "10.1016/j.ipemt.2022.100008\n10.1038/s41591-020-0931-3\n10.1007/s00330-020-06801-0\n10.1016/j.tmaid.2020.101623\n10.1148/radiol.2020200823\n10.1007/s42600-021-00151-6\n10.3390/ai1030027\n10.3390/electronics8030292\n10.1155/2021/8829829\n10.22061/jecei.2022.8200.491\n10.1038/s41598-021-99015-3\n10.1109/UEMCON47517.2019.8993089\n10.1186/s40537-021-00444-8\n10.1186/s13634-021-00755-1\n10.1016/j.media.2021.102253\n10.1148/radiol.2020201491\n10.1109/ACCESS.2021.3086020\n10.3390/electronics11152296\n10.1109/ACCESS.2020.2994762\n10.1109/TMI.2020.2994459\n10.1007/s40477-020-00458-7\n10.1148/radiol.2020200490\n10.1007/s10489-020-01900-3\n10.1021/ci0342472\n10.1109/IACC.2016.25\n10.1016/S0031-3203(02)00121-8\n10.1109/TKDE.2009.191\n10.4018/978-1-60566-766-9.ch011\n10.1016/j.cmpb.2020.105608\n10.1108/IJPCC-06-2020-0060\n10.1007/978-3-540-75171-7_2\n10.1007/978-3-540-28650-9_5\n10.1093/nsr/nwx106\n10.1109/TMI.2020.2994908\n10.1109/TMI.2020.3000314\n10.1109/TMI.2020.2996645\n10.1038/s41746-021-00399-3\n10.1109/ACCESS.2020.3003810\n10.3390/app10165683\n10.1080/07391102.2020.1767212\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103869\n10.1038/nbt1206-1565\n10.1007/s12010-021-03728-0\n10.7326/M20-1495\n10.1016/j.bea.2021.100003\n10.1148/radiol.2020200432\n10.1016/j.clinimag.2021.02.003\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1148/rg.2020200159\n10.1016/j.clinimag.2020.04.001\n10.1016/j.chaos.2020.109947\n10.1186/s12890-020-01286-5\n10.1016/j.chaos.2020.109944\n10.1016/j.compbiomed.2020.103805\n10.1007/s10489-020-01829-7\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105532\n10.1016/j.cmpb.2020.105581\n10.1007/s40846-020-00529-4\n10.3390/sym12040651\n10.1007/s13246-020-00865-4\n10.1016/j.imu.2020.100360\n10.1007/s00264-020-04609-7\n10.1007/s13246-020-00888-x\n10.1109/TMI.2020.2993291\n10.1016/j.imu.2020.100405\n10.3389/fmed.2020.00427\n10.3892/etm.2020.8797\n10.1016/j.asoc.2020.106580\n10.1007/s10489-020-01867-1\n10.1016/j.radi.2020.10.018\n10.1007/s00521-020-05636-6\n10.1016/j.bbe.2020.08.008\n10.1016/j.chaos.2020.110245\n10.1007/s12652-020-02688-3\n10.1016/j.compbiomed.2021.104252\n10.1007/s11571-021-09712-y\n10.1016/j.compbiomed.2021.104927\n10.1007/s10489-020-01714-3\n10.1080/07391102.2020.1788642\n10.1016/j.ejrad.2020.109041\n10.1038/s41467-020-17971-2\n10.1016/j.compbiomed.2020.103795\n10.1109/TMI.2020.2995508\n10.21037/atm.2020.03.132\n10.1109/TMI.2020.2996256\n10.3390/e22050517\n10.1007/s00330-020-06956-w\n10.1016/j.irbm.2020.05.003\n10.1007/s10096-020-03901-z\n10.1007/s00259-020-04929-1\n10.1109/TMI.2020.2995108\n10.1109/ACCESS.2020.3005510\n10.1183/13993003.00775-2020\n10.2196/19569\n10.1007/s00330-021-07715-1\n10.1038/s41598-020-76282-0\n10.1109/TCBB.2021.3065361\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2021.104837\n10.1016/j.chaos.2021.111310\n10.1016/j.patcog.2021.107826\n10.1007/s00521-020-05410-8\n10.1007/s10916-018-1088-1\n10.34133/2021/8786793\n10.4018/978-1-7998-8929-8.ch001\n10.1007/s00530-021-00794-6\n10.1109/ICIRCA48905.2020.9183278\n10.1007/s10462-020-09825-6\n10.1002/acm2.13121\n10.1007/s00521-021-06344-5\n10.1002/mp.15419\n10.1038/s41598-021-88807-2\n10.2196/19673\n10.1109/TIP.2021.3058783\n10.1097/MCP.0000000000000765\n10.3390/diagnostics11071155\n10.1016/j.ultrasmedbio.2020.07.003\n10.1002/int.22504\n10.1016/j.acra.2020.04.032\n10.1016/j.mri.2021.03.005\n10.1117/1.JBO.19.1.010901\n10.1021/acsnano.1c05226\n10.3389/fmicb.2020.02014\n10.1016/B978-0-08-100040-3.00002-X\n10.1016/j.asoc.2021.107150\n10.1016/j.compbiomed.2020.104037\n10.1101/2020.03.19.20039354\n10.1177/0846537120913033\n10.1186/s13244-020-00933-z\n10.1016/j.ijid.2020.06.026\n10.1016/j.diii.2020.03.014\n10.1093/cid/ciaa247\n10.1007/s10462-021-09975-1\n10.1007/978-1-4419-9326-7_1\n10.1109/ICC40277.2020.9148817\n10.1148/radiol.2020200330"}
{"title": "A deep transfer learning-based convolution neural network model for COVID-19 detection using computed tomography scan images for medical applications.", "abstract": "The Coronavirus (COVID-19) has become a critical and extreme epidemic because of its international dissemination. COVID-19 is the world's most serious health, economic, and survival danger. This disease affects not only a single country but the entire planet due to this infectious disease. Illnesses of Covid-19 spread at a much faster rate than usual influenza cases. Because of its high transmissibility and early diagnosis, it isn't easy to manage COVID-19. The popularly used RT-PCR method for COVID-19 disease diagnosis may provide false negatives. COVID-19 can be detected non-invasively using medical imaging procedures such as chest CT and chest x-ray. Deep learning is the most effective machine learning approach for examining a considerable quantity of chest computed tomography (CT) pictures that can significantly affect Covid-19 screening. Convolutional neural network (CNN) is one of the most popular deep learning techniques right now, and its gaining traction due to its potential to transform several spheres of human life. This research aims to develop conceptual transfer learning enhanced CNN framework models for detecting COVID-19 with CT scan images. Though with minimal datasets, these techniques were demonstrated to be effective in detecting the presence of COVID-19. This proposed research looks into several deep transfer learning-based CNN approaches for detecting the presence of COVID-19 in chest CT images.VGG16, VGG19, Densenet121, InceptionV3, Xception, and Resnet50 are the foundation models used in this work. Each model's performance was evaluated using a confusion matrix and various performance measures such as accuracy, recall, precision, f1-score, loss, and ROC. The VGG16 model performed much better than the other models in this study (98.00 % accuracy). Promising outcomes from experiments have revealed the merits of the proposed model for detecting and monitoring COVID-19 patients. This could help practitioners and academics create a tool to help minimal health professionals decide on the best course of therapy.", "journal": "Advances in engineering software (Barking, London, England : 1992)", "date": "2022-11-01", "authors": ["Nirmala DeviKathamuthu", "ShanthiSubramaniam", "Quynh HoangLe", "SureshMuthusamy", "HiteshPanchal", "Suma Christal MarySundararajan", "Ali JawadAlrubaie", "Musaddak Maher AbdulZahra"], "doi": "10.1016/j.advengsoft.2022.103317"}
{"title": "A robust semantic lung segmentation study for CNN-based COVID-19 diagnosis.", "abstract": "This paper aims to diagnose COVID-19 by using Chest X-Ray (CXR) scan images in a deep learning-based system. First of all, COVID-19 Chest X-Ray Dataset is used to segment the lung parts in CXR images semantically. DeepLabV3+ architecture is trained by using the masks of the lung parts in this dataset. The trained architecture is then fed with images in the COVID-19 Radiography Database. In order to improve the output images, some image preprocessing steps are applied. As a result, lung regions are successfully segmented from CXR images. The next step is feature extraction and classification. While features are extracted with modified AlexNet (mAlexNet), Support Vector Machine (SVM) is used for classification. As a result, 3-class data consisting of Normal, Viral Pneumonia and COVID-19 class are classified with 99.8% success. Classification results show that the proposed method is superior to previous state-of-the-art methods.", "journal": "Chemometrics and intelligent laboratory systems : an international journal sponsored by the Chemometrics Society", "date": "2022-11-01", "authors": ["Muhammet FatihAslan"], "doi": "10.1016/j.chemolab.2022.104695\n10.1109/RBME.2020.2987975\n10.1007/s11760-022-02302-3\n10.1016/j.eng.2020.04.010"}
{"title": "Deep-learning-based hepatic fat assessment (DeHFt) on non-contrast chest CT and its association with disease severity in COVID-19 infections: A multi-site retrospective study.", "abstract": "Hepatic steatosis (HS) identified on CT may provide an integrated cardiometabolic and COVID-19 risk assessment. This study presents a deep-learning-based hepatic fat assessment (DeHFt) pipeline for (a) more standardised measurements and (b) investigating the association between HS (liver-to-spleen attenuation ratio <1 in CT) and COVID-19 infections severity, wherein severity is defined as requiring invasive mechanical ventilation, extracorporeal membrane oxygenation, death.\nDeHFt comprises two steps. First, a deep-learning-based segmentation model (3D residual-UNet) is trained (N.\u00df=.\u00df80) to segment the liver and spleen. Second, CT attenuation is estimated using slice-based and volumetric-based methods. DeHFt-based mean liver and liver-to-spleen attenuation are compared with an expert's ROI-based measurements. We further obtained the liver-to-spleen attenuation ratio in a large multi-site cohort of patients with COVID-19 infections (D1, N.\u00df=.\u00df805; D2, N.\u00df=.\u00df1917; D3, N.\u00df=.\u00df169) using the DeHFt pipeline and investigated the association between HS and COVID-19 infections severity.\nThe DeHFt pipeline achieved a dice coefficient of 0.95, 95% CI [0.93...0.96] on the independent validation cohort (N.\u00df=.\u00df49). The automated slice-based and volumetric-based liver and liver-to-spleen attenuation estimations strongly correlated with expert's measurement. In the COVID-19 cohorts, severe infections had a higher proportion of patients with HS than non-severe infections (pooled OR.\u00df=.\u00df1.50, 95% CI [1.20...1.88], P.\u00df<.\u00df.001).\nThe DeHFt pipeline enabled accurate segmentation of liver and spleen on non-contrast CTs and automated estimation of liver and liver-to-spleen attenuation ratio. In three cohorts of patients with COVID-19 infections (N.\u00df=.\u00df2891), HS was associated with disease severity. Pending validation, DeHFt provides an automated CT-based metabolic risk assessment.\nFor a full list of funding bodies, please see the Acknowledgements.", "journal": "EBioMedicine", "date": "2022-10-30", "authors": ["GouravModanwal", "SadeerAl-Kindi", "JonathanWalker", "RohanDhamdhere", "LeiYuan", "MengyaoJi", "ChengLu", "PingfuFu", "SanjayRajagopalan", "AnantMadabhushi"], "doi": "10.1016/j.ebiom.2022.104315\n10.1016/j.acra.2012.02.022\n10.48550/arXiv.1901.04056"}
{"title": "Wasserstein-based texture analysis in radiomic studies.", "abstract": "The emerging field of radiomics that transforms standard-of-care images to quantifiable scalar statistics endeavors to reveal the information hidden in these macroscopic images. The concept of texture is widely used and essential in many radiomic-based studies. Practice usually reduces spatial multidimensional texture matrices, e.g., gray-level co-occurrence matrices (GLCMs), to summary scalar features. These statistical features have been demonstrated to be strongly correlated and tend to contribute redundant information; and does not account for the spatial information hidden in the multivariate texture matrices. This study proposes a novel pipeline to deal with spatial texture features in radiomic studies. A new set of textural features that preserve the spatial information inherent in GLCMs is proposed and used for classification purposes. The set of the new features uses the Wasserstein metric from optimal mass transport theory (OMT) to quantify the spatial similarity between samples within a given label class. In particular, based on a selected subset of texture GLCMs from the training cohort, we propose new representative spatial texture features, which we incorporate into a supervised image classification pipeline. The pipeline relies on the support vector machine (SVM) algorithm along with Bayesian optimization and the Wasserstein metric. The selection of the best GLCM references is considered for each classification label and is performed during the training phase of the SVM classifier using a Bayesian optimizer. We assume that sample fitness is defined based on closeness (in the sense of the Wasserstein metric) and high correlation (Spearman's rank sense) with other samples in the same class. Moreover, the newly defined spatial texture features consist of the Wasserstein distance between the optimally selected references and the remaining samples. We assessed the performance of the proposed classification pipeline in diagnosing the coronavirus disease 2019 (COVID-19) from computed tomographic (CT) images. To evaluate the proposed spatial features' added value, we compared the performance of the proposed classification pipeline with other SVM-based classifiers that account for different texture features, namely: statistical features only, optimized spatial features using Euclidean metric, non-optimized spatial features with Wasserstein metric. The proposed technique, which accounts for the optimized spatial texture feature with Wasserstein metric, shows great potential in classifying new COVID CT images that the algorithm has not seen in the training step. The MATLAB code of the proposed classification pipeline is made available. It can be used to find the best reference samples in other data cohorts, which can then be employed to build different prediction models.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-10-30", "authors": ["ZehorBelkhatir", "Ra\u00fal San Jos\u00e9Est\u00e9par", "Allen RTannenbaum"], "doi": "10.1016/j.compmedimag.2022.102129"}
{"title": "Conceptualising a channel-based overlapping CNN tower architecture for COVID-19 identification from CT-scan images.", "abstract": "Convolutional Neural Network (CNN) has been employed in classifying the COVID cases from the lungs' CT-Scan with promising quantifying metrics. However, SARS COVID-19 has been mutated, and we have many versions of the virus B.1.1.7, B.1.135, and P.1, hence there is a need for a more robust architecture that will classify the COVID positive patients from COVID negative patients with less training. We have developed a neural network based on the number of channels present in the images. The CNN architecture is developed in accordance with the number of the channels present in the dataset and are extracting the features separately from the channels present in the CT-Scan dataset. In the tower architecture, the first tower is dedicated for only the first channel present in the image; the second CNN tower is dedicated to the first and second channel feature maps, and finally the third channel takes account of all the feature maps from all three channels. We have used two datasets viz. one from Tongji Hospital, Wuhan, China and another SARS-CoV-2 dataset to train and evaluate our CNN architecture. The proposed model brought about an average accuracy of 99.4%, F1 score 0.988, and AUC 0.99.", "journal": "Scientific reports", "date": "2022-10-29", "authors": ["Ravi ShekharTiwari", "LakshmiD", "Tapan KumarDas", "KathiravanSrinivasan", "Chuan-YuChang"], "doi": "10.1038/s41598-022-21700-8\n10.1148/ryct.2020200034\n10.1038/s41598-021-95561-y\n10.1148/radiol.2020200642\n10.1155/2017/8314740\n10.1164/rccm.201705-0860OC\n10.1007/s00330-020-06801-0\n10.1038/s41598-021-93832-2\n10.1016/j.imu.2020.100427\n10.1088/2632-2153/abf22c\n10.1007/s00330-021-07715-1\n10.1007/s10140-020-01886-y\n10.1007/s10489-020-01943-6\n10.1038/s41598-020-74164-z\n10.1038/s41467-020-18685-1\n10.1007/s10916-021-01747-2\n10.1007/s10916-020-01562-1\n10.1101/2020.04.24.20078584\n10.5201/ipol.2021.344\n10.1109/ACCESS.2020.3037073"}
{"title": "Artificial Intelligence and Deep Learning Assisted Rapid Diagnosis of COVID-19 from Chest Radiographical Images: A Survey.", "abstract": "Artificial Intelligence (AI) has been applied successfully in many real-life domains for solving complex problems. With the invention of Machine Learning (ML) paradigms, it becomes convenient for researchers to predict the outcome based on past data. Nowadays, ML is acting as the biggest weapon against the COVID-19 pandemic by detecting symptomatic cases at an early stage and warning people about its futuristic effects. It is observed that COVID-19 has blown out globally so much in a short period because of the shortage of testing facilities and delays in test reports. To address this challenge, AI can be effectively applied to produce fast as well as cost-effective solutions. Plenty of researchers come up with AI-based solutions for preliminary diagnosis using chest CT Images, respiratory sound analysis, voice analysis of symptomatic persons with asymptomatic ones, and so forth. Some AI-based applications claim good accuracy in predicting the chances of being COVID-19-positive. Within a short period, plenty of research work is published regarding the identification of COVID-19. This paper has carefully examined and presented a comprehensive survey of more than 110 papers that came from various reputed sources, that is, Springer, IEEE, Elsevier, MDPI, arXiv, and medRxiv. Most of the papers selected for this survey presented candid work to detect and classify COVID-19, using deep-learning-based models from chest X-Rays and CT scan images. We hope that this survey covers most of the work and provides insights to the research community in proposing efficient as well as accurate solutions for fighting the pandemic.", "journal": "Contrast media & molecular imaging", "date": "2022-10-29", "authors": ["DeepakSinwar", "Vijaypal SinghDhaka", "Biniyam AlemuTesfaye", "GhanshyamRaghuwanshi", "AshishKumar", "Sunil KrMaakar", "SanjayAgrawal"], "doi": "10.1155/2022/1306664\n10.1016/j.jbi.2008.05.013\n10.1038/s41591-020-0931-3\n10.1016/j.asoc.2020.106282\n10.1016/j.chaos.2020.110055\n10.1007/s42979-021-00923-y\n10.3390/ai1020009\n10.1016/j.chaos.2020.110059\n10.1016/S2214-109X(20)30068-1\n10.1016/j.clinimag.2020.02.008\n10.1148/ryct.2020200082\n10.1007/978-3-030-00889-5_1\n10.1101/2020.03.12.20027185\n10.1109/TCBB.2021.3065361\n10.1016/j.cell.2020.04.045\n10.1155/2022/7377502\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103795\n10.1038/s41598-020-76282-0\n10.1007/s00330-021-07715-1\n10.1016/j.irbm.2020.05.003\n10.1007/s00330-020-06713-z\n10.1016/j.ejrad.2020.109041\n10.1109/RBME.2020.2987975\n10.12669/pjms.36.covid19-s4.2778\n10.1155/2022/8549707\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1016/j.chaos.2020.109944\n10.1155/2019/4180949\n10.1038/s41598-020-76550-z\n10.37896/jxu14.8/061\n10.1007/s10489-020-01867-1\n10.1007/s40846-020-00529-4\n10.1109/CVPR.2018.00474\n10.1016/j.compbiomed.2020.103805\n10.1016/j.compbiomed.2020.103792\n10.1007/s10044-021-00984-y\n10.1109/CVPR.2015.7298594\n10.1089/pop.2014.0089\n10.33889/ijmems.2020.5.4.052\n10.1109/CVPR.2017.243\n10.1016/j.cmpb.2020.105581\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2020.3010287\n10.1371/journal.pmed.1002686\n10.1007/s13246-020-00865-4\n10.1016/j.chaos.2020.110071\n10.1016/j.cmpb.2020.105608\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103869\n10.1016/j.cmpb.2020.105532\n10.1007/s00330-020-07044-9\n10.1007/s10489-020-01826-w\n10.1007/s10489-020-01831-z\n10.1007/s00264-020-04609-7\n10.1007/s00500-020-05275-y\n10.1155/2022/9697285\n10.1007/s42979-021-00823-1\n10.1101/2020.04.02.20051136v1\n10.1101/2020.04.02.20051136\n10.1088/1361-6560/abe838\n10.1007/s41870-022-00949-2\n10.1101/2020.02.29.20029603\n10.1101/2020.03.25.20043331\n10.1101/2020.04.04.20052092\n10.1016/j.chaos.2020.110086\n10.1186/s12859-018-2340-x\n10.3390/a13100249\n10.1007/s41870-022-00967-0\n10.1101/2020.07.02.20145474\n10.1186/s12911-020-01266-z\n10.1101/2020.05.20.20107847\n10.1101/2020.06.25.20140004\n10.1101/2020.06.01.20119560\n10.1080/09720502.2020.1833443\n10.1016/j.patter.2020.100145\n10.1101/2020.05.23.20110189\n10.1016/j.chaos.2020.110058\n10.1016/j.iot.2020.100222\n10.1016/j.dsx.2020.04.012\n10.1007/s41870-022-00973-2\n10.1016/j.dsx.2020.04.032\n10.1016/S2589-7500(20)30059-5\n10.1007/s42247-020-00102-4\n10.1109/access.2020.2992341\n10.1088/1757-899x/1099/1/012005\n10.1007/s00259-020-04953-1\n10.1007/s10489-020-01770-9\n10.1007/s42979-020-00410-w\n10.1007/s41870-020-00571-0\n10.1101/2020.04.24.20078584"}
{"title": "A CNN-transformer fusion network for COVID-19 CXR image classification.", "abstract": "The global health crisis due to the fast spread of coronavirus disease (Covid-19) has caused great danger to all aspects of healthcare, economy, and other aspects. The highly infectious and insidious nature of the new coronavirus greatly increases the difficulty of outbreak prevention and control. The early and rapid detection of Covid-19 is an effective way to reduce the spread of Covid-19. However, detecting Covid-19 accurately and quickly in large populations remains to be a major challenge worldwide. In this study, A CNN-transformer fusion framework is proposed for the automatic classification of pneumonia on chest X-ray. This framework includes two parts: data processing and image classification. The data processing stage is to eliminate the differences between data from different medical institutions so that they have the same storage format; in the image classification stage, we use a multi-branch network with a custom convolution module and a transformer module, including feature extraction, feature focus, and feature classification sub-networks. Feature extraction subnetworks extract the shallow features of the image and interact with the information through the convolution and transformer modules. Both the local and global features are extracted by the convolution module and transformer module of feature-focus subnetworks, and are classified by the feature classification subnetworks. The proposed network could decide whether or not a patient has pneumonia, and differentiate between Covid-19 and bacterial pneumonia. This network was implemented on the collected benchmark datasets and the result shows that accuracy, precision, recall, and F1 score are 97.09%, 97.16%, 96.93%, and 97.04%, respectively. Our network was compared with other researchers' proposed methods and achieved better results in terms of accuracy, precision, and F1 score, proving that it is superior for Covid-19 detection. With further improvements to this network, we hope that it will provide doctors with an effective tool for diagnosing Covid-19.", "journal": "PloS one", "date": "2022-10-28", "authors": ["KaiCao", "TaoDeng", "ChuanlinZhang", "LimengLu", "LinLi"], "doi": "10.1371/journal.pone.0276758\n10.1155/2021/2560388\n10.1001/jama.2020.1585\n10.1148/radiol.2020200432\n10.1155/2022/1465173\n10.1002/mp.13264\n10.1148/radiol.2020200463\n10.1136/bmj.m1328\n10.1016/j.media.2017.07.005\n10.1109/TPAMI.2019.2913372\n10.1016/j.compbiomed.2020.103792\n10.3389/fmed.2020.00427/full\n10.1016/j.media.2020.101794\n10.1038/s41598-020-76550-z\n10.3389/fpubh.2022.948205/full\n10.3389/fpubh.2022.948205\n10.1155/2022/4254631\n10.1016/j.mlwa.2021.100138\n10.1007/978-981-33-4673-4_55\n10.1148/radiol.2021203957\n10.1007/s10278-013-9622-7"}
{"title": "Checklist for Artificial Intelligence in Medical Imaging Reporting Adherence in Peer-Reviewed and Preprint Manuscripts With the Highest Altmetric Attention Scores: A Meta-Research Study.", "abstract": "", "journal": "Canadian Association of Radiologists journal = Journal l'Association canadienne des radiologistes", "date": "2022-10-28", "authors": ["UmasehSivanesan", "KayWu", "Matthew D FMcInnes", "KiretDhindsa", "FatemeSalehi", "Christian Bvan der Pol"], "doi": "10.1177/08465371221134056"}
{"title": "Classification and Detection of COVID-19 and Other Chest-Related Diseases Using Transfer Learning.", "abstract": "COVID-19 has infected millions of people worldwide over the past few years. The main technique used for COVID-19 detection is reverse transcription, which is expensive, sensitive, and requires medical expertise. X-ray imaging is an alternative and more accessible technique. This study aimed to improve detection accuracy to create a computer-aided diagnostic tool. Combining other artificial intelligence applications techniques with radiological imaging can help detect different diseases. This study proposes a technique for the automatic detection of COVID-19 and other chest-related diseases using digital chest X-ray images of suspected patients by applying transfer learning (TL) algorithms. For this purpose, two balanced datasets, Dataset-1 and Dataset-2, were created by combining four public databases and collecting images from recently published articles. Dataset-1 consisted of 6000 chest X-ray images with 1500 for each class. Dataset-2 consisted of 7200 images with 1200 for each class. To train and test the model, TL with nine pretrained convolutional neural networks (CNNs) was used with augmentation as a preprocessing method. The network was trained to classify using five classifiers: two-class classifier (normal and COVID-19); three-class classifier (normal, COVID-19, and viral pneumonia), four-class classifier (normal, viral pneumonia, COVID-19, and tuberculosis (Tb)), five-class classifier (normal, bacterial pneumonia, COVID-19, Tb, and pneumothorax), and six-class classifier (normal, bacterial pneumonia, COVID-19, viral pneumonia, Tb, and pneumothorax). For two, three, four, five, and six classes, our model achieved a maximum accuracy of 99.83, 98.11, 97.00, 94.66, and 87.29%, respectively.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-28", "authors": ["Muhammad TahirNaseem", "TajmalHussain", "Chan-SuLee", "Muhammad AdnanKhan"], "doi": "10.3390/s22207977\n10.1016/j.jaut.2020.102433\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2017.2788044\n10.1155/2022/6112815\n10.1007/s11042-019-07820-w\n10.1109/JBHI.2015.2425041\n10.1016/j.irbm.2020.05.003\n10.1016/j.compbiomed.2020.103795\n10.1007/s10916-021-01745-4\n10.1007/s10044-021-00984-y\n10.3390/sym12040651\n10.1007/s42600-021-00151-6\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1007/s11263-015-0816-y\n10.1016/j.chaos.2020.109944\n10.1016/j.cmpb.2020.105581\n10.1038/s41598-020-76550-z\n10.1016/j.eswa.2020.114054\n10.1007/s00264-020-04609-7\n10.1155/2018/2908517\n10.1016/j.cell.2018.02.010\n10.1109/TMI.2013.2284099\n10.1109/TMI.2013.2290491\n10.1016/j.compbiomed.2019.04.024\n10.1145/3065386\n10.1186/s40537-019-0192-5\n10.1016/j.compbiomed.2021.104608"}
{"title": "Machine Learning Sensors for Diagnosis of COVID-19 Disease Using Routine Blood Values for Internet of Things Application.", "abstract": "Healthcare digitalization requires effective applications of human sensors, when various parameters of the human body are instantly monitored in everyday life due to the Internet of Things (IoT). In particular, machine learning (ML) sensors for the prompt diagnosis of COVID-19 are an important option for IoT application in healthcare and ambient assisted living (AAL). Determining a COVID-19 infected status with various diagnostic tests and imaging results is costly and time-consuming. This study provides a fast, reliable and cost-effective alternative tool for the diagnosis of COVID-19 based on the routine blood values (RBVs) measured at admission. The dataset of the study consists of a total of 5296 patients with the same number of negative and positive COVID-19 test results and 51 routine blood values. In this study, 13 popular classifier machine learning models and the LogNNet neural network model were exanimated. The most successful classifier model in terms of time and accuracy in the detection of the disease was the histogram-based gradient boosting (HGB) (accuracy: 100%, time: 6.39 sec). The HGB classifier identified the 11 most important features (LDL, cholesterol, HDL-C, MCHC, triglyceride, amylase, UA, LDH, CK-MB, ALP and MCH) to detect the disease with 100% accuracy. In addition, the importance of single, double and triple combinations of these features in the diagnosis of the disease was discussed. We propose to use these 11 features and their binary combinations as important biomarkers for ML sensors in the diagnosis of the disease, supporting edge computing on Arduino and cloud IoT service.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-28", "authors": ["AndreiVelichko", "Mehmet TahirHuyut", "MaksimBelyaev", "YuriyIzotov", "DmitryKorzun"], "doi": "10.3390/s22207886\n10.4103/2045-9912.325992\n10.1080/00365513.2020.1855470\n10.12659/MSM.926178\n10.1016/j.intimp.2021.108127\n10.1016/j.intimp.2021.107838\n10.4103/2045-9912.326002\n10.1515/cclm-2020-1294\n10.1038/s41564-020-0761-6\n10.1038/s41379-020-00700-x\n10.1039/C7LC00955K\n10.1016/j.irbm.2022.05.006\n10.3390/s22134820\n10.1016/S0140-6736(20)30211-7\n10.1016/j.intimp.2022.108542\n10.2174/2666082218666220325105504\n10.1056/NEJMoa2002032\n10.1016/j.intimp.2020.106705\n10.5505/ejm.2022.35336\n10.1007/s12026-020-09145-5\n10.1016/S0140-6736(20)30628-0\n10.3389/fimmu.2019.00055\n10.1002/jmv.26543\n10.1186/1741-7015-11-185\n10.1016/j.cca.2020.03.022\n10.1002/jcla.23618\n10.1111/ijlh.13309\n10.1016/S0140-6736(20)30566-3\n10.19166/med.v8i2.3444\n10.1016/j.ajem.2020.12.076\n10.1016/j.compbiomed.2019.103375\n10.1111/bjh.16817\n10.1515/cclm-2020-0377\n10.1016/S0004-3702(03)00079-1\n10.1007/3-540-57868-4_57\n10.1007/978-3-540-88192-6_8\n10.1111/j.2517-6161.1996.tb02080.x\n10.1002/emp2.12205\n10.4018/978-1-6684-5295-0.CH047\n10.1145/3491208\n10.48550/arxiv.2206.03266\n10.3390/s22124362\n10.1186/s12916-020-01761-0\n10.1002/ajh.25847\n10.1515/cclm-2020-0398\n10.1101/2020.04.02.20051136\n10.1016/j.measurement.2019.05.027\n10.1109/EMBC.2012.6347211\n10.1016/j.alit.2019.04.010\n10.3390/app10010421\n10.1016/j.ekir.2017.11.002\n10.1016/j.cmpb.2018.12.032\n10.1093/bioinformatics/bty949\n10.1371/journal.pone.0194085\n10.1093/clinchem/hvaa200\n10.1007/s10916-020-01597-4\n10.1016/j.jointm.2021.04.001\n10.1371/journal.pone.0264785\n10.1186/s40560-021-00531-1\n10.1038/s41598-021-90265-9\n10.1038/s41591-020-0931-3\n10.1016/j.cca.2020.06.033\n10.1182/blood-2018-06-856500\n10.20944/preprints202002.0424.v1\n10.1016/j.prp.2021.153443\n10.1042/BSR20200817\n10.1080/10408363.2020.1776675\n10.1002/hep.31480\n10.1002/ajh.25829\n10.1007/s00277-021-04426-x\n10.1016/j.bbalip.2020.158849\n10.3389/fpubh.2021.705916\n10.1016/j.jacl.2020.04.008\n10.1093/bjs/znaa168\n10.1186/s12931-020-01427-8\n10.1186/s12902-021-00745-2\n10.1016/j.advms.2021.07.001\n10.22037/ghfbb.v13i4.2129\n10.1101/2020.02.27.20029009\n10.1016/j.adengl.2019.09.003\n10.1007/s13555-020-00372-0\n10.1093/jamia/ocaa258\n10.1109/ACCESS.2020.3045115\n10.1109/ACCESS.2022.3197164\n10.1016/j.matpr.2021.07.379\n10.4018/IJERTCS.2019040108"}
{"title": "Role of Drone Technology Helping in Alleviating the COVID-19 Pandemic.", "abstract": "The COVID-19 pandemic, caused by a new coronavirus, has affected economic and social standards as governments and healthcare regulatory agencies throughout the world expressed worry and explored harsh preventative measures to counteract the disease's spread and intensity. Several academics and experts are primarily concerned with halting the continuous spread of the unique virus. Social separation, the closing of borders, the avoidance of big gatherings, contactless transit, and quarantine are important methods. Multiple nations employ autonomous, digital, wireless, and other promising technologies to tackle this coronary pneumonia. This research examines a number of potential technologies, including unmanned aerial vehicles (UAVs), artificial intelligence (AI), blockchain, deep learning (DL), the Internet of Things (IoT), edge computing, and virtual reality (VR), in an effort to mitigate the danger of COVID-19. Due to their ability to transport food and medical supplies to a specific location, UAVs are currently being utilized as an innovative method to combat this illness. This research intends to examine the possibilities of UAVs in the context of the COVID-19 pandemic from several angles. UAVs offer intriguing options for delivering medical supplies, spraying disinfectants, broadcasting communications, conducting surveillance, inspecting, and screening patients for infection. This article examines the use of drones in healthcare as well as the advantages and disadvantages of strict adoption. Finally, challenges, opportunities, and future work are discussed to assist in adopting drone technology to tackle COVID-19-like diseases.", "journal": "Micromachines", "date": "2022-10-28", "authors": ["Syed Agha HassnainMohsan", "Qurat Ul AinZahra", "Muhammad AsgharKhan", "Mohammed HAlsharif", "Ismail AElhaty", "AbuJahid"], "doi": "10.3390/mi13101593\n10.1109/IOTM.0011.2100053\n10.1017/dmp.2021.9\n10.3390/drones6010015\n10.1007/s10796-021-10131-x\n10.3389/frcmn.2020.566853\n10.1080/14649365.2021.1921245\n10.1177/2043820620934267\n10.1016/j.future.2020.08.046\n10.1016/j.retram.2020.01.002\n10.2139/ssrn.3565463\n10.1109/ACCESS.2018.2875739\n10.1109/ACCESS.2019.2905347\n10.3390/drones4040068\n10.1109/TMM.2021.3075566\n10.1109/TITS.2021.3113787\n10.1109/TII.2022.3174160\n10.3389/fpubh.2022.855994\n10.1007/s11655-020-3192-6\n10.1038/s41598-020-73510-5\n10.1002/er.6007\n10.3390/drones6060147\n10.3390/rs11070820\n10.3390/rs12213539\n10.3390/mi13060977\n10.1016/j.iot.2020.100218\n10.1145/3001836\n10.1016/j.adhoc.2020.102324\n10.1080/17538947.2021.1952324\n10.1049/ntw2.12040\n10.3390/drones5030058\n10.1002/ett.4255\n10.3390/drones5010018\n10.3390/drones6050109\n10.3390/ijerph18052637\n10.53553/JCH.v09i01.009\n10.1016/j.trip.2021.100453\n10.1016/j.ijhm.2020.102758\n10.1016/j.ajic.2022.03.004\n10.1016/j.vaccine.2016.06.022\n10.3390/ijerph17239117\n10.28991/HIJ-2020-01-02-03\n10.1007/s13205-020-02581-y\n10.1155/2022/9718580\n10.1109/MNET.011.2000439\n10.1007/s10462-021-10106-z\n10.1007/s41666-020-00080-6\n10.1016/j.ceh.2020.03.001\n10.4108/eai.13-7-2018.163997\n10.1016/j.clineuro.2021.106655\n10.1109/MCE.2020.2992034\n10.1101/2020.04.06.20039909\n10.1002/ett.4245\n10.1108/IJPCC-05-2020-0046\n10.1016/j.scs.2020.102589\n10.1136/bmjsem-2020-000943\n10.1016/j.jnca.2022.103341\n10.1109/TII.2021.3101651\n10.1007/s10586-022-03722-z\n10.1016/j.ijhcs.2020.102573\n10.1007/s43926-021-00005-8\n10.1016/j.bspc.2022.103658\n10.3390/app12062828\n10.3389/fnbot.2022.840594\n10.1016/j.techfore.2020.120431\n10.32604/cmc.2022.021850\n10.1007/s00607-021-01022-9\n10.1186/s12909-020-02245-8\n10.3389/fneur.2021.646902\n10.1007/s12311-020-01139-1\n10.1002/adma.202103646\n10.1016/j.jnlssr.2020.06.011\n10.1109/JBHI.2021.3103404\n10.1109/IOTM.1100.2000068\n10.1109/MWC.001.2000429\n10.1109/JSEN.2022.3188929\n10.1109/ACCESS.2021.3133796\n10.1007/s00607-022-01064-7\n10.3991/ijim.v15i22.22623\n10.1007/s11036-018-1193-x\n10.3390/drones4040065\n10.1016/j.jnca.2016.12.012\n10.1109/JIOT.2020.3007518\n10.1111/1758-5899.13007"}
{"title": "Machine Learning and Deep Learning in Cardiothoracic Imaging: A Scoping Review.", "abstract": "Machine-learning (ML) and deep-learning (DL) algorithms are part of a group of modeling algorithms that grasp the hidden patterns in data based on a training process, enabling them to extract complex information from the input data. In the past decade, these algorithms have been increasingly used for image processing, specifically in the medical domain. Cardiothoracic imaging is one of the early adopters of ML/DL research, and the COVID-19 pandemic resulted in more research focus on the feasibility and applications of ML/DL in cardiothoracic imaging. In this scoping review, we systematically searched available peer-reviewed medical literature on cardiothoracic imaging and quantitatively extracted key data elements in order to get a big picture of how ML/DL have been used in the rapidly evolving cardiothoracic imaging field. During this report, we provide insights on different applications of ML/DL and some nuances pertaining to this specific field of research. Finally, we provide general suggestions on how researchers can make their research more than just a proof-of-concept and move toward clinical adoption.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-10-28", "authors": ["BardiaKhosravi", "PouriaRouzrokh", "ShahriarFaghani", "ManaMoassefi", "SanazVahdati", "ElhamMahmoudi", "HamidChalian", "Bradley JErickson"], "doi": "10.3390/diagnostics12102512\n10.1007/s12525-021-00475-2\n10.4997/jrcpe.2020.309\n10.1093/eurheartj/ehw302\n10.3389/fdata.2018.00006\n10.3348/kjr.2017.18.4.570\n10.1001/jama.2018.1150\n10.1007/s00256-021-03876-8\n10.1038/s42256-021-00305-2\n10.1016/j.acra.2021.12.032\n10.1007/s00330-021-07781-5\n10.11622/smedj.2019141\n10.1016/j.jacr.2017.12.021\n10.1097/RTI.0000000000000453\n10.1038/s41598-021-84698-5\n10.1016/j.acra.2018.10.007\n10.1259/bjro.20200037\n10.1007/s00247-021-05177-7\n10.1016/j.media.2017.06.015\n10.1007/978-981-16-3783-4_15\n10.7326/M18-0850\n10.1007/s10462-021-10106-z\n10.1145/3065386\n10.5555/1593511\n10.1186/s12874-021-01404-9\n10.1007/s00330-020-07450-z\n10.1148/radiol.2020200432\n10.1148/radiol.2020204226\n10.1016/j.compbiomed.2021.104304\n10.1148/ryai.2019180041\n10.1118/1.3528204\n10.1016/j.cmpb.2021.106373\n10.3348/kjr.2021.0148\n10.3390/s21217059\n10.1007/s00330-019-06628-4\n10.1609/aaai.v33i01.3301590\n10.1186/s41747-018-0068-z\n10.1186/s13244-020-00887-2\n10.1148/ryai.220010\n10.1016/j.cmpb.2019.05.020\n10.1016/j.compbiomed.2022.105466\n10.1007/s00431-021-04061-8\n10.1155/2021/6050433\n10.1016/j.cmpb.2022.106815\n10.1155/2022/4185835\n10.1016/j.ejmp.2019.11.026\n10.1002/mp.15019\n10.3390/tomography7040054\n10.1055/a-1717-2703\n10.1016/j.cmpb.2019.105288\n10.1016/j.media.2020.101823\n10.1016/j.compbiomed.2021.104689\n10.1109/TMI.2018.2833385\n10.1016/j.artmed.2020.101975\n10.1016/j.morpho.2019.09.002\n10.1002/mp.14066\n10.1016/j.media.2022.102362\n10.1109/TMI.2021.3053008\n10.1088/1361-6560/ab1cee\n10.1371/journal.pone.0244745\n10.1016/j.media.2022.102491\n10.1088/1361-6560/ab18db\n10.3233/XST-17358\n10.1148/ryai.220061\n10.1148/radiol.2018182294\n10.1148/rg.2020200040\n10.1016/j.ejmp.2021.08.011\n10.1016/j.lungcan.2021.01.027\n10.1016/j.media.2022.102389\n10.1097/RLI.0000000000000707\n10.1148/ryai.2020190043\n10.1148/ryai.2021200267\n10.1007/s00530-022-00960-4\n10.1148/ryai.210290\n10.1016/j.acra.2021.09.007\n10.1097/RLI.0000000000000763\n10.1145/3531146.3533193"}
{"title": "[The role of artificial intelligence in assessing the progression of fibrosing lung diseases].", "abstract": "The widespread use of artificial intelligence (AI) programs during the COVID-19 pandemic to assess the exact volume of lung tissue damage has allowed them to train a large number of radiologists. The simplicity of the program for determining the volume of the affected lung tissue in acute interstitial pneumonia, which has density indicators in the range from -200 HU to -730 HU, which includes the density indicators of \"ground glass\" and reticulation (the main radiation patterns in COVID-19) allows you to accurately determine the degree of prevalence process. The characteristics of chronic interstitial pneumonia, which are progressive in nature, fit into the same density framework. \u0410im. To \u0430ssess AI's ability to assess the progression of fibrosing lung disease using lung volume counting programs used for COVID-19 and chronic obstructive pulmonary disease.\nRetrospective analysis of computed tomography data during follow-up of 75 patients with progressive fibrosing lung disease made it possible to assess the prevalence and growth of interstitial lesions.\nUsing the experience of using AI programs to assess acute interstitial pneumonia in COVID-19 can be applied to chronic interstitial pneumonia.\n\u041f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u043f\u0435\u0440\u0438\u043e\u0434 \u043f\u0430\u043d\u0434\u0435\u043c\u0438\u0438 COVID-19 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c \u0438\u0441\u043a\u0443\u0441\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0430 (\u0418\u0418) \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043c\u0430 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u043e \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u0438\u043c \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0447\u0438\u0441\u043b\u043e \u0440\u0435\u043d\u0442\u0433\u0435\u043d\u043e\u043b\u043e\u0433\u043e\u0432. \u041f\u0440\u043e\u0441\u0442\u043e\u0442\u0430 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u043d\u043e\u0439 \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u043f\u0440\u0438 \u043e\u0441\u0442\u0440\u043e\u0439 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043f\u043d\u0435\u0432\u043c\u043e\u043d\u0438\u0438, \u0438\u043c\u0435\u044e\u0449\u0435\u0439 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u0432 \u043f\u0440\u043e\u043c\u0435\u0436\u0443\u0442\u043a\u0435 \u043e\u0442 -200 \u0435\u0434\u0438\u043d\u0438\u0446 \u0425\u0430\u0443\u043d\u0441\u0444\u0438\u043b\u0434\u0430 (HU) \u0434\u043e -730 HU, \u043a\u0443\u0434\u0430 \u0432\u0445\u043e\u0434\u044f\u0442 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u0438 \u043c\u0430\u0442\u043e\u0432\u043e\u0433\u043e \u0441\u0442\u0435\u043a\u043b\u0430 \u0438 \u0440\u0435\u0442\u0438\u043a\u0443\u043b\u044f\u0446\u0438\u0438 (\u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0445 \u043b\u0443\u0447\u0435\u0432\u044b\u0445 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u043e\u0432 \u043f\u0440\u0438 COVID-19), \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0442\u043e\u0447\u043d\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430. \u0412 \u0442\u0435 \u0436\u0435 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u043d\u044b\u0435 \u0440\u0430\u043c\u043a\u0438 \u0443\u043a\u043b\u0430\u0434\u044b\u0432\u0430\u044e\u0442\u0441\u044f \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u0439 \u043b\u0435\u0433\u043a\u0438\u0445, \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u0443\u044e\u0449\u0438\u0439 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043a\u0438\u0441\u0442\u043e\u0437\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u0431\u0441\u0447\u0435\u0442\u0430 \u044d\u043c\u0444\u0438\u0437\u0435\u043c\u044b \u0434\u0430\u0435\u0442 \u043f\u043e\u043b\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431 \u043e\u0431\u044a\u0435\u043c\u0435 \u0443\u0442\u0440\u0430\u0447\u0435\u043d\u043d\u043e\u0439 \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438. \u0426\u0435\u043b\u044c. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0418\u0418 \u0432 \u043e\u0446\u0435\u043d\u043a\u0435 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0431\u043e\u043b\u0435\u0437\u043d\u0435\u0439 \u043b\u0435\u0433\u043a\u0438\u0445. \u041c\u0430\u0442\u0435\u0440\u0438\u0430\u043b\u044b \u0438 \u043c\u0435\u0442\u043e\u0434\u044b. \u0420\u0435\u0442\u0440\u043e\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u043e\u0439 \u0442\u043e\u043c\u043e\u0433\u0440\u0430\u0444\u0438\u0438 \u043f\u0440\u0438 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0438 75 \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u0441 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u0443\u044e\u0449\u0438\u043c\u0438 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u043c\u0438 \u0431\u043e\u043b\u0435\u0437\u043d\u044f\u043c\u0438 \u043b\u0435\u0433\u043a\u0438\u0445 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0438 \u043d\u0430\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u0435 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \u0420\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b. \u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043e\u0431\u044a\u0435\u043c\u0430 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u043b\u0438 \u0442\u043e\u0447\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0431\u043e\u043b\u0435\u0437\u043d\u044f\u0445 \u043b\u0435\u0433\u043a\u0438\u0445 \u0438 \u043a\u043e\u0440\u0440\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043b\u0438 \u0441 \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0432\u0430\u0436\u043d\u044b\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0433\u043e \u0440\u0435\u0441\u0443\u0440\u0441\u0430 \u0434\u0438\u0444\u0444\u0443\u0437\u0438\u043e\u043d\u043d\u043e\u0439 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c\u044e \u043b\u0435\u0433\u043a\u0438\u0445. \u0421\u0442\u0435\u043f\u0435\u043d\u044c \u043d\u0430\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u0438 \u043e\u0431\u043e\u0441\u0442\u0440\u0435\u043d\u0438\u0438 \u0438\u043c\u0435\u043b\u0430 \u0432\u044b\u0441\u043e\u043a\u0443\u044e \u0441\u0432\u044f\u0437\u044c \u043f\u043e \u0448\u043a\u0430\u043b\u0435 \u0427\u0435\u0434\u0434\u043e\u043a\u0430 (p0,05; r=0,72) \u0441\u043e \u0441\u0442\u0435\u043f\u0435\u043d\u044c\u044e \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u044f \u0434\u0438\u0444\u0444\u0443\u0437\u0438\u043e\u043d\u043d\u043e\u0439 \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u0438 \u043b\u0435\u0433\u043a\u0438\u0445. \u0421\u043e\u0447\u0435\u0442\u0430\u043d\u0438\u0435 \u0438\u0434\u0438\u043e\u043f\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0433\u043e \u0444\u0438\u0431\u0440\u043e\u0437\u0430 \u0441 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0431\u0441\u0442\u0440\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0431\u043e\u043b\u0435\u0437\u043d\u044c\u044e \u043b\u0435\u0433\u043a\u0438\u0445, \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u043d\u043e\u0435 \u0443 23 (30,6%) \u043f\u0430\u0446\u0438\u0435\u043d\u0442\u043e\u0432, \u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u044c \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0442\u044f\u0436\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0441\u043e\u0442\u043e\u0432\u043e\u0433\u043e \u043b\u0435\u0433\u043a\u043e\u0433\u043e \u0441 \u043a\u0440\u0443\u043f\u043d\u044b\u043c \u0434\u0438\u0430\u043c\u0435\u0442\u0440\u043e\u043c \u0441\u043e\u0442, \u043e\u0442\u043c\u0435\u0447\u0435\u043d\u043d\u043e\u0433\u043e \u0443 42 (56%) \u0431\u043e\u043b\u044c\u043d\u044b\u0445, \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043b\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0441\u043e\u0447\u0435\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0434\u0441\u0447\u0435\u0442\u0430 \u043e\u0431\u044a\u0435\u043c\u0430 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u0439 \u0442\u043a\u0430\u043d\u0438 \u0441 \u0432\u044b\u0447\u043b\u0435\u043d\u0435\u043d\u0438\u0435\u043c \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u0435\u0439, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0445 \u043a\u0430\u043a \u0434\u043b\u044f COVID-19-\u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043b\u0435\u0433\u043a\u0438\u0445, \u0442\u0430\u043a \u0438 \u0434\u043b\u044f \u043a\u0438\u0441\u0442\u043e\u0437\u043d\u043e-\u044d\u043c\u0444\u0438\u0437\u0435\u043c\u0430\u0442\u043e\u0437\u043d\u043e\u0433\u043e \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044f \u0442\u043e\u0447\u043d\u043e \u043e\u0446\u0435\u043d\u0438\u0442\u044c \u0441\u0442\u0435\u043f\u0435\u043d\u044c \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0438 \u043a\u0438\u0441\u0442\u043e\u0437\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u043f\u0440\u0438 \u0438\u0434\u0438\u043e\u043f\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u043c \u043b\u0435\u0433\u043e\u0447\u043d\u043e\u043c \u0444\u0438\u0431\u0440\u043e\u0437\u0435. \u0417\u0430\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435. \u041e\u043f\u044b\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c \u0418\u0418 \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0440\u0430\u0441\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u043e\u0441\u0442\u0440\u043e\u0439 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0439 \u043f\u043d\u0435\u0432\u043c\u043e\u043d\u0438\u0438 \u043f\u0440\u0438 COVID-19 \u0438 \u044d\u043c\u0444\u0438\u0437\u0435\u043c\u044b \u043f\u0440\u0438 \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u043e\u0431\u0441\u0442\u0440\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0439 \u0431\u043e\u043b\u0435\u0437\u043d\u0438 \u043b\u0435\u0433\u043a\u0438\u0445 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0445\u0440\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0444\u0438\u0431\u0440\u043e\u0437\u0438\u0440\u0443\u044e\u0449\u0438\u0445 \u0438\u043d\u0442\u0435\u0440\u0441\u0442\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u0431\u043e\u043b\u0435\u0432\u0430\u043d\u0438\u0439 \u043b\u0435\u0433\u043a\u0438\u0445, \u0447\u0442\u043e \u0432\u0430\u0436\u043d\u043e \u0434\u043b\u044f \u0441\u0432\u043e\u0435\u0432\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0433\u043e \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0430\u043d\u0442\u0438\u0444\u0438\u0431\u0440\u043e\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0442\u0435\u0440\u0430\u043f\u0438\u0438.", "journal": "Terapevticheskii arkhiv", "date": "2022-10-27", "authors": ["A ASperanskaia"], "doi": "10.26442/00403660.2022.03.201407"}
{"title": "Comprehensive Survey of Machine Learning Systems for COVID-19 Detection.", "abstract": "The last two years are considered the most crucial and critical period of the COVID-19 pandemic affecting most life aspects worldwide. This virus spreads quickly within a short period, increasing the fatality rate associated with the virus. From a clinical perspective, several diagnosis methods are carried out for early detection to avoid virus propagation. However, the capabilities of these methods are limited and have various associated challenges. Consequently, many studies have been performed for COVID-19 automated detection without involving manual intervention and allowing an accurate and fast decision. As is the case with other diseases and medical issues, Artificial Intelligence (AI) provides the medical community with potential technical solutions that help doctors and radiologists diagnose based on chest images. In this paper, a comprehensive review of the mentioned AI-based detection solution proposals is conducted. More than 200 papers are reviewed and analyzed, and 145 articles have been extensively examined to specify the proposed AI mechanisms with chest medical images. A comprehensive examination of the associated advantages and shortcomings is illustrated and summarized. Several findings are concluded as a result of a deep analysis of all the previous works using machine learning for COVID-19 detection, segmentation, and classification.", "journal": "Journal of imaging", "date": "2022-10-27", "authors": ["BayanAlsaaidah", "Moh'd RasoulAl-Hadidi", "HebaAl-Nsour", "RajaMasadeh", "NaelAlZubi"], "doi": "10.3390/jimaging8100267\n10.1177/2347631120983481\n10.1016/j.jbusres.2020.05.030\n10.1080/16549716.2020.1788263\n10.11591/ijece.v10i5.pp4738-4744\n10.1148/rg.2017160130\n10.1016/j.jmir.2019.09.005\n10.1147/rd.33.0210\n10.1109/MCE.2016.2640698\n10.25046/aj030435\n10.1007/s10514-022-10039-8\n10.1016/j.crad.2018.05.015\n10.1038/nature14539\n10.1021/acs.chas.0c00075\n10.1561/2000000039\n10.1016/j.neunet.2014.09.003\n10.1109/TSMC.1971.4308320\n10.1109/5.726791\n10.1016/j.knosys.2018.10.034\n10.1145/2347736.2347755\n10.1016/j.jinf.2020.04.004\n10.1016/j.imu.2020.100449\n10.1038/s41598-021-93719-2\n10.3390/s22062224\n10.1093/clinchem/hvaa200\n10.1016/j.chaos.2020.110120\n10.1007/s10462-021-10106-z\n10.1016/j.eswa.2022.116540\n10.34306/ajri.v3i2.659\n10.1111/exsy.12759\n10.1007/s11042-020-09894-3\n10.1109/TIP.2021.3058783\n10.1109/JBHI.2021.3074893\n10.1007/s12559-020-09785-7\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3010287\n10.1016/j.cmpb.2020.105608\n10.1109/JBHI.2020.3037127\n10.1016/j.patrec.2020.09.010\n10.1007/s11042-021-10783-6\n10.3389/fmed.2020.00427\n10.1007/s10489-020-01826-w\n10.1038/s41598-020-74539-2\n10.1109/ACCESS.2020.2994762\n10.1145/3422622\n10.1016/j.compbiomed.2012.12.004\n10.1002/mp.14676\n10.1109/TMI.2020.2996645\n10.1016/j.patcog.2021.108109\n10.1155/2021/5544742\n10.3390/s21217116\n10.1016/j.compbiomed.2020.103869\n10.1016/j.compbiomed.2020.104066\n10.1016/j.compbiomed.2021.104348\n10.1016/j.compbiomed.2021.104781\n10.1016/j.chaos.2020.110170\n10.1016/j.eswa.2020.114054\n10.1016/j.aej.2021.06.024\n10.1016/j.compbiomed.2021.104572\n10.1016/j.imu.2020.100412\n10.20944/preprints202003.0300.v1\n10.1007/s11042-021-11299-9\n10.1101/2020.04.11.20054643\n10.1016/j.asoc.2021.108190\n10.1101/2020.05.10.20097063\n10.1101/2020.07.02.20136721\n10.1016/j.imu.2022.100945\n10.1101/2020.11.08.20228080\n10.1016/j.compmedimag.2021.102008\n10.1007/s40846-020-00529-4\n10.1101/2020.03.30.20047787\n10.1080/07391102.2020.1788642\n10.1080/07391102.2021.1875049\n10.1109/TII.2021.3057524\n10.1016/j.radi.2020.10.018\n10.1016/j.imu.2020.100505\n10.1016/j.inffus.2021.04.008\n10.1007/s42600-020-00091-7\n10.1007/s10489-020-02010-w\n10.3390/ijerph18158052\n10.1002/ima.22525\n10.1002/ima.22527\n10.31661/jbpe.v0i0.2008-1153\n10.1002/jemt.23713\n10.1371/journal.pone.0242899\n10.3390/jpm11010028\n10.1007/s40747-020-00199-4\n10.1007/s11760-020-01820-2\n10.1016/j.patcog.2021.107848\n10.1016/j.compbiomed.2020.104037\n10.1007/s10044-021-00984-y\n10.1152/physiolgenomics.00084.2020\n10.14299/ijser.2020.03.02\n10.20944/preprints202009.0524.v1\n10.1148/radiol.2020200905\n10.1007/s12539-020-00403-6\n10.1007/s00530-021-00826-1\n10.1186/s12938-020-00807-x\n10.1186/s12938-020-00831-x\n10.1038/s41598-020-76141-y\n10.1007/s40747-020-00216-6\n10.1007/s10096-020-03901-z\n10.1007/s10489-020-02149-6\n10.1007/s00259-020-04929-1\n10.1007/s00521-021-06219-9\n10.3390/sym12040651\n10.1109/JAS.2020.1003393\n10.1016/j.patcog.2020.107747\n10.1109/ACCESS.2021.3058854\n10.1016/j.bbe.2021.01.002\n10.18517/ijaseit.10.2.11446\n10.1109/TMI.2020.2995965\n10.1007/s10489-020-01829-7\n10.1007/s42600-020-00110-7\n10.1080/02664763.2020.1849057\n10.1016/j.compbiomed.2020.103792\n10.3390/bioengineering8020026\n10.1016/j.asoc.2022.108966\n10.1155/2022/7377502\n10.3389/fdgth.2021.662343\n10.1049/ipr2.12474"}
{"title": "Deep Transfer Learning for COVID-19 Detection and Lesion Recognition Using Chest CT Images.", "abstract": "Starting from December 2019, the global pandemic of coronavirus disease 2019 (COVID-19) is continuously expanding and has caused several millions of deaths worldwide. Fast and accurate diagnostic methods for COVID-19 detection play a vital role in containing the plague. Chest computed tomography (CT) is one of the most commonly used diagnosis methods. However, a complete CT-scan has hundreds of slices, and it is time-consuming for radiologists to check each slice to diagnose COVID-19. This study introduces a novel method for fast and automated COVID-19 diagnosis using the chest CT scans. The proposed models are based on the state-of-the-art deep convolutional neural network (CNN) architecture, and a 2D global max pooling (globalMaxPool2D) layer is used to improve the performance. We compare the proposed models to the existing state-of-the-art deep learning models such as CNN based models and vision transformer (ViT) models. Based off of metric such as area under curve (AUC), sensitivity, specificity, accuracy, and false discovery rate (FDR), experimental results show that the proposed models outperform the previous methods, and the best model achieves an area under curve of 0.9744 and accuracy 94.12% on our test datasets. It is also shown that the accuracy is improved by around 1% by using the 2D global max pooling layer. Moreover, a heatmap method to highlight the lesion area on COVID-19 chest CT images is introduced in the paper. This heatmap method is helpful for a radiologist to identify the abnormal pattern of COVID-19 on chest CT images. In addition, we also developed a freely accessible online simulation software for automated COVID-19 detection using CT images. The proposed deep learning models and software tool can be used by radiologist to diagnose COVID-19 more accurately and efficiently.", "journal": "Computational and mathematical methods in medicine", "date": "2022-10-27", "authors": ["SaiZhang", "Guo-ChangYuan"], "doi": "10.1155/2022/4509394\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1111/all.14238\n10.1016/j.jhin.2020.03.001\n10.1093/cid/ciaa344\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1007/s00330-020-06975-7\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06731-x\n10.1021/acssensors.0c02042\n10.1146/annurev-bioeng-071516-044442\n10.1007/978-1-4471-4929-3\n10.1155/2018/5157020\n10.1016/S2213-2600(18)30286-8\n10.1148/radiol.2020192154\n10.1136/gutjnl-2017-314547\n10.1093/mind/LIX.236.433\n10.1162/089976602760128018\n10.1155/2017/8314740\n10.1109/FG.2017.42\n10.1109/HealthCom.2017.8210843\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2020.103795\n10.1038/s41591-020-0931-3\n10.1016/j.ejrad.2020.109402\n10.1016/j.compbiomed.2021.104348\n10.1016/j.imu.2020.100427\n10.7717/peerj.10086\n10.1016/j.patrec.2020.10.001\n10.3390/healthcare10010085\n10.1109/TENSYMP52854.2021.9550819\n10.1007/978-1-4842-2766-4\n10.1109/ICEngTechnol.2017.8308186\n10.1145/3065386\n10.1609/aaai.v31i1.11231\n10.3390/s22082988\n10.1007/s11263-019-01228-7"}
{"title": "Chest radiograph classification and severity of suspected COVID-19 by different radiologist groups and attending clinicians: multi-reader, multi-case study.", "abstract": "To quantify reader agreement for the British Society of Thoracic Imaging (BSTI) diagnostic and severity classification for COVID-19 on chest radiographs (CXR), in particular agreement for an indeterminate CXR that could instigate CT imaging, from single and paired images.\nTwenty readers (four groups of five individuals)-consultant chest (CCR), general consultant (GCR), and specialist registrar (RSR) radiologists, and infectious diseases clinicians (IDR)-assigned BSTI categories and severity in addition to modified Covid-Radiographic Assessment of Lung Edema Score (Covid-RALES), to 305 CXRs (129 paired; 2 time points) from 176 guideline-defined COVID-19 patients. Percentage agreement with a consensus of two chest radiologists was calculated for (1) categorisation to those needing CT (indeterminate) versus those that did not (classic/probable, non-COVID-19); (2) severity; and (3) severity change on paired CXRs using the two scoring systems.\nAgreement with consensus for the indeterminate category was low across all groups (28-37%). Agreement for other BSTI categories was highest for classic/probable for the other three reader groups (66-76%) compared to GCR (49%). Agreement for normal was similar across all radiologists (54-61%) but lower for IDR (31%). Agreement for a severe CXR was lower for GCR (65%), compared to the other three reader groups (84-95%). For all groups, agreement for changes across paired CXRs was modest.\nAgreement for the indeterminate BSTI COVID-19 CXR category is low, and generally moderate for the other BSTI categories and for severity change, suggesting that the test, rather than readers, is limited in utility for both deciding disposition and serial monitoring.\n\u2022 Across different reader groups, agreement for COVID-19 diagnostic categorisation on CXR varies widely. \u2022 Agreement varies to a degree that may render CXR alone ineffective for triage, especially for indeterminate cases. \u2022 Agreement for serial CXR change is moderate, limiting utility in guiding management.", "journal": "European radiology", "date": "2022-10-26", "authors": ["ArjunNair", "AlexanderProcter", "SteveHalligan", "ThomasParry", "AsiaAhmed", "MarkDuncan", "MagaliTaylor", "ManilChouhan", "TrevorGaunt", "JamesRoberts", "Nielsvan Vucht", "AlanCampbell", "Laura MayDavis", "JosephJacob", "RachelHubbard", "ShankarKumar", "AmmaarahSaid", "XinhuiChan", "TimCutfield", "AkishLuintel", "MichaelMarks", "NeilStone", "SueMallet"], "doi": "10.1007/s00330-022-09172-w\n10.1016/j.crad.2020.03.008\n10.1148/radiol.2020201160\n10.1001/jamainternmed.2020.2033\n10.1097/RTI.0000000000000533\n10.1007/s11547-020-01272-1\n10.4269/ajtmh.20-0535\n10.1148/radiol.2020201754\n10.1007/s00330-020-07270-1\n10.1148/radiol.2020201874\n10.1038/s41598-020-79470-0\n10.1371/journal.pone.0242759\n10.1016/j.crad.2020.06.005\n10.1016/j.ejrad.2020.109272\n10.1186/s12916-020-01810-8\n10.1186/s43055-021-00541-x\n10.1183/13993003.01809-2020"}
{"title": "Diagnostic performance of corona virus disease 2019 chest computer tomography image recognition based on deep learning: Systematic review and meta-analysis.", "abstract": "To analyze the diagnosis performance of deep learning model used in corona virus disease 2019 (COVID-19) computer tomography(CT) chest scans. The included sample contains healthy people, confirmed COVID-19 patients and unconfirmed suspected patients with corresponding symptoms.\nPubMed, Web of Science, Wiley, China National Knowledge Infrastructure, WAN FANG DATA, and Cochrane Library were searched for articles. Three researchers independently screened the literature, extracted the data. Any differences will be resolved by consulting the third author to ensure that a highly reliable and useful research paper is produced. Data were extracted from the final articles, including: authors, country of study, study type, sample size, participant demographics, type and name of AI software, results (accuracy, sensitivity, specificity, ROC, and predictive values), other outcome(s) if applicable.\nAmong the 3891 searched results, 32 articles describing 51,392 confirmed patients and 7686 non-infected individuals met the inclusion criteria. The pooled sensitivity, the pooled specificity, positive likelihood ratio, negative likelihood ratio and the pooled diagnostic odds ratio (OR) is 0.87(95%CI [confidence interval]: 0.85, 0.89), 0.85(95%CI: 0.82, 0.87), 6.7(95%CI: 5.7, 7.8), 0.14(95%CI: 0.12, 0.16), and 49(95%CI: 38, 65). Further, the AUROC (area under the receiver operating characteristic curve) is 0.94(95%CI: 0.91, 0.96). Secondary outcomes are specific sensitivity and specificity within subgroups defined by different models. Resnet has the best diagnostic performance, which has the highest sensitivity (0.91[95%CI: 0.87, 0.94]), specificity (0.90[95%CI: 0.86, 0.93]) and AUROC (0.96[95%CI: 0.94, 0.97]), according to the AUROC, we can get the rank Resnet > Densenet > VGG > Mobilenet > Inception > Effficient > Alexnet.\nOur study findings show that deep learning models have immense potential in accurately stratifying COVID-19 patients and in correctly differentiating them from patients with other types of pneumonia and normal patients. Implementation of deep learning-based tools can assist radiologists in correctly and quickly detecting COVID-19 and, consequently, in combating the COVID-19 pandemic.", "journal": "Medicine", "date": "2022-10-26", "authors": ["QiaolanWang", "JingxuanMa", "LuoningZhang", "LinshenXie"], "doi": "10.1097/MD.0000000000031346"}
{"title": "Automated diagnosis of COVID-19 using radiological modalities and Artificial Intelligence functionalities: A retrospective study based on chest HRCT database.", "abstract": "The spread of coronavirus has been challenging for the healthcare system's proper management and diagnosis during the rapid spread and control of the infection. Real-time reverse transcription-polymerase chain reaction (RT-PCR), though considered the standard testing measure, has low sensitivity and is time-consuming, which restricts the fast screening of individuals. Therefore, computer tomography (CT) is used to complement the traditional approaches and provide fast and effective screening over other diagnostic methods. This work aims to appraise the importance of chest CT findings of COVID-19 and post-COVID in the diagnosis and prognosis of infected patients and to explore the ways and means to integrate CT findings for the development of advanced Artificial Intelligence (AI) tool-based predictive diagnostic techniques.\nThe retrospective study includes a 188 patient database with COVID-19 infection confirmed by RT-PCR testing, including post-COVID patients. Patients underwent chest high-resolution computer tomography (HRCT), where the images were evaluated for common COVID-19 findings and involvement of the lung and its lobes based on the coverage region. The radiological modalities analyzed in this study may help the researchers in generating a predictive model based on AI tools for further classification with a high degree of reliability.\nMild to moderate ground glass opacities (GGO) with or without consolidation, crazy paving patterns, and halo signs were common COVID-19 related findings. A CT score is assigned to every patient based on the severity of lung lobe involvement.\nTypical multifocal, bilateral, and peripheral distributions of GGO are the main characteristics related to COVID-19 pneumonia. Chest HRCT can be considered a standard method for timely and efficient assessment of disease progression and management severity. With its fusion with AI tools, chest HRCT can be used as a one-stop platform for radiological investigation and automated diagnosis system.", "journal": "Biomedical signal processing and control", "date": "2022-10-25", "authors": ["UpasanaBhattacharjya", "Kandarpa KumarSarma", "Jyoti PrakashMedhi", "Binoy KumarChoudhury", "GeetanjaliBarman"], "doi": "10.1016/j.bspc.2022.104297\n10.1101/2020.02\n10.1101/2020.03.12.20027185"}
{"title": "The Deep Learning-Based Framework for Automated Predicting COVID-19 Severity Score.", "abstract": "With the COVID-19 pandemic sweeping the globe, an increasing number of people are working on pandemic research, but there is less effort on predicting its severity. Diagnostic chest imaging is thought to be a quick and reliable way to identify the severity of COVID-19. We describe a deep learning method to automatically predict the severity score of patients by analyzing chest X-rays, with the goal of collaborating with doctors to create corresponding treatment measures for patients and can also be used to track disease change. Our model consists of a feature extraction phase and an outcome prediction phase. The feature extraction phase uses a DenseNet backbone network to extract 18 features related to lung diseases from CXRs; the outcome prediction phase, which employs the MLP regression model, selects several important features for prediction from the features extracted in the previous phase and demonstrates the effectiveness of our model by comparing it with several commonly used regression models. On a dataset of 2373 CXRs, our model predicts the geographic extent score with 1.02 MAE and the lung opacity score with 0.85 MAE.", "journal": "Procedia computer science", "date": "2022-10-25", "authors": ["YongchangZheng", "HongweiDong"], "doi": "10.1016/j.procs.2022.09.165"}
{"title": "Effects of long-term COVID-19 confinement and music stimulation on mental state and brain activity of young people.", "abstract": "The Corona Virus Disease 2019 (COVID-19) pandemic may have had a negative emotional impact on individuals. This study investigated the effect of long-term lockdown and music on young people's mood and neurophysiological responses in the prefrontal cortex (PFC). Fifteen healthy young adults were recruited and PFC activation was acquired using functional near-infrared spectroscopy during the conditions of resting, Stroop and music stimulation. The Depression Anxiety Stress Scales mental scale scores were simultaneously recorded. Mixed effect models, paired t-tests, one-way ANOVAs and Spearman analyses were adopted to analyse the experimental parameters. Stress, anxiety and depression levels increased significantly from Day 30 to Day 40. In terms of reaction time, both Stroop1 and Stroop2 were faster on Day 40 than on Day 30 (P\u00a0=\u00a00.01, P\u00a0=\u00a00.003). The relative concentration changes of oxyhemoglobin were significantly higher during premusic conditions than music stimulation and postmusic Stroop. The intensity of functional connectivity shifted from inter- to intracerebral over time. In conclusion, the reduced hemodynamic response of the PFC in healthy young adults is associated with negative emotions, especially anxiety, during lockdown. Immediate music stimulation appears to improve efficiency by altering the pattern of connections in PFC.", "journal": "Neuroscience letters", "date": "2022-10-23", "authors": ["LinaLuo", "MianjiaShan", "YangminZu", "YufangChen", "LingguoBu", "LejunWang", "MingNi", "WenxinNiu"], "doi": "10.1016/j.neulet.2022.136922"}
{"title": "Automatic deep learning-based consolidation/collapse classification in lung ultrasound images for COVID-19 induced pneumonia.", "abstract": "Our automated deep learning-based approach identifies consolidation/collapse in LUS images to aid in the identification of late stages of COVID-19 induced pneumonia, where consolidation/collapse is one of the possible associated pathologies. A common challenge in training such models is that annotating each frame of an ultrasound video requires high labelling effort. This effort in practice becomes prohibitive for large ultrasound datasets. To understand the impact of various degrees of labelling precision, we compare labelling strategies to train fully supervised models (frame-based method, higher labelling effort) and inaccurately supervised models (video-based methods, lower labelling effort), both of which yield binary predictions for LUS videos on a frame-by-frame level. We moreover introduce a novel sampled quaternary method which randomly samples only 10% of the LUS video frames and subsequently assigns (ordinal) categorical labels to all frames in the video based on the fraction of positively annotated samples. This method outperformed the inaccurately supervised video-based method and more surprisingly, the supervised frame-based approach with respect to metrics such as precision-recall area under curve (PR-AUC) and F1 score, despite being a form of inaccurate learning. We argue that our video-based method is more robust with respect to label noise and mitigates overfitting in a manner similar to label smoothing. The algorithm was trained using a ten-fold cross validation, which resulted in a PR-AUC score of 73% and an accuracy of 89%. While the efficacy of our classifier using the sampled quaternary method significantly lowers the labelling effort, it must be verified on a larger consolidation/collapse dataset, our proposed classifier using the sampled quaternary video-based method is clinically comparable with trained experts' performance.", "journal": "Scientific reports", "date": "2022-10-21", "authors": ["NabeelDurrani", "DamjanVukovic", "Jeroenvan der Burgt", "MariaAntico", "Ruud J Gvan Sloun", "DavidCanty", "MarianSteffens", "AndrewWang", "AlistairRoyse", "ColinRoyse", "KaviHaji", "JasonDowling", "GirijaChetty", "DavideFontanarosa"], "doi": "10.1038/s41598-022-22196-y\n10.1016/j.compbiomed.2021.104742\n10.1186/s12931-020-01504-y\n10.1016/j.hrtlng.2021.02.015\n10.1007/s12630-020-01704-6\n10.1111/1742-6723.13546\n10.1186/s13089-018-0103-6\n10.1016/j.crad.2020.05.001\n10.1109/TMI.2021.3117246\n10.1109/TUFFC.2021.3070696\n10.1109/TMI.2020.2994459\n10.1109/TUFFC.2022.3161716\n10.1016/j.imu.2021.100687\n10.1109/TUFFC.2020.3002249\n10.1109/TUFFC.2020.3005512\n10.1002/jum.16052\n10.1038/s41598-021-90153-2\n10.1016/j.ejmp.2021.02.023\n10.1186/s13063-019-4003-2\n10.1002/jum.15548\n10.1097/00005382-199621000-00002\n10.1093/nsr/nwx106\n10.1118/1.3611983\n10.2307/2095465\n10.1016/j.ipl.2005.11.003\n10.1371/journal.pone.0118432\n10.1007/978-3-642-40994-3_29\n10.11613/BM.2012.031"}
{"title": "COVID19 Diagnosis Using Chest X-rays and Transfer Learning.", "abstract": "A pandemic of respiratory illnesses from a novel coronavirus known as Sars-CoV-2 has swept across the globe since December of 2019. This is calling upon the research community including medical imaging to provide effective tools for use in combating this virus. Research in biomedical imaging of viral patients is already very active with machine learning models being created for diagnosing Sars-CoV-2 infections in patients using CT scans and chest x-rays. We aim to build upon this research. Here we used a transfer-learning approach to develop models capable of diagnosing COVID19 from chest x-ray. For this work we compiled a dataset of 112120 negative images from the Chest X-Ray 14 and 2725 positive images from public repositories. We tested multiple models, including logistic regression and random forest and XGBoost with and without principal components analysis, using five-fold cross-validation to evaluate recall, precision, and f1-score. These models were compared to a pre-trained deep-learning model for evaluating chest x-rays called COVID-Net. Our best model was XGBoost with principal components with a recall, precision, and f1-score of 0.692, 0.960, 0.804 respectively. This model greatly outperformed COVID-Net which scored 0.987, 0.025, 0.048. This model, with its high precision and reasonable sensitivity, would be most useful as \"rule-in\" test for COVID19. Though it outperforms some chemical assays in sensitivity, this model should be studied in patients who would not ordinarily receive a chest x-ray before being used for screening.\nLife and Medical Sciences \u2022 Machine Learning \u2022 Artificial Intelligence.\nJonathan Stubblefield, Jason Causey, Dakota Dale, Jake Qualls, Emily Bellis, Jennifer Fowler, Karl Walker and Xiuzhen Huang. 2022. COVID19 Diagnosis Using Chest X-Rays and Transfer Learning.", "journal": "medRxiv : the preprint server for health sciences", "date": "2022-10-21", "authors": ["JonathanStubblefield", "JasonCausey", "DakotaDale", "JakeQualls", "EmilyBellis", "JenniferFowler", "KarlWalker", "XiuzhenHuang"], "doi": "10.1101/2022.10.09.22280877\n10.1145/2939672.2939785\n10.1109/CVPR.2017.369\n10.1101/2020.02.25.20021568\n10.1101/2020.04.11.20062091\n10.3390/diagnostics10090669\n10.7937/91ah-v663\n10.1148/radiol.2021203957\n10.1007/s10278-013-9622-7\n10.1613/jair.953"}
{"title": "Role of air pollutants in dengue fever incidence: evidence from two southern cities in Taiwan.", "abstract": "Air pollution may be involved in spreading dengue fever (DF) besides rainfalls and warmer temperatures. While particulate matter (PM), especially those with diameter of 10\u2009\u03bcm (PM10) or 2.5\u2009\u03bcm or less (PM25), and NO2 increase the risk of coronavirus 2 infection, their roles in triggering DF remain unclear. We explored if air pollution factors predict DF incidence in addition to the classic climate factors. Public databases and DF records of two southern cities in Taiwan were used in regression analyses. Month order, PM10 minimum, PM2.5 minimum, and precipitation days were retained in the enter mode model, and SO2 minimum, O3 maximum, and CO minimum were retained in the stepwise forward mode model in addition to month order, PM10 minimum, PM2.5 minimum, and precipitation days. While PM2.5 minimum showed a negative contribution to the monthly DF incidence, other variables showed the opposite effects. The sustain of month order, PM10 minimum, PM2.5 minimum, and precipitation days in both regression models confirms the role of classic climate factors and illustrates a potential biological role of the air pollutants in the life cycle of mosquito vectors and dengue virus and possibly human immune status. Future DF prevention should concern the contribution of air pollution besides the classic climate factors.", "journal": "Pathogens and global health", "date": "2022-10-21", "authors": ["Hao-ChunLu", "Fang-YuLin", "Yao-HueiHuang", "Yu-TungKao", "El-WuiLoh"], "doi": "10.1080/20477724.2022.2135711"}
{"title": "Automated system for classification of COVID-19 infection from lung CT images based on machine learning and deep learning techniques.", "abstract": "The objectives of our proposed study were as follows: First objective is to segment the CT images using a k-means clustering algorithm for extracting the region of interest and to extract textural features using gray level co-occurrence matrix (GLCM). Second objective is to implement machine learning classifiers such as Na\u00efve bayes, bagging and Reptree to classify the images into two image classes namely COVID and non-COVID and to compare the performance of the three pre-trained CNN models such as AlexNet, ResNet50 and SqueezeNet with that of the proposed machine learning classifiers. Our dataset consists of 100 COVID and non-COVID images which are pre-processed and segmented with our proposed algorithm. Following the feature extraction process, three machine learning classifiers (Naive Bayes, Bagging, and REPTree) were used to classify the normal and covid patients. We had implemented the three pre-trained CNN models such as AlexNet, ResNet50 and SqueezeNet for comparing their performance with machine learning classifiers. In machine learning, the Naive Bayes classifier achieved the highest accuracy of 97%, whereas the ResNet50 CNN model attained the highest accuracy of 99%. Hence the deep learning networks outperformed well compared to the machine learning techniques in the classification of Covid-19 images.", "journal": "Scientific reports", "date": "2022-10-19", "authors": ["BhargaveeGuhan", "LailaAlmutairi", "SSowmiya", "USnekhalatha", "TRajalakshmi", "Shabnam MohamedAslam"], "doi": "10.1038/s41598-022-20804-5\n10.1016/j.ajem.2020.04.048\n10.1126/scitranslmed.abc1931\n10.1136/bmj.m1403\n10.1002/jmv.25721\n10.1080/14737159.2020.1757437\n10.1148/radiol.2021204522\n10.1136/bmjopen-2020-04294\n10.1016/j.ibmed.2020.100013\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1007/s10140-020-01886-y\n10.1007/s10096-020-03901-z\n10.1007/s10044-021-00984-y\n10.1007/s40009-020-01009-8\n10.1155/2022/5329014\n10.1016/j.jnlest.2022.100161\n10.1016/j.compbiomed.2020.104037\n10.1038/s41598-021-95537-y\n10.1016/j.media.2020.101794\n10.1016/j.bspc.2020.102365\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020191145\n10.1148/rg.2017160130\n10.18201/ijisae.2019252786\n10.30534/ijatcse/2020/221932020\n10.1109/TMI.2018.2806309\n10.1016/j.jocs.2018.11.008"}
{"title": "Active deep learning from a noisy teacher for semi-supervised 3D image segmentation: Application to COVID-19 pneumonia infection in CT.", "abstract": "Supervised deep learning has become a standard approach to solving medical image segmentation tasks. However, serious difficulties in attaining pixel-level annotations for sufficiently large volumetric datasets in real-life applications have highlighted the critical need for alternative approaches, such as semi-supervised learning, where model training can leverage small expert-annotated datasets to enable learning from much larger datasets without laborious annotation. Most of the semi-supervised approaches combine expert annotations and machine-generated annotations with equal weights within deep model training, despite the latter annotations being relatively unreliable and likely to affect model optimization negatively. To overcome this, we propose an active learning approach that uses an example re-weighting strategy, where machine-annotated samples are weighted (i) based on the similarity of their gradient directions of descent to those of expert-annotated data, and (ii) based on the gradient magnitude of the last layer of the deep model. Specifically, we present an active learning strategy with a query function that enables the selection of reliable and more informative samples from machine-annotated batch data generated by a noisy teacher. When validated on clinical COVID-19 CT benchmark data, our method improved the performance of pneumonia infection segmentation compared to the state of the art.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-10-19", "authors": ["Mohammad ArafatHussain", "ZahraMirikharaji", "MohammadMomeny", "MahmoudMarhamati", "Ali AsgharNeshat", "RafeefGarbi", "GhassanHamarneh"], "doi": "10.1016/j.compmedimag.2022.102127\n10.7937/tcia.2020.gqry-nc81"}
{"title": "Attention induction for a CT volume classification of COVID-19.", "abstract": "This study proposes a method to draw attention toward the specific radiological findings of coronavirus disease 2019 (COVID-19) in CT images, such as bilaterality of ground glass opacity (GGO) and/or consolidation, in order to improve the classification accuracy of input CT images.\nWe propose an induction mask that combines a similarity and a bilateral mask. A similarity mask guides attention to regions with similar appearances, and a bilateral mask induces attention to the opposite side of the lung to capture bilaterally distributed lesions. An induction mask for pleural effusion is also proposed in this study. ResNet18 with nonlocal blocks was trained by minimizing the loss function defined by the induction mask.\nThe four-class classification accuracy of the CT images of 1504 cases was 0.6443, where class 1 was the typical appearance of COVID-19 pneumonia, class 2 was the indeterminate appearance of COVID-19 pneumonia, class 3 was the atypical appearance of COVID-19 pneumonia, and class 4 was negative for pneumonia. The four classes were divided into two subgroups. The accuracy of COVID-19 and pneumonia classifications was evaluated, which were 0.8205 and 0.8604, respectively. The accuracy of the four-class and COVID-19 classifications improved when attention was paid to pleural effusion.\nThe proposed attention induction method was effective for the classification of CT images of COVID-19 patients. Improvement of the classification accuracy of class 3 by focusing on features specific to the class remains a topic for future work.", "journal": "International journal of computer assisted radiology and surgery", "date": "2022-10-18", "authors": ["YusukeTakateyama", "TakahitoHaruishi", "MasahiroHashimoto", "YoshitoOtake", "ToshiakiAkashi", "AkinobuShimizu"], "doi": "10.1007/s11548-022-02769-y\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103795\n10.1007/s11356-020-10133-3\n10.1109/ACCESS.2020.3016780\n10.1109/TMI.2020.2994908\n10.1016/j.patrec.2021.06.021\n10.1016/j.media.2012.08.002\n10.1007/s11263-015-0816-y\n10.1007/BF02295996"}
{"title": "The correlation between COVID-19 segmentation volume based on artificial intelligence technology and gastric wall edema: a multi-center study in Wuhan.", "abstract": "This study aimed to investigate manifestations of the gastric wall and related risk factors in COVID-19 patients with gastrointestinal symptoms by CT.\nTwo hundred and forty patients diagnosed with COVID-19 by RT-PCR were enrolled from January 2020 to April 2020. Patients showed gastrointestinal symptoms, including nausea, vomiting, or diarrhea. Results of the initial laboratory examination were performed after admission. Chest CT was performed for all patients, with the lower bound including the gastric antrum. The volume of COVID-19 and lungs was segmented, and the ratio was calculated as follows: PV/LV\u2009=\u2009Volume\nAmong the 240 patients, 109 presented with gastric wall edema (edema group), and 131 showed no gastric wall edema (non-edema group); the PV/LV values between the two groups were significantly different (\nSARS-CoV-2 invades the gastrointestinal tract, gastric wall edema is the primary CT manifestation, and gastric wall edema is more likely to occur with a shorter APTT and severe pneumonia, with a slightly longer hospitalization time. Patients with gastric wall edema observed by CT should intervene early, which may improve digestive function, and further strengthen immune potency against COVID-19.", "journal": "Chinese journal of academic radiology", "date": "2022-10-18", "authors": ["XiaomingLi", "FengxiChen", "JieCheng", "YimanLi", "JunWang", "JianWang", "ChenLiu"], "doi": "10.1007/s42058-022-00104-7\n10.1038/s41564-020-0695-z\n10.1186/s12967-020-02324-w\n10.1038/s41591-020-0968-3\n10.1001/jama.2020.1585\n10.1016/j.cgh.2020.04.032\n10.1136/gutjnl-2020-320832\n10.1016/j.ijid.2020.04.027\n10.1111/apt.15731\n10.1056/NEJMoa2001191\n10.1053/j.gastro.2020.04.006\n10.1126/science.abc1669\n10.1016/j.surg.2020.04.035\n10.1007/s00384-020-03627-6\n10.1016/j.jns.2020.116824\n10.15252/msb.20209610\n10.1007/s00247-020-04616-1\n10.1038/nrmicro.2016.142\n10.3389/fmicb.2020.00301\n10.3390/ijms21145168\n10.1016/j.cell.2020.02.052\n10.1016/S0140-6736(20)30937-5\n10.1056/NEJMcibr1007320\n10.1111/jth.14820\n10.1053/j.gastro.2020.02.055\n10.1007/s00134-020-06079-2\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30211-7\n10.1007/s11427-020-1733-4"}
{"title": "Multi-texture features and optimized DeepNet for COVID-19 detection using chest x-ray images.", "abstract": "The corona virus disease 2019 (COVID-19) pandemic has a severe influence on population health all over the world. Various methods are developed for detecting the COVID-19, but the process of diagnosing this problem from radiology and radiography images is one of the effective procedures for diagnosing the affected patients. Therefore, a robust and effective multi-local texture features (MLTF)-based feature extraction approach and Improved Weed Sea-based DeepNet (IWS-based DeepNet) approach is proposed for detecting the COVID-19 at an earlier stage. The developed IWS-based DeepNet is developed for detecting COVID-19to optimize the structure of the Deep Convolutional Neural Network (Deep CNN). The IWS is devised by incorporating the Improved Invasive Weed Optimization (IIWO) and Sea Lion Optimization (SLnO), respectively. The noises present in the input chest x-ray (CXR) image are discarded using Region of Interest (RoI) extraction by adaptive thresholding technique. For feature extraction, the proposed MLFT is newly developed by considering various texture features for extracting the best features. Finally, the COVID-19 detection is performed using the proposed IWS-based DeepNet. Furthermore, the proposed technique achieved effective performance in terms of True Positive Rate (TPR), True Negative Rate (TNR), and accuracy with the maximum values of 0.933%, 0.890%, and 0.919%.", "journal": "Concurrency and computation : practice & experience", "date": "2022-10-18", "authors": ["AnandbabuGopatoti", "VijayalakshmiP"], "doi": "10.1002/cpe.7157\n10.1007/s12559-021-09848-3\n10.1109/LSP.2018.2817176"}
{"title": "Computer-aided diagnostic for classifying chest X-ray images using deep ensemble learning.", "abstract": "Nowadays doctors and radiologists are overwhelmed with a huge amount of work. This led to the effort to design different Computer-Aided Diagnosis systems (CAD system), with the aim of accomplishing a faster and more accurate diagnosis. The current development of deep learning is a big opportunity for the development of new CADs. In this paper, we propose a novel architecture for a convolutional neural network (CNN) ensemble for classifying chest X-ray (CRX) images into four classes: viral Pneumonia, Tuberculosis, COVID-19, and Healthy. Although Computed tomography (CT) is the best way to detect and diagnoses pulmonary issues, CT is more expensive than CRX. Furthermore, CRX is commonly the first step in the diagnosis, so it's very important to be accurate in the early stages of diagnosis and treatment.\nWe applied the transfer learning technique and data augmentation to all CNNs for obtaining better performance. We have designed and evaluated two different CNN-ensembles: Stacking and Voting. This system is ready to be applied in a CAD system to automated diagnosis such a second or previous opinion before the doctors or radiology's. Our results show a great improvement, 99% accuracy of the Stacking Ensemble and 98% of accuracy for the the Voting Ensemble.\nTo minimize missclassifications, we included six different base CNN models in our architecture (VGG16, VGG19, InceptionV3, ResNet101V2, DenseNet121 and CheXnet) and it could be extended to any number as well as we expect extend the number of diseases to detected. The proposed method has been validated using a large dataset created by mixing several public datasets with different image sizes and quality. As we demonstrate in the evaluation carried out, we reach better results and generalization compared with previous works. In addition, we make a first approach to explainable deep learning with the objective of providing professionals more information that may be valuable when evaluating CRXs.", "journal": "BMC medical imaging", "date": "2022-10-16", "authors": ["LaraVisu\u00f1a", "DandiYang", "JavierGarcia-Blas", "JesusCarretero"], "doi": "10.1186/s12880-022-00904-4\n10.1007/s10489-020-01888-w\n10.1016/j.eswa.2020.114054\n10.1007/s10489-020-01902-1\n10.3389/fmed.2020.00427\n10.1016/j.imu.2020.100405\n10.1007/s10462-020-09825-6\n10.1016/j.media.2021.102121\n10.3233/XST-200831\n10.1055/s-0039-1677911\n10.1186/s43055-021-00524-y\n10.1016/j.cmpb.2020.105608\n10.1016/j.eswa.2021.115141\n10.1109/ACCESS.2020.3031384\n10.1016/j.eswa.2021.115401\n10.1109/TII.2021.3057683\n10.1007/s13246-020-00966-0\n10.1016/j.compeleceng.2019.08.004\n10.1016/j.eswa.2020.113909\n10.1007/s11263-015-0816-y\n10.1148/rg.2017160032\n10.3390/s21051742\n10.1007/s10044-021-00970-4\n10.1109/ACCESS.2020.2971257"}
{"title": "Deep learning of longitudinal chest X-ray and clinical variables predicts duration on ventilator and mortality in COVID-19 patients.", "abstract": "To use deep learning of serial portable chest X-ray (pCXR) and clinical variables to predict mortality and duration on invasive mechanical ventilation (IMV) for Coronavirus disease 2019 (COVID-19) patients.\nThis is a retrospective study. Serial pCXR and serial clinical variables were analyzed for data from day 1, day 5, day 1-3, day 3-5, or day 1-5 on IMV (110 IMV survivors and 76 IMV non-survivors). The outcome variables were duration on IMV and mortality. With fivefold cross-validation, the performance of the proposed deep learning system was evaluated by receiver operating characteristic (ROC) analysis and correlation analysis.\nPredictive models using 5-consecutive-day data outperformed those using 3-consecutive-day and 1-day data. Prediction using data closer to the outcome was generally better (i.e., day 5 data performed better than day 1 data, and day 3-5 data performed better than day 1-3 data). Prediction performance was generally better for the combined pCXR and non-imaging clinical data than either alone. The combined pCXR and non-imaging data of 5 consecutive days predicted mortality with an accuracy of 85\u2009\u00b1\u20093.5% (95% confidence interval (CI)) and an area under the curve (AUC) of 0.87\u2009\u00b1\u20090.05 (95% CI) and predicted the duration needed to be on IMV to within 2.56\u2009\u00b1\u20090.21 (95% CI) days on the validation dataset.\nDeep learning of longitudinal pCXR and clinical data have the potential to accurately predict mortality and duration on IMV in COVID-19 patients. Longitudinal pCXR could have prognostic value if these findings can be validated in a large, multi-institutional cohort.", "journal": "Biomedical engineering online", "date": "2022-10-15", "authors": ["HongyiDuanmu", "ThomasRen", "HaifangLi", "NeilMehta", "Adam JSinger", "Jeffrey MLevsky", "Michael LLipton", "Tim QDuong"], "doi": "10.1186/s12938-022-01045-z\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200642\n10.1148/radiol.2020200463\n10.1148/radiol.2020200432\n10.1148/radiol.2020200370\n10.1016/j.clinimag.2020.04.001\n10.1007/s10140-020-01808-y\n10.1097/RTI.0000000000000533\n10.1148/radiol.2020201160\n10.1001/jama.2020.6775\n10.1177/08850666211033836\n10.1056/NEJMp2006141\n10.1161/CIRCULATIONAHA.115.001593\n10.1590/0100-3984.2019.0049\n10.1016/S1470-2045(19)30333-X\n10.1371/journal.pone.0213653\n10.1371/journal.pone.0221339\n10.1148/radiol.2020201754\n10.1007/s11547-020-01232-9\n10.1371/journal.pone.0236621\n10.7717/peerj.10309\n10.1186/s12938-020-00831-x\n10.1371/journal.pone.0236618\n10.1093/infdis/jiaa447\n10.1161/CIRCRESAHA.120.317134\n10.32604/cmc.2020.010691\n10.1093/cid/ciaa414\n10.1056/NEJMoa2001316\n10.1002/emp2.12205\n10.3389/fmed.2021.661940\n10.7150/ijms.51235\n10.7717/peerj.10337\n10.7717/peerj.11205\n10.1109/TPAMI.2016.2572683"}
{"title": "Dynamic feature learning for COVID-19 segmentation and classification.", "abstract": "Since December 2019, coronavirus SARS-CoV-2 (COVID-19) has rapidly developed into a global epidemic, with millions of patients affected worldwide. As part of the diagnostic pathway, computed tomography (CT) scans are used to help patient management. However, parenchymal imaging findings in COVID-19 are non-specific and can be seen in other diseases. In this work, we propose to first segment lesions from CT images, and further, classify COVID-19 patients from healthy persons and common pneumonia patients. In detail, a novel Dynamic Fusion Segmentation Network (DFSN) that automatically segments infection-related pixels is first proposed. Within this network, low-level features are aggregated to high-level ones to effectively capture context characteristics of infection regions, and high-level features are dynamically fused to model multi-scale semantic information of lesions. Based on DFSN, Dynamic Transfer-learning Classification Network (DTCN) is proposed to distinguish COVID-19 patients. Within DTCN, a pre-trained DFSN is transferred and used as the backbone to extract pixel-level information. Then the pixel-level information is dynamically selected and used to make a diagnosis. In this way, the pre-trained DFSN is utilized through transfer learning, and clinical significance of segmentation results is comprehensively considered. Thus DTCN becomes more sensitive to typical signs of COVID-19. Extensive experiments are conducted to demonstrate effectiveness of the proposed DFSN and DTCN frameworks. The corresponding results indicate that these two models achieve state-of-the-art performance in terms of segmentation and classification.", "journal": "Computers in biology and medicine", "date": "2022-10-15", "authors": ["XiaoqinZhang", "RunhuaJiang", "PengchengHuang", "TaoWang", "MingjunHu", "Andrew FScarsbrook", "Alejandro FFrangi"], "doi": "10.1016/j.compbiomed.2022.106136"}
{"title": "A novel abnormality annotation database for COVID-19 affected frontal lung X-rays.", "abstract": "Consistent clinical observations of characteristic findings of COVID-19 pneumonia on chest X-rays have attracted the research community to strive to provide a fast and reliable method for screening suspected patients. Several machine learning algorithms have been proposed to find the abnormalities in the lungs using chest X-rays specific to COVID-19 pneumonia and distinguish them from other etiologies of pneumonia. However, despite the enormous magnitude of the pandemic, there are very few instances of public databases of COVID-19 pneumonia, and to the best of our knowledge, there is no database with annotation of abnormalities on the chest X-rays of COVID-19 affected patients. Annotated databases of X-rays can be of significant value in the design and development of algorithms for disease prediction. Further, explainability analysis for the performance of existing or new deep learning algorithms will be enhanced significantly with access to ground-truth abnormality annotations. The proposed COVID Abnormality Annotation for X-Rays (CAAXR) database is built upon the BIMCV-COVID19+ database which is a large-scale dataset containing COVID-19+ chest X-rays. The primary contribution of this study is the annotation of the abnormalities in over 1700 frontal chest X-rays. Further, we define protocols for semantic segmentation as well as classification for robust evaluation of algorithms. We provide benchmark results on the defined protocols using popular deep learning models such as DenseNet, ResNet, MobileNet, and VGG for classification, and UNet, SegNet, and Mask-RCNN for semantic segmentation. The classwise accuracy, sensitivity, and AUC-ROC scores are reported for the classification models, and the IoU and DICE scores are reported for the segmentation models.", "journal": "PloS one", "date": "2022-10-15", "authors": ["SurbhiMittal", "Vasantha KumarVenugopal", "Vikash KumarAgarwal", "ManuMalhotra", "Jagneet SinghChatha", "SavinayKapur", "AnkurGupta", "VikasBatra", "PuspitaMajumdar", "AakarshMalhotra", "KartikThakral", "SahebChhabra", "MayankVatsa", "RichaSingh", "SantanuChaudhury"], "doi": "10.1371/journal.pone.0271931\n10.1016/j.tmaid.2020.101623\n10.2214/AJR.20.22969\n10.1007/s13246-020-00865-4\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103869\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1097/RTI.0000000000000533\n10.1016/j.mehy.2020.109761\n10.1038/s41598-020-76550-z\n10.1371/journal.pone.0247176\n10.1109/JBHI.2021.3111415\n10.1007/s13278-021-00731-5\n10.1016/j.media.2021.102046\n10.1007/s10489-020-01900-3\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1109/TPAMI.2016.2644615\n10.1371/journal.pmed.1002683\n10.1126/science.aax2342\n10.1109/TCYB.2019.2905157"}
{"title": "An Intelligent Sensor Based Decision Support System for Diagnosing Pulmonary Ailment through Standardized Chest X-ray Scans.", "abstract": "Academics and the health community are paying much attention to developing smart remote patient monitoring, sensors, and healthcare technology. For the analysis of medical scans, various studies integrate sophisticated deep learning strategies. A smart monitoring system is needed as a proactive diagnostic solution that may be employed in an epidemiological scenario such as COVID-19. Consequently, this work offers an intelligent medicare system that is an IoT-empowered, deep learning-based decision support system (DSS) for the automated detection and categorization of infectious diseases (COVID-19 and pneumothorax). The proposed DSS system was evaluated using three independent standard-based chest X-ray scans. The suggested DSS predictor has been used to identify and classify areas on whole X-ray scans with abnormalities thought to be attributable to COVID-19, reaching an identification and classification accuracy rate of 89.58% for normal images and 89.13% for COVID-19 and pneumothorax. With the suggested DSS system, a judgment depending on individual chest X-ray scans may be made in approximately 0.01 s. As a result, the DSS system described in this study can forecast at a pace of 95 frames per second (FPS) for both models, which is near to real-time.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-15", "authors": ["ShivaniBatra", "HarshSharma", "WadiiBoulila", "VaishaliArya", "PrakashSrivastava", "Mohammad ZubairKhan", "MoezKrichen"], "doi": "10.3390/s22197474\n10.1152/physiolgenomics.00029.2020\n10.1038/s41586-020-2008-3\n10.1126/science.aba9757\n10.1016/j.compbiomed.2020.103670\n10.1016/j.ijid.2020.01.050\n10.3390/e24040533\n10.1016/j.cmpb.2020.105532\n10.1016/j.compbiomed.2020.103792\n10.1016/j.diii.2020.11.008\n10.1148/radiol.2020203465\n10.1016/j.diii.2021.05.006\n10.1007/s10044-021-00958-0\n10.1016/j.cmpb.2018.01.017\n10.1016/j.ijmedinf.2018.06.003\n10.1155/2021/6657533\n10.1016/j.rmcr.2020.101265\n10.1378/chest.125.6.2345\n10.1007/s10140-020-01806-0\n10.1148/radiol.2020202439\n10.3390/s22166312\n10.1016/j.ijmedinf.2022.104791\n10.3390/jpm11100993\n10.1109/TMI.2020.2993291\n10.1109/TMI.2020.2996645\n10.1038/s41598-020-76550-z\n10.1007/s13246-020-00865-4\n10.1007/s10489-020-01826-w\n10.1016/j.cmpb.2020.105581\n10.1007/s10044-021-00984-y\n10.1007/s10489-020-01770-9\n10.3390/s22155738\n10.1016/j.cmpb.2020.105584\n10.1002/jmv.25891\n10.1148/radiol.2020202352\n10.1183/13993003.02697-2020\n10.1038/s41598-020-74164-z\n10.1016/j.media.2021.102216\n10.1016/j.cell.2018.02.010\n10.1016/j.ecoinf.2016.11.006\n10.1016/j.jocs.2017.10.006"}
{"title": "COVID-19 Detection on Chest X-ray and CT Scan: A Review of the Top-100 Most Cited Papers.", "abstract": "Since the beginning of the COVID-19 pandemic, many works have been published proposing solutions to the problems that arose in this scenario. In this vein, one of the topics that attracted the most attention is the development of computer-based strategies to detect COVID-19 from thoracic medical imaging, such as chest X-ray (CXR) and computerized tomography scan (CT scan). By searching for works already published on this theme, we can easily find thousands of them. This is partly explained by the fact that the most severe worldwide pandemic emerged amid the technological advances recently achieved, and also considering the technical facilities to deal with the large amount of data produced in this context. Even though several of these works describe important advances, we cannot overlook the fact that others only use well-known methods and techniques without a more relevant and critical contribution. Hence, differentiating the works with the most relevant contributions is not a trivial task. The number of citations obtained by a paper is probably the most straightforward and intuitive way to verify its impact on the research community. Aiming to help researchers in this scenario, we present a review of the top-100 most cited papers in this field of investigation according to the Google Scholar search engine. We evaluate the distribution of the top-100 papers taking into account some important aspects, such as the type of medical imaging explored, learning settings, segmentation strategy, explainable artificial intelligence (XAI), and finally, the dataset and code availability.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-15", "authors": ["Yandre M GCosta", "Sergio ASilva", "Lucas OTeixeira", "Rodolfo MPereira", "DiegoBertolini", "Alceu SBritto", "Luiz SOliveira", "George D CCavalcanti"], "doi": "10.3390/s22197303\n10.22038/abjs.2020.51846.2556\n10.1177/1178633720962935\n10.32744/pse.2022.3.8\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2644615\n10.1145/2939672.2939778\n10.1371/journal.pone.0130140\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.1038/s41551-021-00704-1\n10.1016/j.eng.2020.04.010\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01829-7\n10.1109/TCBB.2021.3065361\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103795\n10.1038/s41598-020-76282-0\n10.1016/j.mehy.2020.109761\n10.1016/j.patrec.2020.09.010\n10.1016/j.chaos.2020.109944\n10.1148/ryct.2020200075\n10.1016/j.compbiomed.2020.103805\n10.1016/j.cmpb.2020.105532\n10.1183/13993003.00775-2020\n10.1016/j.cmpb.2020.105608\n10.3390/sym12040651\n10.1016/j.imu.2020.100412\n10.1016/j.eswa.2020.114054\n10.1016/j.compbiomed.2020.104037\n10.1016/j.bbe.2021.05.013\n10.1016/j.compbiomed.2021.104319\n10.1016/j.compbiomed.2020.104115\n10.1145/3450439.3451867\n10.3390/s21217116\n10.1007/s11042-022-12156-z\n10.1016/j.inffus.2021.04.008\n10.1007/s00259-020-04953-1\n10.1101/2020.03.12.20027185\n10.1101/2020.04.13.20063941\n10.1101/2020.04.08.20057679\n10.1101/2020.03.30.20047787\n10.1101/2020.04.13.20063461\n10.1101/2020.04.08.20040907\n10.1109/ACCESS.2020.3005510\n10.3233/XST-200715\n10.1016/j.compbiomed.2020.103869\n10.3390/app10134640\n10.1007/s10489-020-01902-1\n10.1016/j.chaos.2020.110245\n10.1109/ACCESS.2020.3016780\n10.1016/j.imu.2020.100427\n10.1007/s40846-020-00529-4\n10.1016/j.chemolab.2020.104054\n10.1016/j.chaos.2020.110071\n10.1007/s10916-021-01745-4\n10.1109/JAS.2020.1003393\n10.1109/ACCESS.2020.3003810\n10.1016/j.asoc.2020.106885\n10.1080/07391102.2020.1767212\n10.3390/e22050517\n10.1007/s10489-020-02055-x\n10.1007/s42600-021-00151-6\n10.1016/j.asoc.2020.106859\n10.1016/j.chaos.2020.110190\n10.1007/s00500-020-05275-y\n10.1007/s10489-020-01826-w\n10.1007/s10489-020-01888-w\n10.2196/19569\n10.1177/2472630320958376\n10.1016/j.bspc.2020.102365\n10.1155/2020/8828855\n10.1016/j.ejrad.2020.109041\n10.1007/s13755-020-00135-3\n10.7759/cureus.9448\n10.1038/s41551-020-00633-5\n10.1016/j.compbiomed.2021.104348\n10.1016/j.chaos.2020.110170\n10.1007/s12559-020-09787-5\n10.3389/fmed.2020.00427\n10.1038/s41598-020-74164-z\n10.1186/s12880-020-00529-5\n10.1016/j.bbe.2020.08.008\n10.1002/ima.22469\n10.3390/ijerph17186933\n10.1016/j.bbe.2020.08.005\n10.1016/j.chaos.2020.110495\n10.1016/j.eswa.2020.113909\n10.3390/s21020455\n10.1007/s00330-020-07044-9\n10.1007/s12539-020-00403-6\n10.1007/s10140-020-01886-y\n10.1109/JSEN.2021.3076767\n10.1007/s00330-020-07042-x\n10.1007/s00500-020-05424-3\n10.3390/v12070769\n10.1038/s41467-020-20657-4\n10.1038/s41598-021-88807-2\n10.1038/s41598-020-74539-2\n10.1016/j.bspc.2021.102588\n10.1007/s10044-021-00970-4\n10.1007/s10489-020-01900-3\n10.1007/s40747-020-00216-6\n10.1007/s00521-020-05410-8\n10.1016/j.knosys.2020.106647\n10.1007/s00264-020-04609-7\n10.1016/j.imu.2020.100505\n10.1109/ACCESS.2020.3025010\n10.1007/s10489-020-01867-1"}
{"title": "Optical Monitoring of Breathing Patterns and Tissue Oxygenation: A Potential Application in COVID-19 Screening and Monitoring.", "abstract": "The worldwide outbreak of the novel Coronavirus (COVID-19) has highlighted the need for a screening and monitoring system for infectious respiratory diseases in the acute and chronic phase. The purpose of this study was to examine the feasibility of using a wearable near-infrared spectroscopy (NIRS) sensor to collect respiratory signals and distinguish between normal and simulated pathological breathing. Twenty-one healthy adults participated in an experiment that examined five separate breathing conditions. Respiratory signals were collected with a continuous-wave NIRS sensor (PortaLite, Artinis Medical Systems) affixed over the sternal manubrium. Following a three-minute baseline, participants began five minutes of imposed difficult breathing using a respiratory trainer. After a five minute recovery period, participants began five minutes of imposed rapid and shallow breathing. The study concluded with five additional minutes of regular breathing. NIRS signals were analyzed using a machine learning model to distinguish between normal and simulated pathological breathing. Three features: breathing interval, breathing depth, and O", "journal": "Sensors (Basel, Switzerland)", "date": "2022-10-15", "authors": ["Aaron JamesMah", "ThienNguyen", "LeiliGhazi Zadeh", "AtrinaShadgan", "KosarKhaksari", "MehdiNourizadeh", "AliZaidi", "SoonghoPark", "Amir HGandjbakhche", "BabakShadgan"], "doi": "10.3390/s22197274\n10.3389/fmed.2020.00311\n10.1377/hlthaff.9.3.66\n10.1080/14737159.2020.1757437\n10.1016/S1473-3099(21)00048-7\n10.4037/aacnacc2022448\n10.1513/AnnalsATS.202005-418FR\n10.1183/13993003.01217-2020\n10.1007/s001340101064\n10.1177/0885066608324380\n10.1016/S0952-8180(97)00039-1\n10.1136/bmjresp-2020-000778\n10.1371/journal.pone.0241955\n10.1007/s15010-021-01603-y\n10.1080/10408363.2020.1860895\n10.1016/S2213-2600(20)30349-0\n10.1016/j.rmed.2021.106541\n10.1155/2021/6692409\n10.1155/2009/719604\n10.1016/j.jtcvs.2005.02.058\n10.1093/bja/aep299\n10.1255/jnirs.963\n10.1155/2005/951895\n10.1097/HCR.0000000000000185\n10.1007/s00421-016-3334-x\n10.1016/j.resp.2011.06.001\n10.1259/bjr/31200593\n10.1007/s10877-019-00410-z\n10.1016/j.jaut.2020.102433\n10.1148/radiol.2020200236\n10.1056/NEJMoa2001316\n10.1080/14767058.2018.1489535\n10.1186/cc11146\n10.1093/ehjci/jeaa163\n10.1001/jama.2020.22813\n10.1002/jmv.27101\n10.1186/s12931-020-01429-6\n10.1016/j.bios.2021.113486\n10.1021/acssensors.1c00312\n10.1016/j.bios.2020.112454\n10.1021/acsnano.0c05657"}
{"title": "Multilevel segmentation of 2D and volumetric medical images using hybrid Coronavirus Optimization Algorithm.", "abstract": "Medical image segmentation is a crucial step in Computer-Aided Diagnosis systems, where accurate segmentation is vital for perfect disease diagnoses. This paper proposes a multilevel thresholding technique for 2D and 3D medical image segmentation using Otsu and Kapur's entropy methods as fitness functions to determine the optimum threshold values. The proposed algorithm applies the hybridization concept between the recent Coronavirus Optimization Algorithm (COVIDOA) and Harris Hawks Optimization Algorithm (HHOA) to benefit from both algorithms' strengths and overcome their limitations. The improved performance of the proposed algorithm over COVIDOA and HHOA algorithms is demonstrated by solving 5 test problems from IEEE CEC 2019 benchmark problems. Medical image segmentation is tested using two groups of images, including 2D medical images and volumetric (3D) medical images, to demonstrate its superior performance. The utilized test images are from different modalities such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), and X-ray images. The proposed algorithm is compared with seven well-known metaheuristic algorithms, where the performance is evaluated using four different metrics, including the best fitness values, Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Normalized Correlation Coefficient (NCC). The experimental results demonstrate the superior performance of the proposed algorithm in terms of convergence to the global optimum and making a good balance between exploration and exploitation properties. Moreover, the quality of the segmented images using the proposed algorithm at different threshold levels is better than the other methods according to PSNR, SSIM, and NCC values. Additionally, the Wilcoxon rank-sum test is conducted to prove the statistical significance of the proposed algorithm.", "journal": "Computers in biology and medicine", "date": "2022-10-14", "authors": ["Khalid MHosny", "Asmaa MKhalid", "Hanaa MHamza", "SeyedaliMirjalili"], "doi": "10.1016/j.compbiomed.2022.106003\n10.21203/rs.3.rs-1592094/v1"}
{"title": "The natural language processing of radiology requests and reports of chest imaging: Comparing five transformer models' multilabel classification and a proof-of-concept study.", "abstract": "Radiology requests and reports contain valuable information about diagnostic findings and indications, and transformer-based language models are promising for more accurate text classification.\nIn a retrospective study, 2256 radiologist-annotated radiology requests (8 classes) and reports (10 classes) were divided into training and testing datasets (90% and 10%, respectively) and used to train 32 models. Performance metrics were compared by model type (LSTM, Bertje, RobBERT, BERT-clinical, BERT-multilingual, BERT-base), text length, data prevalence, and training strategy. The best models were used to predict the remaining 40,873 cases' categories of the datasets of requests and reports.\nThe RobBERT model performed the best after 4000 training iterations, resulting in AUC values ranging from 0.808 [95% CI (0.757-0.859)] to 0.976 [95% CI (0.956-0.996)] for the requests and 0.746 [95% CI (0.689-0.802)] to 1.0 [95% CI (1.0-1.0)] for the reports. The AUC for the classification of normal reports was 0.95 [95% CI (0.922-0.979)]. The predicted data demonstrated variability of both diagnostic yield for various request classes and request patterns related to COVID-19 hospital admission data.\nTransformer-based natural language processing is feasible for the multilabel classification of chest imaging request and report items. Diagnostic yield varies with the information in the requests.", "journal": "Health informatics journal", "date": "2022-10-14", "authors": ["Allard WOlthof", "Peter Mavan Ooijen", "Ludo JCornelissen"], "doi": "10.1177/14604582221131198"}
{"title": "Role of Imaging and AI in the Evaluation of COVID-19 Infection: A Comprehensive Survey.", "abstract": "Coronavirus disease 2019 (COVID-19) is a respiratory illness that started and rapidly became the pandemic of the century, as the number of people infected with it globally exceeded 253.4 million. Since the beginning of the pandemic of COVID-19, over two years have passed. During this hard period, several defies have been coped by the scientific society to know this novel disease, evaluate it, and treat affected patients. All these efforts are done to push back the spread of the virus. This article provides a comprehensive review to learn about the COVID-19 virus and its entry mechanism, its main repercussions on many organs and tissues of the body, identify its symptoms in the short and long terms, in addition to recognize the role of diagnosis imaging in COVID-19. Principally, the quick evolution of active vaccines act an exceptional accomplishment where leaded to decrease rate of death worldwide. However, some hurdels still have to be overcome. Many proof referrers that infection with CoV-19 causes neurological dis function in a substantial ratio of influenced patients, where these symptoms appear severely during the infection and still less is known about the potential long term consequences for the brain, where Loss of smell is a neurological sign and rudimentary symptom of COVID-19. Hence, we review the causes of olfactory bulb dysfunction and Anosmia associated with COVID-19, the latest appropriate therapeutic strategies for the COVID-19 treatment (e.g., the ACE2 strategy and the Ang II receptor), and the tests through the follow-up phases. Additionally, we discuss the long-term complications of the virus and thus the possibility of improving therapeutic strategies. Moreover, the main steps of artificial intelligence that have been used to foretell and early diagnose COVID-19 are presented, where Artificial intelligence, especially machine learning is emerging as an effective approach for diagnostic image analysis with performance in the discriminate diagnosis of injuries of COVID-19 on multiple organs, comparable to that of human practitioners. The followed methodology to prepare the current survey is to search the related work concerning the mentioned topic from different journals, such as Springer, Wiley, and Elsevier. Additionally, different studies have been compared, the results are collected and then reported as shown. The articles are selected based on the year (i.e., the last three years). Also, different keywords were checked (e.g., COVID-19, COVID-19 Treatment, COVID-19 Symptoms, and COVID-19 and Anosmia).", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2022-10-13", "authors": ["MayadaElgendy", "Hossam MagdyBalaha", "MohamedShehata", "AhmedAlksas", "MahitabGhoneim", "FatmaSherif", "AliMahmoud", "AhmedElgarayhi", "FatmaTaher", "MohammedSallah", "MohammedGhazal", "AymanEl-Baz"], "doi": "10.31083/j.fbl2709276"}
{"title": "Predicting necessity of daily online adaptive replanning based on wavelet image features for MRI guided adaptive radiation therapy.", "abstract": "Online adaptive replanning (OLAR) is generally labor-intensive and time-consuming during MRI-guided adaptive radiation therapy (MRgART). This work aims to develop a method to determine OLAR necessity during MRgART.\nA machine learning classifier was developed to predict OLAR necessity based on wavelet multiscale texture features extracted from daily MRIs and was trained and tested with data from 119 daily MRI datasets acquired during MRgART for 24 pancreatic cancer patients treated on a 1.5\u00a0T MR-Linac. Spearman correlations, interclass correlation (ICC), coefficient of variance (COV), t-test (p\u00a0<\u00a00.05), self-organized map (SOM) and maximum stable extremal region (MSER) algorithm were used to determine candidate features, which were used to build the prediction models using Bayesian classifiers. The model performance was judged using the AUC of the ROC curve.\nSpearman correlation identified 123 features that were not redundant (r\u00a0<\u00a00.9). Of them 82 showed high ICC for repositioning\u00a0>\u00a00.6, 67 had a COV greater than 9% for OLAR. Among the 38 features passed the t-test, 25 passed the SOM and 12 passed the MSER. These final 12 features were used to build the classifier model. The combination of 2-3 features at a time was used to build the classifier models. The best performing model was a 3-feature combination, which can predict OLAR necessity with a CV-AUC of 0.98.\nA machine learning classifier model based on the wavelet features extracted from daily MRI for pancreatic cancer was developed to automatically and objectively determine if OLAR is necessary for a treatment fraction avoiding unnecessary effort during MRgART.", "journal": "Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology", "date": "2022-10-11", "authors": ["Haidy GNasief", "Abdul KParchur", "EenasOmari", "YingZhang", "XinfengChen", "EricPaulson", "William AHall", "BethErickson", "X AllenLi"], "doi": "10.1016/j.radonc.2022.10.001"}
{"title": "Two-step machine learning to diagnose and predict involvement of lungs in COVID-19 and pneumonia using CT radiomics.", "abstract": "To develop a two-step machine learning (ML) based model to diagnose and predict involvement of lungs in COVID-19 and non COVID-19 pneumonia patients using CT chest radiomic features.\nThree hundred CT scans (3-classes: 100 COVID-19, 100 pneumonia, and 100 healthy subjects) were enrolled in this study. Diagnostic task included 3-class classification. Severity prediction score for COVID-19 and pneumonia was considered as mild (0-25%), moderate (26-50%), and severe (>50%). Whole lungs were segmented utilizing deep learning-based segmentation. Altogether, 107 features including shape, first-order histogram, second and high order texture features were extracted. Pearson correlation coefficient (PCC\u226590%) followed by different features selection algorithms were employed. ML-based supervised algorithms (Na\u00efve Bays, Support Vector Machine, Bagging, Random Forest, K-nearest neighbors, Decision Tree and Ensemble Meta voting) were utilized. The optimal model was selected based on precision, recall and area-under-curve (AUC) by randomizing the training/validation, followed by testing using the test set.\nNine pertinent features (2 shape, 1 first-order, and 6 second-order) were obtained after features selection for both phases. In diagnostic task, the performance of 3-class classification using Random Forest was 0.909\u00b10.026, 0.907\u00b10.056, 0.902\u00b10.044, 0.939\u00b10.031, and 0.982\u00b10.010 for precision, recall, F1-score, accuracy, and AUC, respectively. The severity prediction task using Random Forest achieved 0.868\u00b10.123 precision, 0.865\u00b10.121 recall, 0.853\u00b10.139 F1-score, 0.934\u00b10.024 accuracy, and 0.969\u00b10.022 AUC.\nThe two-phase ML-based model accurately classified COVID-19 and pneumonia patients using CT radiomics, and adequately predicted severity of lungs involvement. This 2-steps model showed great potential in assessing COVID-19 CT images towards improved management of patients.", "journal": "Computers in biology and medicine", "date": "2022-10-11", "authors": ["PegahMoradi Khaniabadi", "YassineBouchareb", "HumoudAl-Dhuhli", "IsaacShiri", "FaizaAl-Kindi", "BitaMoradi Khaniabadi", "HabibZaidi", "ArmanRahmim"], "doi": "10.1016/j.compbiomed.2022.106165\n10.1016/j.pdpdt.2021.102287\n10.1016/j.compbiomed.2021.104665\n10.1148/radiol.2020201160\n10.1148/radiol.2021202553\n10.1016/S1120-1797(22)00087-4\n10.1016/j.compbiomed.2021.104304\n10.1038/s41598-021-88807-2\n10.1007/s42979-020-00394-7\n10.48550/arXiv.2003.11988\n10.1186/s12967-020-02692-3\n10.1007/s11432-020-2849-3\n10.1016/j.acra.2020.09.004\n10.1016/j.compbiomed.2022.105467\n10.1038/s41598-022-18994-z\n10.1002/ima.22672\n10.1148/radiol.2020201473\n10.1148/radiol.2020200370\n10.1148/radiol.2462070712\n10.1148/radiol.11092149\n10.1148/ryct.2020200322\n10.1148/radiol.2020200463\n10.30476/ijms.2021.88036.1858.20\n10.1148/radiol.2020191145\n10.1186/s40644-020-00311-4\n10.1002/mp.13649\n10.1016/j.ejro.2020.100271\n10.1101/2021.12.07.21267367\n10.2967/jnumed.121.262567\n10.1007/s00330-020-06829-2\n10.1088/1361-6560/abe838\n10.1109/ICSPIS51611.2020.9349605\n10.21037/atm-20-3026\n10.1155/2021/2263469\n10.1016/j.media.2020.10182\n10.1007/s12539-020-00410-7\n10.1186/s12880-021-00564-w\n10.1016/j.smhl.2020.100178\n10.1016/j.csbj.2021.06.022\n10.1038/s41598-021-96755-0\n10.1016/j.bspc.2022.103662\n10.21037/qims.2020.02.21\n10.3389/fdgth.2021.662343\n10.1186/s43055-021-00592-0\n10.1007/s00259-020-05075-4\n10.1038/s41598-021-99015-3\n10.1016/j.ijid.2021.03.008\n10.1016/j.compbiomed.2021.104531\n10.1148/ryct.2020200047"}
{"title": "Evaluation of federated learning variations for COVID-19 diagnosis using chest radiographs from 42 US and European hospitals.", "abstract": "Federated learning (FL) allows multiple distributed data holders to collaboratively learn a shared model without data sharing. However, individual health system data are heterogeneous. \"Personalized\" FL variations have been developed to counter data heterogeneity, but few have been evaluated using real-world healthcare data. The purpose of this study is to investigate the performance of a single-site versus a 3-client federated model using a previously described Coronavirus Disease 19 (COVID-19) diagnostic model. Additionally, to investigate the effect of system heterogeneity, we evaluate the performance of 4 FL variations.\nWe leverage a FL healthcare collaborative including data from 5 international healthcare systems (US and Europe) encompassing 42 hospitals. We implemented a COVID-19 computer vision diagnosis system using the Federated Averaging (FedAvg) algorithm implemented on Clara Train SDK 4.0. To study the effect of data heterogeneity, training data was pooled from 3 systems locally and federation was simulated. We compared a centralized/pooled model, versus FedAvg, and 3 personalized FL variations (FedProx, FedBN, and FedAMP).\nWe observed comparable model performance with respect to internal validation (local model: AUROC 0.94 vs FedAvg: 0.95, P\u2009=\u2009.5) and improved model generalizability with the FedAvg model (P\u2009<\u2009.05). When investigating the effects of model heterogeneity, we observed poor performance with FedAvg on internal validation as compared to personalized FL algorithms. FedAvg did have improved generalizability compared to personalized FL algorithms. On average, FedBN had the best rank performance on internal and external validation.\nFedAvg can significantly improve the generalization of the model compared to other personalization FL algorithms; however, at the cost of poor internal validity. Personalized FL may offer an opportunity to develop both internal and externally validated algorithms.", "journal": "Journal of the American Medical Informatics Association : JAMIA", "date": "2022-10-11", "authors": ["LePeng", "GaoxiangLuo", "AndrewWalker", "ZacharyZaiman", "Emma KJones", "HemantGupta", "KristopherKersten", "John LBurns", "Christopher AHarle", "TanjaMagoc", "BenjaminShickel", "Scott DSteenburg", "TylerLoftus", "Genevieve BMelton", "Judy WawiraGichoya", "JuSun", "Christopher JTignanelli"], "doi": "10.1093/jamia/ocac188"}
{"title": "Development and validation of chest CT-based imaging biomarkers for early stage COVID-19 screening.", "abstract": "Coronavirus Disease 2019 (COVID-19) is currently a global pandemic, and early screening is one of the key factors for COVID-19 control and treatment. Here, we developed and validated chest CT-based imaging biomarkers for COVID-19 patient screening from two independent hospitals with 419 patients. We identified the vasculature-like signals from CT images and found that, compared to healthy and community acquired pneumonia (CAP) patients, COVID-19 patients display a significantly higher abundance of these signals. Furthermore, unsupervised feature learning led to the discovery of clinical-relevant imaging biomarkers from the vasculature-like signals for accurate and sensitive COVID-19 screening that have been double-blindly validated in an independent hospital (sensitivity: 0.941, specificity: 0.920, AUC: 0.971, accuracy 0.931, F1 score: 0.929). Our findings could open a new avenue to assist screening of COVID-19 patients.", "journal": "Frontiers in public health", "date": "2022-10-11", "authors": ["Xiao-PingLiu", "XuYang", "MiaoXiong", "XuanyuMao", "XiaoqingJin", "ZhiqiangLi", "ShuangZhou", "HangChang"], "doi": "10.3389/fpubh.2022.1004117\n10.1111/tmi.13383\n10.1002/jmv.25722\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020200343\n10.1111/eci.13706\n10.1371/journal.pone.0242958\n10.1016/j.radi.2020.09.010\n10.1016/j.ijid.2020.04.023\n10.1007/s11604-020-00948-y\n10.2214/AJR.20.22961\n10.1148/radiol.2020200642\n10.1148/radiol.2020200330\n10.1148/radiol.2020200432\n10.1177/0846537120913033\n10.1371/journal.pone.0263916\n10.1016/j.compbiomed.2022.105298\n10.2196/27468\n10.1155/2021/9868517\n10.1200/CCI.19.00155\n10.1016/j.patcog.2014.10.005\n10.1109/83.902291\n10.1007/s11263-014-0790-9\n10.1016/j.acra.2020.03.003\n10.1097/TP.0000000000003412\n10.1056/NEJMoa2015432\n10.1038/s41591-020-0822-7\n10.1016/j.tjem.2018.08.001\n10.1089/ars.2012.5149\n10.1093/cvr/cvaa078\n10.1016/j.jcv.2020.104362\n10.1146/annurev-virology-031413-085548\n10.1016/0891-5849(96)00131-1\n10.1371/journal.ppat.1008536\n10.1038/s41577-020-0311-8\n10.1016/S0140-6736(20)30937-5\n10.1016/j.bcp.2009.04.029\n10.1148/radiol.2020200905\n10.1016/j.cell.2020.04.045\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2996256\n10.21037/qims.2020.04.02\n10.1016/S1473-3099(20)30241-3\n10.1148/radiol.2020201845\n10.1148/radiol.2020201365\n10.1038/d41586-020-01001-8\n10.1136/bmj.m1367"}
{"title": "RadImageNet: An Open Radiologic Deep Learning Research Dataset for Effective Transfer Learning.", "abstract": "To demonstrate the value of pretraining with millions of radiologic images compared with ImageNet photographic images on downstream medical applications when using transfer learning.\nThis retrospective study included patients who underwent a radiologic study between 2005 and 2020 at an outpatient imaging facility. Key images and associated labels from the studies were retrospectively extracted from the original study interpretation. These images were used for RadImageNet model training with random weight initiation. The RadImageNet models were compared with ImageNet models using the area under the receiver operating characteristic curve (AUC) for eight classification tasks and using Dice scores for two segmentation problems.\nThe RadImageNet database consists of 1.35 million annotated medical images in 131\u2009872 patients who underwent CT, MRI, and US for musculoskeletal, neurologic, oncologic, gastrointestinal, endocrine, abdominal, and pulmonary pathologic conditions. For transfer learning tasks on small datasets-thyroid nodules (US), breast masses (US), anterior cruciate ligament injuries (MRI), and meniscal tears (MRI)-the RadImageNet models demonstrated a significant advantage (\nRadImageNet pretrained models demonstrated better interpretability compared with ImageNet models, especially for smaller radiologic datasets.", "journal": "Radiology. Artificial intelligence", "date": "2022-10-08", "authors": ["XueyanMei", "ZelongLiu", "Philip MRobson", "BrettMarinelli", "MingqianHuang", "AmishDoshi", "AdamJacobi", "ChendiCao", "Katherine ELink", "ThomasYang", "YingWang", "HayitGreenspan", "TimothyDeyer", "Zahi AFayad", "YangYang"], "doi": "10.1148/ryai.210315"}
{"title": "Deep learning models for COVID-19 chest x-ray classification: Preventing shortcut learning using feature disentanglement.", "abstract": "In response to the COVID-19 global pandemic, recent research has proposed creating deep learning based models that use chest radiographs (CXRs) in a variety of clinical tasks to help manage the crisis. However, the size of existing datasets of CXRs from COVID-19+ patients are relatively small, and researchers often pool CXR data from multiple sources, for example, using different x-ray machines in various patient populations under different clinical scenarios. Deep learning models trained on such datasets have been shown to overfit to erroneous features instead of learning pulmonary characteristics in a phenomenon known as shortcut learning. We propose adding feature disentanglement to the training process. This technique forces the models to identify pulmonary features from the images and penalizes them for learning features that can discriminate between the original datasets that the images come from. We find that models trained in this way indeed have better generalization performance on unseen data; in the best case we found that it improved AUC by 0.13 on held out data. We further find that this outperforms masking out non-lung parts of the CXRs and performing histogram equalization, both of which are recently proposed methods for removing biases in CXR datasets.", "journal": "PloS one", "date": "2022-10-07", "authors": ["AnusuaTrivedi", "CalebRobinson", "MarianBlazes", "AnthonyOrtiz", "JocelynDesbiens", "SunilGupta", "RahulDodhia", "Pavan KBhatraju", "W ConradLiles", "JayashreeKalpathy-Cramer", "Aaron YLee", "Juan MLavista Ferres"], "doi": "10.1371/journal.pone.0274098\n10.1016/j.dsx.2020.04.012\n10.1016/j.compbiomed.2020.103792\n10.3389/fmed.2020.00427\n10.1007/s13246-020-00865-4\n10.1136/bmj.m1328\n10.1016/j.cmpb.2020.105532\n10.1101/2020.09.13.20193565\n10.1016/j.cell.2020.04.045"}
{"title": "Exploration of the Potential Link, Hub Genes, and Potential Drugs for Coronavirus Disease 2019 and Lung Cancer Based on Bioinformatics Analysis.", "abstract": "The ongoing pandemic of coronavirus disease 2019 (COVID-19) has a huge influence on global public health and the economy. Lung cancer is one of the high-risk factors of COVID-19, but the molecular mechanism of lung cancer and COVID-19 is still unclear, and further research is needed. Therefore, we used the transcriptome information of the public database and adopted bioinformatics methods to identify the common pathways and molecular biomarkers of lung cancer and COVID-19 to further understand the connection between them. The two RNA-seq data sets in this study-GSE147507 (COVID-19) and GSE33532 (lung cancer)-were both derived from the Gene Expression Omnibus (GEO) database and identified differentially expressed genes (DEGs) for lung cancer and COVID-19 patients. We conducted Gene Ontology (GO) functions and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways enrichment analysis and found some common features between lung cancer and COVID-19. We also performed TFs-gene, miRNAs-gene, and gene-drug analyses. In total, 32 DEGs were found. A protein-protein interaction (PPI) network was constructed by DEGs, and 10 hub genes were screened. Finally, the identified drugs may be helpful for COVID-19 treatment.", "journal": "Journal of oncology", "date": "2022-10-07", "authors": ["YeWang", "QingLi", "JianfangZhang", "HuiXie"], "doi": "10.1155/2022/8124673\n10.1111/ahg.12418\n10.1016/j.biopha.2017.03.018\n10.1016/j.arcmed.2020.11.006\n10.1177/1073274820960467\n10.1016/s1470-2045(20)30096-6\n10.1186/s12885-020-07418-8\n10.1016/s2352-3018(16)30215-6\n10.1186/s12890-019-0910-y\n10.1002/jmv.25685\n10.1111/bjh.16659\n10.1016/j.ebiom.2020.102763\n10.1001/jamaoncol.2020.0980\n10.1111/bjh.17116\n10.1016/j.cell.2020.04.026\n10.18632/aging.203159\n10.1007/s11033-021-06149-8\n10.1159/000444125\n10.21037/atm-21-927\n10.1042/bsr20182377\n10.1084/jem.20201129\n10.1038/s41467-018-05928-5\n10.1111/1759-7714.13303\n10.1038/s41598-020-61424-1\n10.3390/nu12113448\n10.1002/mgg3.1115\n10.2217/bmm-2021-0337\n10.1155/2020/8931419\n10.1242/dev.174037\n10.3390/cells10040882\n10.1167/iovs.61.14.12\n10.1021/acscentsci.0c01537\n10.1369/0022155417753363\n10.1038/s41591-019-0750-6\n10.1038/nrd4099\n10.4274/tjh.galenos.2019.2018.0420\n10.1002/ibd.20850\n10.3389/fneur.2020.540156\n10.3390/ijms20143401\n10.1242/jcs.239715\n10.3390/ijms21165795\n10.3892/etm.2014.1829\n10.1002/stem.1969\n10.3892/mmr.2019.10543\n10.1038/s41598-020-66469-w\n10.1186/s12967-020-02561-z\n10.1016/j.biopha.2018.08.040\n10.1042/BSR20200884\n10.1017/S2045796021000597\n10.1016/S2215-0366(20)30564-2\n10.1038/s41598-020-58207-z\n10.1016/j.eclinm.2020.100688\n10.1016/j.jhep.2020.06.006\n10.3389/fped.2020.602083\n10.3748/wjg.v26.i31.4694\n10.1186/s12929-020-00703-5\n10.1158/2159-8290.CD-20-0941\n10.1016/j.xcrm.2021.100288\n10.1001/jamapsychiatry.2018.3412\n10.3389/fpsyt.2021.653662\n10.1210/me.2015-1274\n10.1136/hrt.2006.096412\n10.1158/2159-8290.CD-15-1177\n10.1038/s41593-019-0372-9"}
{"title": "Artificial intelligence-based model for COVID-19 prognosis incorporating chest radiographs and clinical data; a retrospective model development and validation study.", "abstract": "The purpose of this study was to develop an artificial intelligence-based model to prognosticate COVID-19 patients at admission by combining clinical data and chest radiographs.\nThis retrospective study used the Stony Brook University COVID-19 dataset of 1384 inpatients. After exclusions, 1356 patients were randomly divided into training (1083) and test datasets (273). We implemented three artificial intelligence models, which classified mortality, ICU admission, or ventilation risk. Each model had three submodels with different inputs: clinical data, chest radiographs, and both. We showed the importance of the variables using SHapley Additive exPlanations (SHAP) values.\nThe mortality prediction model was best overall with area under the curve, sensitivity, specificity, and accuracy of 0.79 (0.72-0.86), 0.74 (0.68-0.79), 0.77 (0.61-0.88), and 0.74 (0.69-0.79) for the clinical data-based model; 0.77 (0.69-0.85), 0.67 (0.61-0.73), 0.81 (0.67-0.92), 0.70 (0.64-0.75) for the image-based model, and 0.86 (0.81-0.91), 0.76 (0.70-0.81), 0.77 (0.61-0.88), 0.76 (0.70-0.81) for the mixed model. The mixed model had the best performance (\nThese results suggest that prognosis models become more accurate if AI-derived chest radiograph features and clinical data are used together.\nThis AI model evaluates chest radiographs together with clinical data in order to classify patients as having high or low mortality risk. This work shows that chest radiographs taken at admission have significant COVID-19 prognostic information compared to clinical data other than age and sex.", "journal": "The British journal of radiology", "date": "2022-10-05", "authors": ["Shannon LWalston", "ToshimasaMatsumoto", "YukioMiki", "DaijuUeda"], "doi": "10.1259/bjr.20220058\n10.1136/bmj.m1328\n10.1080/23744235.2020.1784457\n10.1016/j.jcrc.2020.04.004\n10.1001/jama.2016.0287\n10.1148/radiol.2020201433\n10.1148/radiol.2020201160\n10.1183/13993003.01104-2020\n10.1016/S2589-7500(21)00039-X\n10.1038/s41467-020-17280-8\n10.1093/jamia/ocab029\n10.1001/jama.2018.11100\n10.1148/radiol.2020200905\n10.1007/s00330-020-07269-8\n10.1186/s12938-020-00807-x\n10.1148/ryai.2020200029\n10.1007/s10278-013-9622-7\n10.1016/j.jiph.2021.09.023\n10.1371/journal.pone.0241955\n10.1109/CVPR.2015.7298594\n10.1109/ICCV.2017.74\n10.1002/1097-0142(1950)3:1<32::aid-cncr2820030106>3.0.co;2-3\n10.1148/ryai.2020190043\n10.1001/jamainternmed.2020.2033\n10.1016/j.atherosclerosis.2021.05.015\n10.1007/s11739-020-02475-0\n10.3390/jcm9061668\n10.1172/jci.insight.139024\n10.1017/S0950268820001727\n10.1088/1361-6560/abbf9e\n10.1183/13993003.03498-2020\n10.1016/j.media.2021.102216\n10.1038/s41746-021-00446-z\n10.1109/JBHI.2021.3103389\n10.1038/s41746-021-00546-w\n10.1148/ryai.2020200098\n10.1038/s41746-021-00453-0\n10.1038/s41467-020-20657-4\n10.1016/j.rxeng.2021.09.004\n10.1186/s12879-022-07617-7\n10.1038/s41598-022-10136-9\n10.3390/app12083903\n10.1007/s10278-022-00691-y\n10.1007/s00330-021-08049-8\n10.1007/s40477-021-00559-x\n10.1016/j.ejca.2016.03.081\n10.1002/9780471420194\n10.1038/s41591-021-01506-3"}
{"title": "Unraveling the mystery of efficacy in Chinese medicine formula: New approaches and technologies for research on pharmacodynamic substances.", "abstract": "Traditional Chinese medicine (TCM) is the key to unlock treasures of Chinese civilization. TCM and its compound play a beneficial role in medical activities to cure diseases, especially in major public health events such as novel coronavirus epidemics across the globe. The chemical composition in Chinese medicine formula is complex and diverse, but their effective substances resemble \"mystery boxes\". Revealing their active ingredients and their mechanisms of action has become focal point and difficulty of research for herbalists. Although the existing research methods are numerous and constantly updated iteratively, there is remain a lack of prospective reviews. Hence, this paper provides a comprehensive account of existing new approaches and technologies based on previous studies with an ", "journal": "Arabian journal of chemistry", "date": "2022-10-04", "authors": ["YaoleiLi", "ZhijianLin", "YuWang", "ShanshanJu", "HaoWu", "HongyuJin", "ShuangchengMa", "BingZhang"], "doi": "10.1016/j.arabjc.2022.104302\n10.1039/c4lc00999a\n10.1016/j.jtice.2020.08.028\n10.1021/acs.analchem.7b04833\n10.1016/j.talanta.2020.121411\n10.1016/j.trac.2020.115923\n10.3390/cancers14010190\n10.1016/j.jddst.2021.102794\n10.1177/2472555219830087\n10.1186/s13073-016-0303-2\n10.1039/c8cc08530g\n10.4268/cjcmm20151720\n10.1016/j.indcrop.2020.112673\n10.1038/ncomms8489\n10.1039/D0NJ03221B\n10.3389/fphar.2017.00228\n10.1016/j.jpba.2017.06.001\n10.1016/j.aca.2019.11.042\n10.1016/j.apsb.2020.10.002\n10.1016/j.jep.2021.114172\n10.1016/j.talanta.2019.120554\n10.1021/acs.analchem.6b03557\n10.1016/j.scib.2021.01.005\n10.1016/j.yexcr.2020.112139\n10.1016/j.talanta.2020.121453\n10.1093/jmcb/mjaa036\n10.1007/s11101-019-09635-x\n10.3390/antiox11040658\n10.1002/pca.2888\n10.1038/ncb3312\n10.1016/j.chroma.2021.462237\n10.1007/s10911-016-9359-2\n10.1016/j.chroma.2018.11.024\n10.3390/pharmaceutics13091373\n10.1016/j.jep.2020.112886\n10.1021/ac00276a003\n10.1016/j.jpba.2018.02.005\n10.1080/13880209.2020.1822421\n10.1016/j.apsb.2020.01.019\n10.11842/wst.2015.05.025\n10.1016/j.ijms.2019.03.001\n10.1016/j.apsb.2021.11.007\n10.1016/j.phrs.2020.105077\n10.1016/j.phymed.2018.01.019\n10.1016/j.chroma.2021.462178\n10.1016/j.apsb.2021.08.030\n10.1021/acs.analchem.0c05277\n10.1038/s41467-018-07594-z\n10.3321/j.issn:0513-4870.2009.03.008\n10.3390/biom9100577\n10.1016/j.freeradbiomed.2021.11.023\n10.1016/j.jep.2019.111900\n10.1016/j.jchromb.2019.04.032\n10.1021/acs.analchem.0c03956\n10.1126/science.abn6158\n10.1186/s40580-021-00270-x\n10.1038/s41589-020-0555-4\n10.1126/science.1247125\n10.1021/acs.analchem.0c00227\n10.1016/j.jchromb.2018.03.046\n10.1016/j.apsb.2019.10.001\n10.1021/acsnano.9b01346\n10.1016/j.foodchem.2021.131528\n10.1016/j.procbio.2013.02.005\n10.1016/j.jpba.2019.112795\n10.1016/j.talanta.2021.122873\n10.1007/s00216-013-7612-8\n10.11669/cpj.2019.13.001\n10.1073/pnas.1706778114\n10.1016/j.jff.2018.09.024\n10.1016/j.jep.2021.114439\n10.1016/j.jchromb.2013.02.009\n10.1016/j.ijbiomac.2021.08.231\n10.1007/s11095-013-1283-1\n10.1016/j.jep.2020.112687\n10.1038/s41467-019-11370-y\n10.1016/j.apsb.2020.04.001\n10.1038/s41419-019-1638-6\n10.1016/j.jchromb.2021.122793\n10.1016/j.chroma.2019.03.053\n10.1016/j.phymed.2021.153790\n10.1002/anie.202002151\n10.1016/j.microc.2020.104949\n10.1038/srep35544\n10.1007/s00204-022-03234-0\n10.1038/s41587-021-01055-7\n10.3390/molecules25143256\n10.1016/j.copbio.2018.03.011\n10.1016/j.ijbiomac.2020.07.297\n10.19540/j.cnki.cjcmm.20210129.601\n10.3389/fphar.2018.00622\n10.1016/j.aca.2021.338452\n10.3969/j.issn.1001-859X.1999.03.001\n10.1136/gutjnl-2019-319114\n10.1007/s42242-020-00124-1\n10.1038/523266a\n10.1080/07391102.2020.1715260\n10.1016/j.biopha.2022.112895\n10.1016/j.chroma.2021.462021\n10.1016/j.ijbiomac.2020.10.018\n10.1016/j.chroma.2017.08.037\n10.1039/c4cc08728c\n10.1016/j.chroma.2019.460501\n10.1021/jasms.9b00097\n10.1016/j.aca.2018.12.020\n10.1155/2019/1983780\n10.16438/j.0513-4870.2019-0413\n10.1016/j.ijantimicag.2017.10.001\n10.1007/s00210-019-01786-0\n10.3969/j.issn.1674-3849.2002.02.001\n10.1021/acsami.1c06968\n10.1016/j.apsb.2019.12.004\n10.1016/j.jep.2020.112733\n10.1021/acs.jafc.7b05280\n10.1039/c9fo02475a\n10.1016/j.phrs.2021.105913\n10.1016/j.pharmthera.2021.107824\n10.1016/j.talanta.2019.120367\n10.1155/2019/5135692\n10.1039/c4ay01809e\n10.1016/j.chroma.2017.08.072\n10.1073/pnas.0712365105\n10.1016/j.jchromb.2021.122889\n10.1016/j.jpba.2021.113999\n10.1039/d0tb02661a\n10.1136/gutjnl-2017-315458\n10.1016/j.talanta.2019.03.047\n10.1038/srep37471\n10.16438/j.0513-4870.2018-1008\n10.1039/d0fo01262a\n10.1016/j.chroma.2018.09.027\n10.1038/d41586-018-01079-1\n10.3389/fphar.2020.00508\n10.1186/s13045-018-0662-9\n10.1016/j.talanta.2019.02.011\n10.1016/j.patcog.2020.107558\n10.7501/j.issn.0253-2670.2015.08.001\n10.4268/cjcmm20151719\n10.1016/j.phrs.2020.105034\n10.1016/j.jff.2017.11.049\n10.1039/c004590j\n10.1016/j.apsb.2021.02.017\n10.1038/s41374-018-0025-8\n10.19540/j.cnki.cjcmm.20190222.011\n10.1038/s41467-022-28494-3\n10.16438/j.0513-4870.2018-0912\n10.7501/j.issn.1674-6376.2019.07.036\n10.1016/j.eng.2019.03.009\n10.3389/fphar.2019.00650\n10.1016/j.eng.2018.11.008\n10.1186/s13007-020-00571-y\n10.1016/j.phymed.2018.03.003\n10.1021/acscentsci.9b01125\n10.1016/j.jpba.2020.113419\n10.1016/j.apsb.2019.12.011\n10.19540/j.cnki.cjcmm.20210222.201\n10.1016/j.phrs.2020.104935\n10.1016/j.jchromb.2021.122979\n10.3390/pharmaceutics13081280\n10.3390/molecules24193431"}
{"title": "Automatic Detection of Cases of COVID-19 Pneumonia from Chest X-ray Images and Deep Learning Approaches.", "abstract": "Machine learning has already been used as a resource for disease detection and health care as a complementary tool to help with various daily health challenges. The advancement of deep learning techniques and a large amount of data-enabled algorithms to outperform medical teams in certain imaging tasks, such as pneumonia detection, skin cancer classification, hemorrhage detection, and arrhythmia detection. Automated diagnostics, which are enabled by images extracted from patient examinations, allow for interesting experiments to be conducted. This research differs from the related studies that were investigated in the experiment. These works are capable of binary categorization into two categories. COVID-Net, for example, was able to identify a positive case of COVID-19 or a healthy person with 93.3% accuracy. Another example is CHeXNet, which has a 95% accuracy rate in detecting cases of pneumonia or a healthy state in a patient. Experiments revealed that the current study was more effective than the previous studies in detecting a greater number of categories and with a higher percentage of accuracy. The results obtained during the model's development were not only viable but also excellent, with an accuracy of nearly 96% when analyzing a chest X-ray with three possible diagnoses in the two experiments conducted.", "journal": "Computational intelligence and neuroscience", "date": "2022-10-04", "authors": ["FahimaHajjej", "SarraAyouni", "MalekHasan", "TanvirAbir"], "doi": "10.1155/2022/7451551\n10.1186/s41256-020-00135-6\n10.1186/s13643-021-01648-y\n10.21203/rs.3.rs-198847/v1\n10.1080/22221751.2020.1772678\n10.1016/j.media.2020.101794\n10.1155/2021/8148772\n10.21931/RB/2020.05.03.19\n10.1109/access.2020.3010226\n10.3233/XST-200715\n10.2991/ijcis.d.210518.001\n10.1016/j.cmpb.2020.105532\n10.1155/2022/4569879\n10.1504/ijcsm.2022.122146\n10.1016/j.matpr.2021.05.553\n10.1016/j.ijin.2020.12.002\n10.1007/s00521-022-06918-x\n10.3390/s22031211\n10.1155/2021/5759184\n10.32604/csse.2022.022014\n10.1016/j.bbe.2020.08.008"}
{"title": "Augmentation of literature review of COVID-19 radiology.", "abstract": "We suggest an augmentation of the excellent comprehensive review article titled \"Comprehensive literature review on the radiographic findings, imaging modalities, and the role of radiology in the coronavirus disease 2019 (COVID-19) pandemic\" under the following categories: (1) \"Inclusion of additional radiological features, related to pulmonary infarcts and to COVID-19 pneumonia\"; (2) \"Amplified discussion of cardiovascular COVID-19 manifestations and the role of cardiac magnetic resonance imaging in monitoring and prognosis\"; (3) \"Imaging findings related to fluorodeoxyglucose positron emission tomography, optical, thermal and other imaging modalities/devices, including 'intelligent edge' and other remote monitoring devices\"; (4) \"Artificial intelligence in COVID-19 imaging\"; (5) \"Additional annotations to the radiological images in the manuscript to illustrate the additional signs discussed\"; and (6) \"A minor correction to a passage on pulmonary destruction\".", "journal": "World journal of radiology", "date": "2022-10-04", "authors": ["Suleman AdamMerchant", "PrakashNadkarni", "Mohd Javed SaifullahShaikh"], "doi": "10.4329/wjr.v14.i9.342"}
{"title": "Wavelet transformation can enhance computed tomography texture features: a multicenter radiomics study for grade assessment of COVID-19 pulmonary lesions.", "abstract": "This study set out to develop a computed tomography (CT)-based wavelet transforming radiomics approach for grading pulmonary lesions caused by COVID-19 and to validate it using real-world data.\nThis retrospective study analyzed 111 patients with 187 pulmonary lesions from 16 hospitals; all patients had confirmed COVID-19 and underwent non-contrast chest CT. Data were divided into a training cohort (72 patients with 127 lesions from nine hospitals) and an independent test cohort (39 patients with 60 lesions from seven hospitals) according to the hospital in which the CT was performed. In all, 73 texture features were extracted from manually delineated lesion volumes, and 23 three-dimensional (3D) wavelets with eight decomposition modes were implemented to compare and validate the value of wavelet transformation for grade assessment. Finally, the optimal machine learning pipeline, valuable radiomic features, and final radiomic models were determined. The area under the receiver operating characteristic (ROC) curve (AUC), calibration curve, and decision curve were used to determine the diagnostic performance and clinical utility of the models.\nOf the 187 lesions, 108 (57.75%) were diagnosed as mild lesions and 79 (42.25%) as moderate/severe lesions. All selected radiomic features showed significant correlations with the grade of COVID-19 pulmonary lesions (P<0.05). Biorthogonal 1.1 (bior1.1) LLL was determined as the optimal wavelet transform mode. The wavelet transforming radiomic model had an AUC of 0.910 in the test cohort, outperforming the original radiomic model (AUC =0.880; P<0.05). Decision analysis showed the radiomic model could add a net benefit at any given threshold of probability.\nWavelet transformation can enhance CT texture features. Wavelet transforming radiomics based on CT images can be used to effectively assess the grade of pulmonary lesions caused by COVID-19, which may facilitate individualized management of patients with this disease.", "journal": "Quantitative imaging in medicine and surgery", "date": "2022-10-04", "authors": ["ZekunJiang", "JinYin", "PeilunHan", "NanChen", "QingboKang", "YueQiu", "YiyueLi", "QichengLao", "MiaoSun", "DanYang", "ShanHuang", "JiajunQiu", "KangLi"], "doi": "10.21037/qims-22-252\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(21)02143-7\n10.1148/radiol.2020200823\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020201160\n10.1038/s41551-021-00704-1\n10.1007/s12539-020-00410-7\n10.1016/j.chaos.2020.109944\n10.1038/s41467-020-17971-2\n10.2196/19569\n10.21037/qims-20-782\n10.1109/TMI.2020.3021254\n10.1371/journal.pone.0263916\n10.1016/j.compbiomed.2022.105298\n10.1016/j.media.2020.101910\n10.1148/ryct.2020200047\n10.1109/RBME.2020.2987975\n10.1016/j.acra.2020.09.004\n10.1155/2021/2263469\n10.1186/s40644-021-00406-6\n10.3322/caac.21552\n10.1016/j.ejrad.2020.108991\n10.21037/qims.2020.03.10\n10.1038/s41598-021-01470-5\n10.1186/s12885-020-6523-2\n10.3389/fonc.2018.00096\n10.1016/j.neuroimage.2006.01.015\n10.1158/0008-5472.CAN-17-0339\n10.1148/radiol.2020191145\n10.3390/cancers11121937\n10.18452/2487\n10.18452/2487\n10.1007/s40618-021-01672-8\n10.1017/S0033291719003027\n10.3390/diagnostics11081317\n10.1016/j.compbiomed.2021.104665\n10.1049/ipr2.12153\n10.1080/23808993.2019.1585805\n10.1109/MSP.2018.2877123\n10.1016/j.jksuci.2019.06.012\n10.1148/rg.2017170056\n10.1590/1678-9199-jvatitd-2020-0011\n10.1007/s00330-021-08368-w\n10.1007/s10278-021-00484-9\n10.2967/jnumed.118.222893\n10.1038/s41416-021-01381-2\n10.1038/nrclinonc.2017.141\n10.1007/s00330-019-06360-z"}
{"title": "Biomarkers extracted by fully automated body composition analysis from chest CT correlate with SARS-CoV-2 outcome severity.", "abstract": "The complex process of manual biomarker extraction from body composition analysis (BCA) has far restricted the analysis of SARS-CoV-2 outcomes to small patient cohorts and a limited number of tissue types. We investigate the association of two BCA-based biomarkers with the development of severe SARS-CoV-2 infections for 918 patients (354 female, 564 male) regarding disease severity and mortality (186 deceased). Multiple tissues, such as muscle, bone, or adipose tissue are used and acquired with a deep-learning-based, fully-automated BCA from computed tomography images of the chest. The BCA features and markers were univariately analyzed with a Shapiro-Wilk and two-sided Mann-Whitney-U test. In a multivariate approach, obtained markers were adjusted by a defined set of laboratory parameters promoted by other studies. Subsequently, the relationship between the markers and two endpoints, namely severity and mortality, was investigated with regard to statistical significance. The univariate approach showed that the muscle volume was significant for female (p", "journal": "Scientific reports", "date": "2022-10-01", "authors": ["Ren\u00e9Hosch", "SimoneKattner", "Marc MoritzBerger", "ThorstenBrenner", "JohannesHaubold", "JensKleesiek", "SvenKoitka", "LennardKroll", "AnisaKureishi", "NilsFlaschel", "FelixNensa"], "doi": "10.1038/s41598-022-20419-w\n10.1016/j.dsx.2020.06.060\n10.2196/26075\n10.1038/s41586-020-2521-4\n10.1016/S2213-8587(21)00089-9\n10.1016/j.clnesp.2020.09.018\n10.1186/s12911-021-01576-w\n10.1016/j.imu.2021.100564\n10.1016/j.isci.2021.103523\n10.1016/j.metabol.2020.154378\n10.1093/bja/aev541\n10.1007/s00261-020-02693-2\n10.1186/s12933-021-01327-1\n10.1002/oby.22971\n10.1080/17512433.2017.1347503\n10.1016/j.ejca.2015.12.030\n10.1002/jcsm.12379\n10.1097/RTI.0000000000000428\n10.1002/jcsm.12573\n10.1007/s00330-020-07147-3\n10.1016/j.ejrad.2021.110031\n10.1038/s41598-021-00161-5\n10.1111/nyas.12842\n10.3390/jcm10020356\n10.1038/oby.2008.575\n10.1038/ncpcardio0319\n10.1016/j.numecd.2013.11.010\n10.3389/fphys.2021.651167\n10.1007/s11906-019-0939-6\n10.1093/ehjci/jeu006\n10.1016/j.jcct.2017.11.007\n10.1016/j.amjcard.2016.01.033\n10.5812/ijem.3505\n10.1038/s41592-019-0686-2\n10.1016/j.metabol.2020.154436\n10.1148/radiol.2021204141"}
{"title": "AI in Health science: A Perspective.", "abstract": "By helping practitioners understand complicated and varied types of data, Artificial Intelligence (AI) has influenced medical practice deeply. It is the use of a computer to mimic intelligent behaviour. Many medical professions, particularly those reliant on imaging or surgery, are progressively developing AI. While AI cognitive component outperforms human intellect, it lacks awareness, emotions, intuition, and adaptability. With minimum human participation, AI is quickly growing in healthcare, and numerous AI applications have been created to address current issues. This article explains AI, its various elements and how to utilize them in healthcare. It also offers practical suggestions for developing an AI strategy to assist the digital healthcare transition.", "journal": "Current pharmaceutical biotechnology", "date": "2022-10-01", "authors": ["RaghavMishra", "KajalChaudhary", "IshaMishra"], "doi": "10.2174/1389201023666220929145220"}
{"title": "COVID-19 Semantic Pneumonia Segmentation and Classification Using Artificial Intelligence.", "abstract": "Coronavirus 2019 (COVID-19) has become a pandemic. The seriousness of COVID-19 can be realized from the number of victims worldwide and large number of deaths. This paper presents an efficient deep semantic segmentation network (DeepLabv3Plus). Initially, the dynamic adaptive histogram equalization is utilized to enhance the images. Data augmentation techniques are then used to augment the enhanced images. The second stage builds a custom convolutional neural network model using several pretrained ImageNet models and compares them to repeatedly trim the best-performing models to reduce complexity and improve memory efficiency. Several experiments were done using different techniques and parameters. Furthermore, the proposed model achieved an average accuracy of 99.6% and an area under the curve of 0.996 in the COVID-19 detection. This paper will discuss how to train a customized smart convolutional neural network using various parameters on a set of chest X-rays with an accuracy of 99.6%.", "journal": "Contrast media & molecular imaging", "date": "2022-10-01", "authors": ["Mohammed JAbdulaal", "Ibrahim MMehedi", "Abdullah MAbusorrah", "Abdulah JezaAljohani", "Ahmad HMilyani", "Md MasudRana", "MohamedMahmoud"], "doi": "10.1155/2022/5297709\n10.1109/tnnls.2020.2995800\n10.1109/tnnls.2017.2766168\n10.1016/j.media.2020.101836\n10.1109/access.2020.3016780\n10.1016/j.mehy.2020.109761\n10.1016/j.imu.2020.100360\n10.1016/j.knosys.2020.106270\n10.1016/j.asoc.2020.106580\n10.1109/ICNN.1993.298572\n10.1109/tmi.2016.2528162\n10.1007/s13246-020-00888-x\n10.1016/j.compbiomed.2020.103792\n10.1001/jama.2020.3786\n10.1007/978-981-16-7618-5_3\n10.1016/j.eswa.2020.114054\n10.7717/peerj-cs.358\n10.1109/tgrs.2022.3155765\n10.1504/ijhm.2021.114174\n10.1109/access.2020.2971257\n10.1038/s41598-020-76550-z\n10.1101/2020.03.26.20044610\n10.1016/j.cmpb.2020.105532\n10.3389/fpubh.2020.00437\n10.1109/access.2020.3003810\n10.1016/j.patrec.2020.04.016\n10.1109/access.2019.2941937\n10.1504/ijhm.2021.114173\n10.1109/access.2021.3120717\n10.1504/ijhm.2021.120616\n10.1109/access.2021.3101142"}
{"title": "A novel multimodal fusion framework for early diagnosis and accurate classification of COVID-19 patients using X-ray images and speech signal processing techniques.", "abstract": "COVID-19 outbreak has become one of the most challenging problems for human being. It is a communicable disease caused by a new coronavirus strain, which infected over 375 million people already and caused almost 6 million deaths. This paper aims to develop and design a framework for early diagnosis and fast classification of COVID-19 symptoms using multimodal Deep Learning techniques.\nwe collected chest X-ray and cough sample data from open source datasets, Cohen and datasets and local hospitals. The features are extracted from the chest X-ray images are extracted from chest X-ray datasets. We also used cough audio datasets from Coswara project and local hospitals. The publicly available Coughvid DetectNow and Virufy datasets are used to evaluate COVID-19 detection based on speech sounds, respiratory, and cough. The collected audio data comprises slow and fast breathing, shallow and deep coughing, spoken digits, and phonation of sustained vowels. Gender, geographical location, age, preexisting medical conditions, and current health status (COVID-19 and Non-COVID-19) are recorded.\nThe proposed framework uses the selection algorithm of the pre-trained network to determine the best fusion model characterized by the pre-trained chest X-ray and cough models. Third, deep chest X-ray fusion by discriminant correlation analysis is used to fuse discriminatory features from the two models. The proposed framework achieved recognition accuracy, specificity, and sensitivity of 98.91%, 96.25%, and 97.69%, respectively. With the fusion method we obtained 94.99% accuracy.\nThis paper examines the effectiveness of well-known ML architectures on a joint collection of chest-X-rays and cough samples for early classification of COVID-19. It shows that existing methods can effectively used for diagnosis and suggesting that the fusion learning paradigm could be a crucial asset in diagnosing future unknown illnesses. The proposed framework supports health informatics basis on early diagnosis, clinical decision support, and accurate prediction.", "journal": "Computer methods and programs in biomedicine", "date": "2022-09-30", "authors": ["SantoshKumar", "Mithilesh KumarChaube", "Saeed HamoodAlsamhi", "Sachin KumarGupta", "MohsenGuizani", "RaffaeleGravina", "GiancarloFortino"], "doi": "10.1016/j.cmpb.2022.107109\n10.1109/TDSC.2022.3144657"}
{"title": "A Deep Learning based Solution (Covi-DeteCT) Amidst COVID-19.", "abstract": "The whole world has been severely affected due to the COVID-19 pandemic. The rapid and large-scale spread has caused immense pressure on the medical sector hence increasing the chances of false detection due to human errors and mishandling of reports. At the time of outbreaks of COVID-19, there is a crucial shortage of test kits as well. Quick diagnostic testing has become one of the main challenges. For the detection of COVID-19, many Artificial Intelligence based methodologies have been proposed, a few had suggested integration of the model on a public usable platform, but none had executed this on a working application as per our knowledge.\nKeeping the above comprehension in mind, the objective is to provide an easy-to-use platform for COVID-19 identification. This work would be a contribution to the digitization of health facilities. This work is a fusion of deep learning classifiers and medical images to provide a speedy and accurate identification of the COVID-19 virus by analyzing the user's CT scan images of the lungs. It will assist healthcare workers in reducing their workload and decreasing the possibility of false detection.\nIn this work, various models like Resnet50V2 and Resnet101V2, an adjusted rendition of ResNet101V2 with Feature Pyramid Network, have been applied for classifying the CT scan images into the categories: normal or COVID-19 positive.\nA detailed analysis of all three models' performances have been done on the SARS-CoV-2 dataset with various metrics like precision, recall, F1-score, ROC curve, etc. It was found that Resnet50V2 achieves an accuracy of 96.79%, whereas Resnet101V2 achieves an accuracy of 97.79%. An accuracy of 98.19% has been obtained by ResNet101V2 with Feature Pyramid Network. As ResNet101V2 with Feature Pyramid Network is showing better results, thus, it is further incorporated into a working application that takes CT images as input from the user and feeds into the trained model and detects the presence of COVID-19 infection.\nA mobile application integrated with the deeper variant of ResNet, i.e., ResNet101V2 with FPN checks the presence of COVID-19 in a faster and accurate manner. People can use this application on their smart mobile devices. This automated system would assist healthcare workers as well, which ultimately reduces their workload and decreases the possibility of false detection.", "journal": "Current medical imaging", "date": "2022-09-30", "authors": ["KavitaPandey"], "doi": "10.2174/1573405618666220928145344"}
{"title": "The 2021 SIIM-FISABIO-RSNA Machine Learning COVID-19 Challenge: Annotation and Standard Exam Classification of COVID-19 Chest Radiographs.", "abstract": "We describe the curation, annotation methodology, and characteristics of the dataset used in an artificial intelligence challenge for detection and localization of COVID-19 on chest radiographs. The chest radiographs were annotated by an international group of radiologists into four mutually exclusive categories, including \"typical,\" \"indeterminate,\" and \"atypical appearance\" for COVID-19, or \"negative for pneumonia,\" adapted from previously published guidelines, and bounding boxes were placed on airspace opacities. This dataset and respective annotations are available to researchers for academic and noncommercial use.", "journal": "Journal of digital imaging", "date": "2022-09-29", "authors": ["ParasLakhani", "JMongan", "CSinghal", "QZhou", "K PAndriole", "W FAuffermann", "P MPrasanna", "T XPham", "MichaelPeterson", "P JBergquist", "T SCook", "S FFerraciolli", "G C ACorradi", "M STakahashi", "C SWorkman", "MParekh", "S IKamel", "JGalant", "AMas-Sanchez", "E CBen\u00edtez", "MS\u00e1nchez-Valverde", "LJaques", "MPanadero", "MVidal", "MCulia\u00f1ez-Casas", "DAngulo-Gonzalez", "S GLanger", "Mar\u00edade la Iglesia-Vay\u00e1", "GShih"], "doi": "10.1007/s10278-022-00706-8\n10.1016/S2213-2600(20)30527-0\n10.1016/j.jped.2020.08.001\n10.1177/074823378500100206\n10.1007/s00134-012-2682-1\n10.1148/radiol.2020201365\n10.1148/radiol.2017162326\n10.1371/journal.pmed.1002697\n10.1148/radiol.2020201874\n10.1007/s00330-020-07269-8\n10.2214/AJR.20.24801\n10.1007/s11547-020-01202-1\n10.1016/j.acra.2021.01.016\n10.1148/radiol.2021203957\n10.1148/radiol.2018180736\n10.1007/s10278-019-00299-9\n10.1037/1040-3590.6.4.284\n10.1148/radiol.2020201160"}
{"title": "IEViT: An enhanced vision transformer architecture for chest X-ray image classification.", "abstract": "Chest X-ray imaging is a relatively cheap and accessible diagnostic tool that can assist in the diagnosis of various conditions, including pneumonia, tuberculosis, COVID-19, and others. However, the requirement for expert radiologists to view and interpret chest X-ray images can be a bottleneck, especially in remote and deprived areas. Recent advances in machine learning have made possible the automated diagnosis of chest X-ray scans. In this work, we examine the use of a novel Transformer-based deep learning model for the task of chest X-ray image classification.\nWe first examine the performance of the Vision Transformer (ViT) state-of-the-art image classification machine learning model for the task of chest X-ray image classification, and then propose and evaluate the Input Enhanced Vision Transformer (IEViT), a novel enhanced Vision Transformer model that can achieve improved performance on chest X-ray images associated with various pathologies.\nExperiments on four chest X-ray image data sets containing various pathologies (tuberculosis, pneumonia, COVID-19) demonstrated that the proposed IEViT\u00a0model outperformed ViT for all the data sets and variants examined, achieving an F1-score between 96.39% and 100%, and an improvement over ViT of up to +5.82% in terms of F1-score across the four examined data sets. IEViT's maximum sensitivity (recall) ranged between 93.50% and 100% across the four data sets, with an improvement over ViT of up to +3%, whereas IEViT's maximum precision ranged between 97.96% and 100% across the four data sets, with an improvement over ViT of up to +6.41%.\nResults showed that the proposed IEViT\u00a0model outperformed all ViT's variants for all the examined chest X-ray image data sets, demonstrating its superiority and generalisation ability. Given the relatively low cost and the widespread accessibility of chest X-ray imaging, the use of the proposed IEViT\u00a0model can potentially offer a powerful, but relatively cheap and accessible method for assisting diagnosis using chest X-ray images.", "journal": "Computer methods and programs in biomedicine", "date": "2022-09-27", "authors": ["Gabriel IluebeOkolo", "StamosKatsigiannis", "NaeemRamzan"], "doi": "10.1016/j.cmpb.2022.107141"}
{"title": "Machine learning techniques for CT imaging diagnosis of novel coronavirus pneumonia: a review.", "abstract": "Since 2020, novel coronavirus pneumonia has been spreading rapidly around the world, bringing tremendous pressure on medical diagnosis and treatment for hospitals. Medical imaging methods, such as computed tomography (CT), play a crucial role in diagnosing and treating COVID-19. A large number of CT images (with large volume) are produced during the CT-based medical diagnosis. In such a situation, the diagnostic judgement by human eyes on the thousands of CT images is inefficient and time-consuming. Recently, in order to improve diagnostic efficiency, the machine learning technology is being widely used in computer-aided diagnosis and treatment systems (i.e., CT Imaging) to help doctors perform accurate analysis and provide them with effective diagnostic decision support. In this paper, we comprehensively review these frequently used machine learning methods applied in the CT Imaging Diagnosis for the COVID-19, discuss the machine learning-based applications from the various kinds of aspects including the image acquisition and pre-processing, image segmentation, quantitative analysis and diagnosis, and disease follow-up and prognosis. Moreover, we also discuss the limitations of the up-to-date machine learning technology in the context of CT imaging computer-aided diagnosis.", "journal": "Neural computing & applications", "date": "2022-09-27", "authors": ["JingjingChen", "YixiaoLi", "LinglingGuo", "XiaokangZhou", "YihanZhu", "QingfengHe", "HaijunHan", "QilongFeng"], "doi": "10.1007/s00521-022-07709-0\n10.1016/j.ijid.2020.01.009\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.3906/sag-2004-331\n10.1183/13993003.00334-2020\n10.1183/13993003.00398-2020\n10.1007/s00330-020-06731-x\n10.1145/3411760\n10.1007/s12194-017-0406-5\n10.1038/s41591-018-0177-5\n10.1183/13993003.00986-2018\n10.1016/S2213-2600(18)30286-8\n10.1183/13993003.01216-2019\n10.1016/j.media.2017.06.014\n10.1016/j.bspc.2019.101586\n10.1016/j.cell.2020.04.045\n10.1117/1.JMI.5.3.036501\n10.1016/j.media.2010.02.004\n10.1007/s10278-012-9539-6\n10.1016/j.ijleo.2015.06.011\n10.1016/j.media.2012.02.001\n10.1145/3065386\n10.1007/978-3-030-00889-5_1\n10.1109/TPAMI.2017.2699184\n10.1109/JIOT.2021.3130434\n10.1109/TII.2021.3116085\n10.1109/JIOT.2021.3077449\n10.1109/TMI.2016.2536809\n10.1186/s12880-018-0286-0\n10.1038/s41591-020-0931-3\n10.1016/j.compbiomed.2020.103795\n10.1109/TMI.2020.2996256\n10.2196/19569\n10.1155/2020/8843664\n10.1016/j.ejrad.2020.109041\n10.21037/atm.2020.03.132\n10.1148/radiol.2020200905\n10.1016/j.eng.2020.04.010\n10.1016/j.patrec.2020.10.001\n10.1109/TCBB.2020.2994780\n10.1038/s41598-021-83424-5\n10.1148/radiol.2020201491\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2994908\n10.1016/j.chaos.2020.110153\n10.1038/s41598-021-84561-7\n10.1088/1361-6560/abbf9e\n10.1016/j.jpha.2020.03.004\n10.1109/TII.2021.3061419\n10.1186/s12938-020-00807-x\n10.1109/RBME.2020.2987975\n10.1183/13993003.00775-2020\n10.32604/cmc.2020.010691\n10.1109/TCSS.2020.2987846\n10.1007/s11739-020-02475-0\n10.1007/s00330-020-07042-x\n10.1038/s41467-020-17280-8\n10.3389/fpubh.2020.00357\n10.2196/20259\n10.3390/jcm9061668\n10.1017/S0950268820001727\n10.1038/s42256-021-00307-0\n10.1007/s00330-020-07156-2\n10.1109/ACCESS.2018.2870052\n10.1016/j.inffus.2019.12.012\n10.1038/s41467-020-18685-1\n10.1016/j.radonc.2018.10.019\n10.48550/arXiv.2005.06465\n10.1186/s41747-020-00173-2\n10.1016/j.ijleo.2015.06.011"}
{"title": "E-GCS: Detection of COVID-19 through classification by attention bottleneck residual network.", "abstract": "Recently,\u2009 the coronavirus disease 2019 (COVID-19)\u2009 has caused mortality of many people globally. Thus, there existed a need to detect this disease to prevent its further spread. Hence, the study aims to predict COVID-19 infected patients based on deep learning (DL) and image processing.\nThe study intends to classify the normal and abnormal cases of COVID-19 by considering three different medical imaging modalities namely ultrasound imaging, X-ray images and CT scan images through introduced attention bottleneck residual network (AB-ResNet). It also aims to segment the abnormal infected area from normal images for\u2009localizing localising the disease infected area through the proposed edge based graph cut segmentation (E-GCS).\nAB-ResNet is used for classifying images whereas E-GCS segment the abnormal images. The study possess various advantages as it rely on DL and possess capability for accelerating the training speed of deep networks. It also enhance the network depth leading to minimum parameters, minimising the impact of vanishing gradient issue and attaining effective network performance with respect to better accuracy.\nPerformance and comparative analysis is undertaken to evaluate the efficiency of the introduced system and results explores the efficiency of the proposed system in COVID-19 detection with high accuracy (99%).", "journal": "Engineering applications of artificial intelligence", "date": "2022-09-27", "authors": ["TAhila", "A CSubhajini"], "doi": "10.1016/j.engappai.2022.105398"}
{"title": "Application of Deep Learning Techniques in Diagnosis of Covid-19 (Coronavirus): A Systematic Review.", "abstract": "Covid-19 is now one of the most incredibly intense and severe illnesses of the twentieth century. Covid-19 has already endangered the lives of millions of people worldwide due to its acute pulmonary effects. Image-based diagnostic techniques like X-ray, CT, and ultrasound are commonly employed to get a quick and reliable clinical condition. Covid-19 identification out of such clinical scans is exceedingly time-consuming, labor-intensive, and susceptible to silly intervention. As a result, radiography imaging approaches using Deep Learning (DL) are consistently employed to achieve great results. Various artificial intelligence-based systems have been developed for the early prediction of coronavirus using radiography pictures. Specific DL methods such as CNN and RNN noticeably extract extremely critical characteristics, primarily in diagnostic imaging. Recent coronavirus studies have used these techniques to utilize radiography image scans significantly. The disease, as well as the present pandemic, was studied using public and private data. A total of 64 pre-trained and custom DL models concerning imaging modality as taxonomies are selected from the studied articles. The constraints relevant to DL-based techniques are the sample selection, network architecture, training with minimal annotated database, and security issues. This includes evaluating causal agents, pathophysiology, immunological reactions, and epidemiological illness. DL-based Covid-19 detection systems are the key focus of this review article. Covid-19 work is intended to be accelerated as a result of this study.", "journal": "Neural processing letters", "date": "2022-09-27", "authors": ["Yogesh HBhosale", "K SridharPatnaik"], "doi": "10.1007/s11063-022-11023-0\n10.1101/2020.02.25.20021568\n10.1016/j.scs.2020.102571\n10.1109/ACCESS.2021.3058066\n10.1002/jmv.26709\n10.1109/ACCESS.2020.3003810\n10.33889/IJMEMS.2020.5.4.052\n10.1080/14737159.2021.1962708\n10.1016/j.matpr.2020.06.245\n10.1016/j.jiph.2020.03.019\n10.1080/14737159.2020.1757437\n10.1109/ACCESS.2021.3054484\n10.1080/14737159.2020.1816466\n10.1109/JBHI.2020.3030224\n10.1007/s12559-020-09787-5\n10.1080/07391102.2020.1767212\n10.1016/j.scs.2021.102777\n10.1007/s10489-020-01831-z\n10.1109/RBME.2020.2990959\n10.1007/s00521-020-05437-x\n10.1097/RLI.0000000000000748\n10.1148/radiol.2020200905\n10.1109/ACCESS.2021.3058537\n10.1016/j.csbj.2021.02.016\n10.1016/j.chaos.2020.110120\n10.2196/19569\n10.1007/s00500-020-05424-3\n10.1038/s41746-021-00399-3\n10.1016/j.ijleo.2021.166405\n10.1109/TII.2021.3057683\n10.1016/j.chaos.2020.110245\n10.1109/JBHI.2020.3037127\n10.1016/j.asoc.2021.107184\n10.1016/j.irbm.2021.01.004\n10.1016/j.ejrad.2020.109041\n10.1016/j.aej.2021.01.011\n10.1016/j.asoc.2020.106859\n10.1016/j.chaos.2020.110190\n10.1016/j.asoc.2020.106744\n10.1016/j.asoc.2020.106885\n10.1007/s10489-020-01902-1\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.7937/tcia.2020.6c7y-gq39\n10.7910/DVN/6ACUZJ\n10.1016/j.ejrad.2020.109402\n10.21037/atm.2020.03.132\n10.1016/j.chaos.2020.110495\n10.1016/j.imu.2020.100505\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2021.107160\n10.1109/ACCESS.2020.3016780\n10.1016/j.mehy.2020.109761\n10.1016/j.imu.2020.100427\n10.1007/s10140-020-01886-y\n10.1016/j.media.2020.101913\n10.1016/j.iot.2021.100377\n10.1002/ima.22706,32,2,(419-434)\n10.1007/s00264-020-04609-7\n10.1016/j.bspc.2021.102490\n10.1016/j.knosys.2020.106647\n10.1016/j.ibmed.2020.100013\n10.1016/j.imu.2020.100506\n10.1016/j.scs.2020.102589\n10.3390/app11020672\n10.1016/j.advms.2020.06.005\n10.1109/TMI.2020.2994459\n10.1016/j.media.2021.101993\n10.1016/j.compeleceng.2020.106960\n10.1109/ACCESS.2020.3005510\n10.1109/ACCESS.2020.2994762\n10.3348/kjr.2020.0536\n10.3390/electronics9091439\n10.1007/s42600-021-00132-9\n10.1186/s40537-020-00392-9\n10.1016/j.compbiomed.2020.103792"}
{"title": "Rapid artificial intelligence solutions in a pandemic-The COVID-19-20 Lung CT Lesion Segmentation Challenge.", "abstract": "Artificial intelligence (AI) methods for the automatic detection and quantification of COVID-19 lesions in chest computed tomography (CT) might play an important role in the monitoring and management of the disease. We organized an international challenge and competition for the development and comparison of AI algorithms for this task, which we supported with public data and state-of-the-art benchmark methods. Board Certified Radiologists annotated 295 public images from two sources (A and B) for algorithms training (n=199, source A), validation (n=50, source A) and testing (n=23, source A; n=23, source B). There were 1,096 registered teams of which 225 and 98 completed the validation and testing phases, respectively. The challenge showed that AI models could be rapidly designed by diverse teams with the potential to measure disease or facilitate timely and patient-specific interventions. This paper provides an overview and the major outcomes of the COVID-19 Lung CT Lesion Segmentation Challenge - 2020.", "journal": "Medical image analysis", "date": "2022-09-27", "authors": ["Holger RRoth", "ZiyueXu", "CarlosTor-D\u00edez", "RamonSanchez Jacob", "JonathanZember", "JoseMolto", "WenqiLi", "ShengXu", "BarisTurkbey", "EvrimTurkbey", "DongYang", "AhmedHarouni", "NicolaRieke", "ShishuaiHu", "FabianIsensee", "ClaireTang", "QinjiYu", "JanS\u00f6lter", "TongZheng", "VitaliLiauchuk", "ZiqiZhou", "Jan HendrikMoltz", "BrunoOliveira", "YongXia", "Klaus HMaier-Hein", "QikaiLi", "AndreasHusch", "LuyangZhang", "VassiliKovalev", "LiKang", "AlessaHering", "Jo\u00e3o LVila\u00e7a", "MonaFlores", "DaguangXu", "BradfordWood", "Marius GeorgeLinguraru"], "doi": "10.1016/j.media.2022.102605"}
{"title": "State of the Art in Lung Ultrasound, Shifting from Qualitative to Quantitative Analyses.", "abstract": "Lung ultrasound (LUS) has been increasingly expanding since the 1990s, when the clinical relevance of vertical artifacts was first reported. However, the massive spread of LUS is only recent and is associated with the coronavirus disease 2019 (COVID-19) pandemic, during which semi-quantitative computer-aided techniques were proposed to automatically classify LUS data. In this review, we discuss the state of the art in LUS, from semi-quantitative image analysis approaches to quantitative techniques involving the analysis of radiofrequency data. We also discuss recent in vitro and in silico studies, as well as research on LUS safety. Finally, conclusions are drawn highlighting the potential future of LUS.", "journal": "Ultrasound in medicine & biology", "date": "2022-09-27", "authors": ["FedericoMento", "UmairKhan", "FrancescoFaita", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone", "LibertarioDemi"], "doi": "10.1016/j.ultrasmedbio.2022.07.007"}
{"title": "Advancements in COVID- 19 Testing: An in-depth overview.", "abstract": "COVID-19 rapidly evolved as a pandemic, killing and hospitalising millions of people, creating unprecedented hurdles for communities and health care systems worldwide. The rapidly evolving pandemic prompted the head of the World Health Organisation to deliver a critical message: \"test, test, test.\" The response from the diagnostic industry and researchers worldwide was overwhelming, resulting in more than a thousand commercial tests available in the market worldwide. Several sampling approaches and diagnostic techniques have been employed from the early stages of the pandemic, such as SARS-CoV-2 detection by targeting the viral RNA or protein, indirectly via antibody testing, biochemical estimation, and various imaging techniques, and many are still in the various stages of development and yet to be marketed. Accurate testing techniques and appropriate sampling are the need of the hour to manage, diagnose and treat the pandemic, especially in the current crisis, where SARS-CoV-2 undergoes constant mutation, evolving into various strains, which are pretty challenging. The article discusses various testing techniques as well as screening methods for detection, treatment, and management of COVID-19 transmission, such as NAAT, PCR, isothermal detection including RT-LAMP, RPA, NASBA, RCA, SDA, NEAR, and TMA, CRISPR strategy, nanotechnology approach, metagenomic profiling, point of care tests, virus neutralization test, ELISA, biomarker estimation, utilization of imaging techniques such as CT, ultrasonography, brain MRI in COVID-19 complications, and other novel strategies including microarray methods, microfluidic methods and artificial intelligence with an emphasis on advancements in the testing strategies for the diagnosis, management, and prevention of COVID-19.", "journal": "Current pharmaceutical biotechnology", "date": "2022-09-27", "authors": ["RajeshKumar", "SeethaHarilal", "Abdullah GAl-Sehemi", "MehboobaliPannipara", "Githa ElizabethMathew", "BijoMathew"], "doi": "10.2174/1389201023666220921144150"}
{"title": "Analysis of the Causes of Solitary Pulmonary Nodule Misdiagnosed as Lung Cancer by Using Artificial Intelligence: A Retrospective Study at a Single Center.", "abstract": "Artificial intelligence (AI) adopting deep learning technology has been widely used in the med-ical imaging domain in recent years. It realized the automatic judgment of benign and malig-nant solitary pulmonary nodules (SPNs) and even replaced the work of doctors to some extent. However, misdiagnoses can occur in certain cases. Only by determining the causes can AI play a larger role. A total of 21 Coronavirus disease 2019 (COVID-19) patients were diagnosed with SPN by CT imaging. Their Clinical data, including general condition, imaging features, AI re-ports, and outcomes were included in this retrospective study. Although they were confirmed COVID-19 by testing reverse transcription-polymerase chain reaction (RT-PCR) with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), their CT imaging data were misjudged by AI to be high-risk nodules for lung cancer. Imaging characteristics included burr sign (76.2%), lobulated sign (61.9%), pleural indentation (42.9%), smooth edges (23.8%), and cavity (14.3%). The accuracy of AI was different from that of radiologists in judging the nature of be-nign SPNs (p < 0.001, \u03ba = 0.036 < 0.4, means the two diagnosis methods poor fit). COVID-19 patients with SPN might have been misdiagnosed using the AI system, suggesting that the AI system needs to be further optimized, especially in the event of a new disease outbreak.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-09-24", "authors": ["Xiong-YingWu", "FanDing", "KunLi", "Wen-CaiHuang", "YongZhang", "JianZhu"], "doi": "10.3390/diagnostics12092218\n10.1016/j.chest.2017.01.018\n10.1148/radiol.2017161659\n10.1109/TMI.2016.2629462\n10.1016/j.media.2017.06.015\n10.1002/mp.12846\n10.1109/TBME.2016.2613502\n10.1038/srep24454\n10.1016/j.cell.2020.04.045\n10.3390/cancers12082211\n10.1056/NEJMoa2001316\n10.1155/2017/4067832\n10.1038/srep46479\n10.1371/journal.pone.0248957\n10.1016/j.jtho.2020.04.030\n10.1016/j.jinf.2020.03.033\n10.1007/s00330-020-07042-x\n10.1097/CM9.0000000000000634\n10.3390/cancers11111673\n10.1111/1759-7714.13185\n10.1016/j.cell.2018.02.010\n10.1136/thoraxjnl-2019-214104\n10.1038/s41586-020-2012-7\n10.1016/j.compbiomed.2012.12.004\n10.1038/nbt.4233\n10.1136/bmj.m606"}
{"title": "Segmentation-Based Classification Deep Learning Model Embedded with Explainable AI for COVID-19 Detection in Chest X-ray Scans.", "abstract": "Background and Motivation: COVID-19 has resulted in a massive loss of life during the last two years. The current imaging-based diagnostic methods for COVID-19 detection in multiclass pneumonia-type chest X-rays are not so successful in clinical practice due to high error rates. Our hypothesis states that if we can have a segmentation-based classification error rate <5%, typically adopted for 510 (K) regulatory purposes, the diagnostic system can be adapted in clinical settings. Method: This study proposes 16 types of segmentation-based classification deep learning-based systems for automatic, rapid, and precise detection of COVID-19. The two deep learning-based segmentation networks, namely UNet and UNet+, along with eight classification models, namely VGG16, VGG19, Xception, InceptionV3, Densenet201, NASNetMobile, Resnet50, and MobileNet, were applied to select the best-suited combination of networks. Using the cross-entropy loss function, the system performance was evaluated by Dice, Jaccard, area-under-the-curve (AUC), and receiver operating characteristics (ROC) and validated using Grad-CAM in explainable AI framework. Results: The best performing segmentation model was UNet, which exhibited the accuracy, loss, Dice, Jaccard, and AUC of 96.35%, 0.15%, 94.88%, 90.38%, and 0.99 (p-value <0.0001), respectively. The best performing segmentation-based classification model was UNet+Xception, which exhibited the accuracy, precision, recall, F1-score, and AUC of 97.45%, 97.46%, 97.45%, 97.43%, and 0.998 (p-value <0.0001), respectively. Our system outperformed existing methods for segmentation-based classification models. The mean improvement of the UNet+Xception system over all the remaining studies was 8.27%. Conclusion: The segmentation-based classification is a viable option as the hypothesis (error rate <5%) holds true and is thus adaptable in clinical practice.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-09-24", "authors": ["NoneNillmani", "NeerajSharma", "LucaSaba", "Narendra NKhanna", "Mannudeep KKalra", "Mostafa MFouda", "Jasjit SSuri"], "doi": "10.3390/diagnostics12092132\n10.1371/journal.pone.0249788\n10.1016/j.compbiomed.2020.103960\n10.1186/s13244-022-01176-w\n10.1007/s10554-020-02089-9\n10.1001/jama.2020.3786\n10.1016/j.acra.2015.12.010\n10.1148/radiol.2015150425\n10.1118/1.2836950\n10.1016/j.ejrad.2019.02.038\n10.1038/s42256-020-0186-1\n10.1016/j.zemedi.2018.11.002\n10.1016/j.compbiomed.2018.05.014\n10.3390/cancers11010111\n10.1007/s10916-021-01707-w\n10.1007/s11548-021-02317-0\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01902-1\n10.1016/j.bspc.2020.102365\n10.1148/radiol.2020203511\n10.3390/diagnostics12061482\n10.1016/j.cmpb.2017.07.011\n10.3390/biomedicines9070720\n10.1016/j.compbiomed.2016.06.010\n10.1016/j.compbiomed.2020.103847\n10.1016/j.compbiomed.2021.104721\n10.23736/S0392-9590.21.04771-4\n10.3390/diagnostics12051283\n10.3390/diagnostics11112109\n10.1016/j.compbiomed.2020.103958\n10.1109/ACCESS.2021.3086020\n10.2352/J.ImagingSci.Technol.2020.64.2.020508\n10.1109/TIM.2022.3174270\n10.1007/s11222-009-9153-8\n10.1109/TKDE.2019.2912815\n10.1016/j.neucom.2018.05.011\n10.1109/JBHI.2022.3177854\n10.1109/ACCESS.2020.3031384\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.17632/rscbjbr9sj.2\n10.1016/j.cell.2018.02.010\n10.1016/j.compbiomed.2022.105571\n10.1016/j.compbiomed.2017.10.022\n10.1109/ACCESS.2019.2962617\n10.1007/s13755-021-00166-4\n10.3390/diagnostics12030652\n10.1016/j.eswa.2021.116288\n10.1007/s11277-018-5777-3\n10.1007/s11277-018-5702-9\n10.1186/s12880-020-00514-y\n10.1109/ACCESS.2020.3017915\n10.3390/s21217116\n10.1016/j.cmpb.2019.06.005\n10.1038/s41598-021-99015-3\n10.1016/j.asoc.2020.106912\n10.3390/s22031211\n10.1109/TMI.2020.2993291\n10.1007/s00330-021-08050-1\n10.1016/j.bspc.2021.103182\n10.1016/j.bea.2022.100041\n10.1016/j.compbiomed.2022.105244\n10.1016/j.neucom.2021.03.034\n10.1007/s10916-016-0504-7\n10.1016/j.ejrad.2022.110164\n10.1142/S0219467801000402\n10.1016/j.mri.2012.04.021\n10.1007/s10554-020-02124-9\n10.3390/sym14071310"}
{"title": "Optometrist's perspectives of Artificial Intelligence in eye care.", "abstract": "The application of artificial intelligence (AI) in diagnosing and managing ocular disease has gained popularity as research highlights the utilization of AI to improve personalized medicine and healthcare outcomes. The objective of this study is to describe current optometric perspectives of AI in eye care.\nMembers of the American Academy of Optometry were sent an electronic invitation to complete a 17-item survey. Survey items assessed perceived advantages and concerns regarding AI using a 5-point Likert scale ranging from \"strongly agree\" to \"strongly disagree.\"\nA total of 400 optometrists completed the survey. The mean number of years since optometry school completion was 25 \u00b1 15.1. Most respondents reported familiarity with AI (66.8%). Though half of optometrists had concerns about the diagnostic accuracy of AI (53.0%), most believed it would improve the practice of optometry (72.0%). Optometrists reported their willingness to incorporate AI into practice increased from 53.3% before the COVID-19 pandemic to 65.5% after onset of the pandemic (p<0.001).\nIn this study, optometrists are optimistic about the use of AI in eye care, and willingness to incorporate AI in clinical practice also increased after the onset of the COVID-19 pandemic.", "journal": "Journal of optometry", "date": "2022-09-23", "authors": ["Angelica CScanzera", "EllenShorter", "CharlesKinnaird", "NitaValikodath", "TalaAl-Khaled", "EmilyCole", "SashaKravets", "Joelle AHallak", "TimothyMcMahon", "R V PaulChan"], "doi": "10.1016/j.optom.2022.06.006"}
{"title": "Identification of micro- and nanoplastics released from medical masks using hyperspectral imaging and deep learning.", "abstract": "Apart from other severe consequences, the COVID-19 pandemic has inflicted a surge in personal protective equipment usage, some of which, such as medical masks, have a short effective protection time. Their misdisposition and subsequent natural degradation make them huge sources of micro- and nanoplastic particles. To better understand the consequences of the direct influence of microplastic pollution on biota, there is an urgent need to develop a reliable and high-throughput analytical tool for sub-micrometre plastic identification and visualisation in environmental and biological samples. This study evaluated the application of a combined technique based on dark-field enhanced microscopy and hyperspectral imaging augmented with deep learning data analysis for the visualisation, detection and identification of microplastic particles released from commercially available medical masks after 192 hours of UV-C irradiation. The analysis was performed using a separated blue-coloured spunbond outer layer and white-coloured meltblown interlayer that allowed us to assess the influence of the structure and pigmentation of intact and UV-exposed samples on classification performance. Microscopy revealed strong fragmentation of both layers and the formation of microparticles and fibres of various shapes after UV exposure. Based on the spectral signatures of both layers, it was possible to identify intact materials using a convolutional neural network successfully. However, the further classification of UV-exposed samples demonstrated that the spectral characteristics of samples in the visible to near-infrared range are disrupted, causing a decreased performance of the CNN. Despite this, the application of a deep learning algorithm in hyperspectral analysis outperformed the conventional spectral angle mapper technique in classifying both intact and UV-exposed samples, confirming the potential of the proposed approach in secondary microplastic analysis.", "journal": "The Analyst", "date": "2022-09-21", "authors": ["IlnurIshmukhametov", "SvetlanaBatasheva", "RawilFakhrullin"], "doi": "10.1039/d2an01139e"}
{"title": "Endoscopic capsule robot-based diagnosis, navigation and localization in the gastrointestinal tract.", "abstract": "The proliferation of video capsule endoscopy (VCE) would not have been possible without continued technological improvements in imaging and locomotion. Advancements in imaging include both software and hardware improvements but perhaps the greatest software advancement in imaging comes in the form of artificial intelligence (AI). Current research into AI in VCE includes the diagnosis of tumors, gastrointestinal bleeding, Crohn's disease, and celiac disease. Other advancements have focused on the improvement of both camera technologies and alternative forms of imaging. Comparatively, advancements in locomotion have just started to approach clinical use and include onboard controlled locomotion, which involves miniaturizing a motor to incorporate into the video capsule, and externally controlled locomotion, which involves using an outside power source to maneuver the capsule itself. Advancements in locomotion hold promise to remove one of the major disadvantages of VCE, namely, its inability to obtain targeted diagnoses. Active capsule control could in turn unlock additional diagnostic and therapeutic potential, such as the ability to obtain targeted tissue biopsies or drug delivery. With both advancements in imaging and locomotion has come a corresponding need to be better able to process generated images and localize the capsule's position within the gastrointestinal tract. Technological advancements in computation performance have led to improvements in image compression and transfer, as well as advancements in sensor detection and alternative methods of capsule localization. Together, these advancements have led to the expansion of VCE across a number of indications, including the evaluation of esophageal and colon pathologies including esophagitis, esophageal varices, Crohn's disease, and polyps after incomplete colonoscopy. Current research has also suggested a role for VCE in acute gastrointestinal bleeding throughout the gastrointestinal tract, as well as in urgent settings such as the emergency department, and in resource-constrained settings, such as during the COVID-19 pandemic. VCE has solidified its role in the evaluation of small bowel bleeding and earned an important place in the practicing gastroenterologist's armamentarium. In the next few decades, further improvements in imaging and locomotion promise to open up even more clinical roles for the video capsule as a tool for non-invasive diagnosis of lumenal gastrointestinal pathologies.", "journal": "Frontiers in robotics and AI", "date": "2022-09-20", "authors": ["MarkHanscom", "David RCave"], "doi": "10.3389/frobt.2022.896028\n10.1016/j.gie.2018.10.027\n10.1111/jgh.14941\n10.1159/000525314\n10.1016/j.dld.2013.01.025\n10.1016/j.gie.2020.05.066\n10.1016/j.cgh.2007.12.029\n10.1109/TBME.2007.894729\n10.1109/TBME.2010.2087332\n10.1109/TBME.2009.2013336\n10.1109/IEMBS.2010.5627090\n10.1016/j.cmpb.2011.10.004\n10.1097/MCG.0b013e318288a2cd\n10.1055/a-0856-6845\n10.1055/a-0750-5682\n10.1016/j.gie.2019.04.248\n10.1016/j.gie.2013.02.039\n10.1007/s12213-016-0087-x\n10.1055/s-0029-1243808\n10.1109/TBME.2013.2290018\n10.1055/a-1546-8727\n10.1111/j.1572-0241.2007.01117.x\n10.2310/7290.2012.00014\n10.1053/j.gastro.2019.06.025\n10.1055/s-0032-1310158\n10.1097/01.mcg.0000170764.29202.24\n10.1016/S2589-7500(19)30108-6\n10.1016/j.giec.2003.10.020\n10.1038/ajg.2015.246\n10.1117/1.JBO.21.10.104001\n10.3748/wjg.v17.i41.4590\n10.1053/j.gastro.2017.07.049\n10.1016/j.giec.2020.12.011\n10.1001/jamanetworkopen.2021.18796\n10.14740/gr949w\n10.1155/2013/304723\n10.1016/j.gie.2010.10.016\n10.1016/j.gie.2020.01.027\n10.1109/TBME.2014.2352493\n10.1016/j.gie.2010.08.053\n10.1007/s00464-021-09007-7\n10.3390/jpm12040644\n10.1016/j.gie.2009.12.058\n10.3390/diagnostics11101792\n10.1007/s00261-016-1026-y\n10.1016/j.gie.2019.11.012\n10.4253/wjge.v4.i2.33\n10.1243/09544119JEIM134\n10.14309/ajg.0000000000001245\n10.1109/TBCAS.2010.2079932\n10.1016/j.gie.2018.06.036\n10.1109/TITB.2012.2185807\n10.1111/j.1572-0241.2003.08731.x\n10.4161/jig.23751\n10.1186/s12876-022-02302-0\n10.1016/j.gie.2013.11.022\n10.1016/j.gie.2018.06.016\n10.3390/diagnostics12061445\n10.1002/emp2.12579\n10.1109/IEMBS.2006.260385\n10.1007/s10916-009-9424-0\n10.1109/IEMBS.2006.259668\n10.1016/j.gie.2011.02.011\n10.1007/s00464-021-08689-3\n10.1016/j.ultrasmedbio.2019.12.003\n10.1016/j.gie.2007.11.052\n10.1109/IEMBS.2007.4352917\n10.1016/j.gie.2015.09.015\n10.1016/j.gie.2011.09.030\n10.1016/j.gie.2020.01.054\n10.1186/1471-230X-12-83\n10.1016/j.cgh.2018.07.019\n10.1007/s11938-017-0115-5\n10.1016/j.bios.2015.11.073\n10.1109/TBME.2008.915680\n10.1002/mp.12299\n10.1109/EMBC.2013.6610847\n10.1016/s0016-5085(10)63081-8\n10.3748/wjg.v21.i37.10528\n10.1016/j.gie.2016.04.043\n10.1016/j.gie.2010.01.064\n10.3748/wjg.v28.i20.2227\n10.1055/a-1790-5996\n10.1109/TBME.2012.2201715\n10.1080/13645700903201167\n10.1111/den.13896\n10.3748/wjg.v17.i11.1462\n10.1111/den.13507\n10.1038/s41598-021-90523-w\n10.1055/a-1308-1297\n10.1016/j.gie.2013.06.026\n10.1007/s10544-005-1588-x\n10.1016/j.lanwpc.2020.100072\n10.1021/js970185g\n10.1109/EMBC.2018.8513012\n10.1109/IEMBS.2011.6091642\n10.1109/TBME.2022.3157451\n10.1002/mp.12147\n10.1055/s-0042-122015\n10.1080/17474124.2017.1257384\n10.3390/diagnostics12061333\n10.1016/j.gie.2008.02.023\n10.1016/j.compbiomed.2017.03.031"}
{"title": "Application of machine learning and medical imaging in the detection of COVID-19 patients: A review article.", "abstract": "In the present study, a particular technique of artificial intelligence (AI) is applied for diagnosis and classifying medical images of patients with coronavirus disease (COVID-19). Chest radiography and laboratory-based tests are two of the most important diagnostic approaches for the detection of people with the coronavirus. Recently, a lot of studies have been carried out on using AI techniques for achieving appropriate diagnosis of COVID-19 patients using computed tomography (CT) of the chest. The present study is reviewing all available literature that have investigated the role of chest CT toward AI in the detection of COVID-19. As a novel field of computer science, AI focuses on teaching computers to be capable of learning complex tasks and decide about their solution methods. In this study, we used Matlab, Payton, and Fortran software as well as other software which are suitable for this research. In this regard, the present review study is aimed to collect the information from all the studies conducted on the role of AI as a decisive and comprehensive technology for the detection of coronavirus in patients to have a more accurate diagnosis and investigate its epidemiology.", "journal": "Journal of family medicine and primary care", "date": "2022-09-20", "authors": ["SepidehYadollahi", "SetarehYadollahi", "ElmiraZanjani", "FatemehKhaleghi"], "doi": "10.4103/jfmpc.jfmpc_1715_21"}
{"title": "Automated Lung Segmentation from Computed Tomography Images of Normal and COVID-19 Pneumonia Patients.", "abstract": "Automated image segmentation is an essential step in quantitative image analysis. This study assesses the performance of a deep learning-based model for lung segmentation from computed tomography (CT) images of normal and COVID-19 patients.\nA descriptive-analytical study was conducted from December 2020 to April 2021 on the CT images of patients from various educational hospitals affiliated with Mashhad University of Medical Sciences (Mashhad, Iran). Of the selected images and corresponding lung masks of 1,200 confirmed COVID-19 patients, 1,080 were used to train a residual neural network. The performance of the residual network (ResNet) model was evaluated on two distinct external test datasets, namely the remaining 120 COVID-19 and 120 normal patients. Different evaluation metrics such as Dice similarity coefficient (DSC), mean absolute error (MAE), relative mean Hounsfield unit (HU) difference, and relative volume difference were calculated to assess the accuracy of the predicted lung masks. The Mann-Whitney U test was used to assess the difference between the corresponding values in the normal and COVID-19 patients. P<0.05 was considered statistically significant.\nThe ResNet model achieved a DSC of 0.980 and 0.971 and a relative mean HU difference of -2.679% and -4.403% for the normal and COVID-19 patients, respectively. Comparable performance in lung segmentation of normal and COVID-19 patients indicated the model's accuracy for identifying lung tissue in the presence of COVID-19-associated infections. Although a slightly better performance was observed in normal patients.\nThe ResNet model provides an accurate and reliable automated lung segmentation of COVID-19 infected lung tissue.A preprint version of this article was published on arXiv before formal peer review (https://arxiv.org/abs/2104.02042).", "journal": "Iranian journal of medical sciences", "date": "2022-09-20", "authors": ["FaezeGholamiankhah", "SamanehMostafapour", "NouraddinAbdi Goushbolagh", "SeyedjafarShojaerazavi", "ParvanehLayegh", "Seyyed MohammadTabatabaei", "HosseinArabi"], "doi": "10.30476/IJMS.2022.90791.2178\n10.30476/ijms.2020.85869.1549\n10.30476/ijms.2020.87233.1730\n10.1109/TMI.2020.3001810\n10.1109/TBDATA.2021.3056564\n10.1109/TMI.2020.2996645\n10.1016/j.media.2020.101794\n10.1109/ICBME.2010.5704968\n10.1016/j.radphyschem.2021.109666\n10.1148/radiol.2020200642\n10.1101/2020.03.12.20027185\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2020202439\n10.1186/s41747-020-00173-2\n10.1016/j.media.2016.11.003\n10.1038/s41598-020-80936-4\n10.1016/j.procs.2018.01.104\n10.1186/s41824-020-00086-8\n10.1002/ima.22527\n10.1002/mp.14418\n10.1016/j.media.2020.101759\n10.1016/j.media.2016.02.002\n10.1109/TMI.2021.3066161\n10.1109/TMI.2020.2995965\n10.1016/j.patcog.2021.108109\n10.1016/j.patcog.2020.107747\n10.1515/cdbme-2016-0114\n10.1016/j.cmpb.2018.01.025\n10.1016/j.media.2020.101718\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200432\n10.1002/mp.14676\n10.1109/RBME.2020.2987975\n10.3390/sym12040651\n10.3892/etm.2020.9210\n10.1016/j.imu.2021.100681\n10.3390/diagnostics11081405\n10.1016/j.patcog.2021.108071\n10.1136/neurintsurg-2015-011697"}
{"title": "The role of artificial intelligence in plain chest radiographs interpretation during the Covid-19 pandemic.", "abstract": "Artificial intelligence (AI) plays a crucial role in the future development of all healthcare sectors ranging from clinical assistance of physicians by providing accurate diagnosis, prognosis and treatment to the development of vaccinations and aiding in the combat against the Covid-19 global pandemic. AI has an important role in diagnostic radiology where the algorithms can be trained by large datasets to accurately provide a timely diagnosis of the radiological images given. This has led to the development of several AI algorithms that can be used in regions of scarcity of radiologists during the current pandemic by simply denoting the presence or absence of Covid-19 pneumonia in PCR positive patients on plain chest radiographs as well as in helping to levitate the over-burdened radiology departments by accelerating the time for report delivery. Plain chest radiography is the most common radiological study in the emergency department setting and is readily available, fast and a cheap method that can be used in triaging patients as well as being portable in the medical wards and can be used as the initial radiological examination in Covid-19 positive patients to detect pneumonic changes. Numerous studies have been done comparing several AI algorithms to that of experienced thoracic radiologists in plain chest radiograph reports measuring accuracy of each in Covid-19 patients. The majority of studies have reported performance equal or higher to that of the well-experienced thoracic radiologist in predicting the presence or absence of Covid-19 pneumonic changes in the provided chest radiographs.", "journal": "BJR open", "date": "2022-09-16", "authors": ["DanaAlNuaimi", "ReemAlKetbi"], "doi": "10.1259/bjro.20210075\n10.1016/j.ijsu.2020.02.034\n10.1016/j.dsx.2020.04.012\n10.1016/j.carj.2018.02.002\n10.1038/s41568-018-0016-5\n10.1016/j.jmir.2019.09.005\n10.3390/diagnostics11030530\n10.1148/radiol.2020204226\n10.1016/j.opresp.2020.100078\n10.1007/s12553-021-00520-2\n10.1016/S2589-7500(20)30079-0\n10.1016/j.ibmed.2020.100014\n10.1148/radiol.2020202944\n10.2147/RMI.S292314\n10.1148/radiol.2020201874\n10.1148/radiol.2020204238\n10.1145/3466690\n10.1007/s00146-020-00978-0\n10.1016/S2589-7500(21)00039-X\n10.1101/2020.05.25.20113084\n10.4103/ijri.IJRI_777_20\n10.1148/radiol.2020203511\n10.1038/s42256-021-00307-0"}
{"title": "TOPSIS aided ensemble of CNN models for screening COVID-19 in chest X-ray images.", "abstract": "The novel coronavirus (COVID-19), has undoubtedly imprinted our lives with its deadly impact. Early testing with isolation of the individual is the best possible way to curb the spread of this deadly\u00a0virus. Computer aided diagnosis (CAD) provides an alternative and cheap option for screening of the said virus. In this paper, we propose a convolution neural network (CNN)-based CAD method for COVID-19 and pneumonia detection from chest X-ray images. We consider three input types for three identical base classifiers. To capture maximum possible complementary features, we consider the original RGB image, Red channel image and the original image stacked with Robert's edge information. After that we develop an ensemble strategy based on the technique for order preference by similarity to an ideal solution (TOPSIS) to aggregate the outcomes of base classifiers. The overall framework, called TOPCONet, is very light in comparison with standard CNN models in terms of the number of trainable\u00a0parameters required. TOPCONet achieves state-of-the-art results when evaluated on the three publicly available datasets: (1) IEEE COVID-19 dataset + Kaggle Pneumonia Dataset, (2) Kaggle Radiography dataset and (3) COVIDx.", "journal": "Scientific reports", "date": "2022-09-15", "authors": ["RishavPramanik", "SubhrajitDey", "SamirMalakar", "SeyedaliMirjalili", "RamSarkar"], "doi": "10.1038/s41598-022-18463-7\n10.1016/S0140-6736(21)02000-6\n10.1148/radiol.2020200432\n10.1038/s41563-020-00906-z\n10.1007/s10916-019-1451-x\n10.1016/j.cmpb.2022.106776\n10.1007/s10916-021-01747-2\n10.1148/radiol.2020200642\n10.1007/s10489-020-01943-6\n10.1016/j.neunet.2018.07.011\n10.1016/j.asoc.2021.107160\n10.1109/TNNLS.2019.2906867\n10.1109/ACCESS.2020.2995597\n10.1038/s41598-020-76550-z\n10.1007/s00521-020-05410-8\n10.1016/j.eswa.2022.117812\n10.1109/JBHI.2021.3069798\n10.1016/j.eswa.2020.113909\n10.1016/j.acra.2020.04.034\n10.1109/TMI.2020.2993291\n10.1007/s00521-021-06737-6\n10.1016/j.bdr.2021.100233\n10.1016/j.bbe.2021.12.001\n10.1016/j.rinp.2021.105045\n10.1016/j.chaos.2020.110245\n10.1016/j.compbiomed.2021.104834\n10.1038/s41598-021-88807-2\n10.1109/ACCESS.2021.3061058\n10.1016/j.compbiomed.2021.105047\n10.3390/diagnostics11111972\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01904-z\n10.1016/j.asoc.2020.106912\n10.1016/j.asoc.2021.107918\n10.1016/j.cmpb.2020.105532\n10.1007/s00521-021-06629-9\n10.1038/nature14539\n10.1109/TCSVT.2020.3019293\n10.1109/MCI.2010.938364\n10.1016/j.asoc.2021.107698\n10.1109/ACCESS.2020.3010287\n10.1007/s10489-020-01902-1\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2021.107184\n10.1016/j.compbiomed.2021.104401\n10.1038/s41598-020-79139-8"}
{"title": "Detection of COVID-19 Infection in CT and X-ray images using transfer learning approach.", "abstract": "The infection caused by the SARS-CoV-2 (COVID-19) pandemic is a threat to human lives. An early and accurate diagnosis is necessary for treatment.\nThe study presents an efficient classification methodology for precise identification of infection caused by COVID-19 using CT and X-ray images.\nThe depthwise separable convolution-based model of MobileNet V2 was exploited for feature extraction. The features of infection were supplied to the SVM classifier for training which produced accurate classification results.\nThe accuracies for CT and X-ray images are 99.42% and 98.54% respectively. The MCC score was used to avoid any mislead caused by accuracy and F1 score as it is more mathematically balanced metric. The MCC scores obtained for CT and X-ray were 0.9852 and 0.9657, respectively. The Youden's index showed a significant improvement of more than 2% for both imaging techniques.\nThe proposed transfer learning-based approach obtained the best results for all evaluation metrics and produced reliable results for the accurate identification of COVID-19 symptoms. This study can help in reducing the time in diagnosis of the infection.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2022-09-13", "authors": ["AlokTiwari", "SumitTripathi", "Dinesh ChandraPandey", "NeerajSharma", "ShiruSharma"], "doi": "10.3233/THC-220114"}
{"title": "A Novel Method for COVID-19 Detection Based on DCNNs and Hierarchical Structure.", "abstract": "The worldwide outbreak of the new coronavirus disease (COVID-19) has been declared a pandemic by the World Health Organization (WHO). It has a devastating impact on daily life, public health, and global economy. Due to the highly infectiousness, it is urgent to early screening of suspected cases quickly and accurately. Chest X-ray medical image, as a diagnostic basis for COVID-19, arouses attention from medical engineering. However, due to small lesion difference and lack of training data, the accuracy of detection model is insufficient. In this work, a transfer learning strategy is introduced to hierarchical structure to enhance high-level features of deep convolutional neural networks. The proposed framework consisting of asymmetric pretrained DCNNs with attention networks integrates various information into a wider architecture to learn more discriminative and complementary features. Furthermore, a novel cross-entropy loss function with a penalty term weakens misclassification. Extensive experiments are implemented on the COVID-19 dataset. Compared with the state-of-the-arts, the effectiveness and high performance of the proposed method are demonstrated.", "journal": "Computational and mathematical methods in medicine", "date": "2022-09-13", "authors": ["YuqinLi", "KeZhang", "WeiliShi", "ZhengangJiang"], "doi": "10.1155/2022/2484435\n10.1016/j.bbe.2020.08.008\n10.1016/j.irbm.2020.05.003\n10.1007/s10044-021-00984-y\n10.7717/peerj-cs.313\n10.1080/07391102.2020.1788642\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.05.12.20099937\n10.1117/12.2581496\n10.1155/2022/6185013\n10.1016/j.mehy.2020.109761\n10.1016/j.patrec.2021.11.020\n10.1016/B978-0-12-824536-1.00003-4\n10.1007/s10489-020-01900-3\n10.1007/s10489-020-01826-w\n10.1007/s11517-020-02299-2\n10.1016/j.patrec.2020.09.010\n10.20944/preprints202005.0151.v1\n10.1016/j.bspc.2022.103595\n10.1016/j.compbiomed.2021.105134\n10.1016/j.bspc.2019.04.031\n10.1109/cvpr.2016.90\n10.1109/cvpr.2018.00745\n10.1109/ACCESS.2020.3010287\n10.1186/s40537-019-0197-0\n10.1049/ipr2.12090\n10.1007/s40846-020-00529-4\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1016/j.irbm.2020.07.001"}
{"title": "Hotspots and trends in ophthalmology in recent 5 years: Bibliometric analysis in 2017-2021.", "abstract": "The purpose of this study was to investigate the hotspots and research trends of ophthalmology research.\nOphthalmology research literature published between 2017 and 2021 was obtained in the Web of Science Core Collection database. The bibliometric analysis and network visualization were performed with the VOSviewer and CiteSpace. Publication-related information, including publication volume, citation counts, countries, journals, keywords, subject categories, and publication time, was analyzed.\nA total of 10,469 included ophthalmology publications had been cited a total of 7,995 times during the past 5 years. The top countries and journals for the number of publications were the United States and the Ophthalmology. The top 25 global high-impact documents had been identified using the citation ranking. Keyword co-occurrence analysis showed that the hotspots in ophthalmology research were epidemiological characteristics and treatment modalities of ocular diseases, artificial intelligence and fundus imaging technology, COVID-19-related telemedicine, and screening and prevention of ocular diseases. Keyword burst analysis revealed that \"neural network,\" \"pharmacokinetics,\" \"geographic atrophy,\" \"implementation,\" \"variability,\" \"adverse events,\" \"automated detection,\" and \"retinal images\" were the research trends of research in the field of ophthalmology through 2021. The analysis of the subject categories demonstrated the close cooperation relationships that existed between different subject categories, and collaborations with non-ophthalmology-related subject categories were increasing over time in the field of ophthalmology research.\nThe hotspots in ophthalmology research were epidemiology, prevention, screening, and treatment of ocular diseases, as well as artificial intelligence and fundus imaging technology and telemedicine. Research trends in ophthalmology research were artificial intelligence, drug development, and fundus diseases. Knowledge from non-ophthalmology fields is likely to be more involved in ophthalmology research.", "journal": "Frontiers in medicine", "date": "2022-09-13", "authors": ["YuanTan", "WeiningZhu", "YingshiZou", "BowenZhang", "YinglinYu", "WeiLi", "GuangmingJin", "ZhenzhenLiu"], "doi": "10.3389/fmed.2022.988133\n10.1097/WNO.0000000000001375\n10.1016/j.actbio.2021.02.017\n10.1016/j.exer.2016.10.013\n10.1186/s13643-019-1113-6\n10.1016/j.cbi.2021.109679\n10.1038/s41586-020-2975-4\n10.1016/j.preteyeres.2021.100970\n10.1073/pnas.2117038119\n10.1016/j.bioactmat.2022.02.019\n10.1177/1534735420959442\n10.1080/09737766.2008.10700853\n10.4103/0301-4738.64117\n10.1055/s-0029-1245134\n10.3126/nepjoph.v4i2.6548\n10.1007/s00347-008-1849-1\n10.1046/j.1442-9071.2003.00663.x\n10.12688/f1000research.20673.1\n10.4103/0301-4738.151471\n10.1016/j.clineuro.2020.105740\n10.1016/j.ophtha.2017.05.035\n10.1016/j.ophtha.2019.04.017\n10.1016/j.oret.2016.12.009\n10.1016/j.ophtha.2017.09.028\n10.1136/bjophthalmol-2018-313173\n10.1016/j.ophtha.2018.01.023\n10.1016/j.ophtha.2017.08.015\n10.1016/j.ophtha.2018.04.007\n10.1007/s00592-017-0974-1\n10.1016/j.ophtha.2017.12.011\n10.1016/j.ophtha.2017.03.036\n10.1016/j.ophtha.2017.08.027\n10.1136/annrheumdis-2018-213225\n10.1007/s00417-020-04641-8\n10.1016/j.ophtha.2017.10.031\n10.1016/j.cmpb.2018.02.001\n10.1016/j.preteyeres.2018.07.004\n10.1016/j.ophtha.2017.02.008\n10.1126/sciadv.aat4388\n10.1016/j.preteyeres.2016.12.001\n10.3390/polym10070701\n10.1016/j.ophtha.2016.10.008\n10.1364/BOE.8.001056\n10.1016/j.preteyeres.2017.07.002\n10.1056/NEJMoa1609583\n10.1016/j.diabres.2021.108880\n10.1167/iovs.62.6.5\n10.18240/ijo.2017.09.16\n10.2147/OPTH.S235751\n10.2147/CIA.S297494\n10.1016/j.ophtha.2021.12.017\n10.1016/j.ajo.2020.04.029"}
{"title": "Rapid quantification of COVID-19 pneumonia burden from computed tomography with convolutional long short-term memory networks.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2022-09-13", "authors": ["AdityaKillekar", "KajetanGrodecki", "AndrewLin", "SebastienCadet", "PriscillaMcElhinney", "AryabodRazipour", "CatoChan", "Barry DPressman", "PeterJulien", "PeterChen", "JuditSimon", "PalMaurovich-Horvat", "NicolaGaibazzi", "UditThakur", "ElisabettaMancini", "CeciliaAgalbato", "JiroMunechika", "HidenariMatsumoto", "RobertoMen\u00e8", "GianfrancoParati", "FrancoCernigliaro", "NiteshNerlekar", "CamillaTorlasco", "GianlucaPontone", "DaminiDey", "PiotrSlomka"], "doi": "10.1117/1.JMI.9.5.054001\n10.1016/j.ajic.2020.07.011\n10.1007/s00330-020-07033-y\n10.1038/s41598-020-80061-2\n10.1148/rg.2020200159\n10.1148/radiol.2020200463\n10.1148/radiol.2020200843\n10.1148/radiol.2020200370\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200047\n10.1148/ryai.2020200048\n10.1148/ryct.2020200441\n10.1148/ryct.2020200389\n10.1001/archinternmed.2009.440\n10.2967/jnumed.120.246256\n10.1097/RLU.0000000000003135\n10.1016/j.diii.2020.05.011\n10.1007/s00259-020-05014-3\n10.4103/ijri.IJRI_479_20\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2020201491\n10.1109/TMI.2020.2996645\n10.1016/j.media.2020.101836\n10.1007/978-3-319-46723-8_49\n10.1109/3DV.2016.79\n10.1162/neco.1997.9.8.1735\n10.1148/radiol.10091808\n10.1109/CVPR.2017.243\n10.1109/IVS.2019.8813852\n10.1109/CVPR.2017.19\n10.1109/CVPR.2009.5206848\n10.1007/BF02295996\n10.1148/radiol.2020200642\n10.1148/radiol.2020200905\n10.1016/j.metabol.2020.154436\n10.1109/TMI.2020.3000314\n10.1186/s12880-020-00529-5\n10.1155/2020/4706576\n10.1007/s00521-020-05514-1\n10.1002/int.22586\n10.1117/12.2613272"}
{"title": "Lung image segmentation based on DRD U-Net and combined WGAN with Deep Neural Network.", "abstract": "COVID-19 is a hot issue right now, and it's causing a huge number of infections in people, posing a grave threat to human life. Deep learning-based image diagnostic technology can effectively enhance the deficiencies of the current main detection method. This paper proposes a multi-classification model diagnosis based on segmentation and classification multi-task.\nIn the segmentation task, the end-to-end DRD U-Net model is used to segment the lung lesions to improve the ability of feature reuse and target segmentation. In the classification task, the model combined with WGAN and Deep Neural Network classifier is used to effectively solve the problem of multi-classification of COVID-19 images with small samples, to achieve the goal of effectively distinguishing COVID-19 patients, other pneumonia patients, and normal subjects.\nExperiments are carried out on common X-ray image and CT image data sets. The results display that in the segmentation task, the model is optimal in the key indicators of DSC and HD, and the error is increased by 0.33% and reduced by 3.57 mm compared with the original network U-Net. In the classification task, compared with SMOTE oversampling method, accuracy increased from 65.32% to 73.84%, F-measure increased from 67.65% to 74.65%, G-mean increased from 66.52% to 74.37%. At the same time, compared with other classical multi-task models, the results also have some advantages.\nThis study provides new possibilities for COVID-19 image diagnosis methods, improves the accuracy of diagnosis, and hopes to provide substantial help for COVID-19 diagnosis.", "journal": "Computer methods and programs in biomedicine", "date": "2022-09-12", "authors": ["LuoyuLian", "XinLuo", "CanyuPan", "JinlongHuang", "WenshanHong", "ZhendongXu"], "doi": "10.1016/j.cmpb.2022.107097"}
{"title": "Unmet needs in pneumonia research: a comprehensive approach by the CAPNETZ study group.", "abstract": "Despite improvements in medical science and public health, mortality of community-acquired pneumonia (CAP) has barely changed throughout the last 15\u00a0years. The current SARS-CoV-2 pandemic has once again highlighted the central importance of acute respiratory infections to human health. The \"network of excellence on Community Acquired Pneumonia\" (CAPNETZ) hosts the most comprehensive CAP database worldwide including more than 12,000 patients. CAPNETZ connects physicians, microbiologists, virologists, epidemiologists, and computer scientists throughout Europe. Our aim was to summarize the current situation in CAP research and identify the most pressing unmet needs in CAP research.\nTo identify areas of future CAP research, CAPNETZ followed a multiple-step procedure. First, research members of CAPNETZ were individually asked to identify unmet needs. Second, the top 100 experts in the field of CAP research were asked for their insights about the unmet needs in CAP (Delphi approach). Third, internal and external experts discussed unmet needs in CAP at a scientific retreat.\nEleven topics for future CAP research were identified: detection of causative pathogens, next generation sequencing for antimicrobial treatment guidance, imaging diagnostics, biomarkers, risk stratification, antiviral and antibiotic treatment, adjunctive therapy, vaccines and prevention, systemic and local immune response, comorbidities, and long-term cardio-vascular complications.\nPneumonia is a complex disease where the interplay between pathogens, immune system and comorbidities not only impose an immediate risk of mortality but also affect the patients' risk of developing comorbidities as well as mortality for up to a decade after pneumonia has resolved. Our review of unmet needs in CAP research has shown that there are still major shortcomings in our knowledge of CAP.", "journal": "Respiratory research", "date": "2022-09-11", "authors": ["Mathias WPletz", "Andreas VestergaardJensen", "ChristinaBahrs", "ClaudiaDavenport", "JanRupp", "MartinWitzenrath", "GritBarten-Neiner", "MartinKolditz", "SabineDettmer", "James DChalmers", "DaianaStolz", "NorbertSuttorp", "StefanoAliberti", "Wolfgang MKuebler", "GernotRohde"], "doi": "10.1186/s12931-022-02117-3\n10.1016/S0140-6736(17)31833-0\n10.1016/S0140-6736(21)00630-9\n10.1164/rccm.201908-1581ST\n10.1136/thx.2009.121434\n10.1055/s-0042-101873\n10.1186/s12890-017-0404-8\n10.1136/thoraxjnl-2013-204282\n10.1136/thx.2008.109785\n10.1016/j.rmed.2014.05.004\n10.1007/s10096-018-3224-8\n10.1086/526526\n10.1093/cid/cit254\n10.1186/1471-2334-14-61\n10.1001/jama.1996.03530260048030\n10.7326/0003-4819-142-3-200502010-00006\n10.1136/thorax.58.5.377\n10.1016/j.rmed.2013.04.003\n10.1378/chest.11-2393\n10.1001/archinte.159.9.970\n10.1001/jama.2012.384\n10.1093/cid/cix647\n10.1111/j.1469-0691.2010.03296.x\n10.1001/archinte.159.14.1550\n10.1016/0140-6736(93)91887-R\n10.1086/379712\n10.1164/rccm.200712-1777OC\n10.1097/MD.0b013e318190f444\n10.1007/s15010-004-3107-z\n10.1016/j.chest.2019.10.006\n10.1093/infdis/jiv323\n10.1016/j.cmi.2018.12.037\n10.1016/j.ccm.2011.05.011\n10.1183/13993003.01144-2016\n10.1093/cid/cit734\n10.1136/thoraxjnl-2013-203384\n10.1183/13993003.02086-2016\n10.1038/s41587-019-0156-5\n10.1183/09031936.01.00213501\n10.1016/j.ajem.2012.08.041\n10.1164/rccm.201501-0017OC\n10.2169/internalmedicine.55.5556\n10.1016/j.ajem.2015.01.035\n10.1378/chest.12-0364\n10.1097/RTI.0000000000000524\n10.1016/j.chest.2017.08.028\n10.1183/13993003.00434-2017\n10.1136/bmj.f2450\n10.1007/s00134-007-0895-5\n10.1186/1471-2334-12-90\n10.1136/bmj.e3397\n10.1513/AnnalsATS.201901-007OC\n10.1183/13993003.01389-2017\n10.1164/rccm.201003-0415OC\n10.1164/rccm.201708-1733OC\n10.3390/ijms20082004\n10.1155/2018/7028267\n10.1111/resp.12996\n10.1513/AnnalsATS.201804-286OC\n10.1371/journal.pone.0191750\n10.1007/s001340051129\n10.1055/s-0032-1315637\n10.1111/joim.12349\n10.1136/thoraxjnl-2015-206881\n10.1016/j.cmi.2017.07.007\n10.1016/j.rmed.2016.10.015\n10.1001/jama.2016.0287\n10.1007/s00134-016-4517-y\n10.1097/MD.0000000000012634\n10.1164/rccm.201807-1419OC\n10.1016/j.annemergmed.2018.11.036\n10.1001/jama.2018.19232\n10.1016/j.cmi.2019.02.022\n10.1001/jama.2016.0115\n10.1002/14651858.CD002109.pub4\n10.1097/MCP.0000000000000557\n10.1136/jim-2018-000712\n10.1183/13993003.00048-2021\n10.1038/d41586-021-03379-5\n10.1097/QCO.0000000000000347\n10.3390/toxins11120734\n10.1097/MCC.0000000000000435\n10.1016/j.cmi.2020.07.016\n10.1186/1471-2334-14-13\n10.3389/fmicb.2016.00693\n10.1093/jac/dkp160\n10.1128/JCM.02597-14\n10.1186/s13054-016-1430-2\n10.1056/NEJMp1905589\n10.1007/s11908-017-0565-x\n10.1055/s-0036-1593538\n10.1161/01.RES.0000036603.61868.F9\n10.1183/13993003.01329-2016\n10.1016/S2213-2600(21)00331-3\n10.1056/NEJMoa2101643\n10.1038/s41591-021-01499-z\n10.1001/jama.2021.9508\n10.1016/S2213-2600(20)30556-7\n10.1128/CMR.00078-09\n10.1056/NEJMoa1406330\n10.1038/s41598-018-37186-2\n10.1111/resp.13232\n10.1055/s-0032-1315643\n10.1186/s12879-018-3630-7\n10.1183/13993003.00824-2019\n10.1007/s00134-018-5143-7\n10.1016/j.rmed.2013.09.005\n10.1136/bmjresp-2016-000152\n10.3390/v11030295\n10.1056/NEJMoa2102685\n10.1101/2021.06.15.21258542v1\n10.1183/09031936.00133510\n10.1001/jama.2013.279206\n10.1016/j.ahj.2017.07.020\n10.12688/f1000research.7657.1\n10.7326/M19-0735\n10.1016/S1473-3099(17)30049-X\n10.1016/S0140-6736(00)02377-1\n10.1086/648593\n10.1371/journal.pone.0169370\n10.1016/j.vaccine.2016.03.052\n10.1183/09031936.00183614\n10.1016/j.vaccine.2019.11.026\n10.1016/j.vaccine.2019.05.003\n10.1371/journal.ppat.1007438\n10.1016/j.ijantimicag.2008.01.021\n10.4103/jgid.jgid_88_17\n10.3934/publichealth.2020004\n10.3390/vaccines7010009\n10.1159/000504477\n10.1086/374604\n10.1093/bmb/61.1.1\n10.1016/j.cytogfr.2020.04.002\n10.3389/fimmu.2018.01421\n10.1183/16000617.0034-2015\n10.1038/srep35021\n10.1016/j.gene.2018.08.080\n10.1186/s12920-019-0572-x\n10.1164/rccm.201701-0104OC\n10.1097/MCP.0000000000000584\n10.1183/23120541.00020-2015\n10.1093/cid/cix703\n10.1093/cid/cir840\n10.1093/cid/ciy723\n10.1183/13993003.01520-2016\n10.1016/j.ejim.2013.12.001\n10.1007/s40520-014-0297-9\n10.1111/resp.13233\n10.1016/S0140-6736(12)61266-5\n10.1164/rccm.201501-0140OC\n10.1136/bmj.j413\n10.1183/13993003.00972-2015\n10.1001/jama.2014.18229\n10.1164/rccm.201801-0139WS"}
{"title": "Detection of COVID-19 in Point of Care Lung Ultrasound.", "abstract": "The coronavirus disease 2019 (COVID-19) evolved into a global pandemic, responsible for a significant number of infections and deaths. In this scenario, point-of-care ultrasound (POCUS) has emerged as a viable and safe imaging modality. Computer vision (CV) solutions have been proposed to aid clinicians in POCUS image interpretation, namely detection/segmentation of structures and image/patient classification but relevant challenges still remain. As such, the aim of this study is to develop CV algorithms, using Deep Learning techniques, to create tools that can aid doctors in the diagnosis of viral and bacterial pneumonia (VP and BP) through POCUS exams. To do so, convolutional neural networks were designed to perform in classification tasks. The architectures chosen to build these models were the VGG16, ResNet50, DenseNet169 e MobileNetV2. Patients images were divided in three classes: healthy (HE), BP and VP (which includes COVID-19). Through a comparative study, which was based on several performance metrics, the model based on the DenseNet169 architecture was designated as the best performing model, achieving 78% average accuracy value of the five iterations of 5- Fold Cross-Validation. Given that the currently available POCUS datasets for COVID-19 are still limited, the training of the models was negatively affected by such and the models were not tested in an independent dataset. Furthermore, it was also not possible to perform lesion detection tasks. Nonetheless, in order to provide explainability and understanding of the models, Gradient-weighted Class Activation Mapping (GradCAM) were used as a tool to highlight the most relevant classification regions. Clinical relevance - Reveals the potential of POCUS to support COVID-19 screening. The results are very promising although the dataset is limite.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["JoanaMaximino", "MiguelCoimbra", "JoaoPedrosa"], "doi": "10.1109/EMBC48229.2022.9871235"}
{"title": "Dynamic Classification of Imageless Bioelectrical Impedance Tomography Features with Attention-Driven Spatial Transformer Neural Network.", "abstract": "Point-of-Care monitoring devices have proven to be pivotal in the timely screening and intervention of critical care patients. The urgent demands for their deployment in the COVID-19 pandemic era has translated into the escalation of rapid, reliable, and low-cost monitoring systems research and development. Electrical Impedance Tomography (EIT) is a highly promising modality in providing deep tissue imaging that aids in patient bedside diagnosis and treatment. Motivated to bring forth an accurate and intelligent EIT screening system, we bypassed the complexity and challenges typically associated with its image reconstruction and feature identification processes by solely focusing on the raw data output to extract the embedded knowledge. We developed a novel machine learning architecture based on an attention-driven spatial transformer neural network to specifically accommodate for the patterns and dependencies within EIT raw data. Through elaborate precision-mapped phantom experiments, we validated the reproduction and recognition of features with systemically controlled changes. We demonstrated over 95% accuracy via state-of-the-art machine learning models, and an enhanced performance using our adapted transformer pipeline with shorter training time and greater computational efficiency. Our approach of using imageless EIT driven by a novel attention-focused feature learning algorithm is highly promising in revolutionizing conventional EIT operations and augmenting its practical usage in medicine and beyond.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["MingdeZheng", "HassanJahanandish", "HongweiLi"], "doi": "10.1109/EMBC48229.2022.9870921"}
{"title": "Automated Quantification of Inflamed Lung Regions in Chest CT by UNet++ and SegCaps: A Comparative Analysis in COVID-19 Cases.", "abstract": "During the current COVID-19 pandemic, a high volume of lung imaging has been generated in the aid of the treating clinician. Importantly, lung inflammation severity, associated with the disease outcome, needs to be precisely quantified. Producing consistent and accurate reporting in high-demand scenarios can be a challenge that can compromise patient care with significant inter- or intra-observer variability in quantifying lung inflammation in a chest CT scan. In this backdrop, automated segmentation has recently been attempted using UNet++, a convolutional neural network (CNN), and results comparable to manual methods have been reported. In this paper, we hypothesize that the desired task can be performed with comparable efficiency using capsule networks with fewer parameters that make use of an advanced vector representation of information and dynamic routing. In this paper, we validate this hypothesis using SegCaps, a capsule network, by direct comparison, individual comparison with CT severity score, and comparing the relative effect on a ML(machine learning)-based prognosis model developed elsewhere. We further provide a scenario, where a combination of UNet++ and SegCaps achieves improved performance compared to individual models.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["PriyaBhatia", "AbhisharSinha", "Swati PurohitJoshi", "RahuldebSarkar", "RajeshGhosh", "SoumyaJana"], "doi": "10.1109/EMBC48229.2022.9870901"}
{"title": "Teleoperated Probe Manipulator for Prone-Position Echocardiography Examination.", "abstract": "Echocardiography probe manipulation is a strenuous task. During a procedure, the operator must hold the probe, extend their arm, bend their elbow, and monitor the resulting image simultaneously, which causes strain and introduces variability to the measurement. We propose a teleoperated probe manipulation robot to reduce the burden of handling the probe and minimize the infection risk during the COVID pandemic. The proposed robot utilizes prone position scanning that could enlarge the cardiac windows for easier scanning and eliminate the risk of the robot pressing down on the patient. We derived the robot's requirements based on a real clinical scenario. Initial evaluation showed that the robot could achieve the required range of motion, force, and control. The robot's functionality was tested by a non-clinician, in which the tester could obtain heart images of a volunteer in under one minute.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["Muhammad WildanGifari", "ModarHassan", "KenjiSuzuki"], "doi": "10.1109/EMBC48229.2022.9871021"}
{"title": "Transfer Learning for Automated COVID-19 B-Line Classification in Lung Ultrasound.", "abstract": "Lung ultrasound (LUS) as a diagnostic tool is gaining support for its role in the diagnosis and management of COVID-19 and a number of other lung pathologies. B-lines are a predominant feature in COVID-19, however LUS requires a skilled clinician to interpret findings. To facilitate the interpretation, our main objective was to develop automated methods to classify B-lines as pathologic vs. normal. We developed transfer learning models based on ResNet networks to classify B-lines as pathologic (at least 3 B-lines per lung field) vs. normal using COVID-19 LUS data. Assessment of B-line severity on a 0-4 multi-class scale was also explored. For binary B-line classification, at the frame-level, all ResNet models pretrained with ImageNet yielded higher performance than the baseline nonpretrained ResNet-18. Pretrained ResNet-18 has the best Equal Error Rate (EER) of 9.1% vs the baseline of 11.9%. At the clip-level, all pretrained network models resulted in better Cohen's kappa agreement (linear-weighted) and clip score accuracy, with the pretrained ResNet-18 having the best Cohen's kappa of 0.815 [95% CI: 0.804-0.826], and ResNet-101 the best clip scoring accuracy of 93.6%. Similar results were shown for multi-class scoring, where pretrained network models outperformed the baseline model. A class activation map is also presented to guide clinicians in interpreting LUS findings. Future work aims to further improve the multi-class assessment for severity of B-lines with a more diverse LUS dataset.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["Joseph RPare", "Lars AGjesteby", "Brian ATelfer", "Melinda MTonelli", "Megan MLeo", "EhabBillatos", "JonathanScalera", "Laura JBrattain"], "doi": "10.1109/EMBC48229.2022.9871894"}
{"title": "Data-Efficient Training of Pure Vision Transformers for the Task of Chest X-ray Abnormality Detection Using Knowledge Distillation.", "abstract": "It is generally believed that vision transformers (ViTs) require a huge amount of data to generalize well, which limits their adoption. The introduction of data-efficient algorithms such as data-efficient image transformers (DeiT) provided an opportunity to explore the application of ViTs in medical imaging, where data scarcity is a limiting factor. In this work, we investigated the possibility of using pure transformers for the task of chest x-ray abnormality detection on a small dataset. Our proposed framework is built on a DeiT structure benefiting from a teacher-student scheme for training, with a DenseNet with strong classification performance as the teacher and an adapted ViT as the student. The results show that the performance of transformers is on par with that of convolutional neural networks (CNNs). We achieved a test accuracy of 92.2% for the task of classifying chest x-ray images (normal/pneumonia/COVID-19) on a carefully selected dataset using pure transformers. The results show the capability of transformers to accompany or replace CNNs for achieving state-of-the-art in medical imaging applications. The code and models of this work are available at https://github.com/Ouantimb-Lab/DeiTCovid.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["Seyed AliJalalifar", "AliSadeghi-Naini"], "doi": "10.1109/EMBC48229.2022.9871372"}
{"title": "Wasserstein GAN based Chest X-Ray Dataset Augmentation for Deep Learning Models: COVID-19 Detection Use-Case.", "abstract": "The novel coronavirus infection (COVID-19) is still continuing to be a concern for the entire globe. Since early detection of COVID-19 is of particular importance, there have been multiple research efforts to supplement the current standard RT-PCR tests. Several deep learning models, with varying effectiveness, using Chest X-Ray images for such diagnosis have also been proposed. While some of the models are quite promising, there still remains a dearth of training data for such deep learning models. The present paper attempts to provide a viable solution to the problem of data deficiency in COVID-19 CXR images. We show that the use of a Wasserstein Generative Adversarial Network (WGAN) could lead to an effective and lightweight solution. It is demonstrated that the WGAN generated images are at par with the original images using inference tests on an already proposed COVID-19 detection model.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2022-09-11", "authors": ["B ZahidHussain", "IfrahAndleeb", "Mohammad SamarAnsari", "Amit MaheshJoshi", "NadiaKanwal"], "doi": "10.1109/EMBC48229.2022.9871519"}
{"title": "Distance-based detection of out-of-distribution silent failures for Covid-19 lung lesion segmentation.", "abstract": "Automatic segmentation of ground glass opacities and consolidations in chest computer tomography (CT) scans can potentially ease the burden of radiologists during times of high resource utilisation. However, deep learning models are not trusted in the clinical routine due to failing silently on out-of-distribution (OOD) data. We propose a lightweight OOD detection method that leverages the Mahalanobis distance in the feature space and seamlessly integrates into state-of-the-art segmentation pipelines. The simple approach can even augment pre-trained models with clinically relevant uncertainty quantification. We validate our method across four chest CT distribution shifts and two magnetic resonance imaging applications, namely segmentation of the hippocampus and the prostate. Our results show that the proposed method effectively detects far- and near-OOD samples across all explored scenarios.", "journal": "Medical image analysis", "date": "2022-09-10", "authors": ["CamilaGonz\u00e1lez", "KarolGotkowski", "MoritzFuchs", "AndreasBucher", "ArminDadras", "RicardaFischbach", "Isabel JasminKaltenborn", "AnirbanMukhopadhyay"], "doi": "10.1016/j.media.2022.102596\n10.7937/K9/TCIA.2015.zF0vlOPv\n10.5281/zenodo.3757476\n10.1016/j.cmpb.2021.106236\n10.1055/a-1544-2240"}
{"title": "COVID-19 diagnosis via chest X-ray image classification based on multiscale class residual attention.", "abstract": "Aiming at detecting COVID-19 effectively, a multiscale class residual attention (MCRA) network is proposed via chest X-ray (CXR) image classification. First, to overcome the data shortage and improve the robustness of our network, a pixel-level image mixing of local regions was introduced to achieve data augmentation and reduce noise. Secondly, multi-scale fusion strategy was adopted to extract global contextual information at different scales and enhance semantic representation. Last but not least, class residual attention was employed to generate spatial attention for each class, which can avoid inter-class interference and enhance related features to further improve the COVID-19 detection. Experimental results show that our network achieves superior diagnostic performance on COVIDx dataset, and its accuracy, PPV, sensitivity, specificity and F1-score are 97.71%, 96.76%, 96.56%, 98.96% and 96.64%, respectively; moreover, the heat maps can endow our deep model with somewhat interpretability.", "journal": "Computers in biology and medicine", "date": "2022-09-10", "authors": ["ShangwangLiu", "TongboCai", "XiufangTang", "YangyangZhang", "ChanggengWang"], "doi": "10.1016/j.compbiomed.2022.106065\n10.1016/j.compbiomed.2022.105350\n10.1109/TIP.2021.3058783\n10.1007/s00330-020-07268-9\n10.1109/TNNLS.2021.3086570\n10.1109/TBDATA.2017.2717439\n10.1007/s11063-021-10569-9\n10.32604/cmes.2020.09463\n10.1016/j.micpro.2020.103282\n10.2174/1574893615666200207094357\n10.1016/j.compbiomed.2022.105383\n10.1007/s10489-021-02393-4\n10.1109/TMI.2021.3117564\n10.1016/j.asoc.2021.108041\n10.1016/j.compbiomed.2021.105002\n10.1016/j.media.2020.101839\n10.1007/s11063-022-10742-8\n10.1016/j.compbiomed.2021.105127\n10.1007/s10489-021-02572-3\n10.1109/TMI.2021.3127074\n10.1007/s00521-021-06806-w\n10.1016/j.compbiomed.2022.105604\n10.1016/j.compbiomed.2022.105244\n10.1016/j.compbiomed.2022.105210\n10.1007/s10489-021-02691-x\n10.1016/j.compbiomed.2022.105335\n10.1109/TIM.2021.3128703\n10.1109/TGRS.2021.3080580\n10.1109/TIP.2021.3124668\n10.1109/TIP.2021.3127851\n10.1109/TGRS.2021.3056624\n10.1016/j.neucom.2021.12.077\n10.1109/TIP.2022.3144017\n10.1109/TMI.2021.3140120\n10.1016/j.media.2021.102345\n10.1109/TIP.2021.3139232\n10.1109/TIP.2022.3154931\n10.1016/j.neucom.2021.11.104\n10.1016/j.media.2022.102381\n10.1109/TPAMI.2020.3040258\n10.1109/TPAMI.2020.3026069\n10.1109/CVPR.2016.90"}
{"title": "Machine Learning Model Based on Radiomic Features for Differentiation between COVID-19 and Pneumonia on Chest X-ray.", "abstract": "Machine learning approaches are employed to analyze differences in real-time reverse transcription polymerase chain reaction scans to differentiate between COVID-19 and pneumonia. However, these methods suffer from large training data requirements, unreliable images, and uncertain clinical diagnosis. Thus, in this paper, we used a machine learning model to differentiate between COVID-19 and pneumonia via radiomic features using a bias-minimized dataset of chest X-ray scans. We used logistic regression (LR), naive Bayes (NB), support vector machine (SVM), k-nearest neighbor (KNN), bagging, random forest (RF), extreme gradient boosting (XGB), and light gradient boosting machine (LGBM) to differentiate between COVID-19 and pneumonia based on training data. Further, we used a grid search to determine optimal hyperparameters for each machine learning model and 5-fold cross-validation to prevent overfitting. The identification performances of COVID-19 and pneumonia were compared with separately constructed test data for four machine learning models trained using the maximum probability, contrast, and difference variance of the gray level co-occurrence matrix (GLCM), and the skewness as input variables. The LGBM and bagging model showed the highest and lowest performances; the GLCM difference variance showed a high overall effect in all models. Thus, we confirmed that the radiomic features in chest X-rays can be used as indicators to differentiate between COVID-19 and pneumonia using machine learning.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-09-10", "authors": ["Young JaeKim"], "doi": "10.3390/s22176709\n10.1007/s00253-020-11061-5\n10.1016/j.prp.2021.153443\n10.1148/radiol.2020203173\n10.1007/s11547-020-01232-9\n10.1016/S0140-6736(20)30183-5\n10.1016/j.diii.2020.03.014\n10.1016/j.ejmp.2021.04.016\n10.1016/j.rinp.2021.105045\n10.1016/j.bbe.2021.05.013\n10.1007/s10489-020-01902-1\n10.1007/s10489-020-02055-x\n10.1007/s13347-021-00477-0\n10.1158/0008-5472.CAN-17-0339\n10.2967/jnumed.118.222893\n10.4236/jsip.2012.32019\n10.1016/j.ejmp.2017.05.071\n10.3390/tomography7020022\n10.1007/s00521-020-05017-z\n10.1002/jmri.26524\n10.1016/j.procs.2018.05.057\n10.1186/s40644-019-0243-3\n10.1007/s00521-018-3754-0\n10.4236/ojs.2015.57075\n10.1016/j.inffus.2018.11.008\n10.1016/S1532-0464(03)00034-0\n10.1155/2021/5594899\n10.1007/s41870-018-0233-x\n10.1177/1536867X20909688\n10.3390/diagnostics11091714\n10.1016/j.cpc.2018.02.018\n10.1109/TKDE.2019.2912815"}
{"title": "Fine-Grained Assessment of COVID-19 Severity Based on Clinico-Radiological Data Using Machine Learning.", "abstract": "The severe and critical cases of COVID-19 had high mortality rates. Clinical features, laboratory data, and radiological features provided important references for the assessment of COVID-19 severity. The machine learning analysis of clinico-radiological features, especially the quantitative computed tomography (CT) image analysis results, may achieve early, accurate, and fine-grained assessment of COVID-19 severity, which is an urgent clinical need.\nTo evaluate if machine learning algorithms using CT-based clinico-radiological features could achieve the accurate fine-grained assessment of COVID-19 severity.\nThe clinico-radiological features were collected from 78 COVID-19 patients with different severities. A neural network was developed to automatically measure the lesion volume from CT images. The severity was clinically diagnosed using two-type (severe and non-severe) and fine-grained four-type (mild, regular, severe, critical) classifications, respectively. To investigate the key features of COVID-19 severity, statistical analyses were performed between patients' clinico-radiological features and severity. Four machine learning algorithms (decision tree, random forest, SVM, and XGBoost) were trained and applied in the assessment of COVID-19 severity using clinico-radiological features.\nThe CT imaging features (CTscore and lesion volume) were significantly related with COVID-19 severity (\nCT-based clinico-radiological features can provide an important reference for the accurate fine-grained assessment of illness severity using machine learning to achieve the early triage of COVID-19 patients.", "journal": "International journal of environmental research and public health", "date": "2022-09-10", "authors": ["HaipengLiu", "JiangtaoWang", "YayuanGeng", "KunweiLi", "HanWu", "JianChen", "XiangfeiChai", "ShaolinLi", "DingchangZheng"], "doi": "10.3390/ijerph191710665\n10.1016/S0140-6736(20)30566-3\n10.1002/jmv.25770\n10.1186/s13613-020-00650-2\n10.1002/jmv.25748\n10.1101/2020.03.24.20042283\n10.1038/s41392-020-0148-4\n10.18632/aging.103372\n10.1016/j.kint.2020.03.005\n10.34133/2020/2402961\n10.2214/AJR.20.22954\n10.1148/radiol.2020200463\n10.7150/thno.45985\n10.2147/IDR.S264541\n10.21037/atm-20-2464\n10.1109/RBME.2020.2987975\n10.2139/ssrn.3564426\n10.1016/j.acra.2020.09.004\n10.1097/IM9.0000000000000022\n10.1007/s00330-020-06817-6\n10.1080/01605682.2020.1865846\n10.1186/s12967-020-02692-3\n10.1002/mp.14609\n10.1007/s00330-020-07013-2\n10.1503/cmaj.200648\n10.1148/radiol.2020200490\n10.1056/NEJMsb2005114\n10.1016/j.dsx.2020.12.029\n10.7150/thno.51471"}
{"title": "Imaging of Intimate Partner Violence, From the ", "abstract": "Intimate partner violence (IPV) is a highly prevalent public health issue with multiple adverse health effects. Radiologists are well suited to assessing a patient's likelihood of IPV. Recognition of common IPV injury mechanisms and resulting target and defensive injury patterns on imaging and understanding of differences between patients who have experienced IPV and those who have not with respect to use of imaging will aid radiologists in accurate IPV diagnosis. Target injuries often involve the face and neck as a result of blunt trauma or strangulation; defensive injuries often involve an extremity. Awareness of differences in injury patterns resulting from IPV-related and accidental trauma can aid radiologists in detecting a mismatch between the provided clinical history and imaging findings to support suspicion of IPV. Radiologists should consider all available current and prior imaging in assessing the likelihood of IPV; this process may be aided by machine learning methods. Even if correctly suspecting IPV on the basis of imaging, radiologists face challenges in acting on that suspicion, including appropriately documenting the findings, without compromising the patient's confidentiality and safety. However, through a multidisciplinary approach with appropriate support mechanisms, radiologists may serve as effective frontline physicians for raising suspicion of IPV.", "journal": "AJR. American journal of roentgenology", "date": "2022-09-08", "authors": ["AnjiTang", "AndrewWong", "BhartiKhurana"], "doi": "10.2214/AJR.22.27973"}
{"title": "Ensemble of Deep Neural Networks based on Condorcet's Jury Theorem for screening Covid-19 and Pneumonia from radiograph images.", "abstract": "COVID-19 detection using Artificial Intelligence and Computer-Aided Diagnosis has been the subject of several studies. Deep Neural Networks with hundreds or even millions of parameters (weights) are referred to as \"black boxes\" because their behavior is difficult to comprehend, even when the model's structure and weights are visible. On the same dataset, different Deep Convolutional Neural Networks perform differently. So, we do not necessarily have to rely on just one model; instead, we can evaluate our final score by combining multiple models. While including multiple models in the voter pool, it is not always true that the accuracy will improve. So, In this regard, the authors proposed a novel approach to determine the voting ensemble score of individual classifiers based on Condorcet's Jury Theorem (CJT). The authors demonstrated that the theorem holds while ensembling the N number of classifiers in Neural Networks. With the help of CJT, the authors proved that a model's presence in the voter pool would improve the likelihood that the majority vote will be accurate if it is more accurate than the other models. Besides this, the authors also proposed a Domain Extended Transfer Learning (DETL) ensemble model as a soft voting ensemble method and compared it with CJT based ensemble method. Furthermore, as deep learning models typically fail in real-world testing, a novel dataset has been used with no duplicate images. Duplicates in the dataset are quite problematic since they might affect the training process. Therefore, having a dataset devoid of duplicate images is considered to prevent data leakage problems that might impede the thorough assessment of the trained models. The authors also employed an algorithm for faster training to save computational efforts. Our proposed method and experimental results outperformed the state-of-the-art with the DETL-based ensemble model showing an accuracy of 97.26%, COVID-19, sensitivity of 98.37%, and specificity of 100%. CJT-based ensemble model showed an accuracy of 98.22%, COVID-19, sensitivity of 98.37%, and specificity of 99.79%.", "journal": "Computers in biology and medicine", "date": "2022-09-06", "authors": ["GauravSrivastava", "NiteshPradhan", "YashwinSaini"], "doi": "10.1016/j.compbiomed.2022.105979\n10.1109/TII.2021.3057683\n10.1109/TII.2021.3057524"}
{"title": "Deep learning framework for prediction of infection severity of COVID-19.", "abstract": "With the onset of the COVID-19 pandemic, quantifying the condition of positively diagnosed patients is of paramount importance. Chest CT scans can be used to measure the severity of a lung infection and the isolate involvement sites in order to increase awareness of a patient's disease progression. In this work, we developed a deep learning framework for lung infection severity prediction. To this end, we collected a dataset of 232 chest CT scans and involved two public datasets with an additional 59 scans for our model's training and used two external test sets with 21 scans for evaluation. On an input chest Computer Tomography (CT) scan, our framework, in parallel, performs a lung lobe segmentation utilizing a pre-trained model and infection segmentation using three distinct trained ", "journal": "Frontiers in medicine", "date": "2022-09-06", "authors": ["MehdiYousefzadeh", "MasoudHasanpour", "MozhdehZolghadri", "FatemehSalimi", "AvaYektaeian Vaziri", "AbolfazlMahmoudi Aqeel Abadi", "RamezanJafari", "ParsaEsfahanian", "Mohammad-RezaNazem-Zadeh"], "doi": "10.3389/fmed.2022.940960\n10.1016/j.idm.2020.02.002\n10.1101/2020.02.27.20028027\n10.1101/2020.02.07.937862\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.1371/journal.pone.0250952\n10.1038/nature14539\n10.1038/s41598-019-51503-3\n10.1038/s41591-019-0447-x\n10.1038/s41598-019-56589-3\n10.1109/TNNLS.2019.2892409\n10.1038/s41591-020-0931-3\n10.1016/j.patcog.2018.07.031\n10.1016/j.dsx.2020.04.012\n10.1109/RBME.2020.2987975\n10.1016/S2589-7500(20)30054-6\n10.1148/ryct.2020200075\n10.1101/2020.03.24.20041020\n10.1136/bmj.m1328\n10.1007/s42979-020-00216-w\n10.1007/s10916-020-01582-x\n10.1109/TAI.2020.3020521\n10.1007/s10278-019-00227-x\n10.1038/srep46479\n10.1109/JBHI.2017.2725903\n10.1002/mp.14676\n10.1007/s00330-020-07042-x\n10.1016/j.knosys.2020.106647\n10.1109/TMI.2020.2995108\n10.1109/ISBI.2019.8759468\n10.1007/s10278-019-00223-1\n10.1186/s41747-020-00173-2\n10.1038/s41598-020-76282-0\n10.48550/arXiv.2003.11988\n10.1007/s10489-020-01829-7\n10.1016/j.patcog.2020.107747\n10.3389/fpubh.2020.00357\n10.48550/arXiv.2006.05018\n10.1101/2020.05.20.20108159\n10.1101/2020.05.20.20100362\n10.1007/978-3-319-24574-4_28\n10.48550/arXiv.1511.07122\n10.1109/TPAMI.2017.2699184\n10.1118/1.3611983\n10.1016/j.imu.2022.100935"}
{"title": "Deep Convolutional Neural Network Mechanism Assessment of COVID-19 Severity.", "abstract": "As an epidemic, COVID-19's core test instrument still has serious flaws. To improve the present condition, all capabilities and tools available in this field are being used to combat the pandemic. Because of the contagious characteristics of the unique coronavirus (COVID-19) infection, an overwhelming comparison with patients queues up for pulmonary X-rays, overloading physicians and radiology and significantly impacting the quality of care, diagnosis, and outbreak prevention. Given the scarcity of clinical services such as intensive care and motorized ventilation systems in the aspect of this vastly transmissible ailment, it is critical to categorize patients as per their risk categories. This research describes a novel use of the deep convolutional neural network (CNN) technique to COVID-19 illness assessment seriousness. Utilizing chest X-ray images as contribution, an unsupervised DCNN model is constructed and suggested to split COVID-19 individuals into four seriousness classrooms: low, medium, serious, and crucial with an accuracy level of 96 percent. The efficiency of the DCNN model developed with the proposed methodology is demonstrated by empirical findings on a suitably huge sum of chest X-ray scans. To the evidence relating, it is the first COVID-19 disease incidence evaluation research with four different phases, to use a reasonably high number of X-ray images dataset and a DCNN with nearly all hyperparameters dynamically adjusted by the variable selection optimization task.", "journal": "BioMed research international", "date": "2022-09-03", "authors": ["JNirmaladevi", "MVidhyalakshmi", "E BijolinEdwin", "NVenkateswaran", "VinayAvasthi", "Abdullah AAlarfaj", "Abdurahman HajinurHirad", "R KRajendran", "TegegneAyalewHailu"], "doi": "10.1155/2022/1289221\n10.1109/ISMSIT50672.2020.9255149\n10.3390/electronics10141677\n10.1101/2020.06.25.20140004\n10.1002/pa.2537\n10.1016/j.asoc.2020.106912\n10.1109/SMART-TECH49988.2020.00041\n10.1155/2022/4352730\n10.3390/a13100249\n10.1016/j.idm.2020.03.002\n10.1155/2021/5709257\n10.1007/s42600-020-00105-4\n10.4066/biomedicalresearch.29-18-886\n10.1007/s40747-021-00312-1\n10.1155/2020/8856801\n10.1007/s10489-020-01829-7\n10.1155/2021/6927985\n10.1155/2021/5587188\n10.1016/j.chaos.2020.110056\n10.3390/jpm11050343\n10.3390/jcm9061668\n10.1016/j.cmpb.2019.06.023\n10.3390/info12030109\n10.1016/j.chaos.2020.110059\n10.1016/j.ipm.2021.102809\n10.1371/journal.pone.0241332\n10.3389/fimmu.2020.01581\n10.1109/ACCESS.2020.2997311\n10.1016/j.patter.2020.100074\n10.1148/radiol.2021204531\n10.1016/S2589-7500(20)30162-X"}
{"title": "High-dimensional multinomial multiclass severity scoring of COVID-19 pneumonia using CT radiomics features and machine learning algorithms.", "abstract": "We aimed to construct a prediction model based on computed tomography (CT) radiomics features to classify COVID-19 patients into severe-, moderate-, mild-, and non-pneumonic. A total of 1110 patients were studied from a publicly available dataset with 4-class severity scoring performed by a radiologist (based on CT images and clinical features). The entire lungs were segmented and followed by resizing, bin discretization and radiomic features extraction. We utilized two feature selection algorithms, namely bagging random forest (BRF) and multivariate adaptive regression splines (MARS), each coupled to a classifier, namely multinomial logistic regression (MLR), to construct multiclass classification models. The dataset was divided into 50% (555 samples), 20% (223 samples), and 30% (332 samples) for training, validation, and untouched test datasets, respectively. Subsequently, nested cross-validation was performed on train/validation to select the features and tune the models. All predictive power indices were reported based on the testing set. The performance of multi-class models was assessed using precision, recall, F1-score, and accuracy based on the 4\u2009\u00d7\u20094 confusion matrices. In addition, the areas under the receiver operating characteristic curves (AUCs) for multi-class classifications were calculated and compared for both models. Using BRF, 23 radiomic features were selected, 11 from first-order, 9 from GLCM, 1 GLRLM, 1 from GLDM, and 1 from shape. Ten features were selected using the MARS algorithm, namely 3 from first-order, 1 from GLDM, 1 from GLRLM, 1 from GLSZM, 1 from shape, and 3 from GLCM features. The mean absolute deviation, skewness, and variance from first-order and flatness from shape, and cluster prominence from GLCM features and Gray Level Non Uniformity Normalize from GLRLM were selected by both BRF and MARS algorithms. All selected features by BRF or MARS were significantly associated with four-class outcomes as assessed within MLR (All p values\u2009<\u20090.05). BRF\u2009+\u2009MLR and MARS\u2009+\u2009MLR resulted in pseudo-R", "journal": "Scientific reports", "date": "2022-09-02", "authors": ["IsaacShiri", "ShayanMostafaei", "AtlasHaddadi Avval", "YazdanSalimi", "AmirhosseinSanaat", "AzadehAkhavanallaf", "HosseinArabi", "ArmanRahmim", "HabibZaidi"], "doi": "10.1038/s41598-022-18994-z\n10.1016/j.ijid.2020.09.1464\n10.1016/j.rbmo.2020.06.001\n10.1289/ehp.120-a118\n10.2214/ajr.20.22954\n10.1016/j.ejrad.2020.108961\n10.1186/s12904-016-0105-8\n10.1016/j.contraception.2010.08.022\n10.1093/bjaceaccp/mkn033\n10.1038/s41591-020-0979-0\n10.1016/j.jaci.2020.04.006\n10.1164/rccm.201105-0816OC\n10.4103/ijri.IJRI_300_16\n10.1007/s00261-007-9315-0\n10.1148/radiol.202020147310.1148/radiol.2020201473\n10.1186/s13244-020-00901-7\n10.1007/s00330-020-07033-y\n10.2214/ajr.20.22976\n10.1097/rli.0000000000000672\n10.1148/radiol.2015151169\n10.1053/j.semnuclmed.2022.04.004\n10.7150/thno.30309\n10.1088/0031-9155/61/13/R150\n10.1016/j.ijrobp.2014.11.030\n10.1016/j.compbiomed.2021.104304\n10.1016/j.compbiomed.2021.105145\n10.1016/j.ejro.2020.100271\n10.1016/j.media.2020.101910\n10.1016/j.compbiomed.2022.105467\n10.1097/rct.0000000000001094\n10.1148/radiol.2020200843\n10.1038/s41467-020-18685-1\n10.1002/ima.22672\n10.1148/radiol.2020191145\n10.1158/0008-5472.Can-17-0339\n10.1186/s13244-021-01105-3\n10.1016/j.compbiomed.2021.104752\n10.1155/2021/6919483\n10.1007/s12559-021-09848-310.1007/s12559-021-09848-3\n10.1155/2021/9996737\n10.1155/2022/4694567\n10.1016/j.compbiomed.2021.104834\n10.1186/s12938-020-00831-x10.1186/s12938-020-00831-x\n10.1148/ryct.2020200322\n10.1007/s00330-020-07012-3\n10.1148/ryai.2020200048\n10.1016/j.clon.2021.11.014\n10.1016/j.compbiomed.2022.105230\n10.1007/s10278-021-00500-y"}
{"title": "Deep learning-based patient re-identification is able to exploit the biometric nature of medical chest X-ray data.", "abstract": "With the rise and ever-increasing potential of deep learning techniques in recent years, publicly available medical datasets became a key factor to enable reproducible development of diagnostic algorithms in the medical domain. Medical data contains sensitive patient-related information and is therefore usually anonymized by removing patient identifiers, e.g., patient names before publication. To the best of our knowledge, we are the first to show that a well-trained deep learning system is able to recover the patient identity from chest X-ray data. We demonstrate this using the publicly available large-scale ChestX-ray14 dataset, a collection of 112,120 frontal-view chest X-ray images from 30,805 unique patients. Our verification system is able to identify whether two frontal chest X-ray images are from the same person with an AUC of 0.9940 and a classification accuracy of 95.55%. We further highlight that the proposed system is able to reveal the same person even ten and more years after the initial scan. When pursuing a retrieval approach, we observe an mAP@R of 0.9748 and a precision@1 of 0.9963. Furthermore, we achieve an AUC of up to 0.9870 and a precision@1 of up to 0.9444 when evaluating our trained networks on external datasets such as CheXpert and the COVID-19 Image Data Collection. Based on this high identification rate, a potential attacker may leak patient-related information and additionally cross-reference images to obtain more information. Thus, there is a great risk of sensitive content falling into unauthorized hands or being disseminated against the will of the concerned patients. Especially during the COVID-19 pandemic, numerous chest X-ray datasets have been published to advance research. Therefore, such data may be vulnerable to potential attacks by deep learning-based re-identification algorithms.", "journal": "Scientific reports", "date": "2022-09-02", "authors": ["KaiPackh\u00e4user", "SebastianG\u00fcndel", "NicolasM\u00fcnster", "ChristopherSyben", "VincentChristlein", "AndreasMaier"], "doi": "10.1038/s41598-022-19045-3\n10.1378/chest.10-1302\n10.1038/s41598-019-56847-4\n10.2214/AJR.12.10375\n10.1038/nature14539\n10.1016/j.zemedi.2018.12.003\n10.1016/j.acra.2019.10.006\n10.1016/S0197-2456(00)00097-0\n10.1007/s40256-020-00420-2\n10.1148/radiol.2020192224\n10.1007/s10278-006-1051-4\n10.1142/S0218488502001648\n10.1016/j.csl.2019.06.001\n10.1145/1866739.1866758\n10.1561/0400000042\n10.1038/s42256-020-0186-1\n10.1109/MSP.2013.2259911\n10.1038/s41746-020-00323-1\n10.1038/s42256-021-00337-8\n10.1126/science.aab3050\n10.1109/5.726791\n10.1016/j.patrec.2005.10.010"}
{"title": "Point-of-care SARS-CoV-2 sensing using lens-free imaging and a deep learning-assisted quantitative agglutination assay.", "abstract": "The persistence of the global COVID-19 pandemic caused by the SARS-CoV-2 virus has continued to emphasize the need for point-of-care (POC) diagnostic tests for viral diagnosis. The most widely used tests, lateral flow assays used in rapid antigen tests, and reverse-transcriptase real-time polymerase chain reaction (RT-PCR), have been instrumental in mitigating the impact of new waves of the pandemic, but fail to provide both sensitive and rapid readout to patients. Here, we present a portable lens-free imaging system coupled with a particle agglutination assay as a novel biosensor for SARS-CoV-2. This sensor images and quantifies individual microbeads undergoing agglutination through a combination of computational imaging and deep learning as a way to detect levels of SARS-CoV-2 in a complex sample. SARS-CoV-2 pseudovirus in solution is incubated with acetyl cholinesterase 2 (ACE2)-functionalized microbeads then loaded into an inexpensive imaging chip. The sample is imaged in a portable in-line lens-free holographic microscope and an image is reconstructed from a pixel superresolved hologram. Images are analyzed by a deep-learning algorithm that distinguishes microbead agglutination from cell debris and viral particle aggregates, and agglutination is quantified based on the network output. We propose an assay procedure using two images which results in the accurate determination of viral concentrations greater than the limit of detection (LOD) of 1.27 \u00d7 10", "journal": "Lab on a chip", "date": "2022-09-02", "authors": ["Colin JPotter", "YanmeiHu", "ZhenXiong", "JunWang", "EuanMcLeod"], "doi": "10.1039/d2lc00289b"}
{"title": "Automated COVID-19 Classification Using Heap-Based Optimization with the Deep Transfer Learning Model.", "abstract": "The outbreak of the COVID-19 pandemic necessitates prompt identification of affected persons to restrict the spread of the COVID-19 epidemic. Radiological imaging such as computed tomography (CT) and chest X-rays (CXR) is considered an effective way to diagnose COVID-19. However, it needs an expert's knowledge and consumes more time. At the same time, artificial intelligence (AI) and medical images are discovered to be helpful in effectively assessing and providing treatment for COVID-19 infected patients. In particular, deep learning (DL) models act as a vital part of a high-performance classification model for COVID-19 recognition on CXR images. This study develops a heap-based optimization with the deep transfer learning model for detection and classification (HBODTL-DC) of COVID-19. The proposed HBODTL-DC system majorly focuses on the identification of COVID-19 on CXR images. To do so, the presented HBODTL-DC model initially exploits the Gabor filtering (GF) technique to enhance the image quality. In addition, the HBO algorithm with a neural architecture search network (NasNet) large model is employed for the extraction of feature vectors. Finally, Elman Neural Network (ENN) model gets the feature vectors as input and categorizes the CXR images into distinct classes. The experimental validation of the HBODTL-DC model takes place on the benchmark CXR image dataset from the Kaggle repository, and the outcomes are checked in numerous dimensions. The experimental outcomes stated the supremacy of the HBODTL-DC model over recent approaches with a maximum accuracy of 0.9992.", "journal": "Computational intelligence and neuroscience", "date": "2022-09-02", "authors": ["BahjatFakieh", "MahmoudRagab"], "doi": "10.1155/2022/7508836\n10.3390/jpm12020309\n10.3390/s21217286\n10.3390/ijerph18063056\n10.1016/j.patrec.2021.08.018\n10.3390/app11199023\n10.1016/j.bbe.2020.08.008\n10.1155/2020/8828855\n10.1016/j.eswa.2020.114054\n10.1007/s11760-020-01820-2\n10.1016/j.cmpb.2020.105581\n10.1109/SSCI47803.2020.9308571\n10.3390/s21041480\n10.1109/access.2020.3025010\n10.3390/diagnostics11050895\n10.1016/j.patcog.2014.01.006\n10.1016/j.eswa.2020.113702\n10.1016/j.asej.2022.101728\n10.1177/0361198120967943\n10.1016/j.compbiomed.2021.104816\n10.1155/2022/6074538\n10.1155/2022/6185013"}
{"title": "Multithreshold Segmentation and Machine Learning Based Approach to Differentiate COVID-19 from Viral Pneumonia.", "abstract": "Coronavirus disease (COVID-19) has created an unprecedented devastation and the loss of millions of lives globally. Contagious nature and fatalities invariably pose challenges to physicians and healthcare support systems. Clinical diagnostic evaluation using reverse transcription-polymerase chain reaction and other approaches are currently in use. The Chest X-ray (CXR) and CT images were effectively utilized in screening purposes that could provide relevant data on localized regions affected by the infection. A step towards automated screening and diagnosis using CXR and CT could be of considerable importance in these turbulent times. The main objective is to probe a simple threshold-based segmentation approach to identify possible infection regions in CXR images and investigate intensity-based, wavelet transform (WT)-based, and Laws based texture features with statistical measures. Further feature selection strategy using Random Forest (RF) then selected features used to create Machine Learning (ML) representation with Support Vector Machine (SVM) and a Random Forest (RF) to make different COVID-19 from viral pneumonia (VP). The results obtained clearly indicate that the intensity and WT-based features vary in the two pathologies that are better differentiated with the combined features trained using SVM and RF classifiers. Classifier performance measures like an Area Under the Curve (AUC) of 0.97 and by and large classification accuracy of 0.9 using the RF model clearly indicate that the methodology implemented is useful in characterizing COVID-19 and Viral Pneumonia.", "journal": "Computational intelligence and neuroscience", "date": "2022-08-31", "authors": ["ShaikMahaboob Basha", "Alo\u00edsio VieiraLira Neto", "SamahAlshathri", "Mohamed AbdElaziz", "ShaikHashmitha Mohisin", "Victor Hugo CDe Albuquerque"], "doi": "10.1155/2022/2728866\n10.23750/abm.v91i1.9397\n10.1038/s41551-021-00704-1\n10.1186/s12938-018-0544-y\n10.1038/s41598-020-74539-2\n10.1007/s13755-021-00146-8\n10.1016/j.measurement.2019.05.076\n10.1016/j.crad.2018.12.015\n10.1109/access.2020.3010287\n10.1007/s12559-020-09787-5\n10.1109/access.2020.2994762\n10.1186/s12938-020-00831-x\n10.1007/978-3-030-87196-3_54\n10.1109/tmi.2020.2993291\n10.1016/j.jart.2017.07.005\n10.1109/JIOT.2020.3038009\n10.1109/anziis.2001.974061\n10.1007/s12065-019-00327-1\n10.1109/42.141636\n10.3390/jimaging7110245\n10.32604/iasc.2021.014369\n10.1109/access.2020.2980942\n10.1016/j.aei.2017.09.007\n10.1002/ima.22393\n10.1007/s10334-018-0674-z\n10.1155/2015/457906\n10.22266/ijies2016.0930.09\n10.1109/jsac.2020.3020598\n10.1002/ima.22518\n10.1109/jas.2020.1003393\n10.1016/j.rinp.2021.105045\n10.5573/ieiespc.2017.6.6.401\n10.1016/j.bspc.2014.01.008\n10.1515/jisys-2016-0111\n10.1016/j.eswa.2016.04.029\n10.1016/j.scs.2020.102589\n10.1155/2022/3167717\n10.1007/978-981-15-1286-5_66\n10.1109/primeasia.2012.6458659\n10.1109/sibircon.2010.5555323\n10.3390/electronics9020274\n10.1007/s12652-020-01963-7\n10.1007/s13042-020-01248-7\n10.1109/tsmc.1979.4310076\n10.1007/978-3-030-74575-2_14\n10.1109/tmi.2020.2993291\n10.1177/2472630320958376\n10.1080/01431160412331269698\n10.1155/2021/7517313\n10.1007/978-3-030-87196-3_54\n10.1109/access.2018.2817614\n10.1002/cpa.3160450502\n10.1016/j.ipm.2009.03.002"}
{"title": "COV-RadNet: A Deep Convolutional Neural Network for Automatic Detection of COVID-19 from Chest X-Rays and CT Scans.", "abstract": "With the increase in severity of COVID-19 pandemic situation, the world is facing a critical fight to cope up with the impacts on human health, education and economy. The ongoing battle with the novel corona virus, is showing much priority to diagnose and provide rapid treatment to the patients. The rapid growth of COVID-19 has broken the healthcare system of the affected countries, creating a shortage in ICUs, test kits, ventilation support system. etc. This paper aims at finding an automatic COVID-19 detection approach which will assist the medical practitioners to diagnose the disease quickly and effectively. In this paper, a deep convolutional neural network, 'COV-RadNet' is proposed to detect COVID positive, viral pneumonia, lung opacity and normal, healthy people by analyzing their Chest Radiographic (X-ray and CT scans) images. Data augmentation technique is applied to balance the dataset 'COVID 19 Radiography Dataset' to make the classifier more robust to the classification task. We have applied transfer learning approach using four deep learning based models: VGG16, VGG19, ResNet152 and ResNext 101 to detect COVID-19 from chest X-ray images. We have achieved 97% classification accuracy using our proposed COV-RadNet model for COVID/Viral Pneumonia/Lungs Opacity/Normal, 99.5% accuracy to detect COVID/Viral Pneumonia/Normal and 99.72% accuracy to detect COVID and non-COVID people. Using chest CT scan images, we have found 99.25% accuracy to classify between COVID and non-COVID classes. Among the performance of the pre-trained models, ResNext 101 has shown the highest accuracy of 98.5% for multiclass classification (COVID, viral pneumonia, Lungs opacity and normal).", "journal": "Computer methods and programs in biomedicine update", "date": "2022-08-31", "authors": ["Md KhairulIslam", "Sultana UmmeHabiba", "Tahsin AhmedKhan", "FarzanaTasnim"], "doi": "10.1016/j.cmpbup.2022.100064"}
{"title": "Constructing custom-made radiotranscriptomic signatures of vascular inflammation from routine CT angiograms: a prospective outcomes validation study in COVID-19.", "abstract": "Direct evaluation of vascular inflammation in patients with COVID-19 would facilitate more efficient trials of new treatments and identify patients at risk of long-term complications who might respond to treatment. We aimed to develop a novel artificial intelligence (AI)-assisted image analysis platform that quantifies cytokine-driven vascular inflammation from routine CT angiograms, and sought to validate its prognostic value in COVID-19.\nFor this prospective outcomes validation study, we developed a radiotranscriptomic platform that uses RNA sequencing data from human internal mammary artery biopsies to develop novel radiomic signatures of vascular inflammation from CT angiography images. We then used this platform to train a radiotranscriptomic signature (C19-RS), derived from the perivascular space around the aorta and the internal mammary artery, to best describe cytokine-driven vascular inflammation. The prognostic value of C19-RS was validated externally in 435 patients (331 from study arm 3 and 104 from study arm 4) admitted to hospital with or without COVID-19, undergoing clinically indicated pulmonary CT angiography, in three UK National Health Service (NHS) trusts (Oxford, Leicester, and Bath). We evaluated the diagnostic and prognostic value of C19-RS for death in hospital due to COVID-19, did sensitivity analyses based on dexamethasone treatment, and investigated the correlation of C19-RS with systemic transcriptomic changes.\nPatients with COVID-19 had higher C19-RS than those without (adjusted odds ratio [OR] 2\u00b797 [95% CI 1\u00b743-6\u00b727], p=0\u00b70038), and those infected with the B.1.1.7 (alpha) SARS-CoV-2 variant had higher C19-RS values than those infected with the wild-type SARS-CoV-2 variant (adjusted OR 1\u00b789 [95% CI 1\u00b717-3\u00b720] per SD, p=0\u00b7012). C19-RS had prognostic value for in-hospital mortality in COVID-19 in two testing cohorts (high [\u22656\u00b799] vs low [<6\u00b799] C19-RS; hazard ratio [HR] 3\u00b731 [95% CI 1\u00b749-7\u00b733], p=0\u00b70033; and 2\u00b758 [1\u00b710-6\u00b705], p=0\u00b7028), adjusted for clinical factors, biochemical biomarkers of inflammation and myocardial injury, and technical parameters. The adjusted HR for in-hospital mortality was 8\u00b724 (95% CI 2\u00b716-31\u00b736, p=0\u00b70019) in patients who received no dexamethasone treatment, but 2\u00b727 (0\u00b769-7\u00b755, p=0\u00b718) in those who received dexamethasone after the scan, suggesting that vascular inflammation might have been a therapeutic target of dexamethasone in COVID-19. Finally, C19-RS was strongly associated (r=0\u00b761, p=0\u00b700031) with a whole blood transcriptional module representing dysregulation of coagulation and platelet aggregation pathways.\nRadiotranscriptomic analysis of CT angiography scans introduces a potentially powerful new platform for the development of non-invasive imaging biomarkers. Application of this platform in routine CT pulmonary angiography scans done in patients with COVID-19 produced the radiotranscriptomic signature C19-RS, a marker of cytokine-driven inflammation driving systemic activation of coagulation and responsible for adverse clinical outcomes, which predicts in-hospital mortality and might allow targeted therapy.\nEngineering and Physical Sciences Research Council, British Heart Foundation, Oxford BHF Centre of Research Excellence, Innovate UK, NIHR Oxford Biomedical Research Centre, Wellcome Trust, Onassis Foundation.", "journal": "The Lancet. Digital health", "date": "2022-08-30", "authors": ["Christos PKotanidis", "ChengXie", "DonnaAlexander", "Jonathan C LRodrigues", "KatieBurnham", "AlexanderMentzer", "DanielO'Connor", "JulianKnight", "MuhammadSiddique", "HelenLockstone", "SheenaThomas", "RafailKotronias", "Evangelos KOikonomou", "IleanaBadi", "MariaLyasheva", "CheeragShirodaria", "Sheila FLumley", "BedeConstantinides", "NicholasSanderson", "GillianRodger", "Kevin KChau", "ArchieLodge", "MariaTsakok", "FergusGleeson", "DavidAdlam", "PraveenRao", "DasIndrajeet", "AparnaDeshpande", "AmritaBajaj", "Benjamin JHudson", "VivekSrivastava", "ShakilFarid", "GeorgeKrasopoulos", "RanaSayeed", "Ling-PeiHo", "StefanNeubauer", "David ENewby", "Keith MChannon", "JohnDeanfield", "CharalambosAntoniades", "NoneNone", "NoneNone"], "doi": "10.1016/S2589-7500(22)00132-7"}
{"title": "Chest X-ray analysis empowered with deep learning: A systematic review.", "abstract": "Chest radiographs are widely used in the medical domain and at present, chest X-radiation particularly plays an important role in the diagnosis of medical conditions such as pneumonia and COVID-19 disease. The recent developments of deep learning techniques led to a promising performance in medical image classification and prediction tasks. With the availability of chest X-ray datasets and emerging trends in data engineering techniques, there is a growth in recent related publications. Recently, there have been only a few survey papers that addressed chest X-ray classification using deep learning techniques. However, they lack the analysis of the trends of recent studies. This systematic review paper explores and provides a comprehensive analysis of the related studies that have used deep learning techniques to analyze chest X-ray images. We present the state-of-the-art deep learning based pneumonia and COVID-19 detection solutions, trends in recent studies, publicly available datasets, guidance to follow a deep learning process, challenges and potential future research directions in this domain. The discoveries and the conclusions of the reviewed work have been organized in a way that researchers and developers working in the same domain can use this work to support them in taking decisions on their research.", "journal": "Applied soft computing", "date": "2022-08-30", "authors": ["DulaniMeedeniya", "HasharaKumarasinghe", "ShammiKolonne", "ChamodiFernando", "Isabel De la TorreD\u00edez", "Gon\u00e7aloMarques"], "doi": "10.1016/j.asoc.2022.109319\n10.1038/s41392-020-00243-2\n10.1148/ryct.2020200028\n10.1016/j.compmedimag.2019.05.005\n10.1109/42.974918\n10.1109/ACCESS.2021.3065965\n10.3390/app10020559\n10.1007/s13246-020-00865-4\n10.1016/B978-0-12-819061-6.00013-6\n10.1007/s11633-020-1231-6\n10.3390/jimaging6120131\n10.1016/j.media.2021.102125\n10.30534/ijeter/2021/09972021\n10.1016/j.scs.2020.102589\n10.1109/MCI.2020.3019873\n10.1109/ic-ETITE47903.2020.152\n10.1016/j.compbiomed.2020.103898\n10.1097/01.NAJ.0000444496.24228.2c\n10.1016/j.chaos.2020.110337\n10.1007/978-981-15-7219-7_22\n10.1109/EMBC44109.2020.9175594\n10.3390/diagnostics10060417\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.48550/arXiv.1602.07360\n10.1109/CVPR.2017.195\n10.1007/978-3-319-24574-4_28\n10.1016/j.compmedimag.2017.04.001\n10.1016/j.asoc.2020.106580\n10.1016/j.asoc.2020.106691\n10.1017/9781139061773\n10.1109/IES50839.2020.9231540\n10.1109/ICOSEC49089.2020.9215257\n10.1145/3431804\n10.1109/KSE.2018.8573404\n10.1038/s41598-020-76550-z\n10.1016/j.chaos.2020.109944\n10.1109/INDIACom51348.2021.00137\n10.1109/ICCCNT49239.2020.9225543\n10.1109/ISRITI51436.2020.9315478\n10.1007/s10044-021-00970-4\n10.1155/2021/5513679\n10.1002/ima.22566\n10.1016/j.compbiomed.2020.103805\n10.1016/j.knosys.2020.106062\n10.1117/12.2547635\n10.1109/TMI.2013.2290491\n10.1109/TMI.2013.2284099\n10.1007/978-3-030-32254-0_74\n10.1109/ICVEE50212.2020.9243290\n10.1109/EMBC44109.2020.9176517\n10.1109/EBBT.2019.8741582\n10.1007/s00264-020-04609-7\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2020.100405\n10.1109/ICECOCS50124.2020.9314567\n10.1016/j.jjimei.2021.100020\n10.1101/2020.03.26.20044610\n10.1109/ACCESS.2021.3086229\n10.1117/12.2581314\n10.1109/RIVF48685.2020.9140733\n10.1109/ACCESS.2020.2974242\n10.31661/jbpe.v0i0.2008-1153\n10.1109/ICISS49785.2020.9316100\n10.1109/DASA53625.2021.9682248\n10.48550/arXiv.1711.05225\n10.1109/ICECCT.2019.8869364\n10.1016/j.chaos.2020.110122\n10.1016/j.patrec.2020.09.010\n10.1007/s42600-021-00151-6\n10.1016/j.eswa.2021.114883\n10.1016/j.mehy.2020.109761\n10.1109/ICECCE49384.2020.9179404\n10.1155/2020/8828855\n10.48550/arXiv.1409.1556\n10.1109/CVPR.2018.00474\n10.5614/itbj.ict.res.appl.2019.13.3.5\n10.1117/12.2581882\n10.17632/rscbjbr9sj.3\n10.1155/2019/4180949\n10.5220/0007404301120119\n10.1007/s12559-020-09795-5\n10.1016/j.cmpb.2020.105581\n10.1177/2472630320958376\n10.3390/s21041480\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1016/j.chaos.2020.110245\n10.1109/Confluence47617.2020.9057809\n10.1109/CCECE.2019.8861969\n10.1007/978-981-15-3369-3_36\n10.1142/S0218001421510046\n10.1109/EBBT.2019.8742050\n10.34740/kaggle/dsv/1019469\n10.17632/2fxz4px6d8.4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.measurement.2019.05.076\n10.1109/EIT48999.2020.9208232\n10.12928/TELKOMNIKA.v18i3.14751\n10.5220/0007346600760083\n10.32604/cmc.2021.018514\n10.1016/j.sysarc.2019.101635\n10.1007/978-3-642-21219-2_1\n10.1007/978-3-030-32248-9_45\n10.1109/ICARC54489.2022.9753811\n10.3991/ijoe.v18i07.30807\n10.48550/arXiv.1701.03757\n10.1016/j.simpa.2022.100340\n10.1148/ryai.2020190043\n10.1007/s00521-021-06396-7"}
{"title": "SEL-COVIDNET: An intelligent application for the diagnosis of COVID-19 from chest X-rays and CT-scans.", "abstract": "COVID-19 detection from medical imaging is a difficult challenge that has piqued the interest of experts worldwide. Chest X-rays and computed tomography (CT) scanning are the essential imaging modalities for diagnosing COVID-19. All researchers focus their efforts on developing viable methods and rapid treatment procedures for this pandemic. Fast and accurate automated detection approaches have been devised to alleviate the need for medical professionals. Deep Learning (DL) technologies have successfully recognized COVID-19 situations. This paper proposes a developed set of nine deep learning models for diagnosing COVID-19 based on transfer learning and implementation in a novel architecture (SEL-COVIDNET). We include a global average pooling layer, flattening, and two dense layers that are fully connected. The model's effectiveness is evaluated using balanced and unbalanced COVID-19 radiography datasets. After that, our model's performance is analyzed using six evaluation measures: accuracy, sensitivity, specificity, precision, F1-score, and Matthew's correlation coefficient (MCC). Experiments demonstrated that the proposed SEL-COVIDNET with tuned DenseNet121, InceptionResNetV2, and MobileNetV3Large models outperformed the results of comparative SOTA for multi-class classification (COVID-19 vs. No-finding vs. Pneumonia) in terms of accuracy (98.52%), specificity (98.5%), sensitivity (98.5%), precision (98.7%), F1-score (98.7%), and MCC (97.5%). For the COVID-19 vs. No-finding classification, our method had an accuracy of 99.77%, a specificity of 99.85%, a sensitivity of 99.85%, a precision of 99.55%, an F1-score of 99.7%, and an MCC of 99.4%. The proposed model offers an accurate approach for detecting COVID-19 patients, which aids in the containment of the COVID-19 pandemic.", "journal": "Informatics in medicine unlocked", "date": "2022-08-30", "authors": ["Ahmad AlSmadi", "AhedAbugabah", "Ahmad MohammadAl-Smadi", "SultanAlmotairi"], "doi": "10.1016/j.imu.2022.101059\n10.1145/3447450.3447458\n10.1109/ACCESS.2020.3010287\n10.48550/ARXIV.1512.03385\n10.48550/ARXIV.1801.04381\n10.1109/ICCV.2019.00140\n10.1109/CVPRW50498.2020.00183"}
{"title": "HADCNet: Automatic segmentation of COVID-19 infection based on a hybrid attention dense connected network with dilated convolution.", "abstract": "the automatic segmentation of lung infections in CT slices provides a rapid and effective strategy for diagnosing, treating, and assessing COVID-19 cases. However, the segmentation of the infected areas presents several difficulties, including high intraclass variability and interclass similarity among infected areas, as well as blurred edges and low contrast. Therefore, we propose HADCNet, a deep learning framework that segments lung infections based on a dual hybrid attention strategy. HADCNet uses an encoder hybrid attention module to integrate feature information at different scales across the peer hierarchy to refine the feature map. Furthermore, a decoder hybrid attention module uses an improved skip connection to embed the semantic information of higher-level features into lower-level features by integrating multi-scale contextual structures and assigning the spatial information of lower-level features to higher-level features, thereby capturing the contextual dependencies of lesion features across levels and refining the semantic structure, which reduces the semantic gap between feature maps at different levels and improves the model segmentation performance. We conducted fivefold cross-validations of our model on four publicly available datasets, with final mean Dice scores of 0.792, 0.796, 0.785, and 0.723. These results show that the proposed model outperforms popular state-of-the-art semantic segmentation methods and indicate its potential use in the diagnosis and treatment of COVID-19.", "journal": "Computers in biology and medicine", "date": "2022-08-28", "authors": ["YingChen", "TaohuiZhou", "YiChen", "LongfengFeng", "ChengZheng", "LanLiu", "LipingHu", "BujianPan"], "doi": "10.1016/j.compbiomed.2022.105981"}
{"title": "The role of artificial intelligence technology analysis of high-resolution computed tomography images in predicting the severity of COVID-19 pneumonia.", "abstract": "High-resolution computed tomography (HRCT) is usually used only for qualitative analysis of COVID-19 pneumonia. However, when coupled with artificial intelligence (AI) it can also automatically provide quantitative data.\nThe purpose of the study was to analyze the role of automatic assessment of COVID\u201119 pneumonia severity on HRCT images by AI technology.\nWe retrospectively studied medical records of consecutive patients admitted to the Krakow University Hospital due to COVID\u201119. Of the 1729 patients, 804 underwent HRCT with automatic analysis of such radiological parameters as absolute inflammation volume, absolute ground glass volume, absolute consolidation volume (ACV), percentage inflammation volume, percentage ground glass volume, percentage consolidation volume (PCV), and severity of pneumonia classified as none, mild, moderate, or critical.\nThe automatically assessed radiological parameters correlated with the clinical parameters that reflected the severity of pneumonia (P <0.05). The patients with critical pneumonia, as compared with mild or moderate one, were more frequently men, had significantly lower oxygen saturation, higher respiratory rate, higher levels of inflammatory markers, as well as more common need for mechanical ventilation and admission to the intensive care unit. They were also more likely to die during hospitalization. Notably, as determined by the receiver operating characteristic curve analysis, radiological parameters above or equal to the cutoff points were independently associated with in\u2011hospital mortality (ACV odds ratio [OR], 4.08; 95% CI, 2.62-6.35; PCV OR, 4.05; 95% CI, 2.60-6.30).\nUsing AI to analyze HRCT images is a simple and valuable approach to predict the severity of COVID\u201119 pneumonia.", "journal": "Polish archives of internal medicine", "date": "2022-08-27", "authors": ["RobertChrzan", "WiktoriaWojciechowska", "Micha\u0142Terlecki", "MarekKlocek", "MarekRajzer", "TadeuszPopiela"], "doi": "10.20452/pamw.16332"}
{"title": "Novel Coronavirus and Common Pneumonia Detection from CT Scans Using Deep Learning-Based Extracted Features.", "abstract": "COVID-19 which was announced as a pandemic on 11 March 2020, is still infecting millions to date as the vaccines that have been developed do not prevent the disease but rather reduce the severity of the symptoms. Until a vaccine is developed that can prevent COVID-19 infection, the testing of individuals will be a continuous process. Medical personnel monitor and treat all health conditions; hence, the time-consuming process to monitor and test all individuals for COVID-19 becomes an impossible task, especially as COVID-19 shares similar symptoms with the common cold and pneumonia. Some off-the-counter tests have been developed and sold, but they are unreliable and add an additional burden because false-positive cases have to visit hospitals and perform specialized diagnostic tests to confirm the diagnosis. Therefore, the need for systems that can automatically detect and diagnose COVID-19 automatically without human intervention is still an urgent priority and will remain so because the same technology can be used for future pandemics and other health conditions. In this paper, we propose a modified machine learning (ML) process that integrates deep learning (DL) algorithms for feature extraction and well-known classifiers that can accurately detect and diagnose COVID-19 from chest CT scans. Publicly available datasets were made available by the China Consortium for Chest CT Image Investigation (CC-CCII). The highest average accuracy obtained was 99.9% using the modified ML process when 2000 features were extracted using GoogleNet and ResNet18 and using the support vector machine (SVM) classifier. The results obtained using the modified ML process were higher when compared to similar methods reported in the extant literature using the same datasets or different datasets of similar size; thus, this study is considered of added value to the current body of knowledge. Further research in this field is required to develop methods that can be applied in hospitals and can better equip mankind to be prepared for any future pandemics.", "journal": "Viruses", "date": "2022-08-27", "authors": ["GhazanfarLatif", "HamdyMorsy", "AsmaaHassan", "JaafarAlghazo"], "doi": "10.3390/v14081667\n10.1007/s10044-021-00984-y\n10.1016/j.idm.2020.02.002\n10.3201/eid1212.060401\n10.1136/bmj.m641\n10.1001/jama.2020.2565\n10.1016/j.dsx.2020.04.012\n10.1109/JBHI.2020.3037127\n10.1016/j.media.2020.101797\n10.1007/s10489-020-02029-z\n10.1016/j.susoc.2021.08.001\n10.1109/ACCESS.2020.3016780\n10.1016/j.eswa.2020.114054\n10.1109/ACCESS.2020.2994762\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01902-1\n10.3390/s21020455\n10.3390/s21041480\n10.3390/diagnostics11111972\n10.1016/j.compbiomed.2022.105233\n10.1007/s11042-022-11913-4\n10.2174/1573405614666180402150218\n10.1016/j.procs.2019.12.110\n10.1155/2022/2665283\n10.3390/math10050796\n10.1002/mp.14609\n10.3390/diagnostics11071155\n10.1016/j.jcp.2020.110010\n10.3233/JIFS-189132\n10.1109/ictcs.2019.8923092\n10.3390/app10134523\n10.1007/s40846-021-00656-6\n10.2139/ssrn.3754116\n10.4316/AECE.2014.01010\n10.1145/3277104.3278311\n10.3390/diagnostics12041018\n10.1007/978-1-4471-7296-3_21\n10.1016/j.cell.2020.04.045\n10.1016/j.media.2020.101913\n10.1002/mp.15044\n10.1016/j.compbiomed.2021.104857"}
{"title": "COVID-19 and Virtual Nutrition: A Pilot Study of Integrating Digital Food Models for Interactive Portion Size Education.", "abstract": "Background and aims: Digital food viewing is a vital skill for connecting dieticians to e-health. The aim of this study was to integrate a novel pedagogical framework that combines interactive three- (3-D) and two-dimensional (2-D) food models into a formal dietetic training course. The level of agreement between the digital food models (first semester) and the effectiveness of educational integration of digital food models during the school closure due to coronavirus disease 2019 (COVID-19) (second semester) were evaluated. Method: In total, 65 second-year undergraduate dietetic students were enrolled in a nutritional practicum course at the School of Nutrition and Health Sciences, Taipei Medical University (Taipei, Taiwan). A 3-D food model was created using Agisoft Metashape. Students\u2019 digital food viewing skills and receptiveness towards integrating digital food models were evaluated. Results: In the first semester, no statistical differences were observed between 2-D and 3-D food viewing skills in food identification (2-D: 89% vs. 3-D: 85%) and quantification (within \u00b110% difference in total calories) (2-D: 19.4% vs. 3-D: 19.3%). A Spearman correlation analysis showed moderate to strong correlations of estimated total calories (0.69~0.93; all p values < 0.05) between the 3-D and 2-D models. Further analysis showed that students who struggled to master both 2-D and 3-D food viewing skills had lower estimation accuracies than those who did not (equal performers: 28% vs. unequal performers:16%, p = 0.041), and interactive 3-D models may help them perform better than 2-D models. In the second semester, the digital food viewing skills significantly improved (food identification: 91.5% and quantification: 42.9%) even for those students who struggled to perform digital food viewing skills equally in the first semester (equal performers: 44% vs. unequal performers: 40%). Conclusion: Although repeated training greatly enhanced students\u2019 digital food viewing skills, a tailored training program may be needed to master 2-D and 3-D digital food viewing skills. Future study is needed to evaluate the effectiveness of digital food models for future \u201ceHealth\u201d care.", "journal": "Nutrients", "date": "2022-08-27", "authors": ["Dang Khanh NganHo", "Yu-ChiehLee", "Wan-ChunChiu", "Yi-TaShen", "Chih-YuanYao", "Hung-KuoChu", "Wei-TaChu", "Nguyen Quoc KhanhLe", "Hung TrongNguyen", "Hsiu-YuehSu", "Jung-SuChang"], "doi": "10.3390/nu14163313\n10.1016/j.appet.2019.104522\n10.1080/09637486.2017.1309521\n10.1017/S1368980012003655\n10.1001/archpedi.156.9.867\n10.1111/jhn.12063\n10.1038/oby.2011.344\n10.3390/nu9010073\n10.1258/jtt.2011.100906\n10.1016/j.clnu.2020.08.002\n10.3390/nu13010175\n10.3390/nu9020114\n10.3390/nu10080984\n10.1079/BJN19960007\n10.1007/s10055-020-00484-0\n10.1515/libri-2017-0024\n10.1186/s12966-017-0516-9\n10.1111/acem.13972\n10.1109/ism.2010.50\n10.1017/S1368980018000344\n10.1111/j.1365-277X.2010.01042.x\n10.3390/nu10060741\n10.1038/s41366-020-00693-2\n10.1089/tmj.2009.0174"}
{"title": "Vascular Implications of COVID-19: Role of Radiological Imaging, Artificial Intelligence, and Tissue Characterization: A Special Report.", "abstract": "The SARS-CoV-2 virus has caused a pandemic, infecting nearly 80 million people worldwide, with mortality exceeding six million. The average survival span is just 14 days from the time the symptoms become aggressive. The present study delineates the deep-driven vascular damage in the pulmonary, renal, coronary, and carotid vessels due to SARS-CoV-2. This special report addresses an important gap in the literature in understanding (i) the pathophysiology of vascular damage and the role of medical imaging in the visualization of the damage caused by SARS-CoV-2, and (ii) further understanding the severity of COVID-19 using artificial intelligence (AI)-based tissue characterization (TC). PRISMA was used to select 296 studies for AI-based TC. Radiological imaging techniques such as magnetic resonance imaging (MRI), computed tomography (CT), and ultrasound were selected for imaging of the vasculature infected by COVID-19. Four kinds of hypotheses are presented for showing the vascular damage in radiological images due to COVID-19. Three kinds of AI models, namely, machine learning, deep learning, and transfer learning, are used for TC. Further, the study presents recommendations for improving AI-based architectures for vascular studies. We conclude that the process of vascular damage due to COVID-19 has similarities across vessel types, even though it results in multi-organ dysfunction. Although the mortality rate is ~2% of those infected, the long-term effect of COVID-19 needs monitoring to avoid deaths. AI seems to be penetrating the health care industry at warp speed, and we expect to see an emerging role in patient care, reduce the mortality and morbidity rate.", "journal": "Journal of cardiovascular development and disease", "date": "2022-08-26", "authors": ["Narendra NKhanna", "MaheshMaindarkar", "AnudeepPuvvula", "SudipPaul", "MrinaliniBhagawati", "PuneetAhluwalia", "ZoltanRuzsa", "AdityaSharma", "SmikshaMunjral", "RaghuKolluri", "Padukone RKrishnan", "Inder MSingh", "John RLaird", "MostafaFatemi", "AzraAlizad", "Surinder KDhanjil", "LucaSaba", "AntonellaBalestrieri", "GavinoFaa", "Kosmas IParaskevas", "Durga PrasannaMisra", "VikasAgarwal", "AmanSharma", "JagjitTeji", "MustafaAl-Maini", "AndrewNicolaides", "VijayRathore", "SubbaramNaidu", "KieraLiblik", "Amer MJohri", "MonikaTurk", "David WSobel", "GyanPareek", "MartinMiner", "KlaudijaViskovic", "GeorgeTsoulfas", "Athanasios DProtogerou", "SophieMavrogeni", "George DKitas", "Mostafa MFouda", "Manudeep KKalra", "Jasjit SSuri"], "doi": "10.3390/jcdd9080268\n10.1007/s10072-021-05756-4\n10.3233/JPD-202038\n10.3389/fpsyt.2020.590134\n10.1001/jama.2020.1585\n10.1016/j.compbiomed.2020.103960\n10.1186/s13054-020-03062-7\n10.1161/CIRCULATIONAHA.112.093245\n10.1161/01.ATV.0000051384.43104.FC\n10.1016/S0140-6736(20)30937-5\n10.1038/s41577-020-0343-0\n10.1016/j.atherosclerosis.2020.10.014\n10.1016/j.cell.2020.04.004\n10.21037/cdt-20-561\n10.1016/j.ejrad.2019.02.038\n10.1007/s10916-017-0797-1\n10.1016/j.compbiomed.2020.103804\n10.1007/s10554-020-02099-7\n10.1007/s00296-020-04691-5\n10.4239/wjd.v12.i3.215\n10.1007/s11517-018-1897-x\n10.1016/j.ultras.2011.11.003\n10.7863/ultra.33.2.245\n10.1007/s10278-012-9553-8\n10.3109/02652048.2013.879932\n10.1016/j.cmpb.2017.07.011\n10.1016/j.eswa.2015.03.014\n10.1109/TIM.2011.2174897\n10.1007/s10916-017-0745-0\n10.1007/s11517-006-0119-0\n10.1016/j.cmpb.2016.02.004\n10.21037/cdt.2016.03.08\n10.3390/diagnostics11122367\n10.3390/jcm9072146\n10.1007/s11517-012-1019-0\n10.1016/j.cmpb.2012.09.008\n10.1007/s11883-018-0736-8\n10.3390/diagnostics11112109\n10.1016/j.compbiomed.2021.105131\n10.1007/s10916-021-01707-w\n10.1109/JBHI.2021.3103839\n10.1016/j.mehy.2020.109603\n10.1007/s13721-017-0155-8\n10.3389/fnagi.2021.633752\n10.1016/j.virol.2015.03.043\n10.1038/s41579-018-0118-9\n10.1038/nrmicro2090\n10.1001/jama.2020.6019\n10.1038/d41573-020-00016-0\n10.1056/NEJMoa2001282\n10.1086/375233\n10.1001/jama.2020.2648\n10.1038/nri2171\n10.1161/CIRCULATIONAHA.104.510461\n10.1152/ajplung.00498.2016\n10.1016/j.cell.2020.02.058\n10.1038/s41598-022-07918-6\n10.1177/03009858221079665\n10.1007/s12250-020-00207-4\n10.1128/JVI.01248-09\n10.1002/rth2.12175\n10.2147/COPD.S329783\n10.1128/mBio.00638-15\n10.1016/S0140-6736(20)30628-0\n10.1002/jmv.23354\n10.1007/s12035-021-02457-z\n10.1002/path.1440\n10.1002/jmv.25709\n10.1056/NEJMoa2015432\n10.1080/20009666.2021.1921908\n10.1136/bmj-2021-069590\n10.1186/s12959-020-00255-6\n10.7326/L20-1275\n10.1148/ryct.2020200277\n10.1148/rg.282075705\n10.1155/2022/4640788\n10.1097/CM9.0000000000000774\n10.1007/s11684-020-0754-0\n10.1159/000342483\n10.1038/s41467-021-22781-1\n10.1016/j.kint.2020.04.003\n10.1681/ASN.2020040419\n10.2139/ssrn.3559601\n10.1016/j.tcm.2020.10.005\n10.1016/j.avsg.2020.07.013\n10.1016/j.acra.2020.07.019\n10.1186/s13054-020-02931-5\n10.1007/s00330-020-07300-y\n10.1016/j.ajem.2020.07.054\n10.1093/eurheartj/ehab314\n10.1016/j.ijcard.2020.04.028\n10.1128/MMBR.05015-11\n10.1016/j.tox.2022.153104\n10.1080/15384101.2019.1662678\n10.1016/j.ebiom.2020.102763\n10.1002/jmv.25900\n10.1016/j.ebiom.2020.102789\n10.1038/s41584-020-0474-5\n10.37899/journallamedihealtico.v3i3.647\n10.1016/j.juro.2015.03.119\n10.1016/j.idcr.2020.e00968\n10.1093/ckj/sfaa141\n10.1007/s00134-020-06153-9\n10.1016/j.biopha.2021.111966\n10.4103/iju.IJU_76_21\n10.1016/j.xkme.2020.07.010\n10.1097/SHK.0000000000001659\n10.1016/j.clinimag.2020.11.011\n10.1148/radiol.2020201623\n10.4269/ajtmh.20-0869\n10.1016/S0140-6736(20)30566-3\n10.1016/j.jacc.2020.04.031\n10.1016/j.immuni.2020.06.017\n10.1016/S0304-4157(98)00018-5\n10.1038/nri3345\n10.2174/138920111798281171\n10.12688/f1000research.9692.1\n10.1111/j.1365-2362.2009.02153.x\n10.1046/j.1523-1755.62.s82.4.x\n10.3390/molecules27072048\n10.1161/01.RES.84.9.1043\n10.1111/ijlh.13829\n10.1161/01.ATV.20.8.2019\n10.1016/j.jacc.2020.06.080\n10.1002/ccd.29056\n10.1111/jocs.14538\n10.1016/j.clinimag.2021.02.016\n10.1007/s00259-021-05375-3\n10.1186/s13244-021-00973-z\n10.3174/ajnr.A6674\n10.1161/CIRCULATIONAHA.120.047525\n10.1016/j.wneu.2020.08.154\n10.1016/j.neurad.2020.04.003\n10.3389/fcvm.2021.671669\n10.1016/j.clinimag.2020.07.007\n10.1111/jon.12803\n10.1155/2020/7397480\n10.1016/j.jvs.2021.11.064\n10.1016/j.cmpb.2017.12.016\n10.1007/s10916-015-0214-6\n10.7785/tcrt.2012.500381\n10.1038/nature14539\n10.1016/j.nicl.2022.103065\n10.1016/j.cmpb.2021.106332\n10.1016/j.ejrad.2020.109041\n10.1007/s00330-020-07044-9\n10.3348/kjr.2020.0536\n10.1109/TMI.2020.2994459\n10.1002/jmri.26887\n10.1148/ryai.2020200048\n10.1148/radiol.2020200905\n10.1007/s10096-020-03901-z\n10.21037/atm.2020.03.132\n10.1109/ACCESS.2020.3005510\n10.1016/j.cell.2020.04.045\n10.1109/TUFFC.2020.3005512\n10.1109/ACCESS.2020.3003810\n10.1016/j.compbiomed.2020.103869\n10.1016/j.cmpb.2020.105608\n10.1080/07391102.2020.1788642\n10.1111/nan.12667\n10.3390/cancers14040987\n10.1016/j.cmpb.2017.09.004\n10.1002/mp.14193\n10.1016/j.ekir.2017.11.002\n10.1038/s41746-019-0104-2\n10.1007/s11517-005-0016-y\n10.7863/jum.2009.28.11.1561\n10.1080/03772063.2019.1604176\n10.1093/ajcn/65.4.1000\n10.1007/s11883-016-0635-9\n10.5853/jos.2017.02922\n10.1016/j.cmpb.2013.07.012\n10.1016/j.bspc.2013.08.008\n10.1109/TMI.2016.2528162\n10.1515/itms-2017-0007\n10.1016/j.compbiomed.2020.103958\n10.1016/j.cmpb.2012.05.008\n10.1016/j.compbiomed.2018.05.014\n10.1177/1544316718806421\n10.1016/j.compbiomed.2020.103847\n10.3390/diagnostics11122257\n10.23736/S0392-9590.21.04771-4\n10.1016/j.compbiomed.2016.11.011\n10.1109/JBHI.2016.2631401\n10.3934/mbe.2022229\n10.1504/IJCSE.2022.120788\n10.1016/j.jacr.2017.12.028\n10.1016/j.ejrad.2017.01.031\n10.21037/atm-20-7676\n10.1007/s10554-020-02124-9\n10.1109/TIM.2021.3052577\n10.3390/jcm11133721\n10.1049/el.2020.2102\n10.3390/electronics11111800\n10.1016/j.compbiomed.2022.105273\n10.1109/RBME.2020.2990959\n10.1016/j.preteyeres.2021.100965\n10.1038/s41467-020-17971-2\n10.1056/NEJMc2011400\n10.1016/j.jacc.2020.05.001\n10.1186/s12882-020-02150-8\n10.7759/cureus.9540\n10.1007/s13204-021-01868-7\n10.1007/s11548-021-02317-0\n10.3389/fphys.2022.832457\n10.1002/cyto.a.24274\n10.1038/s41598-019-54244-5\n10.1148/radiol.2021211483\n10.1016/j.jcmg.2021.10.013\n10.1681/ASN.2020050589\n10.1016/j.ijantimicag.2020.105949\n10.1016/j.mayocp.2020.03.024\n10.1148/radiol.2020203511\n10.1186/s13054-020-03179-9\n10.1055/a-1775-8633\n10.1155/2021/6761364\n10.1681/ASN.2020050597\n10.1093/ejcts/ezac289\n10.36660/abc.20200302\n10.1016/j.compbiomed.2021.104721\n10.1016/j.ejvs.2009.03.013\n10.3348/kjr.2021.0148\n10.1007/s40520-021-01985-x\n10.1007/s00296-021-05062-4\n10.1056/NEJM199412013312202\n10.1016/j.ultrasmedbio.2014.12.024\n10.1227/01.NEU.0000239895.00373.E4\n10.1681/ASN.V92231\n10.1165/rcmb.2007-0441OC\n10.1097/MCA.0000000000000934\n10.3390/biology11020221\n10.2214/AJR.11.6955\n10.1097/MCA.0000000000000914\n10.1097/CCM.0000000000004899\n10.1002/14651858.CD003186.pub3\n10.3390/diagnostics12010166\n10.1016/j.bspc.2016.03.001\n10.1016/j.compbiomed.2021.105204\n10.1109/TIM.2022.3174270\n10.1016/S0140-6736(96)07492-2\n10.1016/j.compbiomed.2022.105571\n10.3390/app10238623\n10.1016/j.clim.2020.108509\n10.1016/j.jpeds.2020.07.039\n10.1007/BF01907940\n10.1200/GO.20.00064\n10.1016/j.irbm.2020.05.003\n10.2214/AJR.20.23034\n10.1007/s11604-021-01120-w\n10.1148/radiol.2020200343\n10.1016/j.asoc.2020.106912\n10.1109/TIP.2021.3058783\n10.1093/eurheartj/ehaa399"}
{"title": "Artificial intelligence model on chest imaging to diagnose COVID-19 and other pneumonias: A systematic review and meta-analysis.", "abstract": "When diagnosing Coronavirus disease 2019(COVID-19), radiologists cannot make an accurate judgments because the image characteristics of COVID-19 and other pneumonia are similar. As machine learning advances, artificial intelligence(AI) models show promise in diagnosing COVID-19 and other pneumonias. We performed a systematic review and meta-analysis to assess the diagnostic accuracy and methodological quality of the models.\nWe searched PubMed, Cochrane Library, Web of Science, and Embase, preprints from medRxiv and bioRxiv to locate studies published before December 2021, with no language restrictions. And a quality assessment (QUADAS-2), Radiomics Quality Score (RQS) tools and CLAIM checklist were used to assess the quality of each study. We used random-effects models to calculate pooled sensitivity and specificity, I\nWe screened 32 studies from the 2001 retrieved articles for inclusion in the meta-analysis. We included 6737 participants in the test or validation group. The meta-analysis revealed that AI models based on chest imaging distinguishes COVID-19 from other pneumonias: pooled area under the curve (AUC) 0.96 (95 % CI, 0.94-0.98), sensitivity 0.92 (95 % CI, 0.88-0.94), pooled specificity 0.91 (95 % CI, 0.87-0.93). The average RQS score of 13 studies using radiomics was 7.8, accounting for 22 % of the total score. The 19 studies using deep learning methods had an average CLAIM score of 20, slightly less than half (48.24 %) the ideal score of 42.00.\nThe AI model for chest imaging could well diagnose COVID-19 and other pneumonias. However, it has not been implemented as a clinical decision-making tool. Future researchers should pay more attention to the quality of research methodology and further improve the generalizability of the developed predictive models.", "journal": "European journal of radiology open", "date": "2022-08-24", "authors": ["Lu-LuJia", "Jian-XinZhao", "Ni-NiPan", "Liu-YanShi", "Lian-PingZhao", "Jin-HuiTian", "GangHuang"], "doi": "10.1016/j.ejro.2022.100438"}
{"title": "A dual-stage deep convolutional neural network for automatic diagnosis of COVID-19 and pneumonia from chest CT images.", "abstract": "In the Coronavirus disease-2019 (COVID-19) pandemic, for fast and accurate diagnosis of a large number of patients, besides traditional methods, automated diagnostic tools are now extremely required. In this paper, a deep convolutional neural network (CNN) based scheme is proposed for automated accurate diagnosis of COVID-19 from lung computed tomography (CT) scan images. First, for the automated segmentation of lung regions in a chest CT scan, a modified CNN architecture, namely SKICU-Net is proposed by incorporating additional skip interconnections in the U-Net model that overcome the loss of information in dimension scaling. Next, an agglomerative hierarchical clustering is deployed to eliminate the CT slices without significant information. Finally, for effective feature extraction and diagnosis of COVID-19 and pneumonia from the segmented lung slices, a modified DenseNet architecture, namely P-DenseCOVNet is designed where parallel convolutional paths are introduced on top of the conventional DenseNet model for getting better performance through overcoming the loss of positional arguments. Outstanding performances have been achieved with an F", "journal": "Computers in biology and medicine", "date": "2022-08-23", "authors": ["FarhanSadik", "Ankan GhoshDastider", "Mohseu RashidSubah", "TanvirMahmud", "Shaikh AnowarulFattah"], "doi": "10.1016/j.compbiomed.2022.105806\n10.1101/2020.02.14.20023028"}
{"title": "New International Guidelines and Consensus on the Use of Lung Ultrasound.", "abstract": "Following the innovations and new discoveries of the last 10\u2009years in the field of lung ultrasound (LUS), a multidisciplinary panel of international LUS experts from six countries and from different fields (clinical and technical) reviewed and updated the original international consensus for point-of-care LUS, dated 2012. As a result, a total of 20 statements have been produced. Each statement is complemented by guidelines and future developments proposals. The statements are furthermore classified based on their nature as technical (5), clinical (11), educational (3), and safety (1) statements.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2022-08-23", "authors": ["LibertarioDemi", "FrankWolfram", "CatherineKlersy", "AnnalisaDe Silvestri", "Virginia ValeriaFerretti", "MarieMuller", "DouglasMiller", "FrancescoFeletti", "MarcinWe\u0142nicki", "NataliaBuda", "AgnieszkaSkoczylas", "AndrzejPomiecko", "DomagojDamjanovic", "RobertOlszewski", "Andrew WKirkpatrick", "RaoulBreitkreutz", "GebhartMathis", "GinoSoldati", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone"], "doi": "10.1002/jum.16088"}
{"title": "Study on the correlations of different clinical types with imaging findings at initial diagnosis and clinical laboratory indexes in COVID-19 patients.", "abstract": "To investigate the correlations of initial lab and imaging findings in COVID-19 patients of different clinical types.\nWe retrospective analyzed patients confirmed with COVID-19 in the Fifth Medical Center of the People's Liberation Army (PLA) General Hospital between February to April 2020, selected a total of 58 (N) patients with lab and imaging examinations that met the study criteria, using Artificial intelligence (AI) software to calculate the percentage of COVID-19 lesions in the volume of the whole lung, then the correlations of general information, initial chest CT examination after admission and laboratory examinations were analyzed.\nThe 58 (N) COVID-19 patients were divided into mild group [41(n) cases]: and severe group [17(n) cases]: according to patient's condition. CT findings of the severe group and mild group mainly included single or multiple ground glass opacity (GGO), with lesions mainly distributed in the periphery of lungs or GGO mixed with consolidation, with lesions involved in peripheral and central areas of both lungs, accompanied other signs. A significant difference in CRP, IL-6, D-D, GGT was observed between the two groups (p < 0.05). The ratios regarding lymphocyte abnormality and neutrophil abnormality in the severe group were higher than those in the mild group (p < 0.05).\nThe CT features at initial diagnosis of COVID-19 were mainly characterized by multiple GGO with or without partial consolidation in both lungs, with the lesions mainly distributed at the subpleural regions. Some lab test indexes were correlated with the clinical types of COVID-19.", "journal": "Pakistan journal of medical sciences", "date": "2022-08-23", "authors": ["HongweiRen", "XiaoboZhang", "YuTang", "TaoYan", "YuanLiu"], "doi": "10.12669/pjms.38.6.5091"}
{"title": "Transforming healthcare through a digital revolution: A review of digital healthcare technologies and solutions.", "abstract": "The COVID-19 pandemic has put a strain on the entire global healthcare infrastructure. The pandemic has necessitated the re-invention, re-organization, and transformation of the healthcare system. The resurgence of new COVID-19 virus variants in several countries and the infection of a larger group of communities necessitate a rapid strategic shift. Governments, non-profit, and other healthcare organizations have all proposed various digital solutions. It's not clear whether these digital solutions are adaptable, functional, effective, or reliable. With the disease becoming more and more prevalent, many countries are looking for assistance and implementation of digital technologies to combat COVID-19. Digital health technologies for COVID-19 pandemic management, surveillance, contact tracing, diagnosis, treatment, and prevention will be discussed in this paper to ensure that healthcare is delivered effectively. Artificial Intelligence (AI), big data, telemedicine, robotic solutions, Internet of Things (IoT), digital platforms for communication (DC), computer vision, computer audition (CA), digital data management solutions (blockchain), digital imaging are premiering to assist healthcare workers (HCW's) with solutions that include case base surveillance, information dissemination, disinfection, and remote consultations, along with many other such interventions.", "journal": "Frontiers in digital health", "date": "2022-08-23", "authors": ["NitheshNaik", "B M ZeeshanHameed", "NilakshmanSooriyaperakasam", "ShankeethVinayahalingam", "VathsalaPatil", "KomalSmriti", "JanhaviSaxena", "MilapShah", "SufyanIbrahim", "AnshumanSingh", "HadisKarimi", "KarthickeyanNaganathan", "Dasharathraj KShetty", "Bhavan PrasadRai", "PiotrChlosta", "Bhaskar KSomani"], "doi": "10.3389/fdgth.2022.919985\n10.1016/S2213-2600(20)30079-5\n10.1016/j.jcrc.2020.04.012\n10.1016/j.tacc.2020.05.002\n10.1371/journal.pone.0116949\n10.1016/S2214-109X(20)30366-1\n10.1016/j.worlddev.2020.105318\n10.1002/wps.20764\n10.3389/fpsyt.2020.00790\n10.7189/jogh.10.05002\n10.1016/j.ijsu.2020.04.018\n10.2471/BLT.20.021120\n10.1038/s41746-021-00412-9\n10.1016/j.ihj.2020.04.001\n10.7554/eLife.57309\n10.4178/epih.e2020027\n10.1016/S0140-6736(20)32651-9\n10.7326/M20-3012\n10.1101/2020.02.29.20029421v1.full.pdf\n10.2139/ssrn.3568314\n10.3389/frai.2020.00065\n10.1016/j.aiopen.2020.07.001\n10.1016/S2589-7500(20)30192-8\n10.7326/M20-1495\n10.1186/s12967-020-02324-w\n10.3389/fbloc.2021.830459\n10.1155/2022/7786441\n10.1109/ACCESS.2020.3032450\n10.1016/S2589-7500(21)00210-7\n10.1002/ett.4255\n10.1002/hbe2.226\n10.1007/s13312-020-1840-8"}
{"title": "Multicenter Study on COVID-19 Lung Computed Tomography Segmentation with varying Glass Ground Opacities using Unseen Deep Learning Artificial Intelligence Paradigms: COVLIAS 1.0 Validation.", "abstract": "Variations in COVID-19 lesions such as glass ground opacities (GGO), consolidations, and crazy paving can compromise the ability of solo-deep learning (SDL) or hybrid-deep learning (HDL) artificial intelligence (AI) models in predicting automated COVID-19 lung segmentation in Computed Tomography (CT) from unseen data leading to poor clinical manifestations. As the first study of its kind, \"COVLIAS 1.0-Unseen\" proves two hypotheses, (i) contrast adjustment is vital for AI, and (ii) HDL is superior to SDL. In a multicenter study, 10,000 CT slices were collected from 72 Italian (ITA) patients with low-GGO, and 80 Croatian (CRO) patients with high-GGO. Hounsfield Units (HU) were automatically adjusted to train the AI models and predict from test data, leading to four combinations-two Unseen sets: (i) train-CRO:test-ITA, (ii) train-ITA:test-CRO, and two Seen sets: (iii) train-CRO:test-CRO, (iv) train-ITA:test-ITA. COVILAS used three SDL models: PSPNet, SegNet, UNet and six HDL models: VGG-PSPNet, VGG-SegNet, VGG-UNet, ResNet-PSPNet, ResNet-SegNet, and ResNet-UNet. Two trained, blinded senior radiologists conducted ground truth annotations. Five types of performance metrics were used to validate COVLIAS 1.0-Unseen which was further benchmarked against MedSeg, an open-source web-based system. After HU adjustment for DS and JI, HDL (Unseen AI)\u2009>\u2009SDL (Unseen AI) by 4% and 5%, respectively. For CC, HDL (Unseen AI)\u2009>\u2009SDL (Unseen AI) by 6%. The COVLIAS-MedSeg difference was\u2009<\u20095%, meeting regulatory guidelines.Unseen AI was successfully demonstrated using automated HU adjustment. HDL was found to be superior to SDL.", "journal": "Journal of medical systems", "date": "2022-08-22", "authors": ["Jasjit SSuri", "SushantAgarwal", "LucaSaba", "Gian LucaChabert", "AlessandroCarriero", "AlessioPasch\u00e8", "PietroDanna", "ArminMehmedovi\u0107", "GavinoFaa", "TanayJujaray", "Inder MSingh", "Narendra NKhanna", "John RLaird", "Petros PSfikakis", "VikasAgarwal", "Jagjit STeji", "RajanikantR Yadav", "FerencNagy", "Zsigmond Tam\u00e1sKincses", "ZoltanRuzsa", "KlaudijaViskovic", "Mannudeep KKalra"], "doi": "10.1007/s10916-022-01850-y\n10.23750/abm.v91i1.9397\n10.26355/eurrev_202012_24058\n10.1016/j.compbiomed.2020.103960\n10.1007/s10554-020-02089-9\n10.4239/wjd.v12.i3.215\n10.26355/eurrev_202108_26464\n10.1016/j.clinimag.2021.05.016\n10.23750/abm.v92i5.10418\n10.52586/5026\n10.1148/radiol.2020200432\n10.1016/j.ejrad.2020.109041\n10.1007/s00330-020-06920-8\n10.21037/atm-2020-cass-13\n10.1016/j.neurad.2017.09.007\n10.21037/atm-20-7676\n10.5152/dir.2020.20304\n10.1007/s00330-020-06915-5\n10.1007/s13244-010-0060-5\n10.1080/07853890.2020.1851044\n10.2214/AJR.20.23034\n10.1148/radiol.2020200343\n10.1007/s11548-020-02286-w\n10.1007/s11548-021-02317-0\n10.1007/s10916-021-01707-w\n10.3390/diagnostics12010166\n10.1109/JBHI.2021.3103839\n10.3390/diagnostics11122257\n10.1016/j.wneu.2016.05.069\n10.1016/j.ejmp.2017.11.036\n10.1016/j.compbiomed.2021.104721\n10.1016/j.compbiomed.2021.104803\n10.23736/S0392-9590.21.04771-4\n10.1016/j.compbiomed.2021.105131\n10.1109/TMI.2020.3002417\n10.11613/BM.2015.015\n10.1080/10408340500526766\n10.1177/875647939000600106\n10.2741/4725\n10.1016/j.ejrad.2019.02.038\n10.1007/s10916-018-0940-7\n10.1016/j.cmpb.2017.09.004\n10.1016/j.cmpb.2019.04.008\n10.1007/s10916-010-9645-2\n10.1016/j.cmpb.2012.09.008\n10.1007/s11517-012-1019-0\n10.1177/0954411913483637\n10.1016/j.cmpb.2017.12.016\n10.1016/j.compbiomed.2017.10.019\n10.7785/tcrt.2012.500346\n10.1016/j.compbiomed.2015.07.021\n10.1016/j.cmpb.2017.07.011\n10.1007/s11517-021-02322-0\n10.1007/s00330-020-06829-2\n10.1109/TNNLS.2021.3054746\n10.1109/TMI.2019.2959609\n10.1186/s12880-020-00529-5\n10.1016/j.acra.2020.09.004\n10.1109/TPAMI.2016.2644615\n10.1006/jmps.1999.1279\n10.1007/978-0-387-39940-9_565\n10.1109/TIM.2011.2174897\n10.1007/s10916-015-0214-6\n10.1016/j.cmpb.2016.02.004\n10.1007/s10916-017-0797-1\n10.1016/j.eswa.2015.03.014\n10.1055/s-0032-1330336\n10.1016/j.bspc.2016.03.001\n10.1109/4233.992158\n10.31083/j.rcm.2020.04.236"}
{"title": "Psoas muscle metastatic disease mimicking a psoas abscess on imaging.", "abstract": "Here, we report a case of malignant psoas syndrome presented to us during the second peak of the COVID-19 pandemic. Our patient had a medical history of hypertension, recently diagnosed with left iliac deep vein thrombosis and previous breast and endometrial cancers. She presented with exquisite pain and a fixed flexion deformity of the left hip. A rim-enhancing lesion was seen within the left psoas muscle and was initially deemed to be a psoas abscess. This failed to respond to medical management and attempts at drainage. Subsequent further imaging revealed the mass was of a malignant nature; histology revealing a probable carcinomatous origin. Following diagnosis, palliative input was obtained and, unfortunately, our patient passed away in a hospice shortly after discharge. We discuss the aetiology, radiological findings and potential treatments of this condition and learning points to prompt clinicians to consider this diagnosis in those with a personal history of cancer.", "journal": "BMJ case reports", "date": "2022-08-20", "authors": ["ChristopherGunn", "MazyarFani"], "doi": "10.1136/bcr-2022-250654\n10.1186/1470-7330-14-21\n10.1007/s00330-009-1577-1\n10.1007/s12094-011-0625-x\n10.3892/mco.2018.1635\n10.1016/j.jpainsymman.2003.12.018\n10.1089/jpm.2014.0387\n10.4103/IJPC.IJPC_205_19\n10.1080/15360288.2017.1301617\n10.1016/S0304-3959(99)00039-1\n10.1111/papr.12643\n10.1159/000360581\n10.2214/ajr.174.2.1740401\n10.1111/j.1440-1673.1990.tb02831.x\n10.11604/pamj.2020.36.231.21137\n10.1016/j.gore.2021.100814\n10.1016/j.spinee.2015.08.001\n10.1136/bcr-2017-223916\n10.1102/1470-7330.2013.0011\n10.1159/000227588\n10.1016/j.ygyno.2006.02.011\n10.3233/BD-140384\n10.1007/s002560050141"}
{"title": "Rapid tissue prototyping with micro-organospheres.", "abstract": "In\u00a0vitro tissue models hold great promise for modeling diseases and drug responses. Here, we used emulsion microfluidics to form micro-organospheres (MOSs), which are droplet-encapsulated miniature three-dimensional (3D) tissue models that can be established rapidly from patient tissues or cells. MOSs retain key biological features and responses to chemo-, targeted, and radiation therapies compared with organoids. The small size and large surface-to-volume ratio of MOSs enable various applications including quantitative assessment of nutrient dependence, pathogen-host interaction for anti-viral drug screening, and a rapid potency assay for chimeric antigen receptor (CAR)-T therapy. An automated MOS imaging pipeline combined with machine learning overcomes plating variation, distinguishes tumorspheres from stroma, differentiates cytostatic versus cytotoxic drug effects, and captures resistant clones and heterogeneity in drug response. This pipeline is capable of robust assessments of drug response at individual-tumorsphere resolution and provides a rapid and high-throughput therapeutic profiling platform for precision medicine.", "journal": "Stem cell reports", "date": "2022-08-20", "authors": ["ZhaohuiWang", "MatteoBoretto", "RosemaryMillen", "NaveenNatesh", "Elena SReckzeh", "CarolynHsu", "MarcosNegrete", "HaipeiYao", "WilliamQuayle", "Brook EHeaton", "Alfred THarding", "ShreeBose", "ElseDriehuis", "JoepBeumer", "Grecia ORivera", "Ravian Lvan Ineveld", "DonaldGex", "JessicaDeVilla", "DaisongWang", "JensPuschhof", "Maarten HGeurts", "AthenaYeung", "CaitHamele", "AmberSmith", "EricBankaitis", "KunXiang", "ShengliDing", "DanielNelson", "DanielDelubac", "AnneRios", "RalphAbi-Hachem", "DavidJang", "Bradley JGoldstein", "CarolynGlass", "Nicholas SHeaton", "DavidHsu", "HansClevers", "XilingShen"], "doi": "10.1016/j.stemcr.2022.07.016\n10.1016/j.copbio.2015.05.003\n10.1038/s41556-020-0472-5\n10.1038/s41556-018-0143-y\n10.1038/s41556-019-0360-z\n10.1016/j.medj.2021.08.005\n10.1038/s41551-020-0565-2\n10.1038/s41596-019-0232-9\n10.1038/nm.3388\n10.1016/j.cell.2018.07.009\n10.1016/j.stem.2022.04.006\n10.1158/2159-8290.CD-18-1522\n10.3390/jcm8111880\n10.1038/nature14415\n10.1002/advs.202102418\n10.1038/s41591-019-0584-2\n10.1038/ng.3127\n10.1038/s41586-020-2575-3\n10.1016/j.jtbi.2011.03.026\n10.1016/j.xcrm.2020.100161\n10.1063/1.4995479\n10.1158/1078-0432.CCR-20-5026\n10.48550/arXiv.1912.08193\n10.1016/j.ccell.2021.12.004\n10.1002/advs.201903739\n10.1038/nprot.2013.046\n10.1158/2326-6066.CIR-18-0428\n10.1158/1078-0432.CCR-20-0073\n10.1016/j.cell.2018.11.021\n10.1126/scitranslmed.aay2574\n10.1016/j.cell.2017.11.010\n10.15252/embj.2018100300\n10.1053/j.gastro.2011.07.050\n10.1016/j.stemcr.2021.04.009\n10.15252/embj.2018100928\n10.1016/j.isci.2020.101372\n10.1158/2159-8290.CD-18-0349\n10.1016/j.celrep.2020.107670\n10.1016/j.cell.2015.03.053\n10.1126/science.aao2774\n10.1039/d0bm01085e\n10.1016/j.stem.2019.10.010"}
{"title": "DAFLNet: Dual Asymmetric Feature Learning Network for COVID-19 Disease Diagnosis in X-Rays.", "abstract": "COVID-19 has become the largest public health event worldwide since its outbreak, and early detection is a prerequisite for effective treatment. Chest X-ray images have become an important basis for screening and monitoring the disease, and deep learning has shown great potential for this task. Many studies have proposed deep learning methods for automated diagnosis of COVID-19. Although these methods have achieved excellent performance in terms of detection, most have been evaluated using limited datasets and typically use a single deep learning network to extract features. To this end, the dual asymmetric feature learning network (DAFLNet) is proposed, which is divided into two modules, DAFFM and WDFM. DAFFM mainly comprises the backbone networks EfficientNetV2 and DenseNet for feature fusion. WDFM is mainly for weighted decision-level fusion and features a new pretrained network selection algorithm (PNSA) for determination of the optimal weights. Experiments on a large dataset were conducted using two schemes, DAFLNet-1 and DAFLNet-2, and both schemes outperformed eight state-of-the-art classification techniques in terms of classification performance. DAFLNet-1 achieved an average accuracy of up to 98.56% for the triple classification of COVID-19, pneumonia, and healthy images.", "journal": "Computational and mathematical methods in medicine", "date": "2022-08-20", "authors": ["JingyaoLiu", "JiashiZhao", "LiyuanZhang", "YuMiao", "WeiHe", "WeiliShi", "YanfangLi", "BaiJi", "KeZhang", "ZhengangJiang"], "doi": "10.1155/2022/3836498\n10.1016/j.cmpb.2020.105532\n10.1148/radiol.2020200642\n10.1016/j.inffus.2020.10.004\n10.22266/ijies2016.1231.24\n10.3390/diagnostics10060358\n10.1016/j.chaos.2020.110190\n10.1016/j.crad.2020.03.004\n10.1016/j.imu.2020.100360\n10.1016/j.compeleceng.2020.106765\n10.1007/s12559-020-09776-8\n10.1016/j.ins.2020.09.041\n10.1016/j.compbiomed.2020.103792\n10.1016/j.bspc.2019.04.031\n10.1016/j.inffus.2020.11.005\n10.1016/j.compbiomed.2020.103805\n10.1016/j.bspc.2020.102257\n10.1016/j.bspc.2022.103677\n10.1007/978-3-030-01234-2_1\n10.1186/s13040-021-00244-z\n10.1016/j.eswa.2022.116540\n10.1007/s11548-020-02283-z\n10.2196/19569\n10.1007/978-3-319-46493-0_38"}
{"title": "MFL-Net: An Efficient Lightweight Multi-Scale Feature Learning CNN for COVID-19 Diagnosis From CT Images.", "abstract": "Timely and accurate diagnosis of coronavirus disease 2019 (COVID-19) is crucial in curbing its spread. Slow testing results of reverse transcription-polymerase chain reaction (RT-PCR) and a shortage of test kits have led to consider chest computed tomography (CT) as an alternative screening and diagnostic tool. Many deep learning methods, especially convolutional neural networks (CNNs), have been developed to detect COVID-19 cases from chest CT scans. Most of these models demand a vast number of parameters which often suffer from overfitting in the presence of limited training data. Moreover, the linearly stacked single-branched architecture based models hamper the extraction of multi-scale features, reducing the detection performance. In this paper, to handle these issues, we propose an extremely lightweight CNN with multi-scale feature learning blocks called as MFL-Net. The MFL-Net comprises a sequence of MFL blocks that combines multiple convolutional layers with 3 \u00d73 filters and residual connections effectively, thereby extracting multi-scale features at different levels and preserving them throughout the block. The model has only 0.78M parameters and requires low computational cost and memory space compared to many ImageNet pretrained CNN architectures. Comprehensive experiments are carried out using two publicly available COVID-19 CT imaging datasets. The results demonstrate that the proposed model achieves higher performance than pretrained CNN models and state-of-the-art methods on both datasets with limited training data despite having an extremely lightweight architecture. The proposed method proves to be an effective aid for the healthcare system in the accurate and timely diagnosis of COVID-19.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-08-19", "authors": ["Amogh ManojJoshi", "Deepak RanjanNayak"], "doi": "10.1109/JBHI.2022.3196489"}
{"title": "Semi-supervised COVID-19 CT image segmentation using deep generative models.", "abstract": "A recurring problem in image segmentation is a lack of labelled data. This problem is especially acute in the segmentation of lung computed tomography (CT) of patients with Coronavirus Disease 2019 (COVID-19). The reason for this is simple: the disease has not been prevalent long enough to generate a great number of labels. Semi-supervised learning promises a way to learn from data that is unlabelled and has seen tremendous advancements in recent years. However, due to the complexity of its label space, those advancements cannot be applied to image segmentation. That being said, it is this same complexity that makes it extremely expensive to obtain pixel-level labels, making semi-supervised learning all the more appealing. This study seeks to bridge this gap by proposing a novel model that utilizes the image segmentation abilities of deep convolution networks and the semi-supervised learning abilities of generative models for chest CT images of patients with the COVID-19.\nWe propose a novel generative model called the shared variational autoencoder (SVAE). The SVAE utilizes a five-layer deep hierarchy of latent variables and deep convolutional mappings between them, resulting in a generative model that is well suited for lung CT images. Then, we add a novel component to the final layer of the SVAE which forces the model to reconstruct the input image using a segmentation that must match the ground truth segmentation whenever it is present. We name this final model StitchNet.\nWe compare StitchNet to other image segmentation models on a high-quality dataset of CT images from COVID-19 patients. We show that our model has comparable performance to the other segmentation models. We also explore the potential limitations and advantages in our proposed algorithm and propose some potential future research directions for this challenging issue.", "journal": "BMC bioinformatics", "date": "2022-08-17", "authors": ["JudahZammit", "Daryl L XFung", "QianLiu", "Carson Kai-SangLeung", "PingzhaoHu"], "doi": "10.1186/s12859-022-04878-6\n10.1016/j.cell.2020.04.045\n10.1109/TPAMI.2016.2644615\n10.1109/TPAMI.2019.2960224\n10.1016/j.patcog.2020.107269\n10.1186/s12967-021-02992-2"}
{"title": "A Novel Convolutional Neural Network Model as an Alternative Approach to Bowel Preparation Evaluation Before Colonoscopy in the COVID-19 Era: A Multicenter, Single-Blinded, Randomized Study.", "abstract": "Adequate bowel preparation is key to a successful colonoscopy, which is necessary for detecting adenomas and preventing colorectal cancer. We developed an artificial intelligence (AI) platform using a convolutional neural network (CNN) model (AI-CNN model) to evaluate the quality of bowel preparation before colonoscopy.\nThis was a colonoscopist-blinded, randomized study. Enrolled patients were randomized into an experimental group, in which our AI-CNN model was used to evaluate the quality of bowel preparation (AI-CNN group), or a control group, which performed self-evaluation per routine practice (control group). The primary outcome was the consistency (homogeneity) between the results of the 2 methods. The secondary outcomes included the quality of bowel preparation according to the Boston Bowel Preparation Scale (BBPS), polyp detection rate, and adenoma detection rate.\nA total of 1,434 patients were enrolled (AI-CNN, n = 730; control, n = 704). No significant difference was observed between the evaluation results (\"pass\" or \"not pass\") of the groups in the adequacy of bowel preparation as represented by BBPS scores. The mean BBPS scores, polyp detection rate, and adenoma detection rate were similar between the groups. These results indicated that the AI-CNN model and routine practice were generally consistent in the evaluation of bowel preparation quality. However, the mean BBPS score of patients with \"pass\" results were significantly higher in the AI-CNN group than in the control group, indicating that the AI-CNN model may further improve the quality of bowel preparation in patients exhibiting adequate bowel preparation.\nThe novel AI-CNN model, which demonstrated comparable outcomes to the routine practice, may serve as an alternative approach for evaluating bowel preparation quality before colonoscopy.", "journal": "The American journal of gastroenterology", "date": "2022-08-17", "authors": ["Yang-BorLu", "Si-CunLu", "Yung-NingHuang", "Shun-TianCai", "Puo-HsienLe", "Fang-YuHsu", "Yan-XingHu", "Hui-ShanHsieh", "Wei-TingChen", "Gui-LiXia", "Hong-ZhiXu", "WeiGong"], "doi": "10.14309/ajg.0000000000001900"}
{"title": "A regularization-driven Mean Teacher model based on semi-supervised learning for medical image segmentation.", "abstract": "", "journal": "Physics in medicine and biology", "date": "2022-08-16", "authors": ["QingWang", "XiangLi", "MingzhiChen", "LingnaChen", "JunxiChen"], "doi": "10.1088/1361-6560/ac89c8"}
{"title": "CovMnet-Deep Learning Model for classifying Coronavirus (COVID-19).", "abstract": "Diagnosing COVID-19, current pandemic disease using Chest X-ray images is widely used to evaluate the lung disorders. As the spread of the disease is enormous many medical camps are being conducted to screen the patients and Chest X-ray is a simple imaging modality to detect presence of lung disorders. Manual lung disorder detection using Chest X-ray by radiologist is a tedious process and may lead to inter and intra-rate errors. Various deep convolution neural network techniques were tested for detecting COVID-19 abnormalities in lungs using Chest X-ray images. This paper proposes deep learning model to classify COVID-19 and normal chest X-ray images. Experiments are carried out for deep feature extraction, fine-tuning of convolutional neural networks (CNN) hyper parameters, and end-to-end training of four variants of the CNN model. The proposed CovMnet provide better classification accuracy of 97.4% for COVID-19 /normal than those reported in the previous studies. The proposed CovMnet model has potential to aid radiologist to monitor COVID-19 disease and proves to be an efficient non-invasive COVID-19 diagnostic tool for lung disorders.", "journal": "Health and technology", "date": "2022-08-16", "authors": ["MalathyJawahar", "Jani AnbarasiL", "VinayakumarRavi", "JPrassanna", "S GracelineJasmine", "RManikandan", "RamesSekaran", "SuthendranKannan"], "doi": "10.1007/s12553-022-00688-1\n10.1016/j.chemolab.2020.104054\n10.1007/s10044-021-00984-y\n10.1007/s13246-020-00865-4\n10.1016/j.ins.2020.09.041\n10.1016/j.chaos.2020.109949\n10.1016/j.chaos.2020.110242\n10.4018/IJSSCI.2020070102\n10.4249/scholarpedia.1717\n10.1113/jphysiol.1970.sp009022\n10.1007/BF00344251\n10.1007/s00521-018-3761-1\n10.1007/s10096-020-03901-z"}
{"title": "Multiclass Classification for Detection of COVID-19 Infection in Chest X-Rays Using CNN.", "abstract": "Coronavirus took the world by surprise and caused a lot of trouble in all the important fields in life. The complexity of dealing with coronavirus lies in the fact that it is highly infectious and is a novel virus which is hard to detect with exact precision. The typical detection method for COVID-19 infection is the RT-PCR but it is a rather expensive method which is also invasive and has a high margin of error. Radiographies are a good alternative for COVID-19 detection given the experience of the radiologist and his learning capabilities. To make an accurate detection from chest X-Rays, deep learning technologies can be involved to analyze the radiographs, learn distinctive patterns of coronavirus' presence, find these patterns in the tested radiograph, and determine whether the sample is actually COVID-19 positive or negative. In this study, we propose a model based on deep learning technology using Convolutional Neural Networks and training it on a dataset containing a total of over 35,000 chest X-Ray images, nearly 16,000 for COVID-19 positive images, 15,000 for normal images, and 5,000 for pneumonia-positive images. The model's performance was assessed in terms of accuracy, precision, recall, and ", "journal": "Computational intelligence and neuroscience", "date": "2022-08-16", "authors": ["Rawan SaqerAlharbi", "Hadeel AysanAlsaadi", "SManimurugan", "TAnitha", "MiniluDejene"], "doi": "10.1155/2022/3289809\n10.1016/j.chaos.2020.110495\n10.1016/j.bspc.2022.103561\n10.1007/s13755-020-00135-3\n10.3390/ijerph18063056\n10.1007/978-3-642-15825-4_10\n10.3390/s2203121\n10.1007/s10489-020-01978-9\n10.1016/j.neucom.2021.03.034\n10.1016/j.bspc.2021.102920"}
{"title": "DCNN-FuzzyWOA: Artificial Intelligence Solution for Automatic Detection of COVID-19 Using X-Ray Images.", "abstract": "Artificial intelligence (AI) techniques have been considered effective technologies in diagnosing and breaking the transmission chain of COVID-19 disease. Recent research uses the deep convolution neural network (DCNN) as the discoverer or classifier of COVID-19 X-ray images. The most challenging part of neural networks is the subject of their training. Descent-based (GDB) algorithms have long been used to train fullymconnected layer (FCL) at DCNN. Despite the ability of GDBs to run and converge quickly in some applications, their disadvantage is the manual adjustment of many parameters. Therefore, it is not easy to parallelize them with graphics processing units (GPUs). Therefore, in this paper, the whale optimization algorithm (WOA) evolved by a fuzzy system called FuzzyWOA is proposed for DCNN training. With accurate and appropriate tuning of WOA's control parameters, the fuzzy system defines the boundary between the exploration and extraction phases in the search space. It causes the development and upgrade of WOA. To evaluate the performance and capability of the proposed DCNN-FuzzyWOA model, a publicly available database called COVID-Xray-5k is used. DCNN-PSO, DCNN-GA, and LeNet-5 benchmark models are used for fair comparisons. Comparative parameters include accuracy, processing time, standard deviation (STD), curves of ROC and precision-recall, and F1-Score. The results showed that the FuzzyWOA training algorithm with 20 epochs was able to achieve 100% accuracy, at a processing time of 880.44\u2009s with an F1-Score equal to 100%. Structurally, the i-6c-2s-12c-2s model achieved better results than the i-8c-2s-16c-2s model. However, the results of using FuzzyWOA for both models have been very encouraging compared to particle swarm optimization, genetic algorithm, and LeNet-5 methods.", "journal": "Computational intelligence and neuroscience", "date": "2022-08-16", "authors": ["AbbasSaffari", "MohammadKhishe", "MokhtarMohammadi", "AdilHussein Mohammed", "ShimaRashidi"], "doi": "10.1155/2022/5677961\n10.1016/j.imu.2020.100427\n10.1148/radiol.2020200642\n10.1155/2020/8889023\n10.1016/j.dsx.2020.04.012\n10.1109/ACCESS.2020.2989273\n10.1016/j.scs.2020.102589\n10.1016/j.eswa.2020.113338\n10.1007/s40430-017-0927-1\n10.1080/0952813x.2021.1960639\n10.1007/978-3-319-25751-8\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1145/3243316\n10.1109/TCYB.2020.2983860\n10.24425/aoa.2020.135281\n10.1016/j.oceaneng.2019.04.013\n10.24425/aoa.2019.126360\n10.1007/s10470-018-1366-3\n10.1007/s11277-019-06520-w\n10.1007/s10470-022-02014-1\n10.1016/j.advengsoft.2016.01.008\n10.21203/rs.3.rs-122787/v1\n10.1155/2020/8856801\n10.1007/s10462-020-09825-6\n10.1016/j.media.2020.101794\n10.1609/aaai.v33i01.3301590\n10.1109/oceans.2018.8604847\n10.1109/CEC48606.2020.9185541"}
{"title": "Innovations in thoracic imaging: CT, radiomics, AI and x-ray velocimetry.", "abstract": "In recent years, pulmonary imaging has seen enormous progress, with the introduction, validation and implementation of new hardware and software. There is a general trend from mere visual evaluation of radiological images to quantification of abnormalities and biomarkers, and assessment of 'non visual' markers that contribute to establishing diagnosis or prognosis. Important catalysts to these developments in thoracic imaging include new indications (like computed tomography [CT] lung cancer screening) and the COVID-19 pandemic. This review focuses on developments in CT, radiomics, artificial intelligence (AI) and x-ray velocimetry for imaging of the lungs. Recent developments in CT include the potential for ultra-low-dose CT imaging for lung nodules, and the advent of a new generation of CT systems based on photon-counting detector technology. Radiomics has demonstrated potential towards predictive and prognostic tasks particularly in lung cancer, previously not achievable by visual inspection by radiologists, exploiting high dimensional patterns (mostly texture related) on medical imaging data. Deep learning technology has revolutionized the field of AI and as a result, performance of AI algorithms is approaching human performance for an increasing number of specific tasks. X-ray velocimetry integrates x-ray (fluoroscopic) imaging with unique image processing to produce quantitative four dimensional measurement of lung tissue motion, and accurate calculations of lung ventilation.", "journal": "Respirology (Carlton, Vic.)", "date": "2022-08-16", "authors": ["RozemarijnVliegenthart", "AndreasFouras", "ColinJacobs", "NickolasPapanikolaou"], "doi": "10.1111/resp.14344\n10.1097/RLI.0000000000000822\n10.1148/radiol.210551\n10.1007/s00247-021-05146-0\n10.1109/CVPR.2017.369"}
{"title": "Classification of lungs infected COVID-19 images based on inception-ResNet.", "abstract": "Nowadays, COVID-19 is spreading rapidly worldwide, and seriously threatening lives . From the perspective of security and economy, the effective control of COVID-19 has a profound impact on the entire society. An effective strategy is to diagnose earlier to prevent the spread of the disease and prompt treatment of severe cases to improve the chance of survival.\nThe method of this paper is as follows: Firstly, the collected data set is processed by chest film image processing, and the bone removal process is carried out in the rib subtraction module. Then, the set preprocessing method performed histogram equalization, sharpening, and other preprocessing operations on the chest film. Finally, shallow and high-level feature mapping through the backbone network extracts the processed chest radiographs. We implement the self-attention mechanism in Inception-Resnet, perform the standard classification, and identify chest radiograph diseases through the classifier to realize the auxiliary COVID-19 diagnosis process at the medical level, all in an effort to further enhance the classification performance of the convolutional neural network. Numerous computer simulations demonstrate that the Inception-Resnet convolutional neural network performs CT image categorization and enhancement with greater efficiency and flexibility than conventional segmentation techniques.\nThe experimental COVID-19 CT dataset obtained in this paper is the new data for CT scans and medical imaging of normal, early COVID-19 patients and severe COVID-19 patients from Jinyintan hospital. The experiment plots the relationship between model accuracy, model loss and epoch, using ACC, TPR, SPE, F1 score and G-mean to measure the image maps of patients with and without the disease. Statistical measurement values are obtained by Inception-Resnet are 88.23%, 83.45%, 89.72%, 95.53% and 88.74%. The experimental results show that Inception-Resnet plays a more effective role than other image classification methods in evaluation indicators, and the method has higher robustness, accuracy and intuitiveness.\nWith CT images in the clinical diagnosis of COVID-19 images being widely used and the number of applied samples continuously increasing, the method in this paper is expected to become an additional diagnostic tool that can effectively improve the diagnostic accuracy of clinical COVID-19 images.", "journal": "Computer methods and programs in biomedicine", "date": "2022-08-15", "authors": ["YunfengChen", "YalanLin", "XiaodieXu", "JinzhenDing", "ChuzhaoLi", "YimingZeng", "WeiliLiu", "WeifangXie", "JianlongHuang"], "doi": "10.1016/j.cmpb.2022.107053\n10.1371/journal.pone.0180830\n10.1016/j.ejphar.2020.173644\n10.1155/2021/6658058"}
{"title": "COVID-19 diagnosis using chest CT scans and deep convolutional neural networks evolved by IP-based sine-cosine algorithm.", "abstract": "The prevalence of the COVID-19 virus and its variants has influenced all aspects of our life, and therefore, the precise diagnosis of this disease is vital. If a polymerase chain reaction test for a subject is negative, but he/she cannot easily breathe, taking a computed tomography (CT) image from his/her lung is urgently recommended. This study aims to optimize a deep convolution neural network (DCNN) structure to increase the COVID-19 diagnosis accuracy in lung CT images. This paper employs the sine-cosine algorithm (SCA) to optimize the structure of DCNN to take raw CT images and determine their status. Three improvements based on regular SCA are proposed to enhance both the accuracy and speed of the results. First, a new encoding approach is proposed based on the internet protocol (IP) address. Then, an enfeebled layer is proposed to generate a variable-length DCNN. The suggested model is examined over the COVID-CT and SARS-CoV-2 datasets. The proposed method is compared to a standard DCNN and seven variable-length models in terms of five known metrics, including sensitivity, accuracy, specificity, F1-score, precision, and receiver operative curve (ROC) and precision-recall curves. The results demonstrate that the proposed DCNN-IPSCA\u00a0surpasses other benchmarks, achieving final accuracy of (98.32%\u00a0and 98.01%), the sensitivity of (97.22% and 96.23%), and specificity of (96.77% and 96.44%) on the SARS-CoV-2 and COVID-CT datasets, respectively. Also, the proposed DCNN-IPSCA performs much better than the standard DCNN, with GPU and CPU training times, which are 387.69 and 63.10 times faster, respectively.", "journal": "Medical & biological engineering & computing", "date": "2022-08-13", "authors": ["BinfengXu", "DiegoMart\u00edn", "MohammadKhishe", "RezaBoostani"], "doi": "10.1007/s11517-022-02637-6\n10.1016/j.chaos.2020.109944\n10.1016/j.future.2019.05.009\n10.1016/j.aej.2021.06.024\n10.3390/math9091002\n10.1007/s10489-020-01888-w\n10.1016/j.bspc.2021.103326\n10.1177/0846537120913033\n10.1038/s41597-021-00900-3\n10.1002/jmv.25726\n10.1016/S0140-6736(13)61492-0\n10.1016/j.neucom.2016.12.038\n10.1016/j.bspc.2021.102764\n10.1162/106365602320169811\n10.1016/j.knosys.2015.12.022\n10.1145/1040132.1040133\n10.1016/j.imavis.2006.02.026\n10.1016/j.eswa.2016.04.005\n10.1016/j.eswa.2021.115732\n10.1007/s11047-019-09735-9\n10.1109/TEVC.2013.2281531"}
{"title": "Quantitative chest computed tomography combined with plasma cytokines predict outcomes in COVID-19 patients.", "abstract": "Despite extraordinary international efforts to dampen the spread and understand the mechanisms behind SARS-CoV-2 infections, accessible predictive biomarkers directly applicable in the clinic are yet to be discovered. Recent studies have revealed that diverse types of assays bear limited predictive power for COVID-19 outcomes. Here, we harness the predictive power of chest computed tomography (CT) in combination with plasma cytokines using a machine learning and k-fold cross-validation approach for predicting death during hospitalization and maximum severity degree in COVID-19 patients. Patients (n = 152) from the Mount Sinai Health System in New York with plasma cytokine assessment and a chest CT within five days from admission were included. Demographics, clinical, and laboratory variables, including plasma cytokines (IL-6, IL-8, and TNF-\u03b1), were collected from the electronic medical record. We found that CT quantitative alone was better at predicting severity (AUC 0.81) than death (AUC 0.70), while cytokine measurements alone better-predicted death (AUC 0.70) compared to severity (AUC 0.66). When combined, chest CT and plasma cytokines were good predictors of death (AUC 0.78) and maximum severity (AUC 0.82). Finally, we provide a simple scoring system (nomogram) using plasma IL-6, IL-8, TNF-\u03b1, ground-glass opacities (GGO) to aerated lung ratio and age as new metrics that may be used to monitor patients upon hospitalization and help physicians make critical decisions and considerations for patients at high risk of death for COVID-19.", "journal": "Heliyon", "date": "2022-08-13", "authors": ["GuillermoCarbonell", "Diane MarieDel Valle", "EdgarGonzalez-Kozlova", "BrettMarinelli", "EmmaKlein", "MariaEl Homsi", "DanielStocker", "MichaelChung", "AdamBernheim", "Nicole WSimons", "JianiXiang", "SharonNirenberg", "PatriciaKovatch", "SaraLewis", "MiriamMerad", "SachaGnjatic", "BachirTaouli"], "doi": "10.1016/j.heliyon.2022.e10166"}
{"title": "The risk profile of patients with COVID-19 as predictors of lung lesions severity and mortality-Development and validation of a prediction model.", "abstract": "We developed and validated a prediction model based on individuals' risk profiles to predict the severity of lung involvement and death in patients hospitalized with coronavirus disease 2019 (COVID-19) infection.\nIn this retrospective study, we studied hospitalized COVID-19 patients with data on chest CT scans performed during hospital stay (February 2020-April 2021) in a training dataset (TD) (\nIn the TD and the eVD, respectively, the mean [standard deviation (\nIn hospitalized patients with COVID-19, the severity of lung involvement is a strong predictor of death. Age, CRP levels, and duration of hospitalizations are the most important predictors of severe lung involvement. A simple prediction model based on available clinical and imaging data provides a validated tool that predicts the severity of lung involvement and death probability among hospitalized patients with COVID-19.", "journal": "Frontiers in microbiology", "date": "2022-08-13", "authors": ["EzatRahimi", "MinaShahisavandi", "Albert CidRoyo", "MohammadAzizi", "SaidEl Bouhaddani", "NasehSigari", "MiriamSturkenboom", "FaribaAhmadizar"], "doi": "10.3389/fmicb.2022.893750\n10.1007/s10140-022-02034-4\n10.3122/jabfm.2021.S1.200429\n10.1016/j.jamda.2020.05.045\n10.1371/journal.pone.0239235\n10.1023/A:1010933404324\n10.1016/j.jclinepi.2014.06.018\n10.1016/j.jiac.2022.02.025\n10.1007/s10238-020-00648-x\n10.3892/etm.2022.11315\n10.1016/j.hlc.2021.10.007\n10.1177/0284185121994695\n10.1016/j.tranpol.2021.07.004\n10.1016/j.acra.2022.02.019\n10.1016/j.cmi.2020.07.016\n10.1007/s10654-020-00698-1\n10.1016/j.jinf.2020.04.008\n10.1016/j.pmedr.2020.101298\n10.1016/j.neurol.2020.04.009\n10.1002/dmrr.3519\n10.3389/fimmu.2018.01302\n10.3389/fpubh.2021.695231\n10.2196/25852\n10.1080/13685538.2020.1774748\n10.1080/10408363.2020.1770685\n10.1007/s00330-020-06865-y\n10.1016/j.jaut.2020.102433\n10.1016/j.ajem.2020.09.017\n10.1093/ije/dyab012\n10.7326/M20-2973\n10.1136/thoraxjnl-2020-215518\n10.1016/j.puhe.2021.01.001\n10.1186/s12879-020-05154-9\n10.1007/s00330-021-08049-8\n10.1136/bmj.m1328\n10.2174/1573405617666210916120355\n10.1177/20503121211050755\n10.1142/11877"}
{"title": "Reinforcement Learning Based Diagnosis and Prediction for COVID-19 by Optimizing a Mixed Cost Function From CT Images.", "abstract": "A novel coronavirus disease (COVID-19) is a pandemic disease has caused 4 million deaths and more than 200 million infections worldwide (as of August 4, 2021). Rapid and accurate diagnosis of COVID-19 infection is critical to controlling the spread of the epidemic. In order to quickly and efficiently detect COVID-19 and reduce the threat of COVID-19 to human survival, we have firstly proposed a detection framework based on reinforcement learning for COVID-19 diagnosis, which constructs a mixed loss function that can integrate the advantages of multiple loss functions. This paper uses the accuracy of the validation set as the reward value, and obtains the initial model for the next epoch by searching the model corresponding to the maximum reward value in each epoch. We also have proposed a prediction framework that integrates multiple detection frameworks using parameter sharing to predict the progression of patients' disease without additional training. This paper also constructed a higher-quality version of the CT image dataset containing 247 cases screened by professional physicians, and obtained more excellent results on this dataset. Meanwhile, we used the other two COVID-19 datasets as external verifications, and still achieved a high accuracy rate without additional training. Finally, the experimental results show that our classification accuracy can reach 98.31%, and the precision, sensitivity, specificity, and AUC (Area Under Curve) are 98.82%, 97.99%, 98.67%, and 0.989, respectively. The accuracy of external verification can reach 93.34% and 91.05%. What's more, the accuracy of our prediction framework is 91.54%. A large number of experiments demonstrate that our proposed method is effective and robust for COVID-19 detection and prediction.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-08-12", "authors": ["SiyingChen", "MinghuiLiu", "PanDeng", "JialiDeng", "YiYuan", "XuanCheng", "TianshuXie", "LiboXie", "WeiZhang", "HaigangGong", "XiaominWang", "LifengXu", "HongPu", "MingLiu"], "doi": "10.1109/JBHI.2022.3197666"}
{"title": "Detection of COVID-19 from chest X-ray images: Boosting the performance with convolutional neural network and transfer learning.", "abstract": "Coronavirus disease (COVID-19) is a pandemic that has caused thousands of casualties and impacts all over the world. Most countries are facing a shortage of COVID-19 test kits in hospitals due to the daily increase in the number of cases. Early detection of COVID-19 can protect people from severe infection. Unfortunately, COVID-19 can be misdiagnosed as pneumonia or other illness and can lead to patient death. Therefore, in order to avoid the spread of COVID-19 among the population, it is necessary to implement an automated early diagnostic system as a rapid alternative diagnostic system. Several researchers have done very well in detecting COVID-19; however, most of them have lower accuracy and overfitting issues that make early screening of COVID-19 difficult. Transfer learning is the most successful technique to solve this problem with higher accuracy. In this paper, we studied the feasibility of applying transfer learning and added our own classifier to automatically classify COVID-19 because transfer learning is very suitable for medical imaging due to the limited availability of data. In this work, we proposed a CNN model based on deep transfer learning technique using six different pre-trained architectures, including VGG16, DenseNet201, MobileNetV2, ResNet50, Xception, and EfficientNetB0. A total of 3886 chest X-rays (1200 cases of COVID-19, 1341 healthy and 1345 cases of viral pneumonia) were used to study the effectiveness of the proposed CNN model. A comparative analysis of the proposed CNN models using three classes of chest X-ray datasets was carried out in order to find the most suitable model. Experimental results show that the proposed CNN model based on VGG16 was able to accurately diagnose COVID-19 patients with 97.84% accuracy, 97.90% precision, 97.89% sensitivity, and 97.89% of ", "journal": "Expert systems", "date": "2022-08-11", "authors": ["SohaibAsif", "YiWenhui", "KamranAmjad", "HouJin", "YiTao", "SiJinhai"], "doi": "10.1111/exsy.13099\n10.1080/07391102.2020.1767212\n10.20944/preprints202003.0300.v1"}
{"title": "A modified DeepLabV3+ based semantic segmentation of chest computed tomography images for COVID-19 lung infections.", "abstract": "Coronavirus disease (COVID-19) affects the lives of billions of people worldwide and has destructive impacts on daily life routines, the global economy, and public health. Early diagnosis and quantification of COVID-19 infection have a vital role in improving treatment outcomes and interrupting transmission. For this purpose, advances in medical imaging techniques like computed tomography (CT) scans offer great potential as an alternative to RT-PCR assay. CT scans enable a better understanding of infection morphology and tracking of lesion boundaries. Since manual analysis of CT can be extremely tedious and time-consuming, robust automated image segmentation is necessary for clinical diagnosis and decision support. This paper proposes an efficient segmentation framework based on the modified DeepLabV3+ using lower atrous rates in the Atrous Spatial Pyramid Pooling (ASPP) module. The lower atrous rates make receptive small to capture intricate morphological details. The encoder part of the framework utilizes a pre-trained residual network based on dilated convolutions for optimum resolution of feature maps. In order to evaluate the robustness of the modified model, a comprehensive comparison with other state-of-the-art segmentation methods was also performed. The experiments were carried out using a fivefold cross-validation technique on a publicly available database containing 100 single-slice CT scans from >40 patients with COVID-19. The modified DeepLabV3+ achieved good segmentation performance using around 43.9\u00a0M parameters. The lower atrous rates in the ASPP module improved segmentation performance. After fivefold cross-validation, the framework achieved an overall Dice similarity coefficient score of 0.881. The results demonstrate that several minor modifications to the DeepLabV3+ pipeline can provide robust solutions for improving segmentation performance and hardware implementation.", "journal": "International journal of imaging systems and technology", "date": "2022-08-10", "authors": ["HasanPolat"], "doi": "10.1002/ima.22772\n10.1002/ima.22566\n10.1002/ima.22525\n10.1016/j.measurement.2020.108288\n10.1016/j.mehy.2020.109761\n10.1148/radiol.2020200642\n10.1016/j.aej.2020.10.046\n10.1016/j.media.2017.07.005\n10.1016/j.tmaid.2020.101623\n10.1016/j.jrid.2020.04.001\n10.1111/exsy.12742\n10.1049/iet-cvi.2018.5129\n10.1049/iet-its.2018.5144\n10.1016/j.specom.2017.02.009\n10.1016/j.eswa.2021.115465\n10.1016/j.compbiomed.2020.104037\n10.1016/j.clinimag.2021.01.019\n10.1109/ICDMW.2018.00176\n10.30897/ijegeo.737993\n10.1016/j.media.2020.101794\n10.1186/s12880-020-00529-5\n10.1016/j.imu.2021.100681\n10.1109/CVPR.2016.90\n10.1007/s11042-020-09634-7\n10.1109/ACCESS.2016.2624938\n10.1016/j.compbiomed.2021.105134\n10.1016/j.compbiomed.2022.105383\n10.1007/s10278-021-00434-5\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2572683\n10.1155/2021/9999368\n10.1145/3453892.3461322\n10.1016/j.asoc.2020.106897\n10.1016/j.cmpb.2021.106004\n10.1016/j.patcog.2022.108538\n10.31590/ejosat.819409\n10.3390/s20113183\n10.1016/j.patrec.2020.07.029\n10.1109/WACV.2018.00163\n10.3390/su13031224\n10.3390/diagnostics11091712\n10.1007/978-3-030-01234-2_49\n10.1007/978-3-319-10578-9_23\n10.1016/j.eswa.2020.114417\n10.1002/mp.14676\n10.48550/arXiv.1412.6980\n10.1007/s10462-020-09854-1\n10.5281/ZENODO.3757476\n10.1109/ACCESS.2021.3067047"}
{"title": "Deep Learning-Based Time-to-Death Prediction Model for COVID-19 Patients Using Clinical Data and Chest Radiographs.", "abstract": "Accurate estimation of mortality and time to death at admission for COVID-19 patients is important and several deep learning models have been created for this task. However, there are currently no prognostic models which use end-to-end deep learning to predict time to event for admitted COVID-19 patients using chest radiographs and clinical data. We retrospectively implemented a new artificial intelligence model combining DeepSurv (a multiple-perceptron implementation of the Cox proportional hazards model) and a convolutional neural network (CNN) using 1356 COVID-19 inpatients. For comparison, we also prepared DeepSurv only\u00a0with clinical data, DeepSurv only with images (CNNSurv), and Cox proportional hazards models. Clinical data and chest radiographs at admission were used to estimate patient outcome (death or discharge) and duration to the outcome. The Harrel's concordance index (c-index) of the DeepSurv with CNN model was 0.82 (0.75-0.88) and this was significantly higher than the DeepSurv only\u00a0with clinical data model (c-index\u2009=\u20090.77 (0.69-0.84), p\u2009=\u20090.011),\u00a0CNNSurv (c-index\u00a0= 0.70 (0.63-0.79), p = 0.001), and the Cox proportional hazards model (c-index\u2009=\u20090.71 (0.63-0.79), p\u2009=\u20090.001). These results suggest that the time-to-event prognosis model became more accurate when chest radiographs and clinical data were used together.", "journal": "Journal of digital imaging", "date": "2022-08-09", "authors": ["ToshimasaMatsumoto", "Shannon LeighWalston", "MichaelWalston", "DaijiroKabata", "YukioMiki", "MasatsuguShiba", "DaijuUeda"], "doi": "10.1007/s10278-022-00691-y\n10.7861/clinmed.2020-0214\n10.1186/s13613-020-00650-2\n10.1093/cid/ciaa414\n10.1016/S2213-8587(21)00089-9\n10.1001/jama.2018.11100\n10.1038/nature14539\n10.1186/s12874-018-0482-1\n10.1016/j.amjmed.2004.03.020\n10.1007/s11547-020-01232-9\n10.1007/s10140-020-01808-y\n10.1148/radiol.2020201754\n10.1148/radiol.2020200823\n10.1007/s00330-020-06827-4\n10.1007/s10278-013-9622-7\n10.1136/bmj.h5527\n10.1136/bmj.n2400\n10.1001/jamainternmed.2021.6203\n10.1056/NEJMoa2103417\n10.1016/j.jiph.2021.09.023\n10.1371/journal.pone.0241955\n10.1038/s41586-020-2521-4\n10.1023/A:1010933404324\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2\n10.1111/j.0006-341X.2005.030814.x\n10.1136/bmj.m1328\n10.2196/25535\n10.2196/24973\n10.1016/j.media.2021.102096\n10.1148/ryai.2020200098\n10.1038/s41598-022-07890-1\n10.1038/s41598-021-93543-8\n10.1038/s41598-019-43372-7\n10.1016/S2589-7500(21)00039-X\n10.1016/j.lfs.2020.117788\n10.1186/s13054-019-2663-7\n10.1001/jamanetworkopen.2020.25881\n10.1001/jamanetworkopen.2020.5842\n10.1002/acm2.12995\n10.1007/BF00344251\n10.1148/radiol.2017171183\n10.1056/NEJMc2104626"}
{"title": "Development and Validation of Machine Models Using Natural Language Processing to Classify Substances Involved in Overdose Deaths.", "abstract": "Overdose is one of the leading causes of death in the US; however, surveillance data lag considerably from medical examiner determination of the death to reporting in national surveillance reports.\nTo automate the classification of deaths related to substances in medical examiner data using natural language processing (NLP) and machine learning (ML).\nDiagnostic study comparing different natural language processing and machine learning algorithms to identify substances related to overdose in 10 health jurisdictions in the US from January 1, 2020, to December 31, 2020. Unstructured text from 35\u202f433 medical examiner and coroners' death records was examined.\nText from each case was manually classified to a substance that was related to the death. Three feature representation methods were used and compared: text frequency-inverse document frequency (TF-IDF), global vectors for word representations (GloVe), and concept unique identifier (CUI) embeddings. Several ML algorithms were trained and best models were selected based on F-scores. The best models were tested on a hold-out test set and results were reported with 95% CIs.\nText data from death certificates were classified as any opioid, fentanyl, alcohol, cocaine, methamphetamine, heroin, prescription opioid, and an aggregate of other substances. Diagnostic metrics and 95% CIs were calculated for each combination of feature extraction method and machine learning classifier.\nOf 35\u202f433 death records analyzed (decedent median age, 58 years [IQR, 41-72 years]; 24\u202f449 [69%] were male), the most common substances related to deaths included any opioid (5739 [16%]), fentanyl (4758 [13%]), alcohol (2866 [8%]), cocaine (2247 [6%]), methamphetamine (1876 [5%]), heroin (1613 [5%]), prescription opioids (1197 [3%]), and any benzodiazepine (1076 [3%]). The CUI embeddings had similar or better diagnostic metrics compared with word embeddings and TF-IDF for all substances except alcohol. ML classifiers had perfect or near perfect performance in classifying deaths related to any opioids, heroin, fentanyl, prescription opioids, methamphetamine, cocaine, and alcohol. Classification of benzodiazepines was suboptimal using all 3 feature extraction methods.\nIn this diagnostic study, NLP/ML algorithms demonstrated excellent diagnostic performance at classifying substances related to overdoses. These algorithms should be integrated into workflows to decrease the lag time in reporting overdose surveillance data.", "journal": "JAMA network open", "date": "2022-08-09", "authors": ["DavidGoodman-Meza", "Chelsea LShover", "Jesus AMedina", "Amber BTang", "StevenShoptaw", "Alex A TBui"], "doi": "10.1001/jamanetworkopen.2022.25593\n10.2105/AJPH.2021.306256\n10.2105/AJPH.2017.304187\n10.15585/mmwr.mm7006a4\n10.1016/j.drugalcdep.2020.108314\n10.1371/journal.pone.0223318\n10.1016/j.drugalcdep.2021.109048\n10.1097/ADM.0000000000000775\n10.1136/amiajnl-2011-000464\n10.1016/j.jbi.2019.103185\n10.1002/pds.4772\n10.1002/pds.4810\n10.35111/wk4f-qt80\n10.1016/j.jbi.2018.09.008\n10.18653/v1/W19-5034\n10.1093/nar/gkh061\n10.15585/mmwr.mm7050e3\n10.48550/arXiv.1810.04805\n10.48550/arXiv.1904.03323"}
{"title": "Deep Learning-Aided Automated Pneumonia Detection and Classification Using CXR Scans.", "abstract": "The COVID-19 pandemic has caused a worldwide catastrophe and widespread devastation that reeled almost all countries. The pandemic has mounted pressure on the existing healthcare system and caused panic and desperation. The gold testing standard for COVID-19 detection, reverse transcription-polymerase chain reaction (RT-PCR), has shown its limitations with 70% accuracy, contributing to the incorrect diagnosis that exaggerated the complexities and increased the fatalities. The new variations further pose unseen challenges in terms of their diagnosis and subsequent treatment. The COVID-19 virus heavily impacts the lungs and fills the air sacs with fluid causing pneumonia. Thus, chest X-ray inspection is a viable option if the inspection detects COVID-19-induced pneumonia, hence confirming the exposure of COVID-19. Artificial intelligence and machine learning techniques are capable of examining chest X-rays in order to detect patterns that can confirm the presence of COVID-19-induced pneumonia. This research used CNN and deep learning techniques to detect COVID-19-induced pneumonia from chest X-rays. Transfer learning with fine-tuning ensures that the proposed work successfully classifies COVID-19-induced pneumonia, regular pneumonia, and normal conditions. Xception, Visual Geometry Group 16, and Visual Geometry Group 19 are used to realize transfer learning. The experimental results were promising in terms of precision, recall, F1 score, specificity, false omission rate, false negative rate, false positive rate, and false discovery rate with a COVID-19-induced pneumonia detection accuracy of 98%. Experimental results also revealed that the proposed work has not only correctly identified COVID-19 exposure but also made a distinction between COVID-19-induced pneumonia and regular pneumonia, as the latter is a very common disease, while COVID-19 is more lethal. These results mitigated the concern and overlap in the diagnosis of COVID-19-induced pneumonia and regular pneumonia. With further integrations, it can be employed as a potential standard model in differentiating the various lung-related infections, including COVID-19.", "journal": "Computational intelligence and neuroscience", "date": "2022-08-09", "authors": ["Deepak KumarJain", "TarishiSingh", "PraneetSaurabh", "DhananjayBisen", "NeerajSahu", "JayantMishra", "HabiburRahman"], "doi": "10.1155/2022/7474304\n10.1109/ICDABI51230.2020.9325626\n10.1001/jama.2020.1585\n10.1016/s0140-6736(20)30211-710.1016/s0140-6736(20)30211-7\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30183-5\n10.1093/clinchem/hvaa029\n10.1148/radiol.2020200230\n10.2214/AJR.20.23034\n10.1007/s10489-020-01826-w\n10.1109/ISMSIT.2019.8932878\n10.1109/NSSMIC.2018.8824292\n10.1109/ICNSC.2018.8361312\n10.1007/s11517-019-01965-4\n10.1109/TMI.2019.2894349\n10.1148/radiol.2019181960\n10.3390/app10020559\n10.1016/j.compbiomed.2020.103869\n10.1145/3195588.3195597\n10.1148/radiol.2020200905\n10.1371/journal.pmed.1002686\n10.1109/ACCESS.2020.3010287\n10.3390/app9194130\n10.1109/TMI.2020.2994459\n10.17632/9xkhgts2s6.1\n10.1007/s40009-020-00979-z\n10.1007/s11042-022-12775-6"}
{"title": "A Novel Multi-Stage Residual Feature Fusion Network for Detection of COVID-19 in Chest X-Ray Images.", "abstract": "To suppress the spread of COVID-19, accurate diagnosis at an early stage is crucial, chest screening with radiography imaging plays an important role in addition to the real-time reverse transcriptase polymerase chain reaction (RT-PCR) swab test. Due to the limited data, existing models suffer from incapable feature extraction and poor network convergence and optimization. Accordingly, a multi-stage residual network, MSRCovXNet, is proposed for effective detection of COVID-19 from chest x-ray (CXR) images. As a shallow yet effective classifier with the ResNet-18 as the feature extractor, MSRCovXNet is optimized by fusing two proposed feature enhancement modules (FEM), i.e., low-level and high-level feature maps (LLFMs and HLFMs), which contain respectively more local information and rich semantic information, respectively. For effective fusion of these two features, a single-stage FEM (MSFEM) and a multi-stage FEM (MSFEM) are proposed to enhance the semantic feature representation of the LLFMs and the local feature representation of the HLFMs, respectively. Without ensembling other deep learning models, our MSRCovXNet has a precision of 98.9% and a recall of 94% in detection of COVID-19, which outperforms several state-of-the-art models. When evaluated on the COVIDGR dataset, an average accuracy of 82.2% is achieved, leading other methods by at least 1.2%.", "journal": "IEEE transactions on molecular, biological, and multi-scale communications", "date": "2022-08-09", "authors": ["ZhenyuFang", "JinchangRen", "CalumMacLellan", "HuihuiLi", "HuiminZhao", "AmirHussain", "GiancarloFortino"], "doi": "10.1109/TMBMC.2021.3099367"}
{"title": "Automated analysis of limited echocardiograms: Feasibility and relationship to outcomes in COVID-19.", "abstract": "As automated echocardiographic analysis is increasingly utilized, continued evaluation within hospital settings is important to further understand its potential value. The importance of cardiac involvement in patients hospitalized with COVID-19 provides an opportunity to evaluate the feasibility and clinical relevance of automated analysis applied to limited echocardiograms.\nIn this multisite US cohort, the feasibility of automated AI analysis was evaluated on 558 limited echocardiograms in patients hospitalized with COVID-19. Reliability of automated assessment of left ventricular (LV) volumes, ejection fraction (EF), and LV longitudinal strain (LS) was assessed against clinically obtained measures and echocardiographic findings. Automated measures were evaluated against patient outcomes using ROC analysis, survival modeling, and logistic regression for the outcomes of 30-day mortality and in-hospital sequelae.\nFeasibility of automated analysis for both LVEF and LS was 87.5% (488/558 patients). AI analysis was performed with biplane method in 300 (61.5%) and single plane apical 4- or 2-chamber analysis in 136 (27.9%) and 52 (10.7%) studies, respectively. Clinical LVEF was assessed using visual estimation in 192 (39.3%), biplane in 163 (33.4%), and single plane or linear methods in 104 (21.2%) of the 488 studies; 29 (5.9%) studies did not have clinically reported LVEF. LV LS was clinically reported in 80 (16.4%). Consistency between automated and clinical values demonstrated Pearson's R, root mean square error (RMSE) and intraclass correlation coefficient (ICC) of 0.61, 11.3% and 0.72, respectively, for LVEF; 0.73, 3.9% and 0.74, respectively for LS; 0.76, 24.4ml and 0.87, respectively, for end-diastolic volume; and 0.82, 12.8 ml, and 0.91, respectively, for end-systolic volume. Abnormal automated measures of LVEF and LS were associated with LV wall motion abnormalities, left atrial enlargement, and right ventricular dysfunction. Automated analysis was associated with outcomes, including survival.\nAutomated analysis was highly feasible on limited echocardiograms using abbreviated protocols, consistent with equivalent clinically obtained metrics, and associated with echocardiographic abnormalities and patient outcomes.", "journal": "Frontiers in cardiovascular medicine", "date": "2022-08-09", "authors": ["Patricia APellikka", "Jordan BStrom", "Gabriel MPajares-Hurtado", "Martin GKeane", "BenjaminKhazan", "SalimaQamruddin", "AustinTutor", "FahadGul", "EricPeterson", "RituThamman", "ShivaniWatson", "DeepaMandale", "Christopher GScott", "TasneemNaqvi", "Gary MWoodward", "WilliamHawkes"], "doi": "10.3389/fcvm.2022.937068\n10.1016/S2589-7500(20)30160-6\n10.1038/s41746-017-0013-1\n10.1109/ACCESS.2020.3010326\n10.1007/978-3-030-01045-4_9\n10.1109/TMI.2017.2690836\n10.1016/j.jcmg.2020.08.034\n10.1109/TUFFC.2020.3003403\n10.1007/978-3-030-39343-4_43\n10.1016/j.echo.2020.09.011\n10.1016/j.jcmg.2019.02.024\n10.1161/CIRCULATIONAHA.118.034338\n10.1038/s41746-019-0216-8\n10.1038/s41586-020-2145-8\n10.1530/ERP-18-0056\n10.1038/s41551-020-00667-9\n10.1016/j.jcmg.2021.04.018\n10.1136/heartjnl-2020-318256\n10.1016/j.echo.2021.05.010\n10.1016/j.jacc.2020.06.007\n10.1007/s11739-020-02604-9\n10.1016/j.jacc.2020.08.069\n10.1016/j.echo.2021.03.010\n10.1016/j.mayocp.2021.01.006\n10.1016/j.echo.2021.10.015\n10.1007/s10554-020-01968-5\n10.1016/j.jcmg.2020.07.026\n10.1016/j.echo.2014.10.003\n10.1001/jamacardio.2021.6059\n10.1161/CIRCIMAGING.119.009303\n10.1016/j.echo.2020.01.008\n10.1093/ehjci/jeaa072\n10.1016/j.mayocp.2020.12.015\n10.1016/j.jacc.2012.09.035\n10.1136/heartjnl-2019-316297\n10.1016/j.jcmg.2017.11.017\n10.1016/j.echo.2019.08.012\n10.1016/S0735-1097(21)04505-8\n10.1016/j.jacc.2020.08.066\n10.1016/j.jcmg.2020.04.014\n10.1001/jamacardio.2020.3538\n10.1136/bmj.m1966\n10.1016/j.echo.2020.04.017\n10.1016/j.jacc.2015.07.052\n10.1016/j.echo.2016.03.002\n10.4330/wjc.v7.i12.948\n10.1001/jamacardio.2019.2952\n10.2147/VHRM.S206747\n10.1016/j.jcmg.2016.06.012"}
{"title": "Primary SARS-CoV-2 Pneumonia Screening in Adults: Analysis of the Correlation between High-Resolution Computed Tomography Pulmonary Patterns and Initial Oxygen Saturation Levels.", "abstract": "Chest high-resolution computed tomography (HRCT) is mandatory for patients with confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection and a high respiratory rate (RR) because sublobar consolidation is the likely pathological pattern in addition to ground glass opacities (GGOs).\nThe present study determined the correlation between the percentage extent of typical pulmonary lesions on HRCT, as a representation of severity, and the RR and peripheral oxygen saturation level (SpO2), as measured through pulse oximetry, in patients with reverse transcriptase polymerase chain reaction (RT-PCR)-confirmed primary (noncomplicated) SARS-CoV-2 pneumonia.\nThe present retrospective study was conducted in 332 adult patients who presented withzdyspnea and hypoxemia and were admitted to Prince Mohammed bin Abdulaziz Hospital, Riyadh, Saudi Arabia between May 15, 2020 and December 15, 2020. All the patients underwent chest HRCT. Of the total, 198 patients with primary noncomplicated SARS-CoV-2 pneumonia were finally selected based on the typical chest HRCT patterns. The main CT patterns, GGO and sublobar consolidation, were individually quantified as a percentage of the total pulmonary involvement through algebraic summation of the percentage of the 19 pulmonary segments affected. Additionally, the statistical correlation strength between the total percentage pulmonary involvement and the age, initial RR, and percentage SpO2 of the patients was determined.\nThe mean \u00b1 standard deviation (SD) age of the 198 patients was 48.9 \u00b1 11.4 years. GGO magnitude alone exhibited a significant weak positive correlation with patients' age (r = 0.2; p = 0.04). Sublobar consolidation extent exhibited a relatively stronger positive correlation with RR than GGO magnitude (r = 0.23; p = 0.002). A relatively stronger negative correlation was observed between the GGO extent and SpO2 (r = - 0.38; p = 0.002) than that between sublobar consolidation and SpO2 (r = - 0.2; p = 0.04). An increase in the correlation strength was demonstrated with increased case segregation with GGO extent (r = - 0.34; p = 0.01).\nThe correlation between the magnitudes of typical pulmonary lesion patterns, particularly GGO, which exhibited an incremental correlation pattern on chest HRCT, and the SpO2 percentage, may allow the establishment of an artificial intelligence program to differentiate primary SARS-CoV-2 pneumonia from other complications and associated pathology influencing SpO2.", "journal": "Current medical imaging", "date": "2022-08-06", "authors": ["BatilAlonazi", "Mohamed AMostafa", "Ahmed MFarghaly", "Salah AZindani", "Jehad AAl-Watban", "FerasAltaimi", "Abdulrahim SAlmotairy", "Moram AFagiry", "Mustafa ZMahmoud"], "doi": "10.2174/1573405618666220802095119"}
{"title": "Deep learning based COVID-19 detection using medical images: Is insufficient data handled well?", "abstract": "The deep learning is a prominent method for automatic detection of COVID-19 disease using medical dataset. This paper aims to give the perspective on the data insufficiency issue that exists in COVID-19 detection associated with deep learning. The extensive study on the available datasets comprising CT and X-ray images are presented in this paper, which can be very much useful in the context of deep learning framework for COVID-19 detection. Moreover, various data handling techniques that are very essential in deep learning models are discussed in detail. Advanced data handling techniques and approaches to modify deep learning models are suggested to handle the data insufficiency problem in deep learning based COVID-19 detection.", "journal": "Current medical imaging", "date": "2022-08-06", "authors": ["CarenBabu", "Rahul ManoharO", "D AbrahamChandy"], "doi": "10.2174/1573405618666220803123626"}
{"title": "Performance of a Chest Radiograph AI Diagnostic Tool for COVID-19: A Prospective Observational Study.", "abstract": "To conduct a prospective observational study across 12 U.S. hospitals to evaluate real-time performance of an interpretable artificial intelligence (AI) model to detect COVID-19 on chest radiographs.\nA total of 95\u2009363 chest radiographs were included in model training, external validation, and real-time validation. The model was deployed as a clinical decision support system, and performance was prospectively evaluated. There were 5335 total real-time predictions and a COVID-19 prevalence of 4.8% (258 of 5335). Model performance was assessed with use of receiver operating characteristic analysis, precision-recall curves, and F1 score. Logistic regression was used to evaluate the association of race and sex with AI model diagnostic accuracy. To compare model accuracy with the performance of board-certified radiologists, a third dataset of 1638 images was read independently by two radiologists.\nParticipants positive for COVID-19 had higher COVID-19 diagnostic scores than participants negative for COVID-19 (median, 0.1 [IQR, 0.0-0.8] vs 0.0 [IQR, 0.0-0.1], respectively; \nAI-based tools have not yet reached full diagnostic potential for COVID-19 and underperform compared with radiologist prediction.", "journal": "Radiology. Artificial intelligence", "date": "2022-08-05", "authors": ["JuSun", "LePeng", "TaihuiLi", "DyahAdila", "ZachZaiman", "Genevieve BMelton-Meaux", "Nicholas EIngraham", "EricMurray", "DanielBoley", "SeanSwitzer", "John LBurns", "KunHuang", "TadashiAllen", "Scott DSteenburg", "Judy WawiraGichoya", "ErichKummerfeld", "Christopher JTignanelli"], "doi": "10.1148/ryai.210217\n10.1101/2020.09.13.20193565v2"}
{"title": "Diagnostic performance of artificial intelligence algorithms for detection of pulmonary involvement by COVID-19 based on portable radiography.", "abstract": "To evaluate the diagnostic performance of different artificial intelligence (AI) algorithms for the identification of pulmonary involvement by SARS-CoV-2 based on portable chest radiography (RX).\nProspective observational study that included patients admitted for suspected COVID-19 infection in a university hospital between July and November 2020. The reference standard of pulmonary involvement by SARS-CoV-2 comprised a positive PCR test and low-tract respiratory symptoms.\n493 patients were included, 140 (28%) with positive PCR and 32 (7%) with SARS-CoV-2 pneumonia. The AI-B algorithm had the best diagnostic performance (areas under the ROC curve AI-B 0.73, vs. AI-A 0.51, vs. AI-C 0.57). Using a detection threshold greater than 55%, AI-B had greater diagnostic performance than the specialist [(area under the curve of 0.68 (95% CI 0.64-0.72), vs. 0.54 (95% CI 0.49-0.59)].\nAI algorithms based on portable RX enabled a diagnostic performance comparable to human assessment for the detection of SARS-CoV-2 lung involvement.\nTo evaluate the diagnostic performance of different artificial intelligence (AI) algorithms for the identification of pulmonary involvement by SARS-CoV-2 based on portable chest radiography (RX).\nProspective observational study that included patients admitted for suspected COVID-19 infection in a university hospital between July and November 2020. The reference standard of pulmonary involvement by SARS-CoV-2 comprised a positive PCR test and low-tract respiratory symptoms.\n493 patients were included, 140 (28%) with positive PCR and 32 (7%) with SARS-CoV-2 pneumonia. The AI-B algorithm had the best diagnostic performance (areas under the ROC curve AI-B 0.73, vs. AI-A 0.51, vs. AI-C 0.57). Using a detection threshold greater than 55%, AI-B had greater diagnostic performance than the specialist [(area under the curve of 0.68 (95% CI 0.64-0.72), vs. 0.54 (95% CI 0.49-0.59)].\nAI algorithms based on portable RX enabled a diagnostic performance comparable to human assessment for the detection of SARS-CoV-2 lung involvement.", "journal": "Medicina clinica", "date": "2022-08-03", "authors": ["Ricardo LuisCobe\u00f1as", "Mar\u00edade Vedia", "JuanFlorez", "DanielaJaramillo", "LucianaFerrari", "RicardoRe"], "doi": "10.1016/j.medcli.2022.04.016\n10.2196/19104\n10.7717/peerj-cs.551\n10.1186/s41747-020-00195-w\n10.1148/radiol.2020201874\n10.1016/j.jiph.2020.06.028"}
{"title": "Development and verification of radiomics framework for computed tomography image segmentation.", "abstract": "Radiomics has been considered an imaging marker for capturing quantitative image information (QII). The introduction of radiomics to image segmentation is desirable but challenging.\nThis study aims to develop and validate a radiomics-based framework for image segmentation (RFIS).\nRFIS is designed using features extracted from volume (svfeatures) created by sliding window (swvolume). The 53 svfeatures are extracted from 11 phantom series. Outliers in the svfeature datasets are detected by isolation forest (iForest) and specified as the mean value. The percentage coefficient of variation (%COV) is calculated to evaluate the reproducibility of svfeatures. RFIS is constructed and applied to the gross target volume (GTV) segmentation from the peritumoral region (GTV with a 10 mm margin) to assess its feasibility. The 127 lung cancer images are enrolled. The test-retest method, correlation matrix, and Mann-Whitney U test (p < 0.05) are used to select non-redundant svfeatures of statistical significance from the reproducible svfeatures. The synthetic minority over-sampling technique is utilized to balance the minority group in the training sets. The support vector machine is employed for RFIS construction, which is tuned in the training set using 10-fold stratified cross-validation and then evaluated in the test sets. The swvolumes with the consistent classification results are grouped and merged. Mode filtering is performed to remove very small subvolumes and create relatively large regions of completely uniform character. In addition, RFIS performance is evaluated by the area under the receiver operating characteristic (ROC) curve (AUC), accuracy, sensitivity, specificity, and Dice similarity coefficient (DSC).\n30249 phantom and 145008 patient image swvolumes were analyzed. Forty-nine (92.45% of 53) svfeatures represented excellent reproducibility(%COV<15). Forty-five features (91.84% of 49) included five categories that passed test-retest analysis. Thirteen svfeatures (28.89% of 45) svfeatures were selected for RFIS construction. RFIS showed an average (95% confidence interval) sensitivity of 0.848 (95% CI:0.844-0.883), a specificity of 0.821 (95% CI: 0.818-0.825), an accuracy of 83.48% (95% CI: 83.27%-83.70%), and an AUC of 0.906 (95% CI: 0.904-0.908) with cross-validation. The sensitivity, specificity, accuracy, and AUC were equal to 0.762 (95% CI: 0.754-0.770), 0.840 (95% CI: 0.837-0.844), 82.29% (95% CI: 81.90%-82.60%), and 0.877 (95% CI: 0.873-0.881) in the test set, respectively. GTV was segmented by grouping and merging swvolume with identical classification results. The mean DSC after mode filtering was 0.707 \u00b1 0.093 in the training sets and 0.688 \u00b1 0.072 in the test sets.\nReproducible svfeatures can capture the differences in QII among swvolumes. RFIS can be applied to swvolume classification, which achieves image segmentation by grouping and merging the swvolume with similar QII.", "journal": "Medical physics", "date": "2022-08-03", "authors": ["JiabingGu", "BaoshengLi", "HuazhongShu", "JianZhu", "QingtaoQiu", "TongBai"], "doi": "10.1002/mp.15904\n10.1007/s00066-017-1175-0\n10.1038/ncomms5006\n10.1186/1748-717X-7-32\n10.1016/j.radonc.2014.08.028\n10.1016/j.radonc.2019.03.004\n10.3390/cancers12061682\n10.1016/j.bspc.2021.102522\n10.1038/nrclinonc.2017.141\n10.1002/mp.15539\n10.1002/mp.15178\n10.1016/j.radonc.2020.10.016\n10.1002/mrm.22572\n10.1148/radiol.13122697\n10.1002/mp.15392\n10.1016/j.ebiom.2019.05.023\n10.1088/1361-6560/ac2ea7\n10.1097/RLI.0000000000000180\n10.1007/s10278-013-9622-7\n10.7937/K9/TCIA.2017.zuzrml5b\n10.1088/0031-9155/60/14/5471\n10.1007/s00330-018-5343-0\n10.1007/s11307-016-0973-6\n10.1038/s41598-020-60868-9\n10.1145/2133360.2133363\n10.1007/s10278-014-9716-x\n10.3389/fonc.2021.692973\n10.3389/fonc.2016.00071\n10.1613/jair.953\n10.1190/1.2431821\n10.1109/PROC.1979.11327\n10.3348/kjr.2018.0070\n10.1016/j.mri.2003.09.001\n10.1016/j.radonc.2019.08.008\n10.1002/mp.12123\n10.1007/s00330-017-4859-z\n10.1088/0031-9155/61/13/r150\n10.2967/jnumed.113.129858\n10.3390/cancers13081814\n10.1016/j.radonc.2017.11.025\n10.1002/mp.15582"}
{"title": "Deep Learning-Based Networks for Detecting Anomalies in Chest X-Rays.", "abstract": "X-ray images aid medical professionals in the diagnosis and detection of pathologies. They are critical, for example, in the diagnosis of pneumonia, the detection of masses, and, more recently, the detection of COVID-19-related conditions. The chest X-ray is one of the first imaging tests performed when pathology is suspected because it is one of the most accessible radiological examinations. Deep learning-based neural networks, particularly convolutional neural networks, have exploded in popularity in recent years and have become indispensable tools for image classification. Transfer learning approaches, in particular, have enabled the use of previously trained networks' knowledge, eliminating the need for large data sets and lowering the high computational costs associated with this type of network. This research focuses on using deep learning-based neural networks to detect anomalies in chest X-rays. Different convolutional network-based approaches are investigated using the ChestX-ray14 database, which contains over 100,000 X-ray images with labels relating to 14 different pathologies, and different classification objectives are evaluated. Starting with the pretrained networks VGG19, ResNet50, and Inceptionv3, networks based on transfer learning are implemented, with different schemes for the classification stage and data augmentation. Similarly, an ad hoc architecture is proposed and evaluated without transfer learning for the classification objective with more examples. The results show that transfer learning produces acceptable results in most of the tested cases, indicating that it is a viable first step for using deep networks when there are not enough labeled images, which is a common problem when working with medical images. The ad hoc network, on the other hand, demonstrated good generalization with data augmentation and an acceptable accuracy value. The findings suggest that using convolutional neural networks with and without transfer learning to design classifiers for detecting pathologies in chest X-rays is a good idea.", "journal": "BioMed research international", "date": "2022-08-03", "authors": ["MalekBadr", "ShahaAl-Otaibi", "NazikAlturki", "TanvirAbir"], "doi": "10.1155/2022/7833516\n10.1201/b10866-37\n10.1109/CVPR.2017.369\n10.1109/ICSCCC.2018.8703316\n10.1016/B978-0-12-816718-2.00008-7\n10.1155/2022/1959371\n10.1007/978-981-15-4112-4_7\n10.3390/jcm11072054\n10.23919/MIPRO48935.2020.9245376\n10.1155/2022/4569879\n10.14569/IJACSA.2021.0121026\n10.1109/ELNANO.2018.8477564\n10.1155/2021/8148772\n10.1155/2022/3294954\n10.1007/s00607-021-00992-0\n10.1155/2021/5759184\n10.1155/2021/6799202\n10.24191/mjoc.v4i1.6095\n10.1007/s11548-020-02305-w\n10.1155/2021/1220374\n10.1117/12.2293971\n10.1007/s10916-021-01745-4"}
{"title": "Detecting COVID-19 patients via MLES-Net deep learning models from X-Ray images.", "abstract": "Corona Virus Disease 2019 (COVID-19) first appeared in December 2019, and spread rapidly around the world. COVID-19 is a pneumonia caused by novel coronavirus infection in 2019. COVID-19 is highly infectious and transmissible. By 7 May 2021, the total number of cumulative number of deaths is 3,259,033. In order to diagnose the infected person in time to prevent the spread of the virus, the diagnosis method for COVID-19 is extremely important. To solve the above problems, this paper introduces a Multi-Level Enhanced Sensation module (MLES), and proposes a new convolutional neural network model, MLES-Net, based on this module.\nAttention has the ability to automatically focus on the key points in various information, and Attention can realize parallelism, which can replace some recurrent neural networks to a certain extent and improve the efficiency of the model. We used the correlation between global and local features to generate the attention mask. First, the feature map was divided into multiple groups, and the initial attention mask was obtained by the dot product of each feature group and the feature after the global pooling. Then the attention masks were normalized. At the same time, there were two scaling and translating parameters in each group so that the normalize operation could be restored. Then, the final attention mask was obtained through the sigmoid function, and the feature of each location in the original feature group was scaled. Meanwhile, we use different classifiers on the network models with different network layers.\nThe network uses three classifiers, FC module (fully connected layer), GAP module (global average pooling layer) and GAPFC module (global average pooling layer and fully connected layer), to improve recognition efficiency. GAPFC as a classifier can obtain the best comprehensive effect by comparing the number of parameters, the amount of calculation and the detection accuracy. The experimental results show that the MLES-Net56-GAPFC achieves the best overall accuracy rate (95.27%) and the best recognition rate for COVID-19 category (100%).\nMLES-Net56-GAPFC has good classification ability for the characteristics of high similarity between categories of COVID-19 X-Ray images and low intra-category variability. Considering the factors such as accuracy rate, number of network model parameters and calculation amount, we believe that the MLES-Net56-GAPFC network model has better practicability.", "journal": "BMC medical imaging", "date": "2022-07-31", "authors": ["WeiWang", "YongbinJiang", "XinWang", "PengZhang", "JiLi"], "doi": "10.1186/s12880-022-00861-y\n10.1016/j.physio.2020.03.003\n10.1109/5.726791\n10.1109/TIP.2017.2710620\n10.2991/ijcis.d.191209.001\n10.1186/s12880-019-0399-0\n10.1109/TUFFC.2020.3005512\n10.1109/ACCESS.2020.3001973\n10.7150/ijms.46684\n10.1109/TMI.2020.2995508\n10.1007/s42979-020-00401-x\n10.1007/s42979-020-00335-4\n10.1007/s42979-020-00300-1\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00216-w\n10.1007/s42979-020-00383-w\n10.2991/ijcis.d.201123.001\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103869\n10.1109/ACCESS.2020.3003810\n10.1371/journal.pone.0235187\n10.1016/j.imu.2020.100412\n10.1049/ipr2.12474\n10.1016/j.imu.2020.100505"}
{"title": "Unsupervised machine learning demonstrates the prognostic value of TAPSE/PASP ratio among hospitalized patients with COVID-19.", "abstract": "The ratio of tricuspid annular plane systolic excursion (TAPSE) to pulmonary artery systolic pressure (PASP) is a validated index of right ventricular-pulmonary arterial (RV-PA) coupling with prognostic value. We determined the predictive value of TAPSE/PASP ratio and adverse clinical outcomes in hospitalized patients with COVID-19.\nTwo hundred and twenty-nine consecutive hospitalized racially/ethnically diverse adults (\u226518 years of age) admitted with COVID-19 between March and June 2020 with clinically indicated transthoracic echocardiograms (TTE) that included adequate tricuspid regurgitation (TR) velocities for calculation of PASP were studied. The exposure of interest was impaired RV-PA coupling as assessed by TAPSE/PASP ratio. The primary outcome was in-hospital mortality. Secondary endpoints comprised of ICU admission, incident acute respiratory distress syndrome (ARDS), and systolic heart failure.\nOne hundred and seventy-six patients had both technically adequate TAPSE measurements and measurable TR velocities for analysis. After adjustment for age, sex, BMI, race/ethnicity, diabetes mellitus, and smoking status, log(TAPSE/PASP) had a significantly inverse association with ICU admission (p = 0.015) and death (p = 0.038). ROC analysis showed the optimal cutoff for TAPSE/PASP for death was 0.51\u00a0mm\u00a0mmHg\nImpaired RV-PA coupling, assessed noninvasively via the TAPSE/PASP ratio, was predictive of need for ICU level care and in-hospital mortality in hospitalized patients with COVID-19 suggesting utility of TAPSE/PASP in identification of poor clinical outcomes in this population both by traditional statistical and unsupervised machine learning based methods.", "journal": "Echocardiography (Mount Kisco, N.Y.)", "date": "2022-07-31", "authors": ["VivekJani", "KaranKapoor", "JosephMeyer", "JimLu", "ErinGoerlich", "Thomas SMetkus", "Jose AMadrazo", "ErinMichos", "KatherineWu", "NicoleBavaro", "ShelbyKutty", "Allison GHays", "MonicaMukherjee"], "doi": "10.1111/echo.15432"}
{"title": "A comparison of Covid-19 early detection between convolutional neural networks and radiologists.", "abstract": "The role of chest radiography in COVID-19 disease has changed since the beginning of the pandemic from a diagnostic tool when microbiological resources were scarce to a different one focused on detecting and monitoring COVID-19 lung involvement. Using chest radiographs, early detection of the disease is still helpful in resource-poor environments. However, the sensitivity of a chest radiograph for diagnosing COVID-19 is modest, even for expert radiologists. In this paper, the performance of a deep learning algorithm on the first clinical encounter is evaluated and compared with a group of radiologists with different years of experience.\nThe algorithm uses an ensemble of four deep convolutional networks, Ensemble4Covid, trained to detect COVID-19 on frontal chest radiographs. The algorithm was tested using images from the first clinical encounter of positive and negative cases. Its performance was compared with five radiologists on a smaller test subset of patients. The algorithm's performance was also validated using the public dataset COVIDx.\nCompared to the consensus of five radiologists, the Ensemble4Covid model achieved an AUC of 0.85, whereas the radiologists achieved an AUC of 0.71. Compared with other state-of-the-art models, the performance of a single model of our ensemble achieved nonsignificant differences in the public dataset COVIDx.\nThe results show that the use of images from the first clinical encounter significantly drops the detection performance of COVID-19. The performance of our Ensemble4Covid under these challenging conditions is considerably higher compared to a consensus of five radiologists. Artificial intelligence can be used for the fast diagnosis of COVID-19.", "journal": "Insights into imaging", "date": "2022-07-29", "authors": ["AlbertoAlbiol", "FranciscoAlbiol", "RobertoParedes", "Juana Mar\u00edaPlasencia-Mart\u00ednez", "AnaBlanco Barrio", "Jos\u00e9 M Garc\u00edaSantos", "SalvadorTortajada", "Victoria MGonz\u00e1lez Monta\u00f1o", "Clara ERodr\u00edguez Godoy", "SarayFern\u00e1ndez G\u00f3mez", "ElenaOliver-Garcia", "Mar\u00edade la Iglesia Vay\u00e1", "Francisca LM\u00e1rquez P\u00e9rez", "Juan IRayo Madrid"], "doi": "10.1186/s13244-022-01250-3\n10.1001/JAMA.2020.21694\n10.1007/S00330-020-07347-X\n10.1007/S00330-020-06967-7\n10.1148/RADIOL.2020201160/ASSET/IMAGES/LARGE/RADIOL.2020201160.FIG6.JPEG\n10.1148/RADIOL.2020202944/ASSET/IMAGES/LARGE/RADIOL.2020202944.TBL4.JPEG\n10.1148/RADIOL.2020203511/ASSET/IMAGES/LARGE/RADIOL.2020203511.FIG6C.JPEG\n10.1148/RADIOL.2021204522/ASSET/IMAGES/LARGE/RADIOL.2021204522.FIG8C.JPEG\n10.1109/TMI.2020.2993291\n10.1007/s00330-020-07354-y\n10.1007/s00330-020-07270-1\n10.1148/RADIOL.2020201874\n10.1016/J.MAYOCP.2020.07.024\n10.1109/TKDE.2009.191\n10.1037/H0031619\n10.1214/ss/1177013815\n10.1002/1097-0142\n10.1148/RADIOL.2020201365/ASSET/IMAGES/LARGE/RADIOL.2020201365.TBL2.JPEG\n10.1016/J.JACR.2019.05.019\n10.1186/S41747-020-00203-Z/FIGURES/3\n10.1148/RADIOL.2020204226"}
{"title": "Automatic scoring of COVID-19 severity in X-ray imaging based on a novel deep learning workflow.", "abstract": "In this study, we propose a two-stage workflow used for the segmentation and scoring of lung diseases. The workflow inherits quantification, qualification, and visual assessment of lung diseases on X-ray images estimated by radiologists and clinicians. It requires the fulfillment of two core stages devoted to lung and disease segmentation as well as an additional post-processing stage devoted to scoring. The latter integrated block is utilized, mainly, for the estimation of segment scores and computes the overall severity score of a patient. The models of the proposed workflow were trained and tested on four publicly available X-ray datasets of COVID-19 patients and two X-ray datasets of patients with no pulmonary pathology. Based on a combined dataset consisting of 580 COVID-19 patients and 784 patients with no disorders, our best-performing algorithm is based on a combination of DeepLabV3\u2009+\u2009, for lung segmentation, and MA-Net, for disease segmentation. The proposed algorithms' mean absolute error (MAE) of 0.30 is significantly reduced in comparison to established COVID-19 algorithms; BS-net and COVID-Net-S, possessing MAEs of 2.52 and 1.83 respectively. Moreover, the proposed two-stage workflow was not only more accurate but also computationally efficient, it was approximately 11 times faster than the mentioned methods. In summary, we proposed an accurate, time-efficient, and versatile approach for segmentation and scoring of lung diseases illustrated for COVID-19 and with broader future applications for pneumonia, tuberculosis, pneumothorax, amongst others.", "journal": "Scientific reports", "date": "2022-07-28", "authors": ["Viacheslav VDanilov", "DianaLitmanovich", "AlexProutski", "AlexanderKirpich", "DatoNefaridze", "AlexKarpovsky", "YuriyGankin"], "doi": "10.1038/s41598-022-15013-z\n10.2139/ssrn.3685938\n10.1093/cid/ciaa1012\n10.1016/j.jaci.2020.04.006\n10.1016/j.cmi.2020.04.012\n10.1148/ryct.2020200034\n10.1148/radiol.2020200527\n10.1016/j.chest.2020.04.003\n10.1007/s11547-020-01202-1\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2021.100835\n10.1016/j.compbiomed.2020.103869\n10.1007/s13755-020-00116-6\n10.1016/j.eswa.2020.114054\n10.1007/s42600-020-00091-7\n10.1109/ACCESS.2020.3025372\n10.17632/8gf9vpkhgy.1\n10.17632/36fjrg9s69.1\n10.1016/j.media.2021.102046\n10.1038/s41598-021-88538-4\n10.1177/0885066603251897\n10.1371/journal.pone.0093885\n10.1186/s12931-019-1201-0\n10.1038/s41572-018-0051-2\n10.1371/journal.pone.0197418\n10.1186/s12880-015-0103-y\n10.1136/thoraxjnl-2017-211280\n10.3389/fphys.2021.672823\n10.1186/s12890-020-01286-5\n10.1148/radiol.2020201160\n10.1038/s41598-020-79470-0\n10.1016/j.ijid.2020.05.021\n10.1007/s00330-020-07270-1\n10.11613/BM.2012.031\n10.1109/TIP.2010.2044963\n10.1016/j.ijmedinf.2014.10.004\n10.1016/j.afjem.2020.09.009"}
{"title": "Mortality Prediction Analysis among COVID-19 Inpatients Using Clinical Variables and Deep Learning Chest Radiography Imaging Features.", "abstract": "The emergence of the COVID-19 pandemic over a relatively brief interval illustrates the need for rapid data-driven approaches to facilitate clinical decision making. We examined a machine learning process to predict inpatient mortality among COVID-19 patients using clinical and chest radiographic data. Modeling was performed with a de-identified dataset of encounters prior to widespread vaccine availability. Non-imaging predictors included demographics, pre-admission clinical history, and past medical history variables. Imaging features were extracted from chest radiographs by applying a deep convolutional neural network with transfer learning. A multi-layer perceptron combining 64 deep learning features from chest radiographs with 98 patient clinical features was trained to predict mortality. The Local Interpretable Model-Agnostic Explanations (LIME) method was used to explain model predictions. Non-imaging data alone predicted mortality with an ROC-AUC of 0.87 \u00b1 0.03 (mean \u00b1 SD), while the addition of imaging data improved prediction slightly (ROC-AUC: 0.91 \u00b1 0.02). The application of LIME to the combined imaging and clinical model found HbA1c values to contribute the most to model prediction (17.1 \u00b1 1.7%), while imaging contributed 8.8 \u00b1 2.8%. Age, gender, and BMI contributed 8.7%, 8.2%, and 7.1%, respectively. Our findings demonstrate a viable explainable AI approach to quantify the contributions of imaging and clinical data to COVID mortality predictions.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-07-28", "authors": ["Xuan VNguyen", "EnginDikici", "SemaCandemir", "Robyn LBall", "Luciano MPrevedello"], "doi": "10.3390/tomography8040151\n10.1038/s41586-020-2008-3\n10.1148/radiol.2020200490\n10.1002/path.5549\n10.1148/radiol.2020200642\n10.1109/RBME.2020.2987975\n10.1016/j.bbe.2020.08.008\n10.1038/s41598-020-76550-z\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2993291\n10.1148/radiol.2020204226\n10.1186/s40537-016-0043-6\n10.1007/s10278-013-9622-7\n10.1007/978-3-319-24574-4_28\n10.2214/ajr.174.1.1740071\n10.1016/j.media.2005.02.002\n10.1371/journal.pone.0190069\n10.1109/ACCESS.2021.3086020\n10.1109/ACCESS.2020.2976199\n10.1016/S2589-7500(21)00039-X\n10.1007/s00330-022-08588-8\n10.1183/13993003.02113-2020\n10.7717/peerj.10337\n10.1186/s12911-021-01742-0\n10.7717/peerj-cs.889\n10.3389/fdgth.2021.681608\n10.3390/diagnostics11081383\n10.1002/dmrr.3476"}
{"title": "Federated Learning Approach with Pre-Trained Deep Learning Models for COVID-19 Detection from Unsegmented CT images.", "abstract": "(1) Background: Coronavirus disease 2019 (COVID-19) is an infectious disease caused by SARS-CoV-2. Reverse transcription polymerase chain reaction (RT-PCR) remains the current gold standard for detecting SARS-CoV-2 infections in nasopharyngeal swabs. In Romania, the first reported patient to have contracted COVID-19 was officially declared on 26 February 2020. (2) Methods: This study proposes a federated learning approach with pre-trained deep learning models for COVID-19 detection. Three clients were locally deployed with their own dataset. The goal of the clients was to collaborate in order to obtain a global model without sharing samples from the dataset. The algorithm we developed was connected to our internal picture archiving and communication system and, after running backwards, it encountered chest CT changes suggestive for COVID-19 in a patient investigated in our medical imaging department on the 28 January 2020. (4) Conclusions: Based on our results, we recommend using an automated AI-assisted software in order to detect COVID-19 based on the lung imaging changes as an adjuvant diagnostic method to the current gold standard (RT-PCR) in order to greatly enhance the management of these patients and also limit the spread of the disease, not only to the general population but also to healthcare professionals.", "journal": "Life (Basel, Switzerland)", "date": "2022-07-28", "authors": ["Lucian MihaiFlorescu", "Costin TeodorStreba", "Mircea-Sebastian\u015eerb\u0103nescu", "M\u0103d\u0103linM\u0103muleanu", "Dan NicolaeFlorescu", "Rossy Vl\u0103du\u0163Teic\u0103", "Raluca ElenaNica", "Ioana AndreeaGheonea"], "doi": "10.3390/life12070958\n10.3389/fmicb.2020.631736\n10.1002/jmv.25766\n10.1056/NEJMoa2001017\n10.3390/life12010077\n10.1148/ryct.2020200034\n10.1016/j.jmoldx.2021.04.009\n10.47162/RJME.61.2.21\n10.1007/s00330-021-07937-3\n10.1111/exsy.12759\n10.1038/nature14539\n10.1016/j.patcog.2021.108081\n10.1016/j.asoc.2020.106912\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103869\n10.3390/diagnostics10060358\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2020.3010287\n10.1007/s00264-020-04609-7\n10.1016/j.cmpb.2020.105608\n10.1016/j.cmpb.2020.105581\n10.1016/j.eswa.2020.114054\n10.1016/j.compbiomed.2020.103795\n10.1007/s10489-020-01826-w\n10.2196/19569\n10.1183/13993003.00775-2020\n10.1016/j.asoc.2021.107330\n10.1109/JSEN.2021.3076767\n10.7910/DVN/6ACUZJ\n10.17632/3y55vgckg6.2\n10.1148/radiol.11092149\n10.12968/hmed.2020.0077\n10.5114/pjr.2021.103237\n10.1016/j.ijid.2014.12.007\n10.1016/j.crad.2016.06.110\n10.1148/radiographics.21.2.g01mr17403\n10.17632/ygvgkdbmvt.1\n10.7937/TCIA.2020.NNC2-0461\n10.1073/pnas.79.8.2554\n10.1109/EMBC.2017.8037515\n10.1007/s10278-021-00508-4\n10.21037/jtd.2017.03.157\n10.1167/tvst.9.2.14\n10.1364/AO.29.004790\n10.1016/j.jbi.2014.05.006\n10.1016/j.jacr.2022.03.015\n10.1109/TCOMM.2020.2990686\n10.11919/j.issn.1002-0829.215010\n10.1007/s11263-019-01228-7"}
{"title": "Online Behaviours during the COVID-19 Pandemic and Their Associations with Psychological Factors: An International Exploratory Study.", "abstract": "This cross-sectional study aimed to explore specific online behaviours and their association with a range of underlying psychological and other behavioural factors during the COVID-19 pandemic. Eight countries (Italy, Spain, the United Kingdom, Lithuania, Portugal, Japan, Hungary, and Brazil) participated in an international investigation involving 2223 participants (", "journal": "International journal of environmental research and public health", "date": "2022-07-28", "authors": ["JuliusBurkauskas", "Naomi AFineberg", "KonstantinosIoannidis", "Samuel RChamberlain", "HenriettaBowden-Jones", "IngaGriskova-Bulanova", "AistePranckeviciene", "Artemisa RDores", "Irene PCarvalho", "FernandoBarbosa", "PierluigiSimonato", "IlariaDe Luca", "RosinMooney", "Maria \u00c1ngelesG\u00f3mez-Mart\u00ednez", "ZsoltDemetrovics", "Krisztina Edina\u00c1bel", "AttilaSzabo", "HironobuFujiwara", "MamiShibata", "Alejandra RMelero-Ventola", "Eva MArroyo-Anll\u00f3", "Ricardo MSantos-Labrador", "KeiKobayashi", "FrancescoDi Carlo", "CristinaMonteiro", "GiovanniMartinotti", "OrnellaCorazza"], "doi": "10.3390/ijerph19148823\n10.1016/j.euroneuro.2018.08.004\n10.5114/biolsport.2020.96857\n10.1016/j.cobeha.2022.101179\n10.1016/j.comppsych.2020.152180\n10.1111/ajad.13066\n10.1037/pas0000870\n10.3389/fpsyt.2020.580977\n10.3389/fpsyt.2021.648501\n10.1371/journal.pone.0213060\n10.3109/00952990.2013.803111\n10.1037/ppm0000264\n10.1016/j.neubiorev.2021.03.005\n10.1007/s11920-021-01271-7\n10.1016/j.jpsychires.2020.11.004\n10.1016/j.paid.2020.110457\n10.1007/s42399-020-00309-w\n10.1007/s12144-022-02824-6\n10.1111/aphw.12051\n10.1007/s12671-020-01505-4\n10.4103/indianjpsychiatry.indianjpsychiatry_409_21\n10.1101/2020.05.10.20095844\n10.3389/fpsyt.2020.577135\n10.3389/fpsyt.2021.634464\n10.3389/fpsyg.2021.685137\n10.3389/fpsyt.2020.565769\n10.1186/s43045-022-00180-6\n10.1016/j.abrep.2020.100311\n10.55319/js.v1i1.9\n10.1089/cyber.2020.0645\n10.1038/d41586-019-02776-1\n10.1556/2006.2020.00016\n10.1556/2006.2020.00040\n10.1007/s11469-020-00358-1\n10.1556/2006.2020.00015\n10.1016/j.heliyon.2020.e05135\n10.1177/0004867412461693\n10.1556/JBA.2.2013.016\n10.1016/j.abrep.2015.03.002\n10.3389/fpsyg.2021.689058\n10.1017/S1352465813000556\n10.1002/cpp.702\n10.1016/j.eclinm.2022.101305\n10.3389/fpubh.2020.00392\n10.1016/j.neuropsychologia.2018.11.015\n10.1016/j.etdah.2021.100010\n10.1186/s12889-021-11398-0\n10.1007/s12671-021-01674-w\n10.1016/j.chb.2017.11.020\n10.1556/2006.8.2019.34\n10.3390/children7090148\n10.1016/j.bodyim.2020.02.010\n10.1097/CIN.0000000000000458\n10.1016/j.addbeh.2018.02.017\n10.1037/adb0000160\n10.1007/s10508-006-9064-0\n10.1556/2006.7.2018.134\n10.1016/j.sexol.2015.09.006\n10.1037/a0035774\n10.3389/fpsyt.2021.623508\n10.1371/journal.pone.0260386\n10.1007/s10508-021-02077-7\n10.3390/jpm11040288\n10.3390/healthcare10050948\n10.1016/j.comppsych.2021.152279\n10.1016/j.ijsu.2020.04.018\n10.15585/mmwr.mm6932a1\n10.1080/10550887.2021.1895962"}
{"title": "Bag of Tricks for Improving Deep Learning Performance on Multimodal Image Classification.", "abstract": "A comprehensive medical image-based diagnosis is usually performed across various image modalities before passing a final decision; hence, designing a deep learning model that can use any medical image modality to diagnose a particular disease is of great interest. The available methods are multi-staged, with many computational bottlenecks in between. This paper presents an improved end-to-end method of multimodal image classification using deep learning models. We present top research methods developed over the years to improve models trained from scratch and transfer learning approaches. We show that when fully trained, a model can first implicitly discriminate the imaging modality and then diagnose the relevant disease. Our developed models were applied to COVID-19 classification from chest X-ray, CT scan, and lung ultrasound image modalities. The model that achieved the highest accuracy correctly maps all input images to their respective modality, then classifies the disease achieving overall 91.07% accuracy.", "journal": "Bioengineering (Basel, Switzerland)", "date": "2022-07-26", "authors": ["Steve AAdeshina", "Adeyinka PAdedigba"], "doi": "10.3390/bioengineering9070312\n10.1111/exd.13777\n10.1109/ACCESS.2020.3016780\n10.3390/diagnostics10080565\n10.1007/s40747-021-00321-0\n10.31083/j.fbl2707198\n10.1101/2020.04.24.20078584\n10.1109/ACCESS.2020.3010287\n10.48550/arXiv.1907.08610\n10.1016/j.ibmed.2021.100034\n10.3390/bioengineering9040161"}
{"title": "A Novel Deep Learning and Ensemble Learning Mechanism for Delta-Type COVID-19 Detection.", "abstract": "Recently, the novel coronavirus disease 2019 (COVID-19) has posed many challenges to the research community by presenting grievous severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that results in a huge number of mortalities and high morbidities worldwide. Furthermore, the symptoms-based variations in virus type add new challenges for the research and practitioners to combat. COVID-19-infected patients comprise trenchant radiographic visual features, including dry cough, fever, dyspnea, fatigue, etc. Chest X-ray is considered a simple and non-invasive clinical adjutant that performs a key role in the identification of these ocular responses related to COVID-19 infection. Nevertheless, the defined availability of proficient radiologists to understand the X-ray images and the elusive aspects of disease radiographic replies to remnant the biggest bottlenecks in manual diagnosis. To address these issues, the proposed research study presents a hybrid deep learning model for the accurate diagnosing of Delta-type COVID-19 infection using X-ray images. This hybrid model comprises visual geometry group 16 (VGG16) and a support vector machine (SVM), where the VGG16 is accustomed to the identification process, while the SVM is used for the severity-based analysis of the infected people. An overall accuracy rate of 97.37% is recorded for the assumed model. Other performance metrics such as the area under the curve (AUC), precision, F-score, misclassification rate, and confusion matrix are used for validation and analysis purposes. Finally, the applicability of the presumed model is assimilated with other relevant techniques. The high identification rates shine the applicability of the formulated hybrid model in the targeted research domain.", "journal": "Frontiers in public health", "date": "2022-07-26", "authors": ["Habib UllahKhan", "SulaimanKhan", "ShahNazir"], "doi": "10.3389/fpubh.2022.875971\n10.1016/j.compbiomed.2020.103805\n10.1016/j.eswa.2020.114054\n10.1109/INMIC50486.2020.9318212\n10.1007/s10489-020-01902-1\n10.1109/MITP.2020.3036820\n10.1016/j.eswa.2020.113909\n10.1056/NEJMoa2001191\n10.1016/j.ijid.2020.01.009\n10.1056/NEJMc2001468\n10.1016/j.compeleceng.2020.106906\n10.32604/cmc.2021.013878\n10.1007/s10044-021-00970-4\n10.1038/s41598-020-76550-z\n10.1093/jamia/ocaa280\n10.3390/sym12040651\n10.1016/j.engappai.2019.03.021\n10.1016/j.advengsoft.2017.05.014\n10.1016/j.engappai.2020.103541\n10.1038/s41598-020-71294-2\n10.22581/muet1982.2101.14\n10.1177/0020294020964826"}
{"title": "A Deep Learning and Handcrafted Based Computationally Intelligent Technique for Effective COVID-19 Detection from X-ray/CT-scan Imaging.", "abstract": "The world has witnessed dramatic changes because of the advent of COVID19 in the last few days of 2019. During the last more than two years, COVID-19 has badly affected the world in diverse ways. It has not only affected human health and mortality rate but also the economic condition on a global scale. There is an urgent need today to cope with this pandemic and its diverse effects. Medical imaging has revolutionized the treatment of various diseases during the last four decades. Automated detection and classification systems have proven to be of great assistance to the doctors and scientific community for the treatment of various diseases. In this paper, a novel framework for an efficient COVID-19 classification system is proposed which uses the hybrid feature extraction approach. After preprocessing image data, two types of features i.e., deep learning and handcrafted, are extracted. For Deep learning features, two pre-trained models namely ResNet101 and DenseNet201 are used. Handcrafted features are extracted using Weber Local Descriptor (WLD). The Excitation component of WLD is utilized and features are reduced using DCT. Features are extracted from both models, handcrafted features are fused, and significant features are selected using entropy. Experiments have proven the effectiveness of the proposed model. A comprehensive set of experiments have been performed and results are compared with the existing well-known methods. The proposed technique has performed better in terms of accuracy and time.", "journal": "Journal of grid computing", "date": "2022-07-26", "authors": ["MohammedHabib", "MuhammadRamzan", "Sajid AliKhan"], "doi": "10.1007/s10723-022-09615-0\n10.1109/ACCESS.2020.2999468\n10.1007/s11063-018-09976-2\n10.1109/ACCESS.2017.2789324\n10.1007/s10723-020-09506-2\n10.1007/s10723-021-09594-8\n10.1007/s10723-021-09564-0\n10.1016/j.compbiomed.2018.03.016\n10.1007/s10723-020-09513-3\n10.1007/s10723-021-09590-y\n10.1016/j.diii.2020.03.014\n10.1016/j.chaos.2020.109944\n10.1016/j.compbiomed.2021.104453\n10.1016/j.chemolab.2020.104054\n10.1016/j.bspc.2021.102987\n10.1016/j.bbe.2021.05.013\n10.1016/j.compbiomed.2021.104306\n10.1016/j.eswa.2021.115650\n10.1016/j.bspc.2021.102602\n10.1016/j.bspc.2021.102588\n10.1016/j.clinimag.2021.07.004\n10.1016/j.iot.2021.100377\n10.1016/j.asoc.2021.107184\n10.1016/j.eswa.2021.114883\n10.1007/s10489-021-02393-4\n10.1007/s10489-020-01867-1\n10.1007/s11042-021-11192-5\n10.1023/B:VLSI.0000028532.53893.82\n10.1109/TPAMI.2009.155\n10.1109/T-C.1974.223784\n10.1109/TNNLS.2020.2966319\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1007/s13246-020-00952-6\n10.1109/ACCESS.2020.2994762"}
{"title": "Artificial intelligence enabled non-invasive T-ray imaging technique for early detection of coronavirus infected patients.", "abstract": "A new artificial intelligence (AI) supported T-Ray imaging system designed and implemented for non-invasive and non-ionizing screening for coronavirus-affected patients. The new system has the potential to replace the standard conventional X-Ray based imaging modality of virus detection. This research article reports the development of solid state room temperature terahertz source for thermograph study. Exposure time and radiation energy are optimized through several real-time experiments. During its incubation period, Coronavirus stays within the cell of the upper respiratory tract and its presence often causes an increased level of blood supply to the virus-affected cells/inter-cellular region that results in a localized increase of water content in those cells & tissues in comparison to its neighbouring normal cells. Under THz-radiation exposure, the incident energy gets absorbed more in virus-affected cells/inter-cellular region and gets heated; thus, the sharp temperature gradient is observed in the corresponding thermograph study. Additionally, structural changes in virus-affected zones make a significant contribution in getting better contrast in thermographs. Considering the effectiveness of the Artificial Intelligence (AI) analysis tool in various medical diagnoses, the authors have employed an explainable AI-assisted methodology to correctly identify and mark the affected pulmonary region for the developed imaging technique and thus validate the model. This AI-enabled non-ionizing THz-thermography method is expected to address the voids in early COVID diagnosis, at the onset of infection.", "journal": "Informatics in medicine unlocked", "date": "2022-07-26", "authors": ["SwarnavaBiswas", "SaikatAdhikari", "RiddhiChawla", "NiladriMaiti", "DineshBhatia", "PranjalPhukan", "MoumitaMukherjee"], "doi": "10.1016/j.imu.2022.101025\n10.17762/ijnpme.v7i03.66\n10.1016/j.aci.2018.08.003"}
{"title": "An efficient deep learning-based framework for tuberculosis detection using chest X-ray images.", "abstract": "Early diagnosis of tuberculosis (TB) is an essential and challenging task to prevent disease, decrease mortality risk, and stop transmission to other people. The chest X-ray (CXR) is the top choice for lung disease screening in clinics because it is cost-effective and easily accessible in most countries. However, manual screening of CXR images is a heavy burden for radiologists, resulting in a high rate of inter-observer variances. Hence, proposing a cost-effective and accurate computer aided diagnosis (CAD) system for TB diagnosis is challenging for researchers. In this research, we proposed an efficient and straightforward deep learning network called TBXNet, which can accurately classify a large number of TB CXR images. The network is based on five dual convolutions blocks with varying filter sizes of 32, 64, 128, 256 and 512, respectively. The dual convolution blocks are fused with a pre-trained layer in the fusion layer of the network. In addition, the pre-trained layer is utilized for transferring pre-trained knowledge into the fusion layer. The proposed TBXNet has achieved an accuracy of 98.98%, and 99.17% on Dataset A and Dataset B, respectively. Furthermore, the generalizability of the proposed work is validated against Dataset C, which is based on normal, tuberculous, pneumonia, and COVID-19 CXR images. The TBXNet has obtained the highest results in Precision (95.67%), Recall (95.10%), F1-score (95.38%), and Accuracy (95.10%), which is comparatively better than all other state-of-the-art methods.", "journal": "Tuberculosis (Edinburgh, Scotland)", "date": "2022-07-26", "authors": ["AhmedIqbal", "MuhammadUsman", "ZohairAhmed"], "doi": "10.1016/j.tube.2022.102234"}
{"title": "Multi-population generalizability of a deep learning-based chest radiograph severity score for COVID-19.", "abstract": "To tune and test the generalizability of a deep learning-based model for assessment of COVID-19 lung disease severity on chest radiographs (CXRs) from different patient populations. A published convolutional Siamese neural network-based model previously trained on hospitalized patients with COVID-19 was tuned using 250 outpatient CXRs. This model produces a quantitative measure of COVID-19 lung disease severity (pulmonary x-ray severity (PXS) score). The model was evaluated on CXRs from 4 test sets, including 3 from the United States (patients hospitalized at an academic medical center (N = 154), patients hospitalized at a community hospital (N = 113), and outpatients (N = 108)) and 1 from Brazil (patients at an academic medical center emergency department (N = 303)). Radiologists from both countries independently assigned reference standard CXR severity scores, which were correlated with the PXS scores as a measure of model performance (Pearson R). The Uniform Manifold Approximation and Projection (UMAP) technique was used to visualize the neural network results. Tuning the deep learning model with outpatient data showed high model performance in 2 United States hospitalized patient datasets (R = 0.88 and R = 0.90, compared to baseline R = 0.86). Model performance was similar, though slightly lower, when tested on the United States outpatient and Brazil emergency department datasets (R = 0.86 and R = 0.85, respectively). UMAP showed that the model learned disease severity information that generalized across test sets. A deep learning model that extracts a COVID-19 severity score on CXRs showed generalizable performance across multiple populations from 2 continents, including outpatients and hospitalized patients.", "journal": "Medicine", "date": "2022-07-23", "authors": ["Matthew DLi", "Nishanth TArun", "MehakAggarwal", "SharutGupta", "PraveerSingh", "Brent PLittle", "Dexter PMendoza", "Gustavo C ACorradi", "Marcelo STakahashi", "Suely FFerraciolli", "Marc DSucci", "MinLang", "Bernardo CBizzo", "IttaiDayan", "Felipe CKitamura", "JayashreeKalpathy-Cramer"], "doi": "10.1097/MD.0000000000029587"}
{"title": "COVIDx-US: An Open-Access Benchmark Dataset of Ultrasound Imaging Data for AI-Driven COVID-19 Analytics.", "abstract": "The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. Apart from the global health crises, the pandemic has also caused significant economic and financial difficulties and socio-physiological implications. Effective screening, triage, treatment planning, and prognostication of outcome play a key role in controlling the pandemic. Recent studies have highlighted the role of point-of-care ultrasound imaging for COVID-19 screening and prognosis, particularly given that it is non-invasive, globally available, and easy-to-sanitize. COVIDx-US Dataset: Motivated by these attributes and the promise of artificial intelligence tools to aid clinicians, we introduce COVIDx-US, an open-access benchmark dataset of COVID-19 related ultrasound imaging data. The COVIDx-US dataset was curated from multiple data sources and its current version, i.e., v1.5., consists of 173 ultrasound videos and 21,570 processed images across 147 patients with COVID-19 infection, non-COVID-19 infection, other lung diseases/conditions, as well as normal control cases.\nThe COVIDx-US dataset was released as part of a large open-source initiative, the COVID-Net initiative, and will be continuously growing, as more data sources become available. To the best of the authors' knowledge, COVIDx-US is the first and largest open-access fully-curated benchmark lung ultrasound imaging dataset that contains a standardized and unified lung ultrasound score per video file, providing better interpretation while enabling other research avenues such as severity assessment. In addition, the dataset is reproducible, easy-to-use, and easy-to-scale thanks to the well-documented modular design.", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2022-07-23", "authors": ["AshkanEbadi", "PengchengXi", "AlexanderMacLean", "AdrianFlorea", "St\u00e9phaneTremblay", "SonnyKohli", "AlexanderWong"], "doi": "10.31083/j.fbl2707198"}
{"title": "Ftl-CoV19: A Transfer Learning Approach to Detect COVID-19.", "abstract": "COVID-19 is an infectious and contagious disease caused by the new coronavirus. The total number of cases is over 19 million and continues to grow. A common symptom noticed among COVID-19 patients is lung infection that results in breathlessness, and the lack of essential resources such as testing, oxygen, and ventilators enhances its severity. Chest X-ray can be used to design and develop a COVID-19 detection mechanism for a quicker diagnosis using AI and machine learning techniques. Due to this silver lining, various new COVID-19 detection techniques and prediction models have been introduced in recent times based on chest radiography images. However, due to a high level of unpredictability and the absence of essential data, standard models have showcased low efficiency and also suffer from overheads and complexities. This paper proposes a model fine tuning transfer learning-coronavirus 19 (Ftl-CoV19) for COVID-19 detection through chest X-rays, which embraces the ideas of transfer learning in pretrained VGG16 model with including combination of convolution, max pooling, and dense layer at different stages of model. Ftl-CoV19 reported promising experimental results; it observed training and validation accuracy of 98.82% and 99.27% with precision of 100%, recall of 98%, and F1 score of 99%. These results outperformed other conventional state of arts such as CNN, ResNet50, InceptionV3, and Xception.", "journal": "Computational intelligence and neuroscience", "date": "2022-07-23", "authors": ["TarishiSingh", "PraneetSaurabh", "DhananjayBisen", "LalitKane", "MayankPathak", "G RSinha"], "doi": "10.1155/2022/1953992\n10.15557/pimr.2020.0024\n10.1016/j.genrep.2020.100756\n10.1109/TAI.2021.3062771\n10.1109/tmi.2020.2995508\n10.1109/ACCESS.2020.2997311\n10.1109/RBME.2020.2987975\n10.1109/CANDO-EPE51100.2020.9337794\n10.1109/TCYB.2019.2950779\n10.1016/j.numecd.2020.07.031\n10.1155/2020/9756518\n10.1101/2020.10.13.20212035\n10.1101/2020.10.13.20212035\n10.1109/mpuls.2020.3008354\n10.1109/access.2020.3009328\n10.1109/tmi.2020.2993291\n10.1109/tmi.2020.2995965\n10.1109/ACCESS.2018.2814605\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2019.2946000\n10.17632/9xkhgts2s6.1\n10.1007/s10489-020-01826-w\n10.48550/arXiv.1409.1556\n10.3390/s19163556\n10.1007/s11042-020-10038-w"}
{"title": "Automated diagnosis and prognosis of COVID-19 pneumonia from initial ER chest X-rays using deep learning.", "abstract": "Airspace disease as seen on chest X-rays is an important point in triage for patients initially presenting to the emergency department with suspected COVID-19 infection. The purpose of this study is to evaluate a previously trained interpretable deep learning algorithm for the diagnosis and prognosis of COVID-19 pneumonia from chest X-rays obtained in the ED.\nThis retrospective study included 2456 (50% RT-PCR positive for COVID-19) adult patients who received both a chest X-ray and SARS-CoV-2 RT-PCR test from January 2020 to March of 2021 in the emergency department at a single U.S.\nA total of 2000 patients were included as an additional training cohort and 456 patients in the randomized internal holdout testing cohort for a previously trained Siemens AI-Radiology Companion deep learning convolutional neural network algorithm. Three cardiothoracic fellowship-trained radiologists systematically evaluated each chest X-ray and generated an airspace disease area-based severity score which was compared against the same score produced by artificial intelligence. The interobserver agreement, diagnostic accuracy, and predictive capability for inpatient outcomes were assessed. Principal statistical tests used in this study include both univariate and multivariate logistic regression.\nOverall ICC was 0.820 (95% CI 0.790-0.840). The diagnostic AUC for SARS-CoV-2 RT-PCR positivity was 0.890 (95% CI 0.861-0.920) for the neural network and 0.936 (95% CI 0.918-0.960) for radiologists. Airspace opacities score by AI alone predicted ICU admission (AUC\u2009=\u20090.870) and mortality (0.829) in all patients. Addition of age and BMI into a multivariate log model improved mortality prediction (AUC\u2009=\u20090.906).\nThe deep learning algorithm provides an accurate and interpretable assessment of the disease burden in COVID-19 pneumonia on chest radiographs. The reported severity scores correlate with expert assessment and accurately predicts important clinical outcomes. The algorithm contributes additional prognostic information not currently incorporated into patient management.", "journal": "BMC infectious diseases", "date": "2022-07-22", "authors": ["Jordan HChamberlin", "GilbertoAquino", "SophiaNance", "AndrewWortham", "NathanLeaphart", "NamrataPaladugu", "SeanBrady", "HenryBaird", "MatthewFiegel", "LoganFitzpatrick", "MadisonKocher", "FlorinGhesu", "AwaisMansoor", "PhilippHoelzer", "MathisZimmermann", "W EnnisJames", "D JamesonDennis", "Brian AHouston", "Ismail MKabakus", "DhirajBaruah", "U JosephSchoepf", "Jeremy RBurt"], "doi": "10.1186/s12879-022-07617-7\n10.1136/bmj.m2426\n10.1007/s11547-020-01232-9\n10.1016/j.ijid.2020.05.021\n10.1186/s41747-020-00195-w\n10.1148/radiol.2021219021\n10.1148/ryct.2020200028\n10.1186/s43055-020-00296-x\n10.1148/ryct.2020200337\n10.1148/radiol.2021219022\n10.1136/bmj.m1328\n10.1016/j.jiph.2020.06.028\n10.1109/RBME.2020.2987975\n10.1038/s41598-021-93719-2\n10.1038/s42256-021-00307-0\n10.1148/radiol.2020202944\n10.1148/ryai.2020200079\n10.1371/journal.pone.0236621\n10.1148/ryct.2020200280\n10.1148/ryai.2020200029\n10.1001/jamanetworkopen.2021.41096\n10.1109/TPAMI.2018.2858826\n10.1371/journal.pmed.1002707\n10.1016/j.chest.2020.04.003\n10.1148/ryai.2020190043\n10.1016/j.compbiomed.2021.104665\n10.18280/ts.370313"}
{"title": "Human versus Artificial Intelligence-Based Echocardiographic Analysis as a Predictor of Outcomes: An Analysis from the World Alliance Societies of Echocardiography COVID Study.", "abstract": "Transthoracic echocardiography is the leading cardiac imaging modality for patients admitted with COVID-19, a condition of high short-term mortality. The aim of this study was to test the hypothesis that artificial intelligence (AI)-based analysis of echocardiographic images could predict mortality more accurately than conventional analysis by a human expert.\nPatients admitted to 13 hospitals for acute COVID-19 who underwent transthoracic echocardiography were included. Left ventricular ejection fraction (LVEF) and left ventricular longitudinal strain (LVLS) were obtained manually by multiple expert readers and by automated AI software. The ability of the manual and AI analyses to predict all-cause mortality was compared.\nIn total, 870 patients were enrolled. The mortality rate was 27.4% after a mean follow-up period of 230\u00a0\u00b1\u00a0115\u00a0days. AI analysis had lower variability than manual analysis for both LVEF (P\u00a0=\u00a0.003) and LVLS (P\u00a0=\u00a0.005). AI-derived LVEF and LVLS were predictors of mortality in univariable and multivariable regression analysis (odds ratio, 0.974 [95% CI, 0.956-0.991; P\u00a0=\u00a0.003] for LVEF; odds ratio, 1.060 [95% CI, 1.019-1.105; P\u00a0=\u00a0.004] for LVLS), but LVEF and LVLS obtained by manual analysis were not. Direct comparison of the predictive value of AI versus manual measurements of LVEF and LVLS showed that AI was significantly better (P\u00a0=\u00a0.005 and P\u00a0=\u00a0.003, respectively). In addition, AI-derived LVEF and LVLS had more significant and stronger correlations to other objective biomarkers of acute disease than manual reads.\nAI-based analysis of LVEF and LVLS had similar feasibility as manual analysis, minimized variability, and consequently increased the statistical power to predict mortality. AI-based, but not manual, analyses were a significant predictor of in-hospital and follow-up mortality.", "journal": "Journal of the American Society of Echocardiography : official publication of the American Society of Echocardiography", "date": "2022-07-22", "authors": ["Federico MAsch", "TineDescamps", "RizwanSarwar", "IlyaKaragodin", "Cristiane CarvalhoSingulane", "MingxingXie", "Edwin STucay", "Ana CTude Rodrigues", "Zuilma YVasquez-Ortiz", "Mark JMonaghan", "Bayardo AOrdonez Salazar", "LaurieSoulat-Dufour", "AzinAlizadehasl", "AtoosaMostafavi", "AntonellaMoreo", "RodolfoCitro", "AkhilNarang", "ChunWu", "KarimaAddetia", "RossUpton", "Gary MWoodward", "Roberto MLang", "NoneNone"], "doi": "10.1016/j.echo.2022.07.004"}
{"title": "Resting-state functional connectome predicts individual differences in depression during COVID-19 pandemic.", "abstract": "Stressful life events are significant risk factors for depression, and increases in depressive symptoms have been observed during the COVID-19 pandemic. The aim of this study is to explore the neural makers for individuals' depression during COVID-19, using connectome-based predictive modeling (CPM). Then we tested whether these neural markers could be used to identify groups at high/low risk for depression with a longitudinal dataset. The results suggested that the high-risk group demonstrated a higher level and increment of depression during the pandemic, as compared to the low-risk group. Furthermore, a support vector machine (SVM) algorithm was used to discriminate major depression disorder patients and healthy controls, using neural features defined by CPM. The results confirmed the CPM's ability for capturing the depression-related patterns with individuals' resting-state functional connectivity signature. The exploration for the anatomy of these functional connectivity features emphasized the role of an emotion-regulation circuit and an interoception circuit in the neuropathology of depression. In summary, the present study augments current understanding of potential pathological mechanisms underlying depression during an acute and unpredictable life-threatening event and suggests that resting-state functional connectivity may provide potential effective neural markers for identifying susceptible populations. (PsycInfo Database Record (c) 2022 APA, all rights reserved).", "journal": "The American psychologist", "date": "2022-07-22", "authors": ["YuMao", "QunlinChen", "DongtaoWei", "WenjingYang", "JiangzhouSun", "YaxuYu", "KaixiangZhuang", "XiaoqinWang", "LiHe", "TingyongFeng", "XuLei", "QinghuaHe", "HongChen", "ShukaiDuan", "JiangQiu"], "doi": "10.1037/amp0001031"}
{"title": "Two-stage hybrid network for segmentation of COVID-19 pneumonia lesions in CT images: a multicenter study.", "abstract": "COVID-19 has been spreading continuously since its outbreak, and the detection of its manifestations in the lung via chest computed tomography (CT) imaging is essential to investigate the diagnosis and prognosis of COVID-19 as an indispensable step. Automatic and accurate segmentation of infected lesions is highly required for fast and accurate diagnosis and further assessment of COVID-19 pneumonia. However, the two-dimensional methods generally neglect the intraslice context, while the three-dimensional methods usually have high GPU memory consumption and calculation cost. To address these limitations, we propose a two-stage hybrid UNet to automatically segment infected regions, which is evaluated on the multicenter data obtained from seven hospitals. Moreover, we train a 3D-ResNet for COVID-19 pneumonia screening. In segmentation tasks, the Dice coefficient reaches 97.23% for lung segmentation and 84.58% for lesion segmentation. In classification tasks, our model can identify COVID-19 pneumonia with an area under the receiver-operating characteristic curve value of 0.92, an accuracy of 92.44%, a sensitivity of 93.94%, and a specificity of 92.45%. In comparison with other state-of-the-art methods, the proposed approach could be implemented as an efficient assisting tool for radiologists in COVID-19 diagnosis from CT images.", "journal": "Medical & biological engineering & computing", "date": "2022-07-21", "authors": ["YaxinShang", "ZechenWei", "HuiHui", "XiaohuLi", "LiangLi", "YongqiangYu", "LigongLu", "LiLi", "HongjunLi", "QiYang", "MeiyunWang", "MeixiaoZhan", "WeiWang", "GuanghaoZhang", "XiangjunWu", "LiWang", "JieLiu", "JieTian", "YunfeiZha"], "doi": "10.1007/s11517-022-02619-8\n10.1016/j.micinf.2020.02.002\n10.1080/20477724.2020.1725339\n10.1148/radiol.2020200230\n10.1007/s11517-020-02299-2\n10.1109/ACCESS.2020.3001973\n10.1109/JBHI.2020.3034296\n10.1016/j.jare.2020.03.005\n10.1148/ryct.2020200075\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.3001810\n10.1109/TMI.2018.2845918\n10.1007/978-3-030-00889-5_1\n10.1109/TMI.2019.2959609\n10.1007/s00521-005-0467-y\n10.1021/acs.jmedchem.9b02147\n10.1016/j.patrec.2019.03.022\n10.1016/j.measurement.2018.11.006\n10.1080/07038992.1998.10874685\n10.1109/TPAMI.2016.2572683\n10.1109/TPAMI.2016.2644615"}
{"title": "The New Brain Age.", "abstract": "The pandemic is transforming neurology. Long COVID will linger, neurologic diseases will increase, and technology, artificial intelligence, and new virtual worlds will usher a new age of the brain and new roles for neurologists. The pandemic has compelled international collaboration, greatly increased communications, and accelerated drug and vaccines approvals. It also dramatized the close interconnection of cognitive, mental, and social health and their relevance to building back better health, education, work, and leisure. Brain health is the key to health, productivity, and well-being. Neurologists are best placed to lead brain knowledge integration and application through the unifying theme of brain health by becoming advocates, healers, and guardians of the brain.", "journal": "Neurology", "date": "2022-07-20", "authors": ["VladimirHachinski"], "doi": "10.1212/WNL.0000000000201059"}
{"title": "Simplified Transfer Learning for Chest Radiography Models Using Less Data.", "abstract": "Background Developing deep learning models for radiology requires large data sets and substantial computational resources. Data set size limitations can be further exacerbated by distribution shifts, such as rapid changes in patient populations and standard of care during the COVID-19 pandemic. A common partial mitigation is transfer learning by pretraining a \"generic network\" on a large nonmedical data set and then fine-tuning on a task-specific radiology data set. Purpose To reduce data set size requirements for chest radiography deep learning models by using an advanced machine learning approach (supervised contrastive [SupCon] learning) to generate chest radiography networks. Materials and Methods SupCon helped generate chest radiography networks from 821\u2009544 chest radiographs from India and the United States. The chest radiography networks were used as a starting point for further machine learning model development for 10 prediction tasks (eg, airspace opacity, fracture, tuberculosis, and COVID-19 outcomes) by using five data sets comprising 684\u2009955 chest radiographs from India, the United States, and China. Three model development setups were tested (linear classifier, nonlinear classifier, and fine-tuning the full network) with different data set sizes from eight to 8", "journal": "Radiology", "date": "2022-07-20", "authors": ["Andrew BSellergren", "ChristinaChen", "ZaidNabulsi", "YuanzhenLi", "AaronMaschinot", "AaronSarna", "JennyHuang", "CharlesLau", "Sreenivasa RajuKalidindi", "MozziyarEtemadi", "FlorenciaGarcia-Vicente", "DavidMelnick", "YunLiu", "KrishEswaran", "DanielTse", "NeeralBeladia", "DilipKrishnan", "ShravyaShetty"], "doi": "10.1148/radiol.212482"}
{"title": "COVID-19 Classification from Chest X-Ray Images: A Framework of Deep Explainable Artificial Intelligence.", "abstract": "COVID-19 detection and classification using chest X-ray images is a current hot research topic based on the important application known as medical image analysis. To halt the spread of COVID-19, it is critical to identify the infection as soon as possible. Due to time constraints and the expertise of radiologists, manually diagnosing this infection from chest X-ray images is a difficult and time-consuming process. Artificial intelligence techniques have had a significant impact on medical image analysis and have also introduced several techniques for COVID-19 diagnosis. Deep learning and explainable AI have shown significant popularity among AL techniques for COVID-19 detection and classification. In this work, we propose a deep learning and explainable AI technique for the diagnosis and classification of COVID-19 using chest X-ray images. Initially, a hybrid contrast enhancement technique is proposed and applied to the original images that are later utilized for the training of two modified deep learning models. The deep transfer learning concept is selected for the training of pretrained modified models that are later employed for feature extraction. Features of both deep models are fused using improved canonical correlation analysis that is further optimized using a hybrid algorithm named Whale-Elephant Herding. Through this algorithm, the best features are selected and classified using an extreme learning machine (ELM). Moreover, the modified deep models are utilized for Grad-CAM visualization. The experimental process was conducted on three publicly available datasets and achieved accuracies of 99.1, 98.2, and 96.7%, respectively. Moreover, the ablation study was performed and showed that the proposed accuracy is better than the other methods.", "journal": "Computational intelligence and neuroscience", "date": "2022-07-19", "authors": ["Muhammad AttiqueKhan", "MariumAzhar", "KainatIbrar", "AbdullahAlqahtani", "ShtwaiAlsubai", "AdelBinbusayyis", "Ye JinKim", "ByoungcholChang"], "doi": "10.1155/2022/4254631\n10.3390/diagnostics12030741\n10.1038/s41598-022-10723-w\n10.1371/journal.pone.0246772\n10.1016/j.genhosppsych.2020.07.006\n10.1016/j.eng.2020.04.010\n10.1002/1096-9071(200103)63:3<259::aid-jmv1010>3.0.co;2-x\n10.1016/j.eswa.2020.114054\n10.7326/m20-1382\n10.1007/s00330-021-07715-1\n10.1109/tmi.2016.2553401\n10.1109/trpms.2019.2896399\n10.1049/trit.2019.0028\n10.1016/j.compag.2021.106081\n10.3390/e23060667\n10.1007/978-3-030-27272-2_14\n10.1007/s11042-019-08111-0\n10.1080/03772063.2017.1331757\n10.1007/s11517-020-02302-w\n10.1016/j.neucom.2021.03.035\n10.1109/tpami.2016.2644615\n10.1016/j.artmed.2021.102114\n10.1109/tip.2021.3058783\n10.3390/s21020455\n10.1038/s41598-021-95680-6\n10.36227/techrxiv.15135846.v1\n10.1109/jbhi.2021.3074893\n10.3390/healthcare9091099\n10.1007/s00521-020-05636-6\n10.3390/a14110337\n10.3390/s21165657\n10.1038/s42256-021-00338-7\n10.1109/cbms52027.2021.00103\n10.1016/j.advengsoft.2016.01.008\n10.1109/iscbi.2015.8\n10.1109/iccv.2017.74"}
{"title": "The effect of different degrees of lockdown and self-identified gender on anxiety, depression and suicidality during the COVID-19 pandemic: Data from the international COMET-G study.", "abstract": "During the COVID-19 pandemic various degrees of lockdown were applied by countries around the world. It is considered that such measures have an adverse effect on mental health but the relationship of measure intensity with the mental health effect has not been thoroughly studied. Here we report data from the larger COMET-G study pertaining to this question.\nDuring the COVID-19 pandemic, data were gathered with an online questionnaire from 55,589 participants from 40 countries (64.85% females aged 35.80\u00a0\u00b1 13.61; 34.05% males aged 34.90\u00b113.29 and 1.10% other aged 31.64\u00b113.15). Anxiety was measured with the STAI, depression with the CES-D and suicidality with the RASS. Distress and probable depression were identified with the use of a previously developed cut-off and algorithm respectively.\nIt included the calculation of Relative Risk (RR), Factorial ANOVA and Multiple backwards stepwise linear regression analysis RESULTS: Approximately two-thirds were currently living under significant restrictions due to lockdown. For both males and females the risk to develop clinical depression correlated significantly with each and every level of increasing lockdown degree (RR 1.72 and 1.90 respectively). The combined lockdown and psychiatric history increased RR to 6.88 The overall relationship of lockdown with severity of depression, though significant was small.\nThe current study is the first which reports an almost linear relationship between lockdown degree and effect in mental health. Our findings, support previous suggestions concerning the need for a proactive targeted intervention to protect mental health more specifically in vulnerable groups.", "journal": "Psychiatry research", "date": "2022-07-16", "authors": ["Konstantinos NFountoulakis", "Grigorios NKarakatsoulis", "SeriAbraham", "KristinaAdorjan", "Helal UddinAhmed", "Renato DAlarc\u00f3n", "KiyomiArai", "Sani SalihuAuwal", "MichaelBerk", "SarahBjedov", "JulioBobes", "TeresaBobes-Bascaran", "JulieBourgin-Duchesnay", "Cristina AnaBredicean", "LaurynasBukelskis", "AkakiBurkadze", "Indira Indiana CabreraAbud", "RubyCastilla-Puentes", "MarceloCetkovich", "HectorColon-Rivera", "RicardoCorral", "CarlaCortez-Vergara", "PiirikaCrepin", "DomenicoDe Berardis", "Sergio ZamoraDelgado", "DavidDe Lucena", "AvinashDe Sousa", "Ramona DiStefano", "SeetalDodd", "Livia PriyankaElek", "AnnaElissa", "BertaErdelyi-Hamza", "GamzeErzin", "Martin JEtchevers", "PeterFalkai", "AdrianaFarcas", "IlyaFedotov", "ViktoriiaFilatova", "Nikolaos KFountoulakis", "IrynaFrankova", "FrancescoFranza", "PedroFrias", "TatianaGalako", "Cristian JGaray", "LeticiaGarcia-\u00c1lvarez", "Maria PazGarc\u00eda-Portilla", "XeniaGonda", "Tomasz MGondek", "Daniela MoreraGonz\u00e1lez", "HilaryGould", "PaoloGrandinetti", "ArturoGrau", "VioletaGroudeva", "MichalHagin", "TakayukiHarada", "Tasdik MHasan", "Nurul AzreenHashim", "JanHilbig", "SahadatHossain", "RossitzaIakimova", "MonaIbrahim", "FeliciaIftene", "YuliaIgnatenko", "MatiasIrarrazaval", "ZalihaIsmail", "JamilaIsmayilova", "AsafJacobs", "MiroJakovljevi\u0107", "NenadJak\u0161i\u0107", "AfzalJaved", "Helin YilmazKafali", "SagarKaria", "OlgaKazakova", "DoaaKhalifa", "OlenaKhaustova", "SteveKoh", "SvetlanaKopishinskaia", "KorneliiaKosenko", "Sotirios AKoupidis", "IllesKovacs", "BarbaraKulig", "AlishaLalljee", "JustineLiewig", "AbdulMajid", "EvgeniiaMalashonkova", "KhameliaMalik", "Najma IqbalMalik", "GulayMammadzada", "BilveshMandalia", "DonatellaMarazziti", "DarkoMar\u010dinko", "StephanieMartinez", "EimantasMatiekus", "GabrielaMejia", "Roha SaeedMemon", "Xarah Elenne MezaMart\u00ednez", "DaliaMickevi\u010di\u016bt\u0117", "RoumenMilev", "MuftauMohammed", "AlejandroMolina-L\u00f3pez", "PetrMorozov", "Nuru SuleimanMuhammad", "FilipMusta\u010d", "Mika SNaor", "AmiraNassieb", "AlvydasNavickas", "TarekOkasha", "MilenaPandova", "Anca-LiviaPanfil", "LiliyaPanteleeva", "IonPapava", "Mikaella EPatsali", "AlexeyPavlichenko", "BojanaPejuskovic", "Mariana PintoDa Costa", "MikhailPopkov", "DinaPopovic", "Nor Jannah NasutionRaduan", "Francisca VargasRam\u00edrez", "ElmarsRancans", "SalmiRazali", "FedericoRebok", "AnnaRewekant", "Elena Ninoska ReyesFlores", "Mar\u00eda TeresaRivera-Encinas", "PilarSaiz", "Manuel S\u00e1nchezde Carmona", "David SaucedoMart\u00ednez", "Jo AnneSaw", "G\u00f6rkemSaygili", "PatriciaSchneidereit", "BhumikaShah", "TomohiroShirasaka", "KetevanSilagadze", "SattiSitanggang", "OlegSkugarevsky", "AnnaSpikina", "Sridevi SiraMahalingappa", "MariaStoyanova", "AnnaSzczegielniak", "Simona ClaudiaTamasan", "GiuseppeTavormina", "Maurilio Giuseppe MariaTavormina", "Pavlos NTheodorakis", "MauricioTohen", "Eva MariaTsapakis", "DinaTukhvatullina", "IrfanUllah", "RatnarajVaidya", "Johann MVega-Dienstmaier", "JelenaVrublevska", "OliveraVukovic", "OlgaVysotska", "NataliaWidiasih", "AnnaYashikhina", "Panagiotis EPrezerakos", "DariaSmirnova"], "doi": "10.1016/j.psychres.2022.114702"}
{"title": "A Deep Learning Approach to Identify Chest Computed Tomography Features for Prediction of SARS-CoV-2 Infection Outcomes.", "abstract": "There is still an urgent need to develop effective treatments to help minimize the cases of severe COVID-19. A number of tools have now been developed and applied to address these issues, such as the use of non-contrast chest computed tomography (CT) for evaluation and grading of the associated lung damage. Here we used a deep learning approach for predicting the outcome of 1078 patients admitted into the Baqiyatallah Hospital in Tehran, Iran, suffering from COVID-19 infections in the first wave of the pandemic. These were classified into two groups of non-severe and severe cases according to features on their CT scans with accuracies of approximately 0.90. We suggest that incorporation of molecular and/or clinical features, such as multiplex immunoassay or laboratory findings, will increase accuracy and sensitivity of the model for COVID-19 -related predictions.", "journal": "Methods in molecular biology (Clifton, N.J.)", "date": "2022-07-16", "authors": ["AmirhosseinSahebkar", "MitraAbbasifard", "SamiraChaibakhsh", "Paul CGuest", "Mohamad AminPourhoseingholi", "AmirVahedian-Azimi", "PrashantKesharwani", "TannazJamialahmadi"], "doi": "10.1007/978-1-0716-2395-4_30\n10.1039/D0LC01156H\n10.1016/j.cie.2021.107235\n10.1016/S0140-6736(20)30360-3\n10.7150/ijms.50568\n10.1186/s12879-021-06528-3\n10.1148/radiol.2020200330\n10.1148/radiol.2020200343\n10.1007/978-3-030-59261-5_24\n10.2214/AJR.20.22975\n10.1148/radiol.2020200230\n10.1021/acsnano.0c02624\n10.5114/pjr.2020.98009\n10.1136/bmjhci-2021-100389\n10.1148/ryct.2021200510\n10.1038/s41598-021-99015-3\n10.3390/diagnostics11091712\n10.1007/978-3-030-71697-4_11\n10.1148/radiol.2462070712\n10.1186/s12879-019-4592-0\n10.1148/radiol.2363040958\n10.1016/j.ejrad.2021.109583\n10.1038/s41467-020-20657-4\n10.3348/kjr.2020.0994\n10.1016/j.compbiomed.2021.104304\n10.3348/kjr.2020.1104\n10.1038/s41598-021-93719-2"}
{"title": "Feature-level ensemble approach for COVID-19 detection using chest X-ray images.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS CoV-2), also known as the coronavirus disease 2019 (COVID-19), has threatened many human beings around the world and capsized economies at unprecedented magnitudes. Therefore, the detection of this disease using chest X-ray modalities has played a pivotal role in producing fast and accurate medical diagnoses, especially in countries that are unable to afford laboratory testing kits. However, identifying and distinguishing COVID-19 from virtually similar thoracic abnormalities utilizing medical images is challenging because it is time-consuming, demanding, and susceptible to human-based errors. Therefore, artificial-intelligence-driven automated diagnoses, which excludes direct human intervention, may potentially be used to achieve consistently accurate performances. In this study, we aimed to (i) obtain a customized dataset composed of a relatively small number of images collected from publicly available datasets; (ii) present the efficient integration of the shallow handcrafted features obtained from local descriptors, radiomics features specialized for medical images, and deep features aggregated from pre-trained deep learning architectures; and (iii) distinguish COVID-19 patients from healthy controls and pneumonia patients using a collection of conventional machine learning classifiers. By conducting extensive experiments, we demonstrated that the feature-based ensemble approach provided the best classification metrics, and this approach explicitly outperformed schemes that used only either local, radiomic, or deep features. In addition, our proposed method achieved state-of-the-art multi-class classification results compared to the baseline reference for the currently available COVID-19 datasets.", "journal": "PloS one", "date": "2022-07-15", "authors": ["Thi Kieu KhanhHo", "JeonghwanGwak"], "doi": "10.1371/journal.pone.0268430\n10.1056/NEJMoa2002032\n10.1128/JCM.00512-20\n10.1093/cid/ciaa310\n10.1002/jmv.26699\n10.1038/nature21056\n10.1002/jmri.26534\n10.1016/j.bspc.2019.101678\n10.1016/j.compmedimag.2019.101673\n10.1016/j.eswa.2018.04.021\n10.1109/ACCESS.2019.2900127\n10.1148/radiol.2019182716\n10.1016/j.media.2018.03.006\n10.3390/app9194130\n10.1080/07391102.2020.1767212\n10.1007/s40846-020-00529-4\n10.1007/s13246-020-00865-4\n10.1109/TMI.2020.2993291\n10.3390/s18030699\n10.1109/ACCESS.2019.2917266\n10.1109/ACCESS.2019.2922691\n10.1109/JBHI.2017.2775662\n10.1016/j.scs.2020.102589\n10.1016/j.eswa.2020.114054\n10.1007/s10489-020-01829-7\n10.3390/electronics9091388\n10.1016/j.mehy.2020.109761\n10.1007/s10489-020-01900-3\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1023/A:1011139631724\n10.1038/ncomms5006\n10.1158/1078-0432.CCR-14-0990\n10.1007/s10115-006-0013-y\n10.1016/j.patcog.2006.12.019\n10.1016/j.jneumeth.2015.09.019\n10.1016/j.patcog.2006.06.008\n10.4310/SII.2009.v2.n3.a8\n10.1016/j.neuroimage.2012.09.065\n10.1016/j.engappai.2015.04.003\n10.7717/peerj-cs.551\n10.1016/j.isatra.2022.02.033\n10.1117/1.JMI.4.4.041305"}
{"title": "Classification of COVID-19 from tuberculosis and pneumonia using deep learning techniques.", "abstract": "Deep learning provides the healthcare industry with the ability to analyse data at exceptional speeds without compromising on accuracy. These techniques are applicable to healthcare domain for accurate and timely prediction. Convolutional neural network is a class of deep learning methods which has become dominant in various computer vision tasks and is attracting interest across a variety of domains, including radiology. Lung diseases such as tuberculosis (TB), bacterial and viral pneumonias, and COVID-19 are not predicted accurately due to availability of very few samples for either of the lung diseases. The disease could be easily diagnosed using X-ray or CT scan images. But the number of images available for each of the disease is not as equally as other resulting in imbalance nature of input data. Conventional supervised machine learning methods do not achieve higher accuracy when trained using a lesser amount of COVID-19 data samples. Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. Data augmentation helped reduce overfitting when training a deep neural network. The SMOTE (Synthetic Minority Oversampling Technique) algorithm is used for the purpose of balancing the classes. The novelty in this research work is to apply combined data augmentation and class balance techniques before classification of tuberculosis, pneumonia, and COVID-19. The classification accuracy obtained with the proposed multi-level classification after training the model is recorded as 97.4% for TB and pneumonia and 88% for bacterial, viral, and COVID-19 classifications. The proposed multi-level classification method produced is ~8 to ~10% improvement in classification accuracy when compared with the existing methods in this area of research. The results reveal the fact that the proposed system is scalable to growing medical data and classifies lung diseases and its sub-types in less time with higher accuracy.", "journal": "Medical & biological engineering & computing", "date": "2022-07-15", "authors": ["LokeswariVenkataramana", "D Venkata VaraPrasad", "SSaraswathi", "C MMithumary", "RKarthikeyan", "NMonika"], "doi": "10.1007/s11517-022-02632-x\n10.1080/01431169508954507\n10.1007/s42979-021-00695-5\n10.3390/app10093233\n10.1016/j.cell.2018.02.010\n10.3390/app8101715\n10.1111/j.1440-1843.2006.00947.x\n10.1007/s40747-020-00199-4\n10.1016/j.bbe.2020.08.008\n10.1613/jair.953\n10.1504/IJKESDP.2011.039875\n10.1613/jair.1.11192\n10.1016/j.eswa.2021.114986\n10.1016/j.knosys.2021.107269\n10.1016/j.ins.2021.03.041\n10.1109/TMI.2020.2993291\n10.1002/ima.22613\n10.1016/j.compbiomed.2021.105134\n10.3390/app10020559\n10.1016/j.patrec.2021.08.018"}
{"title": "RLMD-PA: A Reinforcement Learning-Based Myocarditis Diagnosis Combined with a Population-Based Algorithm for Pretraining Weights.", "abstract": "Myocarditis is heart muscle inflammation that is becoming more prevalent these days, especially with the prevalence of COVID-19. Noninvasive imaging cardiac magnetic resonance (CMR) can be used to diagnose myocarditis, but the interpretation is time-consuming and requires expert physicians. Computer-aided diagnostic systems can facilitate the automatic screening of CMR images for triage. This paper presents an automatic model for myocarditis classification based on a deep reinforcement learning approach called as reinforcement learning-based myocarditis diagnosis combined with population-based algorithm (RLMD-PA) that we evaluated using the Z-Alizadeh Sani myocarditis dataset of CMR images prospectively acquired at Omid Hospital, Tehran. This model addresses the imbalanced classification problem inherent to the CMR dataset and formulates the classification problem as a sequential decision-making process. The policy of architecture is based on convolutional neural network (CNN). To implement this model, we first apply the artificial bee colony (ABC) algorithm to obtain initial values for RLMD-PA weights. Next, the agent receives a sample at each step and classifies it. For each classification act, the agent gets a reward from the environment in which the reward of the minority class is greater than the reward of the majority class. Eventually, the agent finds an optimal policy under the guidance of a particular reward function and a helpful learning environment. Experimental results based on standard performance metrics show that RLMD-PA has achieved high accuracy for myocarditis classification, indicating that the proposed model is suitable for myocarditis diagnosis.", "journal": "Contrast media & molecular imaging", "date": "2022-07-15", "authors": ["Seyed VahidMoravvej", "RoohallahAlizadehsani", "SadiaKhanam", "ZahraSobhaninia", "AfshinShoeibi", "FahimeKhozeimeh", "Zahra AlizadehSani", "Ru-SanTan", "AbbasKhosravi", "SaeidNahavandi", "Nahrizul AdibKadri", "Muhammad MokhzainiAzizan", "NArunkumar", "U RajendraAcharya"], "doi": "10.1155/2022/8733632\n10.1056/nejmra0800028\n10.1016/j.humpath.2005.07.009\n10.1007/978-3-030-92238-2_57\n10.1007/s00500-014-1334-5\n10.1007/s10479-011-0894-3\n10.1016/j.knosys.2017.11.029\n10.1007/11538059_91\n10.1145/1007730.1007735\n10.1007/s10489-020-01637-z\n10.1109/tkde.2005.95\n10.1016/j.knosys.2015.10.012\n10.1109/3477.764879\n10.1016/j.asoc.2007.05.007\n10.1609/aaai.v33i01.33013959\n10.1016/j.jcmg.2009.09.023\n10.12688/f1000research.14857.1\n10.1186/s12968-019-0550-7\n10.1161/circimaging.118.007598\n10.1007/bf03086308\n10.1016/j.acra.2013.01.004\n10.1186/s12968-017-0419-6\n10.1007/bf00994018\n10.1001/jama.2016.7653\n10.1007/978-1-4419-9326-7_5\n10.1109/72.286925\n10.1016/j.advengsoft.2013.12.007\n10.1007/978-3-642-12538-6_6\n10.1016/j.advengsoft.2016.01.008"}
{"title": "Non-iterative learning machine for identifying CoViD19 using chest X-ray images.", "abstract": "CoViD19 is a novel disease which has created panic worldwide by infecting millions of people around the world. The last significant variant of this virus, called as omicron, contributed to majority of cases in the third wave across globe. Though lesser in severity as compared to its predecessor, the delta variant, this mutation has shown higher communicable rate. This novel virus with symptoms of pneumonia is dangerous as it is communicable and hence, has engulfed entire world in a very short span of time. With the help of machine learning techniques, entire process of detection can be automated so that direct contacts can be avoided. Therefore, in this paper, experimentation is performed on CoViD19 chest X-ray images using higher order statistics with iterative and non-iterative models. Higher order statistics provide a way of analyzing the disturbances in the chest X-ray images. The results obtained are quite good with 96.64% accuracy using a non-iterative model. For fast testing of the patients, non-iterative model is preferred because it has advantage over iterative model in terms of speed. Comparison with some of the available state-of-the-art methods and some iterative methods proves efficacy of the work.", "journal": "Scientific reports", "date": "2022-07-14", "authors": ["SahilDalal", "Virendra PVishwakarma", "VarshaSisaudia", "ParulNarwal"], "doi": "10.1038/s41598-022-15268-6\n10.3346/jkms.2020.35.e150\n10.1001/jama.2021.2927\n10.1016/j.clinimag.2020.04.010\n10.1016/j.jcv.2020.104359\n10.1016/j.jcv.2020.104356\n10.5582/bst.2020.01047\n10.1016/j.ajem.2020.09.032\n10.1007/s11046-021-00528-2\n10.1007/s13246-020-00865-4\n10.1109/JBHI.2020.3037127\n10.1007/s10096-020-03901-z\n10.1109/TIP.2021.3058783\n10.1007/s10489-020-01826-w\n10.1007/s12559-020-09787-5\n10.1016/j.media.2020.101824\n10.1016/j.chaos.2020.110495\n10.1007/s10140-020-01886-y\n10.1016/j.bspc.2021.102588\n10.1016/j.compeleceng.2020.106960\n10.1016/j.sysarc.2020.101830\n10.1109/ACCESS.2020.3016780\n10.1016/j.measurement.2020.108288\n10.1016/j.asoc.2020.106912\n10.1080/07391102.2020.1788642\n10.1016/j.media.2020.101794\n10.1007/s00521-020-05437-x\n10.1109/5254.708428\n10.1016/j.ejor.2017.08.040\n10.1049/el.2017.0023\n10.1016/j.neucom.2005.12.126\n10.1016/j.neucom.2007.02.009\n10.1016/j.neucom.2007.10.008\n10.1109/TSMCB.2011.2168604\n10.1007/s13369-020-04566-8\n10.4108/eai.13-7-2018.163575\n10.1007/s11042-019-08537-6\n10.17148/IARJSET.2016.3119\n10.1038/s41598-020-79139-8\n10.1016/j.eng.2020.04.010\n10.1038/s41598-019-56847-4"}
{"title": "Detection of COVID-19 using deep learning techniques and classification methods.", "abstract": "Since the patient is not quarantined during the conclusion of the Polymerase Chain Reaction (PCR) test used in the diagnosis of COVID-19, the disease continues to spread. In this study, it was aimed to reduce the duration and amount of transmission of the disease by shortening the diagnosis time of COVID-19 patients with the use of Computed Tomography (CT). In addition, it is aimed to provide a decision support system to radiologists in the diagnosis of COVID-19. In this study, deep features were extracted with deep learning models such as ResNet-50, ResNet-101, AlexNet, Vgg-16, Vgg-19, GoogLeNet, SqueezeNet, Xception on 1345 CT images obtained from the radiography database of Siirt Education and Research Hospital. These deep features are given to classification methods such as Support Vector Machine (SVM), k Nearest Neighbor (kNN), Random Forest (RF), Decision Trees (DT), Naive Bayes (NB), and their performance is evaluated with test images. Accuracy value, F1-score and ROC curve were considered as success criteria. According to the data obtained as a result of the application, the best performance was obtained with ResNet-50 and SVM method. The accuracy was 96.296%, the F1-score was 95.868%, and the AUC value was 0.9821. The deep learning model and classification method examined in this study and found to be high performance can be used as an auxiliary decision support system by preventing unnecessary tests for COVID-19 disease.", "journal": "Information processing & management", "date": "2022-07-14", "authors": ["\u00c7inareO\u011fuz", "MeteYa\u011fano\u011flu"], "doi": "10.1016/j.ipm.2022.103025\n10.1101/2020.03.12.20027185"}
{"title": "Post-vaccination infection rates and modification of COVID-19 symptoms in vaccinated UK school-aged children and adolescents: A prospective longitudinal cohort study.", "abstract": "We aimed to explore the effectiveness of one-dose BNT162b2 vaccination upon SARS-CoV-2 infection, its effect on COVID-19 presentation, and post-vaccination symptoms in children and adolescents (CA) in the UK during periods of Delta and Omicron variant predominance.\nIn this prospective longitudinal cohort study, we analysed data from 115,775 CA aged 12-17 years, proxy-reported through the Covid Symptom Study (CSS) smartphone application. We calculated post-vaccination infection risk after one dose of BNT162b2, and described the illness profile of CA with post-vaccination SARS-CoV-2 infection, compared to unvaccinated CA, and post-vaccination side-effects.\nBetween August 5, 2021 and February 14, 2022, 25,971 UK CA aged 12-17 years received one dose of BNT162b2 vaccine. The probability of testing positive for infection diverged soon after vaccination, and was lower in CA with prior SARS-CoV-2 infection. Vaccination reduced proxy-reported infection risk (-80\u00b74% (95% CI -0\u00b782 -0\u00b778) and -53\u00b77% (95% CI -0\u00b762 -0\u00b743) at 14-30 days with Delta and Omicron variants respectively, and -61\u00b75% (95% CI -0\u00b774 -0\u00b744) and -63\u00b77% (95% CI -0\u00b768 -0.59) after 61-90 days). Vaccinated CA who contracted SARS-CoV-2 during the Delta period had milder disease than unvaccinated CA; during the Omicron period this was only evident in children aged 12-15 years. Overall disease profile was similar in both vaccinated and unvaccinated CA. Post-vaccination local side-effects were common, systemic side-effects were uncommon, and both resolved within few days (3 days in most cases).\nOne dose of BNT162b2 vaccine reduced risk of SARS-CoV-2 infection for at least 90 days in CA aged 12-17 years. Vaccine protection varied for SARS-CoV-2 variant type (lower for Omicron than Delta variant), and was enhanced by pre-vaccination SARS-CoV-2 infection. Severity of COVID-19 presentation after vaccination was generally milder, although unvaccinated CA also had generally mild disease. Overall, vaccination was well-tolerated.\nUK Government Department of Health and Social Care, Chronic Disease Research Foundation, The Wellcome Trust, UK Engineering and Physical Sciences Research Council, UK Research and Innovation London Medical Imaging & Artificial Intelligence Centre for Value Based Healthcare, UK National Institute for Health Research, UK Medical Research Council, British Heart Foundation and Alzheimer's Society, and ZOE Limited.", "journal": "The Lancet regional health. Europe", "date": "2022-07-14", "authors": ["ErikaMolteni", "Liane SCanas", "KerstinKl\u00e4ser", "JieDeng", "Sunil SBhopal", "Robert CHughes", "LiyuanChen", "BenjaminMurray", "EricKerfoot", "MichelaAntonelli", "Carole HSudre", "Joan CapdevilaPujol", "LorenzoPolidori", "AnnaMay", "Prof AlexanderHammers", "JonathanWolf", "Prof Tim DSpector", "Claire JSteves", "Prof SebastienOurselin", "MichaelAbsoud", "MarcModat", "Prof Emma LDuncan"], "doi": "10.1016/j.lanepe.2022.100429\n10.1016/S2352-4642(22)00022-0\n10.1093/infdis/jiab509\n10.1016/s2352-4642(21)00198-x\n10.1016/j.arcped.2021.07.004\n10.15585/mmwr.mm7046a4\n10.1056/nejmoa2107456\n10.1056/NEJMoa2116298\n10.1056/nejmoa2110475\n10.1056/nejmoa2110737\n10.15585/mmwr.mm7031e1\n10.1056/nejmoa2117995\n10.1056/nejmc2114290\n10.1126/science.abc0473\n10.1038/s41591-020-0916-2\n10.1038/s41597-021-01071-x\n10.3390/children9050652\n10.1016/S1473-3099(21)00224-3\n10.1007/s00415-020-10124-x\n10.1093/cid/ciab1038\n10.1111/apa.16157\n10.1111/apa.15982\n10.3201/eid2711.211886\n10.1016/S1473-3099(22)00177-3\n10.1038/s41591-021-01292-y\n10.1001/jamanetworkopen.2021.43955\n10.1056/nejmoa2107058"}
{"title": "Quantitative ultrasound image analysis of axillary lymph nodes to differentiate malignancy from reactive benign changes due to COVID-19 vaccination.", "abstract": "The aim of this study is to assess the potential of quantitative image analysis and machine learning techniques to differentiate between malignant lymph nodes and benign lymph nodes affected by reactive changes due to COVID-19 vaccination.\nIn this institutional review board-approved retrospective study, we improved our previously published artificial intelligence model, by retraining it with newly collected images and testing its performance on images containing benign lymph nodes affected by COVID-19 vaccination. All the images were acquired and selected by specialized breast-imaging radiologists and the nature of each node (benign or malignant) was assessed through a strict clinical protocol using ultrasound-guided biopsies.\nA total of 180 new images from 154 different patients were recruited: 71 images (10 cases and 61 controls) were used to retrain the old model and 109 images (36 cases and 73 controls) were used to evaluate its performance. The achieved accuracy of the proposed method was 92.7% with 77.8% sensitivity and 100% specificity at the optimal cut-off point. In comparison, the visual node inspection made by radiologists from ultrasound images reached 69.7% accuracy with 41.7% sensitivity and 83.6% specificity.\nThe results obtained in this study show the potential of the proposed techniques to differentiate between malignant lymph nodes and benign nodes affected by reactive changes due to COVID-19 vaccination. These techniques could be useful to non-invasively diagnose lymph node status in patients with suspicious reactive nodes, although larger multicenter studies are needed to confirm and validate the results.", "journal": "European journal of radiology", "date": "2022-07-13", "authors": ["DavidCoronado-Guti\u00e9rrez", "SergiGanau", "XavierBargall\u00f3", "Bel\u00e9n\u00dabeda", "MartaPorta", "EstherSanfeliu", "Xavier PBurgos-Artizzu"], "doi": "10.1016/j.ejrad.2022.110438\n10.1148/radiol.2493071483\n10.3389/fonc.2020.00053\n10.1109/TPAMI.2002.1017623"}
{"title": "Fast Prediction of Binding Affinities of SARS-CoV-2 Spike Protein and Its Mutants with Antibodies through Intermolecular Interaction Modeling-Based Machine Learning.", "abstract": "Since the introduction of the novel SARS-CoV-2 virus (COVID-19) in late 2019, various new variants have appeared with mutations that confer resistance to the vaccines and monoclonal antibodies that were developed in response to the wild-type virus. As we continue through the pandemic, an accurate and efficient methodology is needed to help predict the effects certain mutations will have on both our currently produced therapeutics and those that are in development. Using published cryo-electron microscopy and X-ray crystallography structures of the spike receptor binding domain region with currently known antibodies, in the present study, we created and cross-validated an intermolecular interaction modeling-based multi-layer perceptron machine learning approach that can accurately predict the mutation-caused shifts in the binding affinity between the spike protein (wild-type or mutant) and various antibodies. This validated artificial intelligence (AI) model was used to predict the binding affinity (", "journal": "The journal of physical chemistry. B", "date": "2022-07-12", "authors": ["Alexander HWilliams", "Chang-GuoZhan"], "doi": "10.1021/acs.jpcb.2c02123\n10.7326/m20-3100\n10.3390/vaccines9020160\n10.1038/s41591-021-01459-7\n10.1097/jac.0000000000000360\n10.4161/hv.27243\n10.1126/science.372.6549.1375\n10.1021/acs.jcim.1c01451\n10.1016/j.xcrm.2021.100255\n10.1038/s41586-021-03398-2\n10.21203/rs.3.rs-448370/v1\n10.1126/science.abf9302\n10.1093/infdis/jiab355\n10.1038/s41591-021-01678-y\n10.1038/d41586-021-03827-2\n10.1038/d41586-021-03825-4\n10.1016/j.biopha.2020.110337\n10.1007/s10822-019-00201-3\n10.1261/rna.065896.118\n10.1039/c9cp01674k\n10.1002/jcc.21372\n10.1371/journal.pcbi.1004276\n10.1016/j.sbi.2016.10.016\n10.1021/acs.jpcb.1c00869\n10.1021/acs.jpcb.1c10718\n10.1021/acs.jpcb.1c03639\n10.1093/bib/bbab383\n10.1021/acs.jpcb.2c00708\n10.1039/c6cp03670h\n10.1016/s1352-2310(97)00447-0\n10.1021/acschemneuro.0c00551\n10.1016/j.cell.2021.06.020\n10.1016/0010-4655(95)00041-d\n10.1021/ja077972s\n10.1021/jp900963n\n10.1073/pnas.2010470117\n10.1021/acs.jcim.0c00679\n10.1021/ct300418h\n10.1021/jm0606701\n10.1021/jm800607u\n10.1021/ja8055326\n10.1021/ct400341p\n10.1002/wics.162\n10.1016/s0006-3495(03)75094-2\n10.1371/journal.pone.0191882\n10.7554/eLife.43542\n10.1016/s0304-4157(97)00008-7\n10.1038/s41598-017-16450-x\n10.3390/v12091006\n10.1016/s0303-2647(03)00143-6\n10.1038/s41598-018-35196-8\n10.1126/science.abd2321\n10.1016/j.celrep.2020.108274\n10.1016/j.chom.2020.06.010\n10.1126/scitranslmed.abf1906\n10.1126/science.abd0827\n10.1038/s41586-021-04385-3"}
{"title": "Cov-Net: A computer-aided diagnosis method for recognizing COVID-19 from chest X-ray images via machine vision.", "abstract": "In the context of global pandemic Coronavirus disease 2019 (COVID-19) that threatens life of all human beings, it is of vital importance to achieve early detection of COVID-19 among symptomatic patients. In this paper, a computer aided diagnosis (CAD) model Cov-Net is proposed for accurate recognition of COVID-19 from chest X-ray images via machine vision techniques, which mainly concentrates on powerful and robust feature learning ability. In particular, a modified residual network with asymmetric convolution and attention mechanism embedded is selected as the backbone of feature extractor, after which skip-connected dilated convolution with varying dilation rates is applied to achieve sufficient feature fusion among high-level semantic and low-level detailed information. Experimental results on two public COVID-19 radiography databases have demonstrated the practicality of proposed Cov-Net in accurate COVID-19 recognition with accuracy of 0.9966 and 0.9901, respectively. Furthermore, within same experimental conditions, proposed Cov-Net outperforms other six state-of-the-art computer vision algorithms, which validates the superiority and competitiveness of Cov-Net in building highly discriminative features from the perspective of methodology. Hence, it is deemed that proposed Cov-Net has a good generalization ability so that it can be applied to other CAD scenarios. Consequently, one can conclude that this work has both practical value in providing reliable reference to the radiologist and theoretical significance in developing methods to build robust features with strong presentation ability.", "journal": "Expert systems with applications", "date": "2022-07-12", "authors": ["HanLi", "NianyinZeng", "PeishuWu", "KathyClawson"], "doi": "10.1016/j.eswa.2022.118029\n10.1148/radiol.2020200642\n10.22190/FUME190327035A\n10.1016/j.knosys.2020.106731\n10.1016/j.compbiomed.2021.104665\n10.1016/j.eswa.2020.113909\n10.1109/ACCESS.2020.3010287\n10.1109/ICCV.2019.00200\n10.1016/j.patcog.2021.108055\n10.1016/j.media.2021.102225\n10.1109/CVPR42600.2020.00165\n10.1109/CVPR.2016.90\n10.1109/CVPR.2019.00065\n10.1093/bioinformatics/btu101\n10.1016/j.bspc.2021.102764\n10.1109/CVPR.2017.243\n10.1016/j.eswa.2020.114054\n10.1016/j.bbe.2021.05.013\n10.1007/s00330-021-08050-1\n10.1007/s40846-021-00630-2\n10.1016/j.chb.2013.04.004\n10.1016/j.ijleo.2021.167100\n10.1016/j.inffus.2021.04.008\n10.1016/j.media.2020.101794\n10.1016/j.bspc.2021.102750\n10.1016/j.compbiomed.2021.104319\n10.1038/s42256-021-00307-0\n10.1016/j.asoc.2021.107522\n10.1109/CVPR.2018.00474\n10.1016/j.patcog.2021.108110\n10.1016/j.aej.2021.01.011\n10.1016/j.media.2021.102046\n10.1109/JBHI.2020.3037127\n10.1016/j.bbrc.2020.12.010\n10.1016/j.imavis.2021.104341\n10.1016/j.neucom.2021.03.034\n10.1016/j.neucom.2020.07.144\n10.1007/s00521-021-06149-6\n10.1016/j.neucom.2020.04.001\n10.1109/TIM.2022.3153997"}
{"title": "Computational Intelligence-Based Method for Automated Identification of COVID-19 and Pneumonia by Utilizing CXR Scans.", "abstract": "Chest X-ray (CXR) scans are emerging as an important diagnostic tool for the early spotting of COVID and other significant lung diseases. The recognition of visual symptoms is difficult and can take longer time by radiologists as CXR provides various signs of viral infection. Therefore, artificial intelligence-based method for automated identification of COVID by utilizing X-ray images has been found to be very promising. In the era of deep learning, effective utilization of existing pretrained generalized models is playing a decisive role in terms of time and accuracy. In this paper, the benefits of weights of existing pretrained model VGG16 and InceptionV3 have been taken. Base model has been created using pretrained models (VGG16 and InceptionV3). The last fully connected (FC) layer has been added as per the number of classes for classification of CXR in binary and multi-class classification by appropriately using transfer learning. Finally, combination of layers is made by integrating the FC layer weights of both the models (VGG16 and InceptionV3). The image dataset used for experimentation consists of healthy, COVID, pneumonia viral, and pneumonia bacterial. The proposed weight fusion method has outperformed the existing models in terms of accuracy, achieved 99.5% accuracy in binary classification over 20 epochs, and 98.2% accuracy in three-class classification over 100 epochs.", "journal": "Computational intelligence and neuroscience", "date": "2022-07-09", "authors": ["BhavanaKaushik", "DeepikaKoundal", "NeelamGoel", "AtefZaguia", "AssayeBelay", "HamzaTurabieh"], "doi": "10.1155/2022/7124199\n10.1016/j.ijid.2020.01.009\n10.4018/ijehmc.20220701.oa4\n10.1111/exsy.12749\n10.1109/jiot.2018.2802898\n10.1109/tkde.2009.191\n10.1186/s40537-016-0043-6\n10.1109/access.2018.2845399\n10.1016/j.eng.2020.04.010\n10.1080/07391102.2020.1767212\n10.1016/j.compbiomed.2020.103792\n10.1109/access.2020.2994762\n10.1016/j.mehy.2020.109761\n10.1109/tmi.2020.2993291\n10.1007/s40846-020-00529-4\n10.3390/app10020559\n10.1016/j.measurement.2019.05.076\n10.1155/2020/8828855\n10.1016/j.cmpb.2020.105532\n10.1016/j.imu.2020.100360\n10.1016/j.imu.2020.100412\n10.3233/xst-200720\n10.1016/j.asoc.2020.106580\n10.1016/j.chaos.2020.110071\n10.3390/life11111118\n10.1007/s10489-020-01826-w\n10.1109/access.2021.3095312\n10.1109/ssci50451.2021.9659919\n10.1155/2021/6919483\n10.1155/2021/8828404\n10.1109/CVPR.2016.308\n10.1016/j.jpha.2020.03.001\n10.3238/arztebl.2014.0181\n10.1016/j.chaos.2020.109947\n10.1038/s41598-020-74539-2\n10.1155/2020/8889023\n10.1109/iceiec49280.2020.9152329\n10.1016/j.compbiomed.2020.103869\n10.1007/978-3-030-01424-7_27\n10.3906/elk-2105-243\n10.1016/j.compeleceng.2022.108028\n10.1155/2022/8549707\n10.3390/s22062278"}
{"title": "PCA-Based Incremental Extreme Learning Machine (PCA-IELM) for COVID-19 Patient Diagnosis Using Chest X-Ray Images.", "abstract": "Novel coronavirus 2019 has created a pandemic and was first reported in December 2019. It has had very adverse consequences on people's daily life, healthcare, and the world's economy as well. According to the World Health Organization's most recent statistics, COVID-19 has become a worldwide pandemic, and the number of infected persons and fatalities growing at an alarming rate. It is highly required to have an effective system to early detect the COVID-19 patients to curb the further spreading of the virus from the affected person. Therefore, to early identify positive cases in patients and to support radiologists in the automatic diagnosis of COVID-19 from X-ray images, a novel method PCA-IELM is proposed based on principal component analysis (PCA) and incremental extreme learning machine. The suggested method's key addition is that it considers the benefits of PCA and the incremental extreme learning machine. Further, our strategy PCA-IELM reduces the input dimension by extracting the most important information from an image. Consequently, the technique can effectively increase the COVID-19 patient prediction performance. In addition to these, PCA-IELM has a faster training speed than a multi-layer neural network. The proposed approach was tested on a COVID-19 patient's chest X-ray image dataset. The experimental results indicate that the proposed approach PCA-IELM outperforms PCA-SVM and PCA-ELM in terms of accuracy (98.11%), precision (96.11%), recall (97.50%), F1-score (98.50%), etc., and training speed.", "journal": "Computational intelligence and neuroscience", "date": "2022-07-09", "authors": ["VinodKumar", "SougatamoyBiswas", "Dharmendra SinghRajput", "HarshitaPatel", "BasantTiwari"], "doi": "10.1155/2022/9107430\n10.1016/j.neucom.2012.02.042\n10.1007/s00521-014-1567-3\n10.1016/j.jvcir.2019.05.016\n10.1007/s00521-020-05204-y\n10.1007/s10586-021-03282-8\n10.1007/s11063-012-9253-x\n10.3390/sym11010001\n10.1007/s12559-014-9259-y\n10.1016/j.neucom.2007.02.009\n10.1109/UT.2017.7890275\n10.1109/tgrs.2017.2743102\n10.1016/j.asoc.2021.107482\n10.1016/j.cageo.2021.104877\n10.1007/s13042-015-0419-5\n10.1016/j.neucom.2005.12.126\n10.1007/s13042-020-01232-1\n10.1016/j.neucom.2007.07.025\n10.1016/j.eswa.2016.08.052\n10.3390/en12071223\n10.1504/ijmso.2018.096451\n10.1007/s11042-018-7093-z\n10.1080/07391102.2022.2034668\n10.1016/B978-0-12-816718-2.00016-6\n10.1080/10798587.2017.1316071\n10.1016/j.scs.2022.103713\n10.1155/2016/7293278\n10.1016/j.engappai.2018.07.002\n10.1007/s13042-019-01001-9\n10.1007/s11694-008-9043-3\n10.1016/j.neucom.2020.04.098\n10.1080/21642583.2020.1759156\n10.32604/cmc.2021.016957\n10.1007/s11042-019-07978-3\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.3389/fmed.2020.608525\n10.33889/ijmems.2020.5.4.052\n10.48550/arXiv.2003.11055\n10.48550/arXiv.2110.04160\n10.1101/2020.02.23.20026930\n10.1007/s00330-021-07715-1\n10.1016/j.scs.2020.102589\n10.1016/j.eng.2020.04.010"}
{"title": "Human-to-AI Interrater Agreement for Lung Ultrasound Scoring in COVID-19 Patients.", "abstract": "Lung ultrasound (LUS) has sparked significant interest during COVID-19. LUS is based on the detection and analysis of imaging patterns. Vertical artifacts and consolidations are some of the recognized patterns in COVID-19. However, the interrater reliability (IRR) of these findings has not been yet thoroughly investigated. The goal of this study is to assess IRR in LUS COVID-19 data and determine how many LUS videos and operators are required to obtain a reliable result.\nA total of 1035 LUS videos from 59 COVID-19 patients were included. Videos were randomly selected from a dataset of 1807 videos and scored by six human operators (HOs). The videos were also analyzed by artificial intelligence (AI) algorithms. Fleiss' kappa coefficient results are presented, evaluated at both the video and prognostic levels.\nFindings show a stable agreement when evaluating a minimum of 500 videos. The statistical analysis illustrates that, at a video level, a Fleiss' kappa coefficient of 0.464 (95% confidence interval [CI]\u00a0=\u00a00.455-0.473) and 0.404 (95% CI\u00a0=\u00a00.396-0.412) is obtained for pairs of HOs and for AI versus HOs, respectively. At prognostic level, a Fleiss' kappa coefficient of 0.505 (95% CI\u00a0=\u00a00.448-0.562) and 0.506 (95% CI\u00a0=\u00a00.458-0.555) is obtained for pairs of HOs and for AI versus HOs, respectively.\nTo examine IRR and obtain a reliable evaluation, a minimum of 500 videos are recommended. Moreover, the employed AI algorithms achieve results that are comparable with HOs. This research further provides a methodology that can be useful to benchmark future LUS studies.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2022-07-08", "authors": ["NoreenFatima", "FedericoMento", "AlessandroZanforlin", "AndreaSmargiassi", "ElenaTorri", "TizianoPerrone", "LibertarioDemi"], "doi": "10.1002/jum.16052"}
{"title": "CAD systems for COVID-19 diagnosis and disease stage classification by segmentation of infected regions from CT images.", "abstract": "Here propose a computer-aided diagnosis (CAD) system to differentiate COVID-19 (the coronavirus disease of 2019) patients from normal cases, as well as to perform infection region segmentation along with infection severity estimation using computed tomography (CT) images. The developed system facilitates timely administration of appropriate treatment by identifying the disease stage without reliance on medical professionals. So far, this developed model gives the most accurate, fully automatic COVID-19 real-time CAD framework.\nThe CT image dataset of COVID-19 and non-COVID-19 individuals were subjected to conventional ML stages to perform binary classification. In the feature extraction stage, SIFT, SURF, ORB image descriptors and bag of features technique were implemented for the appropriate differentiation of chest CT regions affected with COVID-19 from normal cases. This is the first work introducing this concept for COVID-19 diagnosis application. The preferred diverse database and selected features that are invariant to scale, rotation, distortion, noise etc. make this framework real-time applicable. Also, this fully automatic approach which is faster compared to existing models helps to incorporate it into CAD systems. The severity score was measured based on the infected regions along the lung field. Infected regions were segmented through a three-class semantic segmentation of the lung CT image. Using severity score, the disease stages were classified as mild if the lesion area covers less than 25% of the lung area; moderate if 25-50% and severe if greater than 50%. Our proposed model resulted in classification accuracy of 99.7% with a PNN classifier, along with area under the curve (AUC) of 0.9988, 99.6% sensitivity, 99.9% specificity and a misclassification rate of 0.0027. The developed infected region segmentation model gave 99.47% global accuracy, 94.04% mean accuracy, 0.8968 mean IoU (intersection over union), 0.9899 weighted IoU, and a mean Boundary F1 (BF) contour matching score of 0.9453, using Deepabv3+ with its weights initialized using ResNet-50.\nThe developed CAD system model is able to perform fully automatic and accurate diagnosis of COVID-19 along with infected region extraction and disease stage identification. The ORB image descriptor with bag of features technique and PNN classifier achieved the superior classification performance.", "journal": "BMC bioinformatics", "date": "2022-07-07", "authors": ["Mohammad HAlshayeji", "SilpaChandraBhasi Sindhu", "Sa'edAbed"], "doi": "10.1186/s12859-022-04818-4\n10.1186/s12859-021-04083-x\n10.1016/j.patrec.2020.10.001\n10.1007/978-3-030-01234-2_49\n10.1007/s11042-022-12608-6\n10.1016/j.imu.2020.100427\n10.1007/s10916-020-01562-1\n10.1038/s41598-020-79139-8\n10.1038/s41598-019-56847-4\n10.1016/j.bbe.2021.05.013\n10.1038/s41598-020-79139-8\n10.1371/journal.pone.0235187\n10.1038/s41598-020-79139-8\n10.1038/s41591-020-0931-3\n10.3390/diagnostics10110901\n10.1109/TMI.2020.2996645\n10.1016/j.eswa.2021.114848\n10.1371/journal.pone.0252384.t001\n10.1371/journal.pone.0236618\n10.3389/fmed.2020.557453\n10.1007/s00330-020-07033-y\n10.1016/j.cell.2020.04.045\n10.1007/s12530-021-09386-1\n10.1007/s00371-020-01814-8\n10.1017/S1431927621001653\n10.1007/978-981-15-5697-5_11\n10.1007/s11760-020-01759-4\n10.3390/bdcc5040053\n10.1007/s10489-021-02731-6\n10.1007/s00330-021-08049-8\n10.3389/fmed.2020.608525\n10.1117/1.JMI.8.S1.017502.full"}
{"title": "Automated diagnosis of COVID stages from lung CT images using statistical features in 2-dimensional flexible analytic wavelet transform.", "abstract": "The COVID-19 epidemic has been causing a global problem since December 2019. COVID-19 is highly contagious and spreads rapidly throughout the world. Thus, early detection is essential. The progression of COVID-19 lung illness has been demonstrated to be aided by chest imaging. The respiratory system is the most vulnerable component of the human body to the COVID virus. COVID can be diagnosed promptly and accurately using images from a chest X-ray and a computed tomography scan. CT scans are preferred over X-rays to rule out other pulmonary illnesses, assist venous entry, and pinpoint any new heart problems. The traditional and trending tools are physical, time-inefficient, and not more accurate. Many techniques for detecting COVID utilizing CT scan images have recently been developed, yet none of them can efficiently detect COVID at an early stage. We proposed a two-dimensional Flexible analytical wavelet transform (FAWT) based on a novel technique in this work. This method is decomposed pre-processed images into sub-bands. Then statistical-based relevant features are extracted, and principal component analysis (PCA) is used to identify robust features. After that, robust features are ranked with the help of the Student's t-value algorithm. Finally, features are applied to Least Square-SVM (RBF) for classification. According to the experimental outcomes, our model beat state-of-the-art approaches for COVID classification. This model attained better classification accuracy of 93.47%, specificity 93.34%, sensitivity 93.6% and F1-score 0.93 using tenfold cross-validation.", "journal": "Biocybernetics and biomedical engineering", "date": "2022-07-07", "authors": ["Rajneesh KumarPatel", "ManishKashyap"], "doi": "10.1016/j.bbe.2022.06.005\n10.1148/RADIOL.2020200463\n10.1007/S12652-020-02669-6\n10.1016/J.BSPC.2021.103076\n10.1016/J.BBE.2021.05.013\n10.1101/2020.04.16.20064709\n10.1007/S11042-020-09894-3\n10.1016/J.BBE.2020.08.005\n10.1016/J.BBE.2021.12.001\n10.1016/J.BBE.2021.04.006\n10.1016/J.NEUCOM.2022.02.018\n10.1016/J.ESWA.2022.116554\n10.1016/J.BSPC.2019.101761\n10.1007/S11042-019-7359-0\n10.1002/IMA.22589\n10.1007/S42417-021-00322-W\n10.1007/3-540-44673-7_8\n10.1007/S13534-017-0051-2\n10.1137/05064182X\n10.1080/01431160412331269698\n10.1016/J.PATCOG.2006.12.019\n10.1016/J.PATCOG.2008.10.023\n10.1016/j.media.2020.101910\n10.1016/J.VIRUSRES.2020.198129\n10.1016/J.PATREC.2008.10.005\n10.1016/J.COMPBIOMED.2016.10.022\n10.1007/S00330-021-07715-1\n10.1016/J.CHAOS.2020.110153\n10.1016/J.CMPBUP.2021.100022"}
{"title": "Topology optimization search of deep convolution neural networks for CT and X-ray image classification.", "abstract": "Covid-19 is a disease that can lead to pneumonia, respiratory syndrome, septic shock, multiple organ failure, and death. This pandemic is viewed as a critical component of the fight against an enormous threat to the human population. Deep convolutional neural networks have recently proved their ability to perform well in classification and dimension reduction tasks. Selecting hyper-parameters is critical for these networks. This is because the search space expands exponentially in size as the number of layers increases. All existing approaches utilize a pre-trained or designed architecture as an input. None of them takes design and pruning into account throughout the process. In fact, there exists a convolutional topology for any architecture, and each block of a CNN corresponds to an optimization problem with a large search space. However, there are no guidelines for designing a specific architecture for a specific purpose; thus, such design is highly subjective and heavily reliant on data scientists' knowledge and expertise. Motivated by this observation, we propose a topology optimization method for designing a convolutional neural network capable of classifying radiography images and detecting probable chest anomalies and infections, including COVID-19. Our method has been validated in a number of comparative studies against relevant state-of-the-art architectures.", "journal": "BMC medical imaging", "date": "2022-07-06", "authors": ["HassenLouati", "AliLouati", "SlimBechikh", "FatmaMasmoudi", "AbdulazizAldaej", "ElhamKariri"], "doi": "10.1186/s12880-022-00847-w\n10.1001/jama.2020.0757\n10.1002/jmv.25681\n10.1162/neco.2006.18.7.1527\n10.1371/journal.pmed.1002686\n10.1016/S1473-3099(20)30086-4\n10.1016/j.neucom.2021.01.094\n10.1148/ryct.2020200034\n10.1007/s00521-022-07052-4\n10.1007/s00521-021-05960-5\n10.1007/s10462-021-10049-5\n10.1186/s40537-021-00444-8\n10.1016/S0004-3702(97)00043-X\n10.3390/cancers13071590\n10.1007/s00521-020-04784-z\n10.1007/s10462-020-09831-8\n10.1002/cpe.6748\n10.3390/app11209551\n10.1007/s11227-020-03435-3\n10.1007/s12652-020-01921-3\n10.1007/s00521-022-07331-0"}
{"title": "FWLICM-Deep Learning: Fuzzy Weighted Local Information C-Means Clustering-Based Lung Lobe Segmentation with Deep Learning for COVID-19 Detection.", "abstract": "Coronavirus (COVID-19) creates an extensive range of respiratory contagions, and it is a kind of ribonucleic acid (RNA) virus, which affects both animals and humans. Moreover, COVID-19 is a new disease, which produces contamination in upper respiration alterritory and lungs. The new COVID is a rapidly spreading pathogen globally, and it threatens billions of humans' lives. However, it is significant to identify positive cases in order to avoid the spread of plague and to speedily treat infected patients. Hence, in this paper, the WSCA-based RMDL approach is devised for COVID-19 prediction by means of chest X-ray images. Moreover, Fuzzy Weighted Local Information C-Means (FWLICM) approach is devised in order to segment lung lobes. The developed FWLICM method is designed by modifying the Fuzzy Local Information C-Means (FLICM) technique. Additionally, random multimodel deep learning (RMDL) classifier is utilized for the COVID-19 prediction process. The new optimization approach, named water sine cosine algorithm (WSCA), is devised in order to obtain an effective prediction. The developed WSCA is newly designed by incorporating sine cosine algorithm (SCA) and water cycle algorithm (WCA). The developed WSCA-driven RMDL approach outperforms other COVID-19 prediction techniques with regard to accuracy, specificity, sensitivity, and dice score of 92.41%, 93.55%, 92.14%, and 90.02%.", "journal": "Journal of digital imaging", "date": "2022-07-06", "authors": ["RRajeswari", "VeerrajuGampala", "BalajeeMaram", "RCristin"], "doi": "10.1007/s10278-022-00667-y\n10.1016/j.compbiomed.2020.103805\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2020.103792\n10.1056/NEJMc2001468\n10.1007/s12098-020-03263-6\n10.1016/S0140-6736(20)30522-5\n10.1038/s41368-020-0075-9\n10.1148/radiol.2020200490\n10.32098/mltj.01.2016.06\n10.1053/j.jfas.2020.11.003\n10.1016/j.media.2017.07.005\n10.1109/ACCESS.2017.2788044\n10.1016/j.cmpb.2018.04.005\n10.1007/s10462-018-9641-3\n10.1016/j.measurement.2019.05.076\n10.46253/j.mr.v3i2.a4\n10.1080/14737159.2020.1757437\n10.1109/TIP.2010.2040763\n10.1016/j.compstruc.2012.07.010\n10.1016/j.knosys.2015.12.022\n10.1016/j.eswa.2020.114054\n10.1007/s13246-020-00952-6"}
{"title": "Developing and Validating Multi-Modal Models for Mortality Prediction in COVID-19 Patients: a Multi-center Retrospective Study.", "abstract": "The unprecedented global crisis brought about by the COVID-19 pandemic has sparked numerous efforts to create predictive models for the detection and prognostication of SARS-CoV-2 infections with the goal of helping health systems allocate resources. Machine learning models, in particular, hold promise for their ability to leverage patient clinical information and medical images for prediction. However, most of the published COVID-19 prediction models thus far have little clinical utility due to methodological flaws and lack of appropriate validation. In this paper, we describe our methodology to develop and validate multi-modal models for COVID-19 mortality prediction using multi-center patient data. The models for COVID-19 mortality prediction were developed using retrospective data from Madrid, Spain (N\u2009=\u20092547) and were externally validated in patient cohorts from a community hospital in New Jersey, USA (N\u2009=\u2009242) and an academic center in Seoul, Republic of Korea (N\u2009=\u2009336). The models we developed performed differently across various clinical settings, underscoring the need for a guided strategy when employing machine learning for clinical decision-making. We demonstrated that using features from both the structured electronic health records and chest X-ray imaging data resulted in better 30-day mortality prediction performance across all three datasets (areas under the receiver operating characteristic curves: 0.85 (95% confidence interval: 0.83-0.87), 0.76 (0.70-0.82), and 0.95 (0.92-0.98)). We discuss the rationale for the decisions made at every step in developing the models and have made our code available to the research community. We employed the best machine learning practices for clinical model development. Our goal is to create a toolkit that would assist investigators and organizations in building multi-modal models for prediction, classification, and/or optimization.", "journal": "Journal of digital imaging", "date": "2022-07-06", "authors": ["Joy Tzung-YuWu", "Miguel \u00c1ngel Armengolde la Hoz", "Po-ChihKuo", "Joseph AlexanderPaguio", "Jasper SethYao", "Edward ChristopherDee", "WesleyYeung", "JerryJurado", "AchintyaMoulick", "CarmeloMilazzo", "PalomaPeinado", "PaulaVillares", "AntonioCubillo", "Jos\u00e9 FelipeVarona", "Hyung-ChulLee", "AlbertoEstirado", "Jos\u00e9 MariaCastellano", "Leo AnthonyCeli"], "doi": "10.1007/s10278-022-00674-z\n10.7150/thno.46428\n10.3348/kjr.2020.0485\n10.1016/j.media.2020.101844\n10.1016/S2589-7500(20)30186-2\n10.1016/S2589-7500(20)30292-2\n10.1136/bmj.m1328\n10.1148/ryai.2020200029\n10.1016/j.chest.2020.06.082\n10.1007/s11263-019-01228-7\n10.1001/jamainternmed.2020.7968\n10.1136/thoraxjnl-2020-216001\n10.1001/jamainternmed.2020.8193\n10.1038/s41597-019-0322-0\n10.1093/jamia/ocx030\n10.1177/0310057X1204000609\n10.1093/jamia/ocz127\n10.1038/s41746-021-00393-9\n10.1371/journal.pone.0243262\n10.1126/science.aaz3873"}
{"title": "Self-evolving vision transformer for chest X-ray diagnosis through knowledge distillation.", "abstract": "Although deep learning-based computer-aided diagnosis systems have recently achieved expert-level performance, developing a robust model requires large, high-quality data with annotations that are expensive to obtain. This situation poses a conundrum that annually-collected chest x-rays cannot be utilized due to the absence of labels, especially in deprived areas. In this study, we present a framework named distillation for self-supervision and self-train learning (DISTL) inspired by the learning process of the radiologists, which can improve the performance of vision transformer simultaneously with self-supervision and self-training through knowledge distillation. In external validation from three hospitals for diagnosis of tuberculosis, pneumothorax, and COVID-19, DISTL offers gradually improved performance as the amount of unlabeled data increase, even better than the fully supervised model with the same amount of labeled data. We additionally show that the model obtained with DISTL is robust to various real-world nuisances, offering better applicability in clinical setting.", "journal": "Nature communications", "date": "2022-07-06", "authors": ["SangjoonPark", "GwanghyunKim", "YujinOh", "Joon BeomSeo", "Sang MinLee", "Jin HwanKim", "SungjunMoon", "Jae-KwangLim", "Chang MinPark", "Jong ChulYe"], "doi": "10.1038/s41467-022-31514-x\n10.1001/jama.2016.17216\n10.1038/s41591-018-0107-6\n10.1038/s41591-018-0029-3\n10.1016/j.jacr.2017.12.028\n10.1186/s41747-018-0061-6\n10.1148/radiol.2017162326\n10.1038/s41598-019-42557-4\n10.1371/journal.pone.0221339\n10.1016/S2589-7500(21)00116-3\n10.2174/1573405617666210127154257\n10.1016/j.media.2019.101539\n10.1109/TMI.2020.2995518\n10.1016/j.media.2020.101797"}
{"title": "Neural architecture search for pneumonia diagnosis from chest X-rays.", "abstract": "Pneumonia is one of the diseases that causes the most fatalities worldwide, especially in children. Recently, pneumonia-caused deaths have increased dramatically due to the novel Coronavirus global pandemic. Chest X-ray (CXR) images are one of the most readily available and common imaging modality for the detection and identification of pneumonia. However, the detection of pneumonia from chest radiography is a difficult task even for experienced radiologists. Artificial Intelligence (AI) based systems have great potential in assisting in quick and accurate diagnosis of pneumonia from chest X-rays. The aim of this study is to develop a Neural Architecture Search (NAS) method to find the best convolutional architecture capable of detecting pneumonia from chest X-rays. We propose a Learning by Teaching framework inspired by the teaching-driven learning methodology from humans, and conduct experiments on a pneumonia chest X-ray dataset with over 5000 images. Our proposed method yields an area under ROC curve (AUC) of 97.6% for pneumonia detection, which improves upon previous NAS methods by 5.1% (absolute).", "journal": "Scientific reports", "date": "2022-07-06", "authors": ["AbhibhaGupta", "ParthSheth", "PengtaoXie"], "doi": "10.1038/s41598-022-15341-0\n10.1109/ACCESS.2018.2798799\n10.3390/mti2030047\n10.1172/JCI33947\n10.5603/ARM.a2021.0036\n10.1136/bmj.j2471\n10.1016/j.cell.2018.02.010\n10.1371/journal.pmed.1002686\n10.1016/j.cmpb.2018.04.025\n10.1186/s12880-018-0286-0\n10.1016/j.compbiomed.2018.10.011\n10.1016/j.artmed.2019.101744\n10.3390/app9194130\n10.1016/j.cmpb.2019.06.005\n10.1109/ACCESS.2018.2885997\n10.1016/j.measurement.2019.05.076\n10.1016/j.cmpb.2019.06.023\n10.1016/j.compeleceng.2019.08.004\n10.1155/2019/4180949\n10.1007/s10916-021-01747-2\n10.1109/TMI.2017.2775636\n10.1007/s11548-016-1359-6\n10.1007/s13246-020-00888-x\n10.1007/s10489-020-01943-6\n10.1109/TCE.2007.381734\n10.3390/app10020559\n10.1109/5.726791\n10.1371/journal.pone.0256630\n10.3390/app11031242"}
{"title": "An ML prediction model based on clinical parameters and automated CT scan features for COVID-19 patients.", "abstract": "Outcome prediction for individual patient groups is of paramount importance in terms of selection of appropriate therapeutic options, risk communication to patients and families, and allocating resource through optimum triage. This has become even more necessary in the context of the current COVID-19 pandemic. Widening the spectrum of predictor variables by including radiological parameters alongside the usually utilized demographic, clinical and biochemical ones can facilitate building a comprehensive prediction model. Automation has the potential to build such models with applications to time-critical environments so that a clinician will be able to utilize the model outcomes in real-time decision making at bedside. We show that amalgamation of computed tomogram (CT) data with clinical parameters (CP) in generating a Machine Learning model from 302 COVID-19 patients presenting to an acute care hospital in India could prognosticate the need for invasive mechanical ventilation. Models developed from CP alone, CP and radiologist derived CT severity score and CP with automated lesion-to-lung ratio had AUC of 0.87 (95% CI 0.85-0.88), 0.89 (95% CI 0.87-0.91), and 0.91 (95% CI 0.89-0.93), respectively. We show that an operating point on the ROC can be chosen to aid clinicians in risk characterization according to the resource availability and ethical considerations. This approach can be deployed in more general settings, with appropriate calibrations, to predict outcomes of severe COVID-19 patients effectively.", "journal": "Scientific reports", "date": "2022-07-06", "authors": ["AbhisharSinha", "Swati PurohitJoshi", "Purnendu SekharDas", "SoumyaJana", "RahuldebSarkar"], "doi": "10.1038/s41598-022-15327-y\n10.1016/S1473-3099(20)30120-1\n10.26599/BDMA.2020.9020012\n10.1016/j.bspc.2020.102365\n10.26599/BDMA.2020.9020013\n10.26599/TST.2021.9010026\n10.1038/s41467-020-20314-w\n10.1148/ryct.2020200047\n10.1016/j.acra.2020.08.038\n10.1007/s00330-020-07273-y\n10.1016/j.compbiomed.2021.104304\n10.1016/S2589-7500(20)30112-6\n10.1007/s10278-013-9622-7\n10.1186/s12911-016-0318-z\n10.1023/A:1007979827043\n10.1109/TPAMI.2013.106\n10.1023/A:1010933404324\n10.1002/widm.1249\n10.7717/peerj.453"}
{"title": "Development and validation of bone-suppressed deep learning classification of COVID-19 presentation in chest radiographs.", "abstract": "Coronavirus disease 2019 (COVID-19) is a pandemic disease. Fast and accurate diagnosis of COVID-19 from chest radiography may enable more efficient allocation of scarce medical resources and hence improved patient outcomes. Deep learning classification of chest radiographs may be a plausible step towards this. We hypothesize that bone suppression of chest radiographs may improve the performance of deep learning classification of COVID-19 phenomena in chest radiographs.\nTwo bone suppression methods (Gusarev \nBone suppression of external test data was found to significantly (P<0.05) improve AUC for one classifier architecture [from 0.698 (non-suppressed) to 0.732 (Rajaraman-suppressed)]. For the other classifier architecture, suppression did not significantly (P>0.05) improve or worsen classifier performance.\nRajaraman suppression significantly improved classification performance in one classification architecture, and did not significantly worsen classifier performance in the other classifier architecture. This research could be extended to explore the impact of bone suppression on classification of different lung pathologies, and the effect of other image enhancement techniques on classifier performance.", "journal": "Quantitative imaging in medicine and surgery", "date": "2022-07-06", "authors": ["Ngo Fung DanielLam", "HongfeiSun", "LimingSong", "DongrongYang", "ShaohuaZhi", "GeRen", "Pak HeiChou", "Shiu Bun NelsonWan", "Man Fung EstherWong", "King KwongChan", "Hoi Ching HaileyTsang", "Feng-Ming SpringKong", "Y\u00ec Xi\u00e1ng JW\u00e1ng", "JingQin", "Lawrence Wing ChiChan", "MichaelYing", "JingCai"], "doi": "10.21037/qims-21-791\n10.1016/S0140-6736(20)30183-5\n10.1016/S2213-2600(20)30076-X\n10.2807/1560-7917.ES.2021.26.24.2100509\n10.1016/S0140-6736(21)01358-1\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200642\n10.1016/j.radi.2020.10.018\n10.1007/s11263-015-0816-y\n10.1155/2018/7068349\n10.1038/s41598-020-76550-z\n10.1007/s10489-020-01829-7\n10.1007/s10489-020-01829-7\n10.1097/RLI.0000000000000748\n10.3348/kjr.2020.0536\n10.1016/j.patrec.2021.06.021\n10.3390/diagnostics11050840\n10.21037/qims-20-1230\n10.1002/mp.14371\n10.1007/s11548-015-1278-y\n10.1109/TMI.2006.871549\n10.1016/j.ejrad.2009.03.046\n10.1148/radiol.11100153\n10.2214/ajr.174.1.1740071\n10.7937/91ah-v663\n10.1148/radiol.2021203957\n10.1148/ryai.2019180041\n10.1109/TIP.2003.819861\n10.1016/j.compbiomed.2021.104319\n10.1038/s42256-021-00307-0\n10.1038/s41598-020-74539-2\n10.2307/2531595\n10.1371/journal.pone.0254809\n10.15585/mmwr.mm7015e2\n10.1038/s41598-021-83237-6"}
{"title": "Automated Screening of COVID-19-Based Tongue Image on Chinese Medicine.", "abstract": "Artificial intelligence-powered screening systems of coronavirus disease 2019 (COVID-19) are urgently demanding since the ongoing outbreak of SARS-CoV-2 worldwide. Chest CT or X-ray is not sufficient to support the large-scale screening of COVID-19 because mildly-infected patients do not have imaging features on these images. Therefore, it is imperative to exploit supplementary medical imaging strategies. Traditional Chinese medicine has played an essential role in the fight against COVID-19.\nIn this paper, we conduct two kinds of verification experiments based on a newly-collected multi-modality dataset, which consists of three types of modalities: tongue images, chest CT scans, and X-ray images. First, we study a binary classification experiment on tongue images to verify the discriminative ability between COVID-19 and non-COVID-19. Second, we design extensive multimodality experiments to validate whether introducing tongue image can improve the screening accuracy of COVID-19 based on chest CT or X-ray images.\nTongue image screening of COVID-19 showed that the accuracy (ACC), sensitivity (SEN), specificity (SPEC), and Matthew correlation coefficient (MCC) of the improved AlexNet and Googlenet both reached 98.39%, 98.97%, 96.67%, and 99.11%. The fusion of chest CT and tongue images used a tandem multimodal classifier fusion strategy to achieve optimal classification, and the results and screening accuracy of COVID-19 reached 98.98%, resulting in a significant improvement of 4.75% the highest accuracy in 375 years compared with the single-modality model. The fusion of chest x-rays and tongue images also had good classification accuracy.\nBoth experimental results demonstrate that tongue image not only has an excellent discriminative ability for screening COVID-19 but also can improve the screening accuracy based on chest CT or X-rays. To the best of our knowledge, it is the first work that verifies the effectiveness of tongue image on screening COVID-19. This paper provides a new perspective and a novel solution that contributes to large-scale screening toward fast stopping the pandemic of COVID-19.", "journal": "BioMed research international", "date": "2022-07-06", "authors": ["GuangZhang", "XueyingHe", "DelinLi", "CuihuanTian", "BenzhengWei"], "doi": "10.1155/2022/6825576\n10.1109/TMI.2020.2996256\n10.1109/TCBB.2021.3065361\n10.1016/j.asoc.2021.107323\n10.1016/j.asoc.2020.106912\n10.1007/s13369-021-06240-z\n10.1038/s41598-021-88807-2\n10.1109/JIOT.2021.3050775\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200230\n10.1016/j.phrs.2020.104743\n10.1007/s11655-020-3192-6\n10.1148/ryct.2020200075\n10.1038/s41598-020-76282-0\n10.1002/hbm.24428\n10.1109/TMI.2019.2913158\n10.3389/fninf.2018.00035\n10.1145/3358331.3358400\n10.1109/JSTQE.2015.2478657\n10.3390/app9153128\n10.1109/ICCIKE47802.2019.9004326\n10.5958/0973-9130.2019.00059.8\n10.1109/ACCESS.2019.2960578\n10.1007/978-3-319-10584-0_20\n10.1016/j.media.2019.01.012"}
{"title": "Multi-branch fusion auxiliary learning for the detection of pneumonia from chest X-ray images.", "abstract": "Lung infections caused by bacteria and viruses are infectious and require timely screening and isolation, and different types of pneumonia require different treatment plans. Therefore, finding a rapid and accurate screening method for lung infections is critical. To achieve this goal, we proposed a multi-branch fusion auxiliary learning (MBFAL) method for pneumonia detection from chest X-ray (CXR) images. The MBFAL method was used to perform two tasks through a double-branch network. The first task was to recognize the absence of pneumonia (normal), COVID-19, other viral pneumonia and bacterial pneumonia from CXR images, and the second task was to recognize the three types of pneumonia from CXR images. The latter task was used to assist the learning of the former task to achieve a better recognition effect. In the process of auxiliary parameter updating, the feature maps of different branches were fused after sample screening through label information to enhance the model's ability to recognize case of pneumonia without impacting its ability to recognize normal cases. Experiments show that an average classification accuracy of 95.61% is achieved using MBFAL. The single class accuracy for normal, COVID-19, other viral pneumonia and bacterial pneumonia was 98.70%, 99.10%, 96.60% and 96.80%, respectively, and the recall was 97.20%, 98.60%, 96.10% and 89.20%, respectively, using the MBFAL method. Compared with the baseline model and the model constructed using the above methods separately, better results for the rapid screening of pneumonia were achieved using MBFAL.", "journal": "Computers in biology and medicine", "date": "2022-07-03", "authors": ["JiaLiu", "JingQi", "WeiChen", "YongjianNian"], "doi": "10.1016/j.compbiomed.2022.105732\n10.1109/TMI.2020.3040950\n10.1128/jcm.02589-20\n10.1186/s13054-015-1083-6\n10.1002/jmv.25674\n10.2807/1560-7917.es.2020.25.3.2000045\n10.1148/radiol.2020200432\n10.1148/radiol.2020200241\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200343\n10.1016/j.clinimag.2020.11.004\n10.1148/rg.2018170048\n10.1148/ryct.2020200034\n10.1109/ICESC51422.2021.9532943\n10.1016/j.compmedimag.2016.11.004\n10.1016/j.crad.2018.12.015\n10.1109/TMI.2020.2994908\n10.1109/TPAMI.2021.3054719\n10.1109/CVPR.2019.00197\n10.1109/CVPR.2019.00332\n10.1023/A:1007379606734\n10.1109/ICCV.2019.00649\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.1007/s00500-020-05424-3\n10.1109/ICCC51575.2020.9345005\n10.1016/j.asoc.2020.106744\n10.1016/j.ins.2020.09.041\n10.1016/j.neucom.2022.01.055\n10.1109/ICCV.2017.89\n10.1109/CVPR.2018.00813\n10.1109/TNNLS.2021.3114747\n10.1109/TMI.2019.2893944\n10.1109/ICCV.2017.324\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1109/WACV.2018.00097\n10.1109/TCBB.2021.3065361\n10.1007/978-3-319-23344-4_37\n10.1007/978-3-319-23117-4_43"}
{"title": "Multi-center validation of an artificial intelligence system for detection of COVID-19 on chest radiographs in symptomatic patients.", "abstract": "While chest radiograph (CXR) is the first-line imaging investigation in patients with respiratory symptoms, differentiating COVID-19 from other respiratory infections on CXR remains challenging. We developed and validated an AI system for COVID-19 detection on presenting CXR.\nA deep learning model (RadGenX), trained on 168,850 CXRs, was validated on a large international test set of presenting CXRs of symptomatic patients from 9 study sites (US, Italy, and Hong Kong SAR) and 2 public datasets from the US and Europe. Performance was measured by area under the receiver operator characteristic curve (AUC). Bootstrapped simulations were performed to assess performance across a range of potential COVID-19 disease prevalence values (3.33 to 33.3%). Comparison against international radiologists was performed on an independent test set of 852 cases.\nRadGenX achieved an AUC of 0.89 on 4-fold cross-validation and an AUC of 0.79 (95%CI 0.78-0.80) on an independent test cohort of 5,894 patients. Delong's test showed statistical differences in model performance across patients from different regions (p < 0.01), disease severity (p < 0.001), gender (p < 0.001), and age (p = 0.03). Prevalence simulations showed the negative predictive value increases from 86.1% at 33.3% prevalence, to greater than 98.5% at any prevalence below 4.5%. Compared with radiologists, McNemar's test showed the model has higher sensitivity (p < 0.001) but lower specificity (p < 0.001).\nAn AI model that predicts COVID-19 infection on CXR in symptomatic patients was validated on a large international cohort providing valuable context on testing and performance expectations for AI systems that perform COVID-19 prediction on CXR.\n\u2022 An AI model developed using CXRs to detect COVID-19 was validated in a large multi-center cohort of 5,894 patients from 9 prospectively recruited sites and 2 public datasets. \u2022 Differences in AI model performance were seen across region, disease severity, gender, and age. \u2022 Prevalence simulations on the international test set demonstrate the model's NPV is greater than 98.5% at any prevalence below 4.5%.", "journal": "European radiology", "date": "2022-07-03", "authors": ["Michael DKuo", "Keith W HChiu", "David SWang", "Anna RitaLarici", "DmytroPoplavskiy", "AdeleValentini", "AlessandroNapoli", "AndreaBorghesi", "GuidoLigabue", "Xin Hao BFang", "Hing Ki CWong", "SailongZhang", "John RHunter", "AbeerMousa", "AmatoInfante", "LorenzoElia", "SalvatoreGolemi", "Leung Ho PYu", "Christopher K MHui", "Bradley JErickson"], "doi": "10.1007/s00330-022-08969-z\n10.1016/S1473-3099(20)30457-6\n10.1001/jamanetworkopen.2020.37067\n10.1016/S2468-2667(20)30308-X\n10.1056/NEJMp2025631\n10.1148/radiol.2020200432\n10.1148/radiol.2020201365\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020203511\n10.1109/TMI.2020.2993291\n10.1016/j.patrec.2020.09.010\n10.1155/2020/8889023\n10.1109/JBHI.2020.3037127\n10.1148/ryai.2020200029\n10.1016/j.media.2021.102225\n10.1148/radiol.2020200038\n10.1038/s41597-020-00741-6\n10.1038/s42256-021-00307-0\n10.1371/journal.pmed.1002683\n10.1038/s41467-020-19802-w\n10.1016/S2213-2600(21)00005-9\n10.1016/S2666-5247(20)30200-7\n10.1126/sciadv.abd5393"}
{"title": "Sequence-assignment validation in cryo-EM models with checkMySequence.", "abstract": "The availability of new artificial intelligence-based protein-structure-prediction tools has radically changed the way that cryo-EM maps are interpreted, but it has not eliminated the challenges of map interpretation faced by a microscopist. Models will continue to be locally rebuilt and refined using interactive tools. This inevitably results in occasional errors, among which register shifts remain one of the most difficult to identify and correct. Here, checkMySequence, a fast, fully automated and parameter-free method for detecting register shifts in protein models built into cryo-EM maps, is introduced. It is shown that the method can assist model building in cases where poorer map resolution hinders visual interpretation. It is also shown that checkMySequence could have helped to avoid a widely discussed sequence-register error in a model of SARS-CoV-2 RNA-dependent RNA polymerase that was originally detected thanks to a visual residue-by-residue inspection by members of the structural biology community. The software is freely available at https://gitlab.com/gchojnowski/checkmysequence.", "journal": "Acta crystallographica. Section D, Structural biology", "date": "2022-07-02", "authors": ["GrzegorzChojnowski"], "doi": "10.1107/S2059798322005009"}
{"title": "Explainable deep learning algorithm for distinguishing incomplete Kawasaki disease by coronary artery lesions on echocardiographic imaging.", "abstract": "Incomplete Kawasaki disease (KD) has often been misdiagnosed due to a lack of the clinical manifestations of classic KD. However, it is associated with a markedly higher prevalence of coronary artery lesions. Identifying coronary artery lesions by echocardiography is important for the timely diagnosis of and favorable outcomes in KD. Moreover, similar to KD, coronavirus disease 2019, currently causing a worldwide pandemic, also manifests with fever; therefore, it is crucial at this moment that KD should be distinguished clearly among the febrile diseases in children. In this study, we aimed to validate a deep learning algorithm for classification of KD and other acute febrile diseases.\nWe obtained coronary artery images by echocardiography of children (n = 138 for KD; n = 65 for pneumonia). We trained six deep learning networks (VGG19, Xception, ResNet50, ResNext50, SE-ResNet50, and SE-ResNext50) using the collected data.\nSE-ResNext50 showed the best performance in terms of accuracy, specificity, and precision in the classification. SE-ResNext50 offered a precision of 81.12%, a sensitivity of 84.06%, and a specificity of 58.46%.\nThe results of our study suggested that deep learning algorithms have similar performance to an experienced cardiologist in detecting coronary artery lesions to facilitate the diagnosis of KD.", "journal": "Computer methods and programs in biomedicine", "date": "2022-07-01", "authors": ["HaeyunLee", "YongsoonEun", "Jae YounHwang", "Lucy YoungminEun"], "doi": "10.1016/j.cmpb.2022.106970"}
{"title": "Real-Time Prediction of Mortality, Cardiac\u00a0Arrest, and Thromboembolic Complications in Hospitalized Patients With COVID-19.", "abstract": "COVID-19 infection carries significant morbidity and mortality. Current risk prediction for complications in COVID-19 is limited, and existing approaches fail to account for the dynamic course of the disease.\nThe purpose of this study was to develop and validate the COVID-HEART predictor, a novel continuously updating risk-prediction technology to forecast adverse events in hospitalized patients with COVID-19.\nRetrospective registry data from patients with severe acute respiratory syndrome coronavirus 2 infection admitted to 5 hospitals were used to train COVID-HEART to predict all-cause mortality/cardiac arrest (AM/CA) and imaging-confirmed thromboembolic events (TEs) (n = 2,550 and n = 1,854, respectively). To assess COVID-HEART's performance in the face of rapidly changing clinical treatment guidelines, an additional 1,100 and 796 patients, admitted after the completion of development data collection, were used for testing. Leave-hospital-out validation was performed.\nOver 20 iterations of temporally divided testing, the mean area under the receiver operating characteristic curve were 0.917 (95% confidence interval [CI]: 0.916-0.919) and 0.757 (95%\u00a0CI: 0.751-0.763) for prediction of AM/CA and TE, respectively. The interquartile ranges of median early warning times were 14 to 21\u00a0hours for AM/CA and 12 to 60\u00a0hours for TE. The mean area under the receiver operating characteristic curve for the left-out hospitals were 0.956 (95%\u00a0CI: 0.936-0.976) and 0.781 (95%\u00a0CI: 0.642-0.919) for prediction of AM/CA and TE, respectively.\nThe continuously updating, fully interpretable COVID-HEART predictor accurately predicts AM/CA and TE within multiple time windows in hospitalized COVID-19 patients. In its current implementation, the predictor can facilitate practical, meaningful changes in patient triage and resource allocation by providing real-time risk scores for these outcomes. The potential utility of the predictor extends to COVID-19 patients after hospitalization and beyond COVID-19.", "journal": "JACC. Advances", "date": "2022-06-28", "authors": ["Julie KShade", "Ashish NDoshi", "EricSung", "Dan MPopescu", "Anum SMinhas", "Nisha AGilotra", "Konstantinos NAronis", "Allison GHays", "Natalia ATrayanova"], "doi": "10.1016/j.jacadv.2022.100043\n10.1038/s41569-020-0360-5\n10.1001/jamacardio.2020.0950\n10.1093/cid/civ1020\n10.1016/S0140-6736(20)30566-3\n10.1001/jamacardio.2020.3557\n10.1111/jth.14768\n10.1016/j.eclinm.2020.100639\n10.3390/v12050527\n10.1152/physiolgenomics.00029.2020\n10.1007/s00134-020-05991-x\n10.1101/2020.02.27.20028027\n10.2196/24018\n10.7717/peerj.10337\n10.48550/arXiv.2008.12683\n10.1016/j.numecd.2020.07.031\n10.3233/SHTI200478\n10.48550/arXiv.2010.02006\n10.1183/13993003.00775-2020\n10.1136/bmj.m1328\n10.1038/s41746-021-00527-z\n10.18053/jctres.06.202005.003\n10.7326/M14-0698\n10.1016/S0140-6736(20)31209-5\n10.31083/j.rcm.2020.03.120\n10.1007/s00134-005-2763-5\n10.1097/01.CCM.0000257337.63529.9F\n10.1016/j.jcrc.2011.08.016\n10.1007/s11239-021-02384-9"}
{"title": "CVD-HNet: Classifying Pneumonia and COVID-19 in Chest X-ray Images Using Deep Network.", "abstract": "The use of computer-assisted analysis to improve image interpretation has been a long-standing challenge in the medical imaging industry. In terms of image comprehension, Continuous advances in AI (Artificial Intelligence), predominantly in DL (Deep Learning) techniques, are supporting in the classification, Detection, and quantification of anomalies in medical images. DL techniques are the most rapidly evolving branch of AI, and it's recently been successfully pragmatic in a variety of fields, including medicine. This paper provides a classification method for COVID 19 infected X-ray images based on new novel deep CNN model. For COVID19 specified pneumonia analysis, two new customized CNN architectures, CVD-HNet1 (COVID-HybridNetwork1) and CVD-HNet2 (COVID-HybridNetwork2), have been designed. The suggested method utilizes operations based on boundaries and regions, as well as convolution processes, in a systematic manner. In comparison to existing CNNs, the suggested classification method achieves excellent Accuracy 98 percent, F Score 0.99 and MCC 0.97. These results indicate impressive classification accuracy on a limited dataset, with more training examples, much better results can be achieved. Overall, our CVD-HNet model could be a useful tool for radiologists in diagnosing and detecting COVID 19 instances early.", "journal": "Wireless personal communications", "date": "2022-06-28", "authors": ["SSuganyadevi", "VSeethalakshmi"], "doi": "10.1007/s11277-022-09864-y\n10.1183/13993003.00775-2020\n10.1038/s41598-020-76282-0\n10.1109/ACCESS.2020.3005510\n10.1007/s13246-020-00865-4\n10.1007/s40846-020-00529-4\n10.1016/j.cmpb.2020.105581\n10.1109/TMI.2020.2993291\n10.1016/j.cmpb.2020.105608\n10.1016/j.pdpdt.2021.102473\n10.1038/s42003-020-01535-7\n10.1109/RBME.2020.2990959\n10.1016/j.mri.2021.03.005\n10.1016/j.clinimag.2020.09.002\n10.1155/2021/6621607\n10.1007/s10044-021-00970-4\n10.2196/23693\n10.1007/s12530-021-09385-2\n10.1038/s42256-021-00307-0\n10.1016/j.imu.2020.100427\n10.1371/journal.pone.0250688\n10.1007/s10140-020-01886-y\n10.1088/2632-2153/abf22c\n10.1371/journal.pone.0235187\n10.1038/s41591-020-0931-3\n10.1080/21681163.2015.1135299\n10.1097/RLI.0000000000000341\n10.1109/TMI.2015.2508280\n10.1016/j.media.2016.05.004\n10.4103/2153-3539.186902\n10.1007/s11042-020-09981-5\n10.1109/JBHI.2016.2631401\n10.1007/978-3-319-0443-0_39\n10.1007/s11277-021-09031-9.(IF-1.671)\n10.1109/TMI.2016.2521800\n10.1109/TMI.2016.2548501\n10.1038/srep38897\n10.3389/fnins.2014.00229\n10.1007/s13735-021-00218-1\n10.1007/s10278-016-9914-9\n10.1109/JBHI.2016.2636665\n10.1118/1.4967345\n10.1002/9781119792253.ch8\n10.1080/03772063.2021.1893231\n10.1007/s11517-016-1590-x\n10.3389/fonc.2020.01621"}
{"title": "Learning deep neural networks' architectures using differential evolution. Case study: Medical imaging processing.", "abstract": "The COVID-19 pandemic has changed the way we practice medicine. Cancer patient and obstetric care landscapes have been distorted. Delaying cancer diagnosis or maternal-fetal monitoring increased the number of preventable deaths or pregnancy complications. One solution is using Artificial Intelligence to help the medical personnel establish the diagnosis in a faster and more accurate manner. Deep learning is the state-of-the-art solution for image classification. Researchers manually design the structure of fix deep learning neural networks structures and afterwards verify their performance. The goal of this paper is to propose a potential method for learning deep network architectures automatically. As the number of networks architectures increases exponentially with the number of convolutional layers in the network, we propose a differential evolution algorithm to traverse the search space. At first, we propose a way to encode the network structure as a candidate solution of fixed-length integer array, followed by the initialization of differential evolution method. A set of random individuals is generated, followed by mutation, recombination, and selection. At each generation the individuals with the poorest loss values are eliminated and replaced with more competitive individuals. The model has been tested on three cancer datasets containing MRI scans and histopathological images and two maternal-fetal screening ultrasound images. The novel proposed method has been compared and statistically benchmarked to four state-of-the-art deep learning networks: VGG16, ResNet50, Inception V3, and DenseNet169. The experimental results showed that the model is competitive to other state-of-the-art models, obtaining accuracies between 78.73% and 99.50% depending on the dataset it had been applied on.", "journal": "Computers in biology and medicine", "date": "2022-06-26", "authors": ["SmarandaBelciug"], "doi": "10.1016/j.compbiomed.2022.105623\n10.1159/000508254\n10.3390/jcm9113749\n10.1016/S2214-109X(21)00079-6\n10.1111/jgh15325\n10.1136/bmjpo-2020-000859\n10.5281/zenodo.3904280\n10.1002/uog.20945\n10.1002/uog.20796\n10.1109/ISBI.2019.8759377\n10.1103/PhysRevE.101.052604\n10.1038/s41467-021-26568-2\n10.1146/annurev-conmatphys-031119-050745\n10.1038/s42256-018-0006-z\n10.1109/TAI.2021.3067574\n10.34740/Kaggle/dsv/1183165\n10.1080/00949655.2010.520163\n10.3390/s21062222\n10.1101/2020.08.15.20175760\n10.10138/s41598-020-67076-5"}
{"title": "Eight pruning deep learning models for low storage and high-speed COVID-19 computed tomography lung segmentation and heatmap-based lesion localization: A multicenter study using COVLIAS 2.0.", "abstract": "COVLIAS 1.0: an automated lung segmentation was designed for COVID-19 diagnosis. It has issues related to storage space and speed. This study shows that COVLIAS 2.0 uses pruned AI (PAI) networks for improving both storage and speed, wiliest high performance on lung segmentation and lesion localization.\nology: The proposed study uses multicenter \u223c9,000 CT slices from two different nations, namely, CroMed from Croatia (80 patients, experimental data), and NovMed from Italy (72 patients, validation data). We hypothesize that by using pruning and evolutionary optimization algorithms, the size of the AI models can be reduced significantly, ensuring optimal performance. Eight different pruning techniques (i) differential evolution (DE), (ii) genetic algorithm (GA), (iii) particle swarm optimization algorithm (PSO), and (iv) whale optimization algorithm (WO) in two deep learning frameworks (i) Fully connected network (FCN) and (ii) SegNet were designed. COVLIAS 2.0 was validated using \"Unseen NovMed\" and benchmarked against MedSeg. Statistical tests for stability and reliability were also conducted.\nPruning algorithms (i) FCN-DE, (ii) FCN-GA, (iii) FCN-PSO, and (iv) FCN-WO showed improvement in storage by 92.4%, 95.3%, 98.7%, and 99.8% respectively when compared against solo FCN, and (v) SegNet-DE, (vi) SegNet-GA, (vii) SegNet-PSO, and (viii) SegNet-WO showed improvement by 97.1%, 97.9%, 98.8%, and 99.2% respectively when compared against solo SegNet. AUC > 0.94 (p\u00a0<\u00a00.0001) on CroMed and > 0.86 (p\u00a0<\u00a00.0001) on NovMed data set for all eight EA model. PAI <0.25\u00a0s per image. DenseNet-121-based Grad-CAM heatmaps showed validation on glass ground opacity lesions.\nEight PAI networks that were successfully validated are five times faster, storage efficient, and could be used in clinical settings.", "journal": "Computers in biology and medicine", "date": "2022-06-26", "authors": ["MohitAgarwal", "SushantAgarwal", "LucaSaba", "Gian LucaChabert", "SuneetGupta", "AlessandroCarriero", "AlessioPasche", "PietroDanna", "ArminMehmedovic", "GavinoFaa", "SaurabhShrivastava", "KanishkaJain", "HarshJain", "TanayJujaray", "Inder MSingh", "MonikaTurk", "Paramjit SChadha", "Amer MJohri", "Narendra NKhanna", "SophieMavrogeni", "John RLaird", "David WSobel", "MartinMiner", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "Durga PrasannaMisra", "VikasAgarwal", "George DKitas", "Jagjit STeji", "MustafaAl-Maini", "Surinder KDhanjil", "AndrewNicolaides", "AdityaSharma", "VijayRathore", "MostafaFatemi", "AzraAlizad", "Pudukode RKrishnan", "Rajanikant RYadav", "FrenceNagy", "Zsigmond Tam\u00e1sKincses", "ZoltanRuzsa", "SubbaramNaidu", "KlaudijaViskovic", "Manudeep KKalra", "Jasjit SSuri"], "doi": "10.1016/j.compbiomed.2022.105571\n10.1002/jmv.25996\n10.1002/jmv.25855\n10.23736/S0392-9590.21.04771-4"}
{"title": "Point-of-care lung ultrasonography for early identification of mild COVID-19: a prospective cohort of outpatients in a Swiss screening center.", "abstract": "Early identification of SARS-CoV-2 infection is important to guide quarantine and reduce transmission. This study evaluates the diagnostic performance of lung ultrasound (LUS), an affordable, consumable-free point-of-care tool, for COVID-19 screening.\nThis prospective observational cohort included adults presenting with cough and/or dyspnoea at a SARS-CoV-2 screening centre of Lausanne University Hospital between 31 March and 8 May 2020.\nInvestigators recorded standardised LUS images and videos in 10 lung zones per patient. Two blinded independent experts reviewed LUS recording and classified abnormal findings according to prespecified criteria to investigate their predictive value to diagnose SARS-CoV-2 infection according to PCR on nasopharyngeal swabs (COVID-19 positive vs COVID-19 negative).\nWe finally combined LUS and clinical findings to derive a multivariate logistic regression diagnostic score.\nOf 134 included patients, 23% (n=30/134) were COVID-19 positive and 77% (n=103/134) were COVID-19 negative; 85%, (n=114/134) cases were previously healthy healthcare workers presenting within 2-5\u2009days of symptom onset (IQR). Abnormal LUS findings were significantly more frequent in COVID-19 positive compared with COVID-19 negative (45% vs 26%, p=0.045) and mostly consisted of focal pathologic B lines. Combining clinical findings in a multivariate logistic regression score had an area under the receiver operating curve of 80.3% to detect COVID-19, and slightly improved to 84.5% with the addition of LUS features.\nCOVID-19-positive patients are significantly more likely to have lung pathology by LUS. However, LUS has an insufficient sensitivity and is not an appropriate screening tool in outpatients. LUS only adds little value to clinical features alone.", "journal": "BMJ open", "date": "2022-06-25", "authors": ["Sim\u00e9onSchaad", "ThomasBrahier", "Mary-AnneHartley", "Jean-BaptisteCordonnier", "LucaBosso", "TanguyEspejo", "OlivierPantet", "OlivierHugli", "Pierre-NicolasCarron", "Jean-YvesMeuwly", "No\u00e9mieBoillat-Blanco"], "doi": "10.1136/bmjopen-2021-060181\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200642\n10.1148/ryct.2020200110\n10.1016/S1473-3099(20)30086-4\n10.1002/14651858.CD013639.pub3\n10.1097/MEJ.0000000000000517\n10.1093/cid/ciaa1408\n10.7326/0003-4819-137-7-200210010-00011\n10.1177/87564793211037607\n10.7860/JCDR/2016/18129.8744\n10.1067/j.cpradiol.2016.12.003\n10.1007/s00134-012-2513-4\n10.1002/ehf2.12907\n10.1007/s00134-019-05725-8\n10.1002/jum.15285\n10.1097/01.ede.0000147512.81966.ba\n10.5811/westjem.2020.5.47743\n10.1111/anae.15175\n10.1007/s11739-020-02524-8\n10.1007/s00134-021-06373-7\n10.1007/s00134-020-06212-1\n10.1016/j.ultrasmedbio.2020.12.021\n10.2214/AJR.20.23202\n10.1148/radiol.2020200432\n10.2214/AJR.20.23034\n10.1186/s40779-020-0233-6\n10.1148/radiol.2020200463\n10.1007/s00330-020-06967-7\n10.7554/eLife.58728\n10.1016/j.cmi.2020.06.019\n10.1093/femspd/ftaa061"}
{"title": "Disease Localization and Severity Assessment in Chest X-Ray Images using Multi-Stage Superpixels Classification.", "abstract": "Chest X-ray (CXR) is a non-invasive imaging modality used in the prognosis and management of chronic lung disorders like tuberculosis (TB), pneumonia, coronavirus disease (COVID-19), etc. The radiomic features associated with different disease manifestations assist in detection, localization, and grading the severity of infected lung regions. The majority of the existing computer-aided diagnosis (CAD) system used these features for the classification task, and only a few works have been dedicated to disease-localization and severity scoring. Moreover, the existing deep learning approaches use class activation map and Saliency map, which generate a rough localization. This study aims to generate a compact disease boundary, infection map, and grade the infection severity using proposed multistage superpixel classification-based disease localization and severity assessment framework.\nThe proposed method uses a simple linear iterative clustering (SLIC) technique to subdivide the lung field into small superpixels. Initially, the different radiomic texture and proposed shape features are extracted and combined to train different benchmark classifiers in a multistage framework. Subsequently, the predicted class labels are used to generate an infection map, mark disease boundary, and grade the infection severity. The performance is evaluated using a publicly available Montgomery dataset and validated using Friedman average ranking and Holm and Nemenyi post-hoc procedures.\nThe proposed multistage classification approach achieved accuracy (ACC)= 95.52%, F-Measure (FM)= 95.48%, area under the curve (AUC)= 0.955 for Stage-I and ACC=85.35%, FM=85.20%, AUC=0.853 for Stage-II using calibration dataset and ACC\u00a0=\u00a093.41%, FM\u00a0=\u00a095.32%, AUC\u00a0=\u00a00.936 for Stage-I and ACC\u00a0=\u00a084.02%, FM\u00a0=\u00a071.01%, AUC\u00a0=\u00a00.795 for Stage-II using validation dataset. Also, the model has demonstrated the average Jaccard Index (JI) of 0.82 and Pearson's correlation coefficient (r) of 0.9589.\nThe obtained classification results using calibration and validation dataset confirms the promising performance of the proposed framework. Also, the average JI shows promising potential to localize the disease, and better agreement between radiologist score and predicted severity score (r) confirms the robustness of the method. Finally, the statistical test justified the significance of the obtained results.", "journal": "Computer methods and programs in biomedicine", "date": "2022-06-25", "authors": ["Tej BahadurChandra", "Bikesh KumarSingh", "DeepakJain"], "doi": "10.1016/j.cmpb.2022.106947\n10.1016/j.media.2020.101847\n10.1016/j.media.2020.101846\n10.1016/j.eswa.2020.113514\n10.1016/j.measurement.2019.05.076\n10.1016/j.eswa.2020.113909\n10.1016/j.media.2021.102046\n10.1016/j.patcog.2020.107613\n10.1016/j.media.2018.12.007\n10.1007/s10916-019-1222-8\n10.1109/ICPC2T48082.2020.9071445\n10.1109/tmi.2020.2993291\n10.1016/j.media.2021.101978\n10.1016/j.media.2020.101911\n10.1016/j.media.2015.09.004\n10.1016/j.media.2020.101913\n10.1109/ACCESS.2020.2971257\n10.1109/ACCESS.2020.3003810\n10.1109/ICCV.2017.74\n10.1007/s13755-021-00146-8\n10.1109/TPAMI.2012.120\n10.3990/2.401\n10.3390/s17071474\n10.1186/s12880-019-0369-6\n10.1016/j.media.2018.05.006\n10.1109/CVPR.2017.369\n10.1109/CVPR.2018.00865\n10.1007/s10723-021-09596-6\n10.1049/ipr2.12153\n10.1109/LSC.2018.8572113\n10.1109/JBHI.2021.3069169\n10.1007/s13755-021-00146-8\n10.3390/diagnostics12010025\n10.1016/j.compbiomed.2021.105002\n10.1016/j.media.2021.102299\n10.1259/bjr.20210759\n10.1016/j.media.2020.101860\n10.1016/j.patcog.2021.107828\n10.1016/j.media.2021.102054\n10.1016/j.compbiomed.2022.105466\n10.1016/j.scs.2021.103252\n10.1007/s13369-021-05958-0\n10.5114/pjr.2022.113435\n10.3390/diagnostics11040616\n10.1088/1742-6596/1767/1/012004\n10.1109/TMI.2020.3040950\n10.1109/JBHI.2020.3037127\n10.1016/j.media.2020.101794\n10.1038/s41598-019-42557-4\n10.1109/TMI.2013.2290491\n10.1109/TMI.2013.2284099\n10.1016/j.measurement.2019.107426\n10.1007/s00330-020-07504-2\n10.1016/j.ijid.2020.05.021\n10.1016/j.ijmedinf.2019.06.017\n10.1109/TSMC.1973.4309314\n10.1016/j.media.2020.101819\n10.1016/j.advengsoft.2013.12.007\n10.1007/s10462-009-9124-7\n10.1016/B978-0-12-809633-8.20473-1\n10.11613/BM.2014.003\n10.1109/TMI.2017.2775636\n10.5152/dir.2020.20205"}
{"title": "Lung Sonography in Critical Care Medicine.", "abstract": "During the last five decades, lung sonography has developed into a core competency of intensive care medicine. It is a highly accurate bedside tool, with clear diagnostic criteria for most causes of respiratory failure (pneumothorax, pulmonary edema, pneumonia, pulmonary embolism, chronic obstructive pulmonary disease, asthma, and pleural effusion). It helps in distinguishing a hypovolemic from a cardiogenic, obstructive, or distributive shock. In addition to diagnostics, it can also be used to guide ventilator settings, fluid administration, and even antimicrobial therapy, as well as to assess diaphragmatic function. Moreover, it provides risk-reducing guidance during invasive procedures, e.g., intubation, thoracocentesis, or percutaneous dilatational tracheostomy. The recent pandemic has further increased its scope of clinical applications in the management of COVID-19 patients, from their initial presentation at the emergency department, during their hospitalization, and after their discharge into the community. Despite its increasing use, a consensus on education, assessment of competencies, and certification is still missing. Deep learning and artificial intelligence are constantly developing in medical imaging, and contrast-enhanced ultrasound enables new diagnostic perspectives. This review summarizes the clinical aspects of lung sonography in intensive care medicine and provides an overview about current training modalities, diagnostic limitations, and future developments.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-06-25", "authors": ["RobertBreitkopf", "BenediktTreml", "SasaRajsic"], "doi": "10.3390/diagnostics12061405\n10.1007/s00134-015-3952-5\n10.1186/s13089-017-0059-y\n10.1016/S2213-2600(14)70135-3\n10.1378/chest.14-2608\n10.1097/MD.0000000000005713\n10.1186/cc13016\n10.1590/S1516-31802010000200009\n10.1097/CCM.0000000000003129\n10.1097/CCM.0b013e31824e68ae\n10.3760/CMA.J.ISSN.2095-4352.2015.07.008\n10.1007/s00134-016-4411-7\n10.1186/cc13859\n10.1164/rccm.201003-0369OC\n10.1056/NEJMra072149\n10.7754/Clin.Lab.2017.170730\n10.1213/ANE.0b013e3181e7cc42\n10.1002/uog.22034\n10.1016/j.ajog.2020.04.020\n10.21106/ijma.294\n10.1016/j.aca.2020.10.009\n10.1016/j.ultrasmedbio.2020.04.033\n10.3390/diagnostics11122202\n10.1016/j.crad.2020.05.001\n10.1007/s00134-021-06506-y\n10.1378/chest.13-0882\n10.1056/NEJMra0909487\n10.1097/01.CCM.0000260624.99430.22\n10.1378/chest.07-2800\n10.1002/jum.15285\n10.1016/j.acra.2020.07.002\n10.1164/rccm.201802-0227LE\n10.15557/JoU.2021.0036\n10.1093/BJACEACCP/MKV012\n10.1186/2110-5820-4-1\n10.4103/0970-2113.156245\n10.1097/00005373-200204000-00029\n10.4103/1658-354X.197351\n10.1164/ajrccm.156.5.96-07096\n10.1007/s00134-012-2513-4\n10.1007/s00134-003-1930-9\n10.1007/s00134-010-2079-y\n10.1016/j.ajem.2016.07.032\n10.5811/westjem.2015.3.24809\n10.1007/s001340000627\n10.1016/j.redar.2020.02.008\n10.1378/chest.108.5.1345\n10.1197/j.aem.2005.05.005\n10.1097/01.TA.0000133565.88871.E4\n10.1007/s00134-014-3402-9\n10.1378/chest.12-1445\n10.1097/01.CCM.0000164542.86954.B4\n10.1097/00005373-200102000-00003\n10.1007/978-3-319-44072-9_4\n10.1186/1472-6920-9-3\n10.1111/j.1553-2712.2008.00347.x\n10.1186/1476-7120-4-34\n10.3389/fphys.2022.838479\n10.1378/chest.09-0001\n10.1186/1476-7120-6-16\n10.7863/jum.2003.22.2.173\n10.1378/chest.10-0435\n10.1007/s001340050771\n10.1159/000074195\n10.1186/s12890-015-0091-2\n10.1097/CCM.0b013e3181b08cdb\n10.1007/s00134-017-4941-7\n10.3390/JCM11051224\n10.1097/CCM.0000000000003340\n10.1186/cc5668\n10.1097/CCM.0000000000003377\n10.1007/978-3-642-37096-0_22\n10.1378/chest.128.3.1531\n10.1016/0301-5629(95)02003-9\n10.1378/chest.13-1087\n10.1378/chest.08-2281\n10.1136/emj.2010.101584\n10.1378/chest.12-0364\n10.1016/j.chest.2015.12.012\n10.3390/jcm10112453\n10.2214/ajr.159.4.1529829\n10.1007/s001340050988\n10.1007/s00134-005-0024-2\n10.1097/01.CCM.0000171532.02639.08\n10.1378/chest.127.1.224\n10.1007/s40477-017-0266-1\n10.1148/radiology.191.3.8184046\n10.1097/00000542-200401000-00006\n10.2214/ajr.159.1.1609716\n10.1097/00063198-200307000-00007\n10.1002/jcu.1870190206\n10.1136/thx.2008.100545\n10.1378/chest.126.1.129\n10.1148/rg.322115127\n10.1016/j.ultrasmedbio.2010.10.004\n10.1111/j.1440-1843.2011.02005.x\n10.1097/CCM.0b013e3182266408\n10.1177/0885066615583639\n10.1186/2036-7902-6-8\n10.1007/s00134-012-2547-7\n10.1186/s13054-015-0894-9\n10.1164/rccm.201004-0670OC\n10.1164/rccm.201602-0367OC\n10.1016/S0009-9260(05)82987-3\n10.1007/s00134-015-4125-2\n10.1136/thoraxjnl-2013-204111\n10.1016/S0012-3692(15)32912-3\n10.1097/00003246-199612000-00020\n10.1007/s12630-014-0301-z\n10.1016/j.annemergmed.2006.07.004\n10.1197/j.aem.2005.08.014\n10.1017/S1049023X00002004\n10.1002/ppul.25955\n10.1378/chest.125.3.1059\n10.1186/1477-7819-12-139\n10.1001/archinternmed.2009.548\n10.1378/chest.12-0447\n10.1378/chest.123.2.436\n10.5402/2012/676524\n10.1378/chest.11-0348\n10.1177/0885066618755334\n10.1007/s00408-019-00230-7\n10.1002/jum.14448\n10.1007/s12028-009-9259-z\n10.1186/cc10344\n10.2139/ssrn.3544750\n10.26355/EURREV_202003_20549\n10.1148/radiol.2020200847\n10.1186/s13089-020-00171-w\n10.4269/ajtmh.20-0280\n10.1002/emp2.12194\n10.1186/S13089-021-00250-6\n10.1378/chest.08-2305\n10.1007/S00134-011-2246-9\n10.1016/j.chest.2019.08.859\n10.1016/j.chest.2019.08.806\n10.1186/s13089-018-0103-6\n10.1186/cc10194\n10.1007/s00134-009-1531-3\n10.1186/s13089-017-0081-0\n10.2214/ajr.168.2.9016251\n10.2214/ajr.164.6.7754907\n10.1109/TUFFC.2021.3094849\n10.1109/TMI.2020.2994459\n10.1109/JBHI.2019.2936151\n10.1109/TUFFC.2020.3002249\n10.1016/S0140-6736(20)31875-4\n10.1055/A-0586-1107\n10.1016/j.jus.2008.05.008\n10.4329/wjr.v5.i10.372\n10.1002/jum.15338\n10.1007/s00134-020-06085-4"}
{"title": "Novel COVID-19 Diagnosis Delivery App Using Computed Tomography Images Analyzed with Saliency-Preprocessing and Deep Learning.", "abstract": "This app project was aimed to remotely deliver diagnoses and disease-progression information to COVID-19 patients to help minimize risk during this and future pandemics. Data collected from chest computed tomography (CT) scans of COVID-19-infected patients were shared through the app. In this article, we focused on image preprocessing techniques to identify and highlight areas with ground glass opacity (GGO) and pulmonary infiltrates (PIs) in CT image sequences of COVID-19 cases. Convolutional neural networks (CNNs) were used to classify the disease progression of pneumonia. Each GGO and PI pattern was highlighted with saliency map fusion, and the resulting map was used to train and test a CNN classification scheme with three classes. In addition to patients, this information was shared between the respiratory triage/radiologist and the COVID-19 multidisciplinary teams with the application so that the severity of the disease could be understood through CT and medical diagnosis. The three-class, disease-level COVID-19 classification results exhibited a macro-precision of more than 94.89% in a two-fold cross-validation. Both the segmentation and classification results were comparable to those made by a medical specialist.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-06-24", "authors": ["SantiagoTello-Mijares", "FomuyWoo"], "doi": "10.3390/tomography8030134\n10.1016/j.radi.2020.05.012\n10.1148/radiol.2020201160\n10.1007/s00330-020-06955-x\n10.1148/radiol.2020200343\n10.2214/AJR.20.22976\n10.1016/j.diii.2020.03.014\n10.1016/j.ejrad.2020.108941\n10.5152/dir.2020.20144\n10.1148/radiol.2020200230\n10.1007/s00330-020-06976-6\n10.1056/NEJMoa2002032\n10.1111/jgh.15094\n10.3390/diagnostics12040846\n10.1016/j.ejrad.2020.109008\n10.1016/j.jrid.2020.04.001\n10.1109/TNNLS.2016.2582924\n10.1021/acs.jcim.8b00706\n10.1109/ACCESS.2020.3009328\n10.48550/arXiv.2004.07407\n10.1016/j.compbiomed.2020.104037\n10.1016/j.patrec.2020.10.001\n10.1148/radiol.2020200905\n10.1109/ACCESS.2021.3058854\n10.1109/TIP.2021.3058783\n10.1007/BF00133570\n10.1109/TIP.2008.925375\n10.1155/2021/8869372\n10.1007/s10140-021-01937-y\n10.1109/TPAMI.1986.4767851\n10.1117/1.JBO.23.5.056005\n10.1148/ryct.2020200213\n10.1097/RLI.0000000000000670\n10.1148/radiol.2020200843\n10.1148/radiol.2020201473\n10.1109/5.726791\n10.1145/3065386\n10.1109/ic-ETITE47903.2020.049\n10.1109/TEVC.2021.3088631\n10.1136/bmj.m998\n10.2196/18810\n10.1109/42.363096"}
{"title": "Exploring COVID-19-Related Stressors: Topic Modeling Study.", "abstract": "The COVID-19 pandemic has affected the lives of people globally for over 2 years. Changes in lifestyles due to the pandemic may cause psychosocial stressors for individuals and could lead to mental health problems. To provide high-quality mental health support, health care organizations need to identify COVID-19-specific stressors and monitor the trends in the prevalence of those stressors.\nThis study aims to apply natural language processing (NLP) techniques to social media data to identify the psychosocial stressors during the COVID-19 pandemic and to analyze the trend in the prevalence of these stressors at different stages of the pandemic.\nWe obtained a data set of 9266 Reddit posts from the subreddit \\rCOVID19_support, from February 14, 2020, to July 19, 2021. We used the latent Dirichlet allocation (LDA) topic model to identify the topics that were mentioned on the subreddit and analyzed the trends in the prevalence of the topics. Lexicons were created for each of the topics and were used to identify the topics of each post. The prevalences of topics identified by the LDA and lexicon approaches were compared.\nThe LDA model identified 6 topics from the data set: (1) \"fear of coronavirus,\" (2) \"problems related to social relationships,\" (3) \"mental health symptoms,\" (4) \"family problems,\" (5) \"educational and occupational problems,\" and (6) \"uncertainty on the development of pandemic.\" According to the results, there was a significant decline in the number of posts about the \"fear of coronavirus\" after vaccine distribution started. This suggests that the distribution of vaccines may have reduced the perceived risks of coronavirus. The prevalence of discussions on the uncertainty about the pandemic did not decline with the increase in the vaccinated population. In April 2021, when the Delta variant became prevalent in the United States, there was a significant increase in the number of posts about the uncertainty of pandemic development but no obvious effects on the topic of fear of the coronavirus.\nWe created a dashboard to visualize the trend in the prevalence of topics about COVID-19-related stressors being discussed on a social media platform (Reddit). Our results provide insights into the prevalence of pandemic-related stressors during different stages of the COVID-19 pandemic. The NLP techniques leveraged in this study could also be applied to analyze event-specific stressors in the future.", "journal": "Journal of medical Internet research", "date": "2022-06-23", "authors": ["Yue TongLeung", "FarzadKhalvati"], "doi": "10.2196/37142\n10.1016/j.jad.2021.02.056\n10.1145/2464464.2464480\n10.1109/JBHI.2015.2403839\n10.1093/ofid/ofaa258\n10.1109/JBHI.2020.3001216\n10.2196/25431\n10.18653/v1/2020.nlpcovid19-2.8\n10.2196/22635\n10.3115/v1/w15-1211\n10.1007/s11606-020-05898-9\n10.1109/wiiat50758.2020.00104\n10.1371/journal.pone.0256406\n10.1371/journal.pone.0256406\n10.1016/j.janxdis.2020.102232\n10.1016/j.jpsychires.2020.10.035"}
{"title": "A meta-analysis of the diagnostic test accuracy of CT-based radiomics for the prediction of COVID-19 severity.", "abstract": "According to the Chinese Health Commission guidelines, coronavirus disease 2019 (COVID-19) severity is classified as mild, moderate, severe, or critical. The mortality rate of COVID-19 is higher among patients with severe and critical diseases; therefore, early identification of COVID-19 prevents disease progression and improves patient survival. Computed tomography (CT) radiomics, as a machine learning method, provides an objective and mathematical evaluation of COVID-19 pneumonia. As CT-based radiomics research has recently focused on COVID-19 diagnosis and severity analysis, this meta-analysis aimed to investigate the predictive power of a CT-based radiomics model in determining COVID-19 severity.\nThis study followed the diagnostic version of PRISMA guidelines. PubMed, Embase databases and the Cochrane Central Register of Controlled Trials, and the Cochrane Database of Systematic Reviews were searched to identify relevant articles in the meta-analysis from inception until July 16, 2021. The sensitivity and specificity were analyzed using forest plots. The overall predictive power was calculated using the summary receiver operating characteristic curve. The bias was evaluated using a funnel plot. The quality of the included literature was assessed using the radiomics quality score and quality assessment of diagnostic accuracy studies tool.\nThe radiomics quality scores ranged from 7 to 16 (achievable score:\u20092212\u20098 to 36). The pooled sensitivity and specificity were 0.800 (95% confidence interval [CI] 0.662-0.891) and 0.874 (95% CI 0.773-0.934), respectively. The pooled area under the receiver operating characteristic curve was 0.908. The quality assessment tool showed favorable results.\nThis meta-analysis demonstrated that CT-based radiomics models might be helpful for predicting the severity of COVID-19 pneumonia.", "journal": "La Radiologia medica", "date": "2022-06-23", "authors": ["Yung-ShuoKao", "Kun-TeLin"], "doi": "10.1007/s11547-022-01510-8\n10.1097/CM9.0000000000000819\n10.1001/jama.2020.2648\n10.5603/ARM.a2021.0036\n10.3389/fmed.2020.00311\n10.1007/s00330-020-06731-x\n10.1097/RLI.0000000000000674\n10.1148/radiol.2020200642\n10.1016/j.jacr.2020.03.006\n10.7150/thno.30309\n10.1007/s00330-020-06666-3\n10.1371/journal.pone.0246582\n10.1038/s41467-020-18685-1\n10.3390/diagnostics11060991\n10.1097/RCT.0000000000001094\n10.1001/jama.2017.19163\n10.3348/kjr.2015.16.6.1188\n10.1038/nrclinonc.2017.141\n10.7326/0003-4819-155-8-201110180-00009\n10.1038/s41598-020-76141-y\n10.3348/kjr.2020.1104\n10.3346/jkms.2021.36.e46\n10.1186/s12879-021-06331-0\n10.1007/s00330-021-07727-x\n10.1097/MD.0000000000025307\n10.1007/s00330-020-07012-3\n10.1016/j.acra.2020.09.004\n10.1088/1361-6560/abbf9e\n10.1109/JBHI.2020.3036722\n10.1148/radiol.2020191145\n10.1148/radiol.2363040958\n10.1148/radiol.2020200370\n10.1080/22221751.2020.1735265\n10.1148/radiol.2019191154"}
{"title": "Reduction in Chest CT Severity and Improved Hospital Outcomes in SARS-CoV-2 Omicron Compared with Delta Variant Infection.", "abstract": "Background The SARS-Cov-2 Omicron variant demonstrates rapid spread but reduced disease severity. Studies evaluating lung imaging findings of Omicron infection versus non-Omicron infection remain lacking. Purpose To compare the Omicron variant with the SARS-CoV-2 Delta variant according to their chest CT radiologic pattern, biochemical parameters, clinical severity, and hospital outcomes after adjusting for vaccination status. Materials and Methods This retrospective study included hospitalized adult patients with reverse transcriptase-polymerase chain reaction test results positive for SARS-CoV-2, with CT pulmonary angiography performed within 7 days of admission between December 1, 2021, and January 14, 2022. Multiple readers performed blinded radiologic analyses that included RSNA CT classification, chest CT severity score (CTSS) (range, 0 [least severe] to 25 [most severe]), and CT imaging features, including bronchial wall thickening. Results A total of 106 patients (Delta group, ", "journal": "Radiology", "date": "2022-06-22", "authors": ["Maria TTsakok", "Robert AWatson", "Shyamal JSaujani", "MarkKong", "ChengXie", "HeikoPeschl", "LouiseWing", "Fiona KMacLeod", "BrianShine", "Nicholas PTalbot", "Rachel EBenamore", "David WEyre", "FergusGleeson"], "doi": "10.1148/radiol.220533\n10.21203/rs.3.rs-1211792/v1"}
{"title": "Convolutional neural network based CT scan classification method for COVID-19 test validation.", "abstract": "Given the novel corona virus discovered in Wuhan, China, in December 2019, due to the high false-negative rate of RT-PCR and the time-consuming to obtain the results, research has proved that computed tomography (CT) has become an auxiliary One of the essential means of diagnosis and treatment of new corona virus pneumonia. Since few COVID-19 CT datasets are currently available, it is proposed to use conditional generative adversarial networks to enhance data to obtain CT datasets with more samples to reduce the risk of over fitting. In addition, a BIN residual block-based method is proposed. The improved U-Net network is used for image segmentation and then combined with multi-layer perception for classification prediction. By comparing with network models such as AlexNet and GoogleNet, it is concluded that the proposed BUF-Net network model has the best performance, reaching an accuracy rate of 93%. Using Grad-CAM technology to visualize the system's output can more intuitively illustrate the critical role of CT images in diagnosing COVID-19. Applying deep learning using the proposed techniques suggested by the above study in medical imaging can help radiologists achieve more effective diagnoses that is the main objective of the research. On the basis of the foregoing, this study proposes to employ CGAN technology to augment the restricted data set, integrate the residual block into the U-Net network, and combine multi-layer perception in order to construct new network architecture for COVID-19 detection using CT images. -19. Given the scarcity of COVID-19 CT datasets, it is proposed that conditional generative adversarial networks be used to augment data in order to obtain CT datasets with more samples and therefore lower the danger of overfitting.", "journal": "Smart health (Amsterdam, Netherlands)", "date": "2022-06-21", "authors": ["MukeshSoni", "Ajay KumarSingh", "K SureshBabu", "SumitKumar", "AkhileshKumar", "ShwetaSingh"], "doi": "10.1016/j.smhl.2022.100296\n10.2174/1573405617666210215143503\n10.1108/WJE-12-2020-0631\n10.1109/ICAS49788.2021.9551169\n10.1108/WJE-09-2020-0450\n10.1109/JBHI.2021.3060035\n10.1109/ASYU52992.2021.9598993\n10.1109/ACCESS.2020.3005510\n10.1109/JBHI.2020.3042523\n10.1155/2021/9293877\n10.1109/JBHI.2021.3051470\n10.1109/ESCI50559.2021.9396773\n10.1109/ISITIA52817.2021.9502217\n10.1109/ICOIACT50329.2020.9332123\n10.4018/IJSIR.287544\n10.1109/TMI.2020.2994908\n10.1109/CAC51589.2020.9326470\n10.1109/TMI.2021.3104474\n10.1109/TBDATA.2021.3056564\n10.1109/JBHI.2021.3067465"}
{"title": "Value of quantitative airspace disease measured on chest CT and chest radiography at initial diagnosis compared to clinical variables for prediction of severe COVID-19.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2022-06-21", "authors": ["Hae-MinJung", "RochelleYang", "Warren BGefter", "Florin CGhesu", "BorisMailhe", "AwaisMansoor", "SasaGrbic", "DorinComaniciu", "SebastianVogt", "Eduardo JMortani Barbosa"], "doi": "10.1117/1.JMI.9.3.034003\n10.1016/j.jpha.2020.03.004\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200075\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103869\n10.1148/radiol.2020200241\n10.1177/0846537120938328\n10.1016/j.cpcardiol.2020.100618\n10.1136/bmj.m1328\n10.1016/j.chaos.2020.109947\n10.7326/M20-3905\n10.1007/s00330-020-07033-y\n10.1016/j.ejrad.2020.109271\n10.1016/j.cell.2020.04.045\n10.3348/kjr.2020.0171\n10.3348/kjr.2020.0215\n10.1016/j.cca.2020.06.051\n10.1016/j.isprsjprs.2016.01.011\n10.1186/s12863-018-0633-8\n10.1142/S0219720005001004\n10.1109/CITA.2015.7349827\n10.1097/RLI.0000000000000763\n10.1109/CVPR.2016.90\n10.1007/978-3-319-24574-4_28\n10.1016/j.media.2021.102087\n10.1093/bioinformatics/btt383\n10.1214/15-EJS1035\n10.1186/1471-2105-12-77"}
{"title": "Improved Analysis of COVID-19 Influenced Pneumonia from the Chest X-Rays Using Fine-Tuned Residual Networks.", "abstract": "COVID-19 has remained a threat to world life despite a recent reduction in cases. There is still a possibility that the virus will evolve and become more contagious. If such a situation occurs, the resulting calamity will be worse than in the past if we act irresponsibly. COVID-19 must be widely screened and recognized early to avert a global epidemic. Positive individuals should be quarantined immediately, as this is the only effective way to prevent a global tragedy that has occurred previously. No positive case should go unrecognized. However, current COVID-19 detection procedures require a significant amount of time during human examination based on genetic and imaging techniques. Apart from RT-PCR and antigen-based tests, CXR and CT imaging techniques aid in the rapid and cost-effective identification of COVID. However, discriminating between diseased and normal X-rays is a time-consuming and challenging task requiring an expert's skill. In such a case, the only solution was an automatic diagnosis strategy for identifying COVID-19 instances from chest X-ray images. This article utilized a deep convolutional neural network, ResNet, which has been demonstrated to be the most effective for image classification. The present model is trained using pretrained ResNet on ImageNet weights. The versions of ResNet34, ResNet50, and ResNet101 were implemented and validated against the dataset. With a more extensive network, the accuracy appeared to improve. Nonetheless, our objective was to balance accuracy and training time on a larger dataset. By comparing the prediction outcomes of the three models, we concluded that ResNet34 is a more likely candidate for COVID-19 detection from chest X-rays. The highest accuracy level reached 98.34%, which was higher than the accuracy achieved by other state-of-the-art approaches examined in earlier studies. Subsequent analysis indicated that the incorrect predictions occurred with approximately 100% certainty. This uncovered a severe weakness in CNN, particularly in the medical area, where critical decisions are made. However, this can be addressed further in a future study by developing a modified model to incorporate uncertainty into the predictions, allowing medical personnel to manually review the incorrect predictions.", "journal": "Computational intelligence and neuroscience", "date": "2022-06-21", "authors": ["AmelKsibi", "MohammedZakariah", "ManelAyadi", "HelaElmannai", "Prashant KumarShukla", "HalifaAwal", "MoniaHamdi"], "doi": "10.1155/2022/9414567\n10.1016/s0140-6736(20)30183-5\n10.1001/jama.2020.3786\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.3201/eid2606.200301\n10.1038/s42256-021-00338-7\n10.5114/pjr.2020.100788\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343\n10.1016/j.media.2020.101794\n10.1146/annurev.bioeng.8.061505.095802\n10.1093/bib/bbx044\n10.1097/rli.0000000000000672\n10.1016/j.bbe.2020.08.008\n10.1016/j.media.2017.07.005\n10.1148/radiol.2019192515\n10.1109/access.2020.3016780\n10.1109/tii.2021.3057524\n10.1371/journal.pone.0255886\n10.1109/cvpr.2017.369\n10.1007/s13246-020-00865-4\n10.1016/j.cell.2018.02.010\n10.1016/j.compbiomed.2020.103792\n10.1109/access.2020.3010287\n10.1016/j.cmpb.2020.105581\n10.1007/s12559-021-09848-3\n10.1109/access.2020.2994762\n10.1016/j.compbiomed.2021.104319\n10.1038/s41598-020-76550-z\n10.1155/2021/8828404\n10.1109/iccc51575.2020.9344870\n10.1155/2021/3281135\n10.1117/1.jmi.8.s1.017503\n10.1109/aset.2018.8379889\n10.1016/j.knosys.2015.01.010\n10.1007/s11263-015-0816-y\n10.1109/CVPR.2016.90\n10.1016/j.patcog.2019.01.006\n10.1371/journal.pone.0118432\n10.1007/978-981-15-5281-6_7\n10.1016/j.mehy.2020.109761\n10.1101/2020.07.08.20149161\n10.1007/s10044-021-00970-4\n10.1155/2021/6799202\n10.1109/cvpr.2015.7298640"}
{"title": "An insight into the current perceptions of UK radiographers on the future impact of AI on the profession: A cross-sectional survey.", "abstract": "As a profession, radiographers have always been keen on adapting and integrating new technologies. The increasing integration of artificial intelligence (AI) into clinical practice in the last five years has been met with scepticism by some, who predict the demise of the profession, whilst others suggest a bright future with AI, full of opportunities and synergies. Post COVID-19 pandemic need for economic recovery and a backlog of medical imaging and reporting may accelerate the adoption of AI. It is therefore timely to appreciate practitioners' perceptions of AI used in clinical practice and their perception of the short-term impact on the profession.\nThis study aims to explore the perceptions of AI in the UK radiography workforce and to investigate its current AI applications and future technological expectations of radiographers.\nAn online survey (Qualtrics\n411 responses were collected (80% diagnostic radiographers (DR); 20% therapeutic radiographers (TR)). Awareness of AI used in clinical practice is low, with DR respondents suggesting AI will have the most value/potential in cross sectional imaging and image reporting. TR responses linked AI as having most value in treatment planning, contouring, and image acquisition/matching. Respondents felt that AI will impact radiographers' daily work (DR, 79.6%; TR, 88.9%) by standardising some aspects of patient care and technical factors of radiography practice. A mixed response about impact on careers was reported.\nRespondents were unsure about the ways in which AI is currently used in practice and how AI will impact on careers in the future. It was felt that AI integration will lead to increased job opportunities to contribute to decision making as an end user. Job security was not identified as a cause for concern.", "journal": "Journal of medical imaging and radiation sciences", "date": "2022-06-18", "authors": ["ClareRainey", "TracyO'Regan", "JacquelineMatthew", "EmilySkelton", "NickWoznitza", "Kwun-YeChu", "SpencerGoodman", "JonathanMcConnell", "CiaraHughes", "RaymondBond", "ChristinaMalamateniou", "SonyiaMcFadden"], "doi": "10.1016/j.jmir.2022.05.010"}
{"title": "Combating COVID-19 Using Generative Adversarial Networks and Artificial Intelligence for Medical Images: Scoping Review.", "abstract": "Research on the diagnosis of COVID-19 using lung images is limited by the scarcity of imaging data. Generative adversarial networks (GANs) are popular for synthesis and data augmentation. GANs have been explored for data augmentation to enhance the performance of artificial intelligence (AI) methods for the diagnosis of COVID-19 within lung computed tomography (CT) and X-ray images. However, the role of GANs in overcoming data scarcity for COVID-19 is not well understood.\nThis review presents a comprehensive study on the role of GANs in addressing the challenges related to COVID-19 data scarcity and diagnosis. It is the first review that summarizes different GAN methods and lung imaging data sets for COVID-19. It attempts to answer the questions related to applications of GANs, popular GAN architectures, frequently used image modalities, and the availability of source code.\nA search was conducted on 5 databases, namely PubMed, IEEEXplore, Association for Computing Machinery (ACM) Digital Library, Scopus, and Google Scholar. The search was conducted from October 11-13, 2021. The search was conducted using intervention keywords, such as \"generative adversarial networks\" and \"GANs,\" and application keywords, such as \"COVID-19\" and \"coronavirus.\" The review was performed following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR) guidelines for systematic and scoping reviews. Only those studies were included that reported GAN-based methods for analyzing chest X-ray images, chest CT images, and chest ultrasound images. Any studies that used deep learning methods but did not use GANs were excluded. No restrictions were imposed on the country of publication, study design, or outcomes. Only those studies that were in English and were published from 2020 to 2022 were included. No studies before 2020 were included.\nThis review included 57 full-text studies that reported the use of GANs for different applications in COVID-19 lung imaging data. Most of the studies (n=42, 74%) used GANs for data augmentation to enhance the performance of AI techniques for COVID-19 diagnosis. Other popular applications of GANs were segmentation of lungs and superresolution of lung images. The cycleGAN and the conditional GAN were the most commonly used architectures, used in 9 studies each. In addition, 29 (51%) studies used chest X-ray images, while 21 (37%) studies used CT images for the training of GANs. For the majority of the studies (n=47, 82%), the experiments were conducted and results were reported using publicly available data. A secondary evaluation of the results by radiologists/clinicians was reported by only 2 (4%) studies.\nStudies have shown that GANs have great potential to address the data scarcity challenge for lung images in COVID-19. Data synthesized with GANs have been helpful to improve the training of the convolutional neural network (CNN) models trained for the diagnosis of COVID-19. In addition, GANs have also contributed to enhancing the CNNs' performance through the superresolution of the images and segmentation. This review also identified key limitations of the potential transformation of GAN-based methods in clinical applications.", "journal": "JMIR medical informatics", "date": "2022-06-17", "authors": ["HazratAli", "ZubairShah"], "doi": "10.2196/37365\n10.1016/S1473-3099(20)30235-8\n10.1016/S1473-3099(20)30235-8\n10.1002/jmv.25786\n10.2196/20756\n10.3389/fmed.2021.704256\n10.3389/fmed.2021.704256\n10.1109/access.2020.3010287\n10.1152/physiolgenomics.00029.2020\n10.1109/tai.2020.3020521\n10.1109/access.2020.3023495\n10.1007/s10916-018-1072-9\n10.1016/j.media.2019.101552\n10.1109/iccv.2017.244\n10.3389/fpubh.2020.00164\n10.3389/fpubh.2020.00164\n10.1002/acm2.13121\n10.2196/27414\n10.7326/m18-0850\n10.1109/jbhi.2020.3042523\n10.1007/s00259-020-04929-1\n10.1038/s41598-021-87994-2\n10.1038/s41598-021-87994-2\n10.1007/s13246-020-00952-6\n10.1016/j.media.2021.102159\n10.1007/s12559-020-09785-7\n10.1007/s00521-020-05437-x\n10.1016/j.compbiomed.2020.104181\n10.1002/mp.15044\n10.1007/s10796-021-10144-6\n10.1007/s12539-020-00403-6\n10.1016/j.eswa.2021.115681\n10.2147/idr.s296346\n10.1007/s00521-020-05636-6\n10.3390/diagnostics11050895\n10.1007/s12559-021-09926-6\n10.1016/j.bspc.2021.102901\n10.1007/s42979-021-00795-2\n10.1016/j.csbj.2021.02.016\n10.1016/j.bspc.2021.103182\n10.1109/conit51480.2021.9498272\n10.1109/isbi48211.2021.9434159\n10.1109/access.2020.2994762\n10.1109/bibm49941.2020.9313466\n10.1109/icassp39728.2021.9414031\n10.1109/cec45853.2021.9504743\n10.1109/prai53619.2021.9551043\n10.1109/bigdata50022.2020.9377878\n10.1109/pic50277.2020.9350813\n10.1109/tem.2021.3103334\n10.1109/jbhi.2021.3067465\n10.1109/csci51800.2020.00160\n10.1109/access.2020.3025010\n10.1109/isbi48211.2021.9433806\n10.3390/sym12040651\n10.1007/s13278-021-00731-5\n10.3390/engproc2021007006\n10.1109/access.2020.3017915\n10.1155/2021/6680455\n10.32604/csse.2021.017191\n10.3390/app11167174\n10.1145/3458744.3474039\n10.1145/3449639.3459319\n10.1007/s00521-021-06344-5\n10.1016/j.aej.2021.01.011\n10.1016/j.eswa.2021.115401\n10.1007/978-3-030-60802-6_36\n10.3390/sym12091530\n10.32604/cmc.2022.018547\n10.32604/cmc.2022.018564\n10.1007/s10489-020-01867-1\n10.1007/978-3-030-86340-1_47\n10.1007/978-3-030-68035-0_12\n10.1117/12.2582162"}
{"title": "Machine learning applications for COVID-19 outbreak management.", "abstract": "Recently, the COVID-19 epidemic has resulted in millions of deaths and has impacted practically every area of human life. Several machine learning (ML) approaches are employed in the medical field in many applications, including detecting and monitoring\u00a0patients, notably in COVID-19 management. Different medical imaging systems, such as computed tomography (CT) and X-ray, offer ML an excellent platform for combating the pandemic. Because of this need, a significant quantity of study has been carried out; thus, in this work, we employed a systematic literature review (SLR) to cover all aspects of outcomes from related papers. Imaging methods, survival analysis, forecasting, economic and geographical issues, monitoring methods, medication development, and hybrid apps are the seven key uses of applications employed in the COVID-19 pandemic. Conventional neural networks (CNNs), long short-term memory networks (LSTM), recurrent neural networks (RNNs), generative adversarial networks (GANs), autoencoders, random forest, and other ML techniques are frequently used in such scenarios. Next, cutting-edge applications\u00a0related to ML\u00a0techniques for pandemic medical issues are discussed. Various problems and challenges linked with ML applications for this pandemic were reviewed. It is expected that additional research will be conducted in the upcoming to limit the spread and catastrophe management. According to the data, most papers are evaluated mainly on characteristics such as flexibility and accuracy, while other factors such as safety are overlooked. Also, Keras was the most often used library in the research studied, accounting for 24.4 percent of the time. Furthermore, medical imaging systems are employed for diagnostic reasons in 20.4 percent of applications.", "journal": "Neural computing & applications", "date": "2022-06-16", "authors": ["ArashHeidari", "NimaJafari Navimipour", "MehmetUnal", "ShivaToumaj"], "doi": "10.1007/s00521-022-07424-w\n10.1016/j.compbiomed.2022.105244\n10.1109/ACCESS.2021.3054484\n10.1109/TSC.2021.3061402\n10.1109/TNNLS.2021.3084250\n10.1109/TFUZZ.2022.3141761\n10.1016/j.cpcardiol.2022.101108\n10.1002/ima.22697\n10.1016/j.dss.2022.113752\n10.1109/ACCESS.2021.3079121\n10.1109/TAI.2022.3142241\n10.1016/j.jiph.2021.11.013\n10.1016/j.csbj.2021.11.040\n10.1016/j.jpha.2021.12.006\n10.1016/j.cpcardiol.2021.101034\n10.1109/JIOT.2021.3066575\n10.1016/j.idm.2021.12.005"}
{"title": "Mandating Limits on Workload, Duty, and Speed in Radiology.", "abstract": "Research has not yet quantified the effects of workload or duty hours on the accuracy of radiologists. With the exception of a brief reduction in imaging studies during the 2020 peak of the COVID-19 pandemic, the workload of radiologists in the United States has seen relentless growth in recent years. One concern is that this increased demand could lead to reduced accuracy. Behavioral studies in species ranging from insects to humans have shown that decision speed is inversely correlated to decision accuracy. A potential solution is to institute workload and duty limits to optimize radiologist performance and patient safety. The concern, however, is that any prescribed mandated limits would be arbitrary and thus no more advantageous than allowing radiologists to self-regulate. Specific studies have been proposed to determine whether limits reduce error, and if so, to provide a principled basis for such limits. This could determine the precise susceptibility of individual radiologists to medical error as a function of speed during image viewing, the maximum number of studies that could be read during a work shift, and the appropriate shift duration as a function of time of day. Before principled recommendations for restrictions are made, however, it is important to understand how radiologists function both optimally and at the margins of adequate performance. This study examines the relationship between interpretation speed and error rates in radiology, the potential influence of artificial intelligence on reading speed and error rates, and the possible outcomes of imposed limits on both caseload and duty hours. This review concludes that the scientific evidence needed to make meaningful rules is lacking and notes that regulating workloads without scientific principles can be more harmful than not regulating at all.", "journal": "Radiology", "date": "2022-06-15", "authors": ["RobertAlexander", "StephenWaite", "Michael ABruno", "Elizabeth AKrupinski", "LeonardBerlin", "StephenMacknik", "SusanaMartinez-Conde"], "doi": "10.1148/radiol.212631"}
{"title": "Chronic lung lesions in COVID-19 survivors: predictive clinical model.", "abstract": "This study aimed to propose a simple, accessible and low-cost predictive clinical model to detect lung lesions due to COVID-19 infection.\nThis prospective cohort study included COVID-19 survivors hospitalised between 30 March 2020 and 31 August 2020 followed-up 6\u2009months after hospital discharge. The pulmonary function was assessed using the modified Medical Research Council (mMRC) dyspnoea scale, oximetry (SpO\nA tertiary hospital in Sao Paulo, Brazil.\n749 eligible RT-PCR-confirmed SARS-CoV-2-infected patients aged \u226518 years.\nA predictive clinical model for lung lesion detection on chest CT.\nThere were 470 patients (63%) that had at least one sign of pulmonary involvement and were eligible for CT. Almost half of them (48%) had significant pulmonary abnormalities, including ground-glass opacities, parenchymal bands, reticulation, traction bronchiectasis and architectural distortion. The machine learning model, including the results of 257 patients with complete data on mMRC, SpO\nA predictive clinical model based on CXR, mMRC, oximetry and spirometry data can accurately screen patients with lung lesions after SARS-CoV-2 infection. Given that these examinations are highly accessible and low cost, this protocol can be automated and implemented in different countries for early detection of COVID-19 sequelae.", "journal": "BMJ open", "date": "2022-06-14", "authors": ["Carlos Roberto RibeiroCarvalho", "Rodrigo CarusoChate", "Marcio Valente YamadaSawamura", "Michelle LouvaesGarcia", "Celina AlmeidaLamas", "Diego Armando CardonaCardenas", "Daniel MarioLima", "Paula GobiScudeller", "Jo\u00e3o MarcosSalge", "Cesar HigaNomura", "Marco AntonioGutierrez", "NoneNone", "NoneNone"], "doi": "10.1136/bmjopen-2021-059110\n10.3390/ijerph18084350\n10.1038/s41591-021-01283-z\n10.1186/s12931-021-01628-9\n10.1080/17476348.2021.1916472\n10.36416/1806-3756/e20200545\n10.1016/S0140-6736(20)32656-8\n10.21203/rs.3.rs-266574/v1\n10.1590/1516-3180.2021.139104022021\n10.1001/jama.2021.3331\n10.1164/rccm.2009-040GL\n10.1001/jama.2020.12603\n10.1186/s13613-021-00882-w\n10.7861/clinmed.2020-0353\n10.1038/d41586-021-00586-y\n10.1186/s41747-020-00203-z\n10.1183/13993003.00775-2020\n10.1007/s10278-021-00421-w\n10.1136/bmj.m3026\n10.1136/bmjopen-2021-051706\n10.1183/09031936.05.00034805\n10.1590/s1806-37132007000400008\n10.1148/radiol.2462070712\n10.1007/s00330-020-07033-y\n10.1155/2021/6692409\n10.1016/S0140-6736(21)01755-4\n10.1148/radiol.2021210972\n10.1148/radiol.2021203153\n10.1016/j.rmed.2021.106421\n10.1148/radiol.2021210834\n10.1183/13993003.03690-2020\n10.1513/AnnalsATS.202004-324OC\n10.1007/s10140-020-01869-z\n10.1183/13993003.03481-2020\n10.1016/j.clinimag.2020.04.001"}
{"title": "Automated Multi-View Multi-Modal Assessment of COVID-19 Patients Using Reciprocal Attention and Biomedical Transform.", "abstract": "Automated severity assessment of coronavirus disease 2019 (COVID-19) patients can help rationally allocate medical resources and improve patients' survival rates. The existing methods conduct severity assessment tasks mainly on a unitary modal and single view, which is appropriate to exclude potential interactive information. To tackle the problem, in this paper, we propose a multi-view multi-modal model to automatically assess the severity of COVID-19 patients based on deep learning. The proposed model receives multi-view ultrasound images and biomedical indices of patients and generates comprehensive features for assessment tasks. Also, we propose a reciprocal attention module to acquire the underlying interactions between multi-view ultrasound data. Moreover, we propose biomedical transform module to integrate biomedical data with ultrasound data to produce multi-modal features. The proposed model is trained and tested on compound datasets, and it yields 92.75% for accuracy and 80.95% for recall, which is the best performance compared to other state-of-the-art methods. Further ablation experiments and discussions conformably indicate the feasibility and advancement of the proposed model.", "journal": "Frontiers in public health", "date": "2022-06-14", "authors": ["YanhanLi", "HongyunZhao", "TianGan", "YangLiu", "LianZou", "TingXu", "XuanChen", "CienFan", "MengWu"], "doi": "10.3389/fpubh.2022.886958\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001191\n10.1148/radiol.2020200642\n10.1148/ryct.2020200034\n10.1016/j.compbiomed.2021.104721\n10.1016/j.advms.2020.06.005\n10.1007/s10439-015-1495-0\n10.1016/S2213-2600(20)30120-X\n10.1007/978-3-030-32245-8_64\n10.1016/j.compmedimag.2019.101688\n10.1007/s00330-019-06163-2\n10.1001/jama.2016.17216\n10.1001/jama.2017.18152\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2020200432\n10.1038/s42256-020-0180-7\n10.1016/j.cell.2020.05.032\n10.1038/s41467-020-17971-2\n10.1038/s41467-020-17280-8\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-76282-0\n10.3390/diagnostics12010025\n10.1016/j.compbiomed.2020.104037\n10.1016/j.media.2021.102299\n10.1016/j.bspc.2021.102622\n10.1016/j.rinp.2021.104495\n10.1038/s41598-022-05052-x\n10.1145/3462635\n10.1038/s41598-021-93543-8\n10.1016/j.bspc.2021.103182\n10.1002/jum.15285\n10.48550/arXiv.1412.6980\n10.1109/ICCV.2017.324\n10.48550/arXiv.1409.1556\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.1109/CVPR.2018.00745\n10.1109/CVPR.2017.195\n10.1109/ICCV.2017.74"}
{"title": "A Deep Learning Model for Diagnosing COVID-19 and Pneumonia through X-ray.", "abstract": "The new global pandemic caused by the 2019 novel coronavirus (COVID-19), novel coronavirus pneumonia, has spread rapidly around the world, causing enormous damage to daily life, public health security, and the global economy. Early detection and treatment of COVID-19 infected patients are critical to prevent the further spread of the epidemic. However, existing detection methods are unable to rapidly detect COVID-19 patients, so infected individuals are not detected in a timely manner, which complicates the prevention and control of COVID-19 to some extent. Therefore, it is crucial to developing a rapid and practical COVID-19 detection method. In this work, we explored the application of deep learning in COVID-19 detection to develop a rapid COVID-19 detection method.\nExisting studies have shown that novel coronavirus pneumonia has significant radiographic performance. In this study, we analyze and select the features of chest radiographs. We propose a chest X-Ray (CXR) classification method based on the selected features and investigate the application of transfer learning in detecting pneumonia and COVID-19. Furthermore, we combine the proposed CXR classification method based on selected features with transfer learning and ensemble learning, and propose an ensemble deep learning model based on transfer learning called COVID-ensemble to diagnose pneumonia and COVID-19 using chest x-ray images. The model aims to provide accurate diagnosis for binary classification (no finding/pneumonia) and multivariate classification (COVID-19/No findings/Pneumonia).\nOur proposed CXR classification method based on selection features can significantly improve the CXR classification accuracy of the CNN model. Using this method, DarkNet19 improved its binary and triple classification accuracies by 3.5% and 5.78%, respectively. In addition, the COVID- ensemble achieved 91.5% accuracy in the binary classification task and 91.11% in the multicategory classification task. The experimental results demonstrate that the COVID-ensemble can quickly and accurately detect COVID-19 and pneumonia automatically through X-ray images, and that the performance of this model is superior to that of several existing methods.\nOur proposed COVID-ensemble can not only overcome the limitations of the conventional COVID-19 detection method RT-PCR and provide convenient and fast COVID-19 detection, but also automatically detect pneumonia, thereby reducing the pressure on the medical staff. Using deep learning models to automatically diagnose COVID-19 and pneumonia from X-ray images can serve as a fast and efficient screening method for COVID-19 and pneumonia.", "journal": "Current medical imaging", "date": "2022-06-14", "authors": ["XiangbinLiu", "WenqianWu", "JerryChun-Wei Lin", "ShuaiLiu"], "doi": "10.2174/1573405618666220610093740"}
{"title": "Multilevel threshold image segmentation for COVID-19 chest radiography: A framework using horizontal and vertical multiverse optimization.", "abstract": "COVID-19 is currently raging worldwide, with more patients being diagnosed every day. It usually is diagnosed by examining pathological photographs of the patient's lungs. There is a lot of detailed and essential information on chest radiographs, but manual processing is not as efficient or accurate. As a result, how efficiently analyzing and processing chest radiography of COVID-19 patients is an important research direction to promote COVID-19 diagnosis. To improve the processing efficiency of COVID-19 chest films, a multilevel thresholding image segmentation (MTIS) method based on an enhanced multiverse optimizer (CCMVO) is proposed. CCMVO is improved from the original Multi-Verse Optimizer by introducing horizontal and vertical search mechanisms. It has a more assertive global search ability and can jump out of the local optimum in optimization. The CCMVO-based MTIS method can obtain higher quality segmentation results than HHO, SCA, and other forms and is less prone to stagnation during the segmentation process. To verify the performance of the proposed CCMVO algorithm, CCMVO is first compared with DE, MVO, and other algorithms by 30 benchmark functions; then, the proposed CCMVO is applied to image segmentation of COVID-19 chest radiography; finally, this paper verifies that the combination of MTIS and CCMVO is very successful with good segmentation results by using the Feature Similarity Index (FSIM), the Peak Signal to Noise Ratio (PSNR), and the Structural Similarity Index (SSIM). Therefore, this research can provide an effective segmentation method for a medical organization to process COVID-19 chest radiography and then help doctors diagnose coronavirus pneumonia (COVID-19).", "journal": "Computers in biology and medicine", "date": "2022-06-12", "authors": ["HangSu", "DongZhao", "HelaElmannai", "Ali AsgharHeidari", "SamiBourouis", "ZongdaWu", "ZhennaoCai", "WenyongGui", "MayunChen"], "doi": "10.1016/j.compbiomed.2022.105618\n10.1155/2020/9756518\n10.1109/tnnls.2021.3105484\n10.1109/tii.2022.3165636\n10.1016/j.eswa.2021.114864\n10.1109/tkde.2020.3028943\n10.1109/TPAMI.2020.3023092\n10.1109/JIOT.2021.3102856\n10.1002/int.22689"}
{"title": "Artificial intelligence-based CT metrics used in predicting clinical outcome of COVID-19 in young and middle-aged adults.", "abstract": "Currently, most researchers mainly analyzed coronavirus disease 2019 (COVID-19) pneumonia visually or qualitatively, probably somewhat time-consuming and not precise enough.\nThis study aimed to excavate more information, such as differences in distribution, density, and severity of pneumonia lesions between males and females in a specific age group using artificial intelligence (AI)-based computed tomography (CT) metrics. Besides, these metrics were incorporated into a clinical regression model to predict the short-term outcome.\nThe clinical, laboratory information and a series of HRCT images from 49 patients, aged from 20 to 50 years and confirmed with COVID-19, were collected. The volumes and percentages of infection (POIs) among bilateral lungs and each bronchopulmonary segment were extracted using uAI-Discover-NCP software (version R001). The POI in three HU ranges (i.e., <-300, -300-49, and \u226550\u00a0HU representing ground-glass opacity [GGO], mixed opacity, and consolidation) were also extracted. Hospital stay was predicted with several POI after adjusting days from illness onset to admission, leucocytes, lymphocytes, C-reactive protein, age, and gender using a multiple linear regression model. A total of 91 patients aged 20-50 from public database were selected.\nRight lower lobes had the highest POI, followed by left lower lobes, right upper lobes, middle lobes, and left upper lobes. The distributions in lung lobes and segments were different between the sexes. Men had a higher total POI and GGO of the lungs, but less consolidation than women in initial CT (all p\u00a0<\u00a00.05). The total POI, percentage of consolidation on initial CT, and changed POI were positively correlated with hospital stay in the model. A total of 91 patients aged 20-50 years in the public database were selected, and AI segmentation was performed. The POI of the lower lobes was obviously higher than that in the upper lobes; the POI of each segment of the right upper lobe in the males was higher than that in the females, which was consistent with the result of the 49 patients previously.\nBoth men and women had characteristic distributions in lung lobes and bronchopulmonary segments. AI-based CT quantitative metrics can provide more precise information regarding lesion distribution and severity to predict clinical outcome.", "journal": "Medical physics", "date": "2022-06-12", "authors": ["YuXudong", "LiuWeihong", "XiaFeng", "LiYanli", "LanWeishun", "ZhangFengjun", "GaoJiao", "LiJiawei", "HuangXiaolu", "HuangHuailiang", "LiangJianye", "ZengSihui", "XieChuanmiao", "LiHanhui", "MaoLiang"], "doi": "10.1002/mp.15803"}
{"title": "Application of a Tele-Ultrasound Robot During COVID-19 Pandemic: A Feasibility Study.", "abstract": "To investigate the accuracy of ultrasonic diagnosis using the tele-ultrasound robot in Leishen Shan Hospital.\nTwenty-two patients with novel coronavirus pneumonia from Leishen Shan Hospital voluntarily participated in this study. Their thyroids, neck vessels, hepatobiliaries and kidneys were scanned by both a tele-ultrasound robot manufactured by Imabot Co., Ltd, Wuhan and conventional method. The ultrasound diagnosis of each patient was compared, and the ultrasound images obtained by the two methods were mixed together and double-blindly diagnosed by an experienced ultrasound radiologist.\nThere were 44 positive lesions in 110 sites of 22 patients. Of which the two methods, 40 positive lesions were detected by the robotic method with 4 lesions missed (2 small polyps of gallbladder, 1 small hemangioma of liver and 1 small cyst of kidney) and 1 lesion misdiagnosed (normal carotid artery was misdiagnosed as carotid atherosclerotic plaque); 44 positive lesions were detected by conventional method with 1 small cyst of the liver was missed. There was no statistically significant difference in the accuracy rate between the robotic method and the conventional method using the chi-square test of the four-grid data (P>.05).\nThe application of tele-ultrasound robot meets the standard of patient care during the pandemic. The method is feasible to provide adequate ultrasound information to diagnose common abdominal, vascular, superficial organ pathologies in patients with COVID-19 with acceptable accuracy compared with a conventional ultrasound scan.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2022-06-12", "authors": ["WenliJiang", "XiaZhao", "TianGan", "YingLiu", "ShuilianLuo", "MeifangZhu", "SikaiChen", "YuJiang", "MengWu"], "doi": "10.1002/jum.16041"}
{"title": "Dual Mode ", "abstract": "The COVID-19 pandemic has brought unprecedented extreme pressure on the medical system due to the physical distance policy, especially for procedures such as ultrasound (US) imaging, which are usually carried out in person. Tele-operation systems are a promising way to avoid physical human-robot interaction (", "journal": "Sensors (Basel, Switzerland)", "date": "2022-06-11", "authors": ["TengLi", "XiaoMeng", "MahdiTavakoli"], "doi": "10.3390/s22114025\n10.1016/j.jcrc.2020.10.014\n10.1038/s42256-020-00238-2\n10.1146/annurev-control-062420-090543\n10.3389/frobt.2021.610677\n10.4329/wjr.v6.i10.794\n10.3390/jimaging7110238\n10.1007/s11548-017-1566-9\n10.3389/frobt.2021.645424\n10.1109/TUFFC.2013.2593\n10.1016/S1361-8415(99)80025-5\n10.1109/TRO.2009.2019785\n10.1016/j.bspc.2021.102900\n10.1109/LRA.2018.2890674\n10.1017/S0263574718001339"}
{"title": "Development of severity and mortality prediction models for covid-19 patients at emergency department including the chest x-ray.", "abstract": "To develop prognosis prediction models for COVID-19 patients attending an emergency department (ED) based on initial chest X-ray (CXR), demographics, clinical and laboratory parameters.\nAll symptomatic confirmed COVID-19 patients admitted to our hospital ED between February 24th and April 24th 2020 were recruited. CXR features, clinical and laboratory variables and CXR abnormality indices extracted by a convolutional neural network (CNN) diagnostic tool were considered potential predictors on this first visit. The most serious individual outcome defined the three severity level: 0) home discharge or hospitalization \u2264 3 days, 1) hospital stay >3 days and 2) intensive care requirement or death. Severity and in-hospital mortality multivariable prediction models were developed and internally validated. The Youden index was used for the optimal threshold selection of the classification model.\nA total of 440 patients were enrolled (median 64 years; 55.9% male); 13.6% patients were discharged, 64% hospitalized, 6.6% required intensive care and 15.7% died. The severity prediction model included oxygen saturation/inspired oxygen fraction (SatO2/FiO2), age, C-reactive protein (CRP), lymphocyte count, extent score of lung involvement on CXR (ExtScoreCXR), lactate dehydrogenase (LDH), D-dimer level and platelets count, with AUC-ROC = 0.94 and AUC-PRC = 0.88. The mortality prediction model included age, SatO2/FiO2, CRP, LDH, CXR extent score, lymphocyte count and D-dimer level, with AUC-ROC = 0.97 and AUC-PRC = 0.78. The addition of CXR CNN-based indices did not improve significantly the predictive metrics.\nThe developed and internally validated severity and mortality prediction models could be useful as triage tools in ED for patients with COVID-19 or other virus infections with similar behaviour.\nDesarrollar modelos de predicci\u00f3n de pron\u00f3stico para pacientes con COVID-19 que acuden a urgencias, basados en la radiograf\u00eda de t\u00f3rax inicial (RXT), par\u00e1metros demogr\u00e1ficos, cl\u00ednicos y de laboratorio.\nSe reclutaron todos los pacientes sintom\u00e1ticos con COVID-19 confirmada, que ingresaron en urgencias de nuestro hospital entre el 24 de febrero y el 24 de abril de 2020. Los par\u00e1metros de la RXT, las variables cl\u00ednicas y de laboratorio y los \u00edndices de hallazgos en RXT extra\u00eddos por una herramienta diagn\u00f3stica de inteligencia artificial en esta primera visita se consideraron potenciales predictores. El desenlace individual m\u00e1s grave defini\u00f3 los tres niveles de gravedad: 0) alta domiciliaria u hospitalizaci\u00f3n de 3 d\u00edas o inferior, 1) hospitalizaci\u00f3n m\u00e1s de 3 d\u00edas y 2) necesidad de cuidados intensivos o muerte. Se desarrollaron y validaron internamente modelos de predicci\u00f3n multivariable de gravedad y mortalidad hospitalaria. El \u00edndice de Youden se utiliz\u00f3 para la selecci\u00f3n del umbral \u00f3ptimo del modelo de clasificaci\u00f3n.\nSe registraron 440 pacientes (mediana de 64 a\u00f1os; 55,9% hombres); el 13,6% de los pacientes fueron dados de alta, el 64% hospitalizo m\u00e1s de 3 d\u00edas, el 6,6% requiri\u00f3 cuidados intensivos y un 15,7% falleci\u00f3. El modelo de predicci\u00f3n de gravedad incluy\u00f3 saturaci\u00f3n de ox\u00edgeno/fracci\u00f3n de ox\u00edgeno inspirado (SatO\nLos modelos de predicci\u00f3n de pron\u00f3stico desarrollados podr\u00edan ser \u00fatiles para clasificar en urgencias a los pacientes con COVID-19 u otras infecciones v\u00edricas con comportamiento similar.", "journal": "Radiologia", "date": "2022-06-09", "authors": ["PCalvillo-Batll\u00e9s", "LCerd\u00e1-Alberich", "CFonfr\u00eda-Esparcia", "ACarreres-Ortega", "C FMu\u00f1oz-N\u00fa\u00f1ez", "LTrilles-Olaso", "LMart\u00ed-Bonmat\u00ed"], "doi": "10.1016/j.rxeng.2021.09.004\n10.1148/radiol.2020200230\n10.1148/radiol.2020200463\n10.1148/radiol.2020200370\n10.1148/radiol.2020200823\n10.2214/AJR.20.23034\n10.1148/radiol.2020201754\n10.1148/radiol.2020202723\n10.1007/s00330-020-07354-y\n10.1007/s11547-020-01200-3\n10.1136/bmj.m1328\n10.1148/radiol.2020201433\n10.7326/M18-1376\n10.2147/IJGM.S310577\n10.1007/s00330-020-06865-y\n10.1016/j.chest.2020.08.2064\n10.1016/j.amjmed.2004.03.020\n10.3348/kjr.2007.8.6.466\n10.1016/j.ijid.2020.04.061\n10.1016/S0140-6736(20)30566-3\n10.1001/jamainternmed.2020.0994\n10.1016/j.jinf.2020.04.021\n10.1111/all.14238\n10.1007/s00134-020-05991-x\n10.1016/j.metabol.2020.154378\n10.1001/jamainternmed.2020.2033\n10.3346/jkms.2020.35.e234\n10.1186/s13049-020-00764-3\n10.1001/jama.2020.2648\n10.1001/jama.2020.6775\n10.1002/sim.6787"}
{"title": "5G in Healthcare: From COVID-19 to Future Challenges.", "abstract": "Worldwide up to May 2022 there have been 515 million cases of COVID-19 infection and over 6 million deaths. The World Health Organization estimated that 115,000 healthcare workers died from COVID-19 from January 2020 to May 2021. This toll on human lives prompted this review on 5G based networking primarily on major components of healthcare delivery: diagnosis, patient monitoring, contact tracing, diagnostic imaging tests, vaccines distribution, emergency medical services, telesurgery and robot-assisted tele-ultrasound. The positive impact of 5G as core technology for COVID-19 applications enabled exchange of huge data sets in fangcang (cabin) hospitals and real-time contact tracing, while the low latency enhanced robot-assisted tele-ultrasound, and telementoring during ophthalmic surgery. In other instances, 5G provided a supportive technology for applications related to COVID-19, e.g., patient monitoring. The feasibility of 5G telesurgery was proven, albeit by a few studies on real patients, in very low samples size in most instances. The important future applications of 5G in healthcare include surveillance of elderly people, the immunosuppressed, and nano- oncology for Internet of Nano Things (IoNT). Issues remain and these require resolution before routine clinical adoption. These include infrastructure and coverage; health risks; security and privacy protection of patients' data; 5G implementation with artificial intelligence, blockchain, and IoT; validation, patient acceptance and training of end-users on these technologies.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-06-09", "authors": ["AndreaMoglia", "KonstantinosGeorgiou", "BlagoiMarinov", "EvangelosGeorgiou", "Raffaella NiceBerchiolli", "Richard MSatava", "AlfredCuschieri"], "doi": "10.1109/JBHI.2022.3181205"}
{"title": "MRI and laboratory monitoring of disease-modifying therapy efficacy and risks.", "abstract": "Increasingly, therapeutic strategy in multiple sclerosis (MS) is informed by imaging and laboratory biomarkers, in addition to traditional clinical factors. Here, we review aspects of monitoring the efficacy and risks of disease-modifying therapy (DMT) with both conventional and emerging MRI and laboratory measures.\nThe adoption of consensus-driven, stable MRI acquisition protocols and artificial intelligence-based, quantitative image analysis is heralding an era of precision monitoring of DMT efficacy. New MRI measures of compartmentalized inflammation, neuro-degeneration and repair complement traditional metrics but require validation before use in individual patients. Laboratory markers of brain cellular injury, such as neurofilament light, are robust outcomes in DMT efficacy trials; their use in clinical practice is being refined. DMT-specific laboratory monitoring for safety is critical and may include lymphocytes, immunoglobulins, autoimmunity surveillance, John Cunningham virus serology and COVID-19 vaccination seroresponse.\nA biomarker-enhanced monitoring strategy has immediate clinical application, with growing evidence of long-term reductions in disability accrual when both clinically symptomatic and asymptomatic inflammatory activity is fully suppressed; and amelioration of the risks associated with therapy. Emerging MRI and blood-based measures will also become important tools for monitoring agents that target the innate immune system and promote neuro-repair.", "journal": "Current opinion in neurology", "date": "2022-06-09", "authors": ["MichaelBarnett", "YaelBarnett", "StephenReddel"], "doi": "10.1097/WCO.0000000000001067\n10.1177/13524585211061339.\n10.1093/brain/awab448."}
{"title": "The Pitfalls of Using Open Data to Develop Deep Learning Solutions for COVID-19 Detection in Chest X-Rays.", "abstract": "Since the emergence of COVID-19, deep learning models have been developed to identify COVID-19 from chest X-rays. With little to no direct access to hospital data, the AI community relies heavily on public data comprising numerous data sources. Model performance results have been exceptional when training and testing on open-source data, surpassing the reported capabilities of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study impactful models are trained on a widely used open-source data and tested on an external test set and a hospital dataset, for the task of classifying chest X-rays into one of three classes: COVID-19, non-COVID pneumonia and no-pneumonia. Classification performance of the models investigated is evaluated through ROC curves, confusion matrices and standard classification metrics. Explainability modules are implemented to explore the image features most important to classification. Data analysis and model evalutions show that the popular open-source dataset COVIDx is not representative of the real clinical problem and that results from testing on this are inflated. Dependence on open-source data can leave models vulnerable to bias and confounding variables, requiring careful analysis to develop clinically useful/viable AI tools for COVID-19 detection in chest X-rays.", "journal": "Studies in health technology and informatics", "date": "2022-06-09", "authors": ["RachaelHarkness", "GeoffHall", "Alejandro FFrangi", "NishantRavikumar", "KieranZucker"], "doi": "10.3233/SHTI220164"}
{"title": "Evaluation of the models generated from clinical features and deep learning-based segmentations: Can thoracic CT on admission help us to predict hospitalized COVID-19 patients who will require intensive care?", "abstract": "The aim of the study was to predict the probability of intensive care unit (ICU) care for inpatient COVID-19 cases using clinical and artificial intelligence segmentation-based volumetric and CT-radiomics parameters on admission.\nTwenty-eight clinical/laboratory features, 21 volumetric parameters, and 74 radiomics parameters obtained by deep learning (DL)-based segmentations from CT examinations of 191 severe COVID-19 inpatients admitted between March 2020 and March 2021 were collected. Patients were divided into Group 1 (117 patients discharged from the inpatient service) and Group 2 (74 patients transferred to the ICU), and the differences between the groups were evaluated with the T-test and Mann-Whitney test. The sensitivities and specificities of significantly different parameters were evaluated by ROC analysis. Subsequently, 152 (79.5%) patients were assigned to the training/cross-validation set, and 39 (20.5%) patients were assigned to the test set. Clinical, radiological, and combined logit-fit models were generated by using the Bayesian information criterion from the training set and optimized via tenfold cross-validation. To simultaneously use all of the clinical, volumetric, and radiomics parameters, a random forest model was produced, and this model was trained by using a balanced training set created by adding synthetic data to the existing training/cross-validation set. The results of the models in predicting ICU patients were evaluated with the test set.\nNo parameter individually created a reliable classifier. When the test set was evaluated with the final models, the AUC values were 0.736, 0.708, and 0.794, the specificity values were 79.17%, 79.17%, and 87.50%, the sensitivity values were 66.67%, 60%, and 73.33%, and the F1 values were 0.67, 0.62, and 0.76 for the clinical, radiological, and combined logit-fit models, respectively. The random forest model that was trained with the balanced training/cross-validation set was the most successful model, achieving an AUC of 0.837, specificity of 87.50%, sensitivity of 80%, and F1 value of 0.80 in the test set.\nBy using a machine learning algorithm that was composed of clinical and DL-segmentation-based radiological parameters and that was trained with a balanced data set, COVID-19 patients who may require intensive care could be successfully predicted.", "journal": "BMC medical imaging", "date": "2022-06-08", "authors": ["MutluG\u00fclbay", "AliyeBa\u015ftu\u011f", "Erdem\u00d6zkan", "B\u00fc\u015fra Y\u00fcce\u00d6zt\u00fcrk", "B\u00f6kebatur Ahmet Ra\u015fitMendi", "H\u00fcrremBodur"], "doi": "10.1186/s12880-022-00833-2\n10.1371/journal.pone.0245272\n10.1111/joim.13091\n10.4266/acc.2020.00381\n10.1371/journal.pone.0243709\n10.1093/cid/ciaa443\n10.1001/jamainternmed.2020.2033\n10.1093/cid/ciaa414\n10.1186/s40560-021-00527-x\n10.7150/ijms.48281\n10.5114/pjr.2020.98009\n10.7150/thno.46465\n10.1148/radiol.2015151169\n10.1038/s41598-021-83237-6\n10.1371/journal.pone.0246582\n10.1038/s41598-021-90991-0\n10.7150/thno.46428\n10.7150/ijbs.58855\n10.1186/s12880-020-00529-5\n10.1016/s0895-4356(96)00236-3\n10.1186/1471-2105-14-106\n10.2307/2531595\n10.1016/j.ijid.2021.12.357\n10.1016/S2589-7500(21)00039-X\n10.1371/journal.pone.0230548\n10.4081/jphr.2021.2270\n10.2214/AJR.20.24044\n10.2214/AJR.20.22976\n10.1148/ryct.2020200322\n10.1007/s11604-020-00956-y\n10.1016/j.ijid.2020.10.095\n10.5152/dir.2020.20451\n10.1016/j.ejrad.2021.109552\n10.1007/s11547-020-01197-9\n10.1148/radiol.2021204522"}
{"title": "Deep learning-based lesion subtyping and prediction of clinical outcomes in COVID-19 pneumonia using chest CT.", "abstract": "The main objective of this work is to develop and evaluate an artificial intelligence system based on deep learning capable of automatically identifying, quantifying, and characterizing COVID-19 pneumonia patterns in order to assess disease severity and predict clinical outcomes, and to compare the prediction performance with respect to human reader severity assessment and whole lung radiomics. We propose\u00a0a deep learning based scheme to automatically segment the different lesion subtypes in nonenhanced CT scans. The automatic lesion quantification was used to predict clinical outcomes. The proposed technique has been independently tested in a multicentric cohort of 103 patients, retrospectively collected between March and July of 2020. Segmentation of lesion subtypes was evaluated using both overlapping (Dice) and distance-based (Hausdorff and average surface) metrics, while the proposed system to predict clinically relevant outcomes was assessed using the area under the curve (AUC). Additionally, other metrics including sensitivity, specificity, positive predictive value and negative predictive value were estimated. 95% confidence intervals were properly calculated. The agreement between the automatic estimate of parenchymal damage (%) and the radiologists' severity scoring was strong, with a Spearman correlation coefficient (R) of 0.83. The automatic quantification of lesion subtypes was able to predict patient mortality, admission to the Intensive Care Units (ICU) and need for mechanical ventilation with an AUC of 0.87, 0.73 and 0.68 respectively. The proposed artificial intelligence system enabled a better prediction of those clinically relevant outcomes when compared to the radiologists' interpretation and to whole lung radiomics. In conclusion, deep learning lesion subtyping in COVID-19 pneumonia from noncontrast chest CT enables quantitative assessment of disease severity and better prediction of clinical outcomes with respect to whole lung radiomics or radiologists' severity score.", "journal": "Scientific reports", "date": "2022-06-08", "authors": ["DavidBermejo-Pel\u00e1ez", "Ra\u00falSan Jos\u00e9 Est\u00e9par", "Mar\u00edaFern\u00e1ndez-Velilla", "CarmeloPalacios Miras", "GuillermoGallardo Madue\u00f1o", "MarianaBenegas", "CarolinaGotera Rivera", "SandraCuerpo", "MiguelLuengo-Oroz", "JacoboSellar\u00e9s", "MarceloS\u00e1nchez", "GorkaBastarrika", "GermanPeces Barba", "Luis MSeijo", "Mar\u00eda JLedesma-Carbayo"], "doi": "10.1038/s41598-022-13298-8\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30566-3\n10.1136/thoraxjnl-2020-216001\n10.2214/AJR.20.22976\n10.1148/radiol.2020201754\n10.1148/ryai.2020200098\n10.1016/j.media.2020.101860\n10.1038/s41598-021-90991-0\n10.1148/radiol.2020200905\n10.1038/s41746-021-00446-z\n10.1038/s41598-021-84561-7\n10.1148/RADIOL.2020202439\n10.1038/s41467-020-17971-2\n10.1016/j.cell.2020.04.045\n10.1038/s41598-019-56989-5\n10.1038/s41746-020-00369-1\n10.1148/radiol.2020200370\n10.1148/rg.2020200159\n10.21037/atm-20-3026\n10.1148/ryct.2020200322\n10.1016/j.ejrad.2021.109583\n10.1007/s00330-021-07957-z\n10.1186/s41747-020-00173-2\n10.1148/ryct.2020200110\n10.1016/j.acra.2011.01.011\n10.1109/TMI.2016.2535865\n10.1016/j.acra.2015.12.021\n10.1038/s41592-020-01008-z\n10.21227/w3aw-rv39"}
{"title": "Vasculopathy and Increased Vascular Congestion in Fatal COVID-19 and Acute Respiratory Distress Syndrome.", "abstract": "", "journal": "American journal of respiratory and critical care medicine", "date": "2022-06-08", "authors": ["Julian AVillalba", "Caroline FHilburn", "Michelle AGarlin", "Grant AElliott", "YijiaLi", "KeikoKunitoki", "SergioPoli", "George AAlba", "EmilioMadrigal", "ManuelTaso", "Melissa CPrice", "Alexis JAviles", "MilagrosAraujo-Medina", "LianaBonanno", "BarisBoyraz", "Samantha NChampion", "Cynthia KHarris", "Timothy LHelland", "BaileyHutchison", "SomaJobbagy", "Michael SMarshall", "Daniel JShepherd", "Jaimie LBarth", "Yin PHung", "AmyLy", "Lida PHariri", "Sarah ETurbett", "Virginia MPierce", "John ABranda", "Eric SRosenberg", "JavierMendez-Pena", "IvanChebib", "Ivy ARosales", "Rex NSmith", "Miles AMiller", "Ivan ORosas", "Charles CHardin", "Lindsey RBaden", "Benjamin DMedoff", "Robert BColvin", "Brent PLittle", "James RStone", "MariMino-Kenudson", "Angela RShih"], "doi": "10.1164/rccm.202109-2150OC"}
{"title": "Towards an effective model for lung disease classification: Using Dense Capsule Nets for early classification of lung diseases.", "abstract": "Machine Learning and computer vision have been the frontiers of the war against the COVID-19 Pandemic. Radiology has vastly improved the diagnosis of diseases, especially lung diseases, through the early assessment of key disease factors. Chest X-rays have thus become among the commonly used radiological tests to detect and diagnose many lung diseases. However, the discovery of lung disease through X-rays is a significantly challenging task depending on the availability of skilled radiologists. There has been a recent increase in attention to the design of Convolution Neural Networks (CNN) models for lung disease classification. A considerable amount of training dataset is required for CNN to work, but the problem is that it cannot handle translation and rotation correctly as input. The recently proposed Capsule Networks (referred to as CapsNets) are new automated learning architecture that aims to overcome the shortcomings in CNN. CapsNets are vital for rotation and complex translation. They require much less training information, which applies to the processing of data sets from medical images, including radiological images of the chest X-rays. In this research, the adoption and integration of CapsNets into the problem of chest X-ray classification have been explored. The aim is to design a deep model using CapsNet that increases the accuracy of the classification problem involved. We have used convolution blocks that take input images and generate convolution layers used as input to capsule block. There are 12 capsule layers operated, and the output of each capsule is used as an input to the next convolution block. The process is repeated for all blocks. The experimental results show that the proposed architecture yields better results when compared with the existing CNN techniques by achieving a better area under the curve (AUC) average. Furthermore, DNet checks the best performance in the ChestXray-14 data set on traditional CNN, and it is validated that DNet performs better with a higher level of total depth.", "journal": "Applied soft computing", "date": "2022-06-07", "authors": ["FaizanKarim", "Munam AliShah", "Hasan AliKhattak", "ZoobiaAmeer", "UmarShoaib", "Hafiz TayyabRauf", "FadiAl-Turjman"], "doi": "10.1016/j.asoc.2022.109077"}
{"title": "Prior-aware autoencoders for lung pathology segmentation.", "abstract": "Segmentation of lung pathology in Computed Tomography (CT) images is of great importance for lung disease screening. However, the presence of different types of lung pathologies with a wide range of heterogeneities in size, shape, location, and texture, on one side, and their visual similarity with respect to surrounding tissues, on the other side, make it challenging to perform reliable automatic lesion segmentation. To leverage segmentation performance, we propose a deep learning framework comprising a Normal Appearance Autoencoder (NAA) model to learn the distribution of healthy lung regions and reconstruct pathology-free images from the corresponding pathological inputs by replacing the pathological regions with the characteristics of healthy tissues. Detected regions that represent prior information regarding the shape and location of pathologies are then integrated into a segmentation network to guide the attention of the model into more meaningful delineations. The proposed pipeline was tested on three types of lung pathologies, including pulmonary nodules, Non-Small Cell Lung Cancer (NSCLC), and Covid-19 lesion on five comprehensive datasets. The results show the superiority of the proposed prior model, which outperformed the baseline segmentation models in all the cases with significant margins. On average, adding the prior model improved the Dice coefficient for the segmentation of lung nodules by 0.038, NSCLCs by 0.101, and Covid-19 lesions by 0.041. We conclude that the proposed NAA model produces reliable prior knowledge regarding the lung pathologies, and integrating such knowledge into a prior segmentation network leads to more accurate delineations.", "journal": "Medical image analysis", "date": "2022-06-03", "authors": ["MehdiAstaraki", "\u00d6rjanSmedby", "ChunliangWang"], "doi": "10.1016/j.media.2022.102491"}
{"title": "Predictors of venous thromboembolism in COVID-19 patients: results of the COVID-19 Brazilian Registry.", "abstract": "Previous studies that assessed risk factors for venous thromboembolism (VTE) in COVID-19 patients have shown inconsistent results. Our aim was to investigate VTE predictors by both logistic regression (LR) and machine learning (ML) approaches, due to their potential complementarity. This cohort study of a large Brazilian COVID-19 Registry included 4120 COVID-19 adult patients from 16 hospitals. Symptomatic VTE was confirmed by objective imaging. LR analysis, tree-based boosting, and bagging were used to investigate the association of variables upon hospital presentation with VTE. Among 4,120 patients (55.5% men, 39.3% critical patients), VTE was confirmed in 6.7%. In multivariate LR analysis, obesity (OR 1.50, 95% CI 1.11-2.02); being an ex-smoker (OR 1.44, 95% CI 1.03-2.01); surgery\u2009\u2264\u200990\u00a0days (OR 2.20, 95% CI 1.14-4.23); axillary temperature (OR 1.41, 95% CI 1.22-1.63); D-dimer\u2009\u2265\u20094 times above the upper limit of reference value (OR 2.16, 95% CI 1.26-3.67), lactate (OR 1.10, 95% CI 1.02-1.19), C-reactive protein levels (CRP, OR 1.09, 95% CI 1.01-1.18); and neutrophil count (OR 1.04, 95% CI 1.005-1.075) were independent predictors of VTE. Atrial fibrillation, peripheral oxygen saturation/inspired oxygen fraction (SF) ratio and prophylactic use of anticoagulants were protective. Temperature at admission, SF ratio, neutrophil count, D-dimer, CRP and lactate levels were also identified as predictors by ML methods. By\u00a0using ML and LR analyses, we showed that D-dimer, axillary temperature, neutrophil count, CRP and lactate levels are risk factors for VTE in COVID-19 patients.", "journal": "Internal and emergency medicine", "date": "2022-06-02", "authors": ["Warley Cezarda Silveira", "Lucas Emanuel FerreiraRamos", "Rafael TavaresSilva", "Bruno Barbosa Mirandade Paiva", "Polianna DelfinoPereira", "Alexandre VargasSchwarzbold", "Andresa FontouraGarbini", "Bruna Schettino MoratoBarreira", "Bruno Mateusde Castro", "Carolina MarquesRamos", "Caroline DanubiaGomes", "Christiane Corr\u00eaa RodriguesCimini", "Elayne CrestaniPereira", "Eliane W\u00fcrdigRoesch", "Emanuele Marianne SouzaKroger", "Felipe Ferraz Martins Gra\u00e7aAranha", "FernandoAnschau", "Fernando AntonioBotoni", "Fernando Gra\u00e7aAranha", "Gabriela PetryCrestani", "Giovanna GrunewaldVietta", "Gisele Alsina NaderBastos", "Jamille Hem\u00e9trio Salles MartinsCosta", "J\u00e9ssica Rayane Corr\u00eaa Silvada Fonseca", "Karen BrasilRuschel", "Leonardo Seixasde Oliveira", "L\u00edlian SantosPinheiro", "Liliane SoutoPacheco", "Luciana BorgesSegala", "Luciana Siuves FerreiraCouto", "LucianeKopittke", "Maiara AnschauFloriani", "Majlla Magalh\u00e3esSilva", "MarceloCarneiro", "Maria Ang\u00e9lica PiresFerreira", "Maria Auxiliadora ParreirasMartins", "Marina Neves Zerbinide Faria", "Matheus Carvalho AlvesNogueira", "Milton HenriquesGuimar\u00e3es J\u00fanior", "Nat\u00e1lia da Cunha SeverinoSampaio", "Neimy Ramosde Oliveira", "Nicole de MoraesPertile", "Pedro Guido SoaresAndrade", "Pedro LedicAssaf", "Reginaldo AparecidoValacio", "Rochele MosmannMenezes", "Saionara CristinaFrancisco", "Silvana Mangeon MeirellesGuimar\u00e3es", "Silvia FerreiraAra\u00fajo", "Suely MeirelesRezende", "Susany Anast\u00e1ciaPereira", "TatianaKurtz", "Tatiani OliveiraFereguetti", "Car\u00edsi AnnePolanczyk", "Magda CarvalhoPires", "Marcos Andr\u00e9Gon\u00e7alves", "Milena SorianoMarcolino"], "doi": "10.1007/s11739-022-03002-z\n10.1016/S2352-3026(15)00202-1\n10.1001/archinte.162.11.1245\n10.1016/j.amjmed.2013.02.024\n10.1378/chest.09-0959\n10.1093/eurheartj/ehaa623\n10.1111/jth.14854\n10.1161/CIRCULATIONAHA.120.050354\n10.1007/s11739-020-02601-y\n10.1148/radiol.2020203557\n10.1177/1076029620967083\n10.1182/bloodadvances.2020003083\n10.1007/s00134-020-06062-x\n10.1111/jth.14869\n10.1001/jama.2020.13372\n10.1016/j.eclinm.2020.100639\n10.15585/mmwr.mm6924e2\n10.1001/jama.2020.1585\n10.1007/s11739-021-02891-w\n10.1016/j.chest.2020.08.2064\n10.1016/j.chest.2020.07.031\n10.1093/eurheartj/ehaa500\n10.1016/j.ajem.2021.09.004\n10.1016/j.ijid.2021.11.038\n10.1016/j.tru.2021.100037\n10.1177/10760296211040868\n10.1016/j.ijid.2021.01.019\n10.1371/journal.pone.0243533\n10.1055/a-1366-9656\n10.1016/j.jbi.2008.08.010\n10.1016/j.jbi.2019.103208\n10.1016/j.apnr.2010.02.004\n10.1111/jth.14929\n10.1093/eurheartj/ehz405\n10.1515/cclm-2020-0573\n10.1111/jth.15261\n10.1016/j.jacc.2020.04.031\n10.1001/jamasurg.2019.3742\n10.1016/j.ijid.2021.06.005\n10.1046/j.1467-789X.2002.00056.x\n10.1016/j.jacc.2006.08.040\n10.1046/j.1538-7836.2003.00279.x\n10.1038/oby.2002.98\n10.1016/S1262-3636(07)70251-3\n10.1038/ijo.2011.19\n10.1007/s11739-020-02355-7\n10.1378/chest.15-0287\n10.1002/rth2.12065\n10.1097/01.CCM.0000201882.23917.B8\n10.1097/CRD.0000000000000347\n10.1016/j.ijid.2021.07.049\n10.1378/chest.07-0617\n10.1177/10760296211008999\n10.1001/jamainternmed.2021.6203\n10.1515/spp-2019-0010\n10.1214/aos/1013203451\n10.1161/CIRCULATIONAHA.120.047407\n10.1111/j.1538-7836.2010.04034.x\n10.1093/cid/ciq125"}
{"title": "Machine learning based COVID -19 disease recognition using CT images of SIRM database.", "abstract": "The COVID-19 pandemic, probably one of the most widespread pandemics humanity has encountered in the twenty first century, caused death to almost 1.75\u2009M people worldwide, impacting almost 80\u2009M lives with direct contact. In order to contain the spread of coronavirus, it is necessary to develop a reliant and quick method to identify those who are affected and isolate them until full recovery is made. The imagery knowledge has been shown to be useful for quick COVID-19 diagnosis. Though the scans of computational tomography (CT) demonstrate a range of viral infection signals, considering the vast number of images, certain visual characteristics are challenging to distinguish and can take a long time to be identified by radiologists. In this study for detection of the COVID-19, a dataset is formed by taking 3764 images. The feature extraction process is applied to the dataset to increase the classification performance. Techniques like Grey Level Co-occurrence Matrix (GLCM) and Discrete Wavelet Transform (DWT) are used for feature extraction. Then various machine learning algorithms applied such as Support Vector Machines (SVM), Linear Discriminant Analysis (LDA), Multi- Level Perceptron, Naive Bayes, K-Nearest Neighbours and Random Forests are used for classification of COVID-19 disease detection. Sensitivity, Specificity, Accuracy, Precision, and F-score are the metrics used to measure the performance of different machine learning models. Among these machine learning models SVM with GLCM as feature extraction technique using 10-fold cross validation gives the best classification result with 99.70% accuracy, 99.80% sensitivity and 97.03% F-score. We also ran these tests on different data sets and found that the results are similar across those too, as discussed later in the results section.", "journal": "Journal of medical engineering & technology", "date": "2022-06-01", "authors": ["Saroj KumarPandey", "Rekh RamJanghel", "Pankaj KumarMishra", "RachanaKaabra"], "doi": "10.1080/03091902.2022.2080883"}
{"title": "An interpretable multi-task system for clinically applicable COVID-19 diagnosis using CXR.", "abstract": "With the emergence of continuously mutating variants of coronavirus, it is urgent to develop a deep learning model for automatic COVID-19 diagnosis at early stages from chest X-ray images. Since laboratory testing is time-consuming and requires trained laboratory personal, diagnosis using chest X-ray (CXR) is a befitting option.\nIn this study, we proposed an interpretable multi-task system for automatic lung detection and COVID-19 screening in chest X-rays to find an alternate method of testing which are reliable, fast and easily accessible, and able to generate interpretable predictions that are strongly correlated with radiological findings.\nThe proposed system consists of image preprocessing and an unsupervised machine learning (UML) algorithm for lung region detection, as well as a truncated CNN model based on deep transfer learning (DTL) to classify chest X-rays into three classes of COVID-19, pneumonia, and normal. The Grad-CAM technique was applied to create class-specific heatmap images in order to establish trust in the medical AI system.\nExperiments were performed with 15,884 frontal CXR images to show that the proposed system achieves an accuracy of 91.94% in a test dataset with 2,680 images including a sensitivity of 94.48% on COVID-19 cases, a specificity of 88.46% on normal cases, and a precision of 88.01% on pneumonia cases. Our system also produced state-of-the-art outcomes with a sensitivity of 97.40% on public test data and 88.23% on a previously unseen clinical data (1,000 cases) for binary classification of COVID-19-positive and COVID-19-negative films.\nOur automatic computerized evaluation for grading lung infections exhibited sensitivity comparable to that of radiologist interpretation in clinical applicability. Therefore, the proposed solution can be used as one element of patient evaluation along with gold-standard clinical and laboratory testing.", "journal": "Journal of X-ray science and technology", "date": "2022-06-01", "authors": ["YanZhuang", "Md FashiarRahman", "YuxinWen", "MichaelPokojovy", "PeterMcCaffrey", "AlexanderVo", "EricWalser", "ScottMoen", "HonglunXu", "Tzu-Liang BillTseng"], "doi": "10.3233/XST-221151"}
{"title": "An Analysis of New Feature Extraction Methods Based on Machine Learning Methods for Classification Radiological Images.", "abstract": "The lungs are COVID-19's most important focus, as it induces inflammatory changes in the lungs that can lead to respiratory insufficiency. Reducing the supply of oxygen to human cells negatively impacts humans, and multiorgan failure with a high mortality rate may, in certain circumstances, occur. Radiological pulmonary evaluation is a vital part of patient therapy for the critically ill patient with COVID-19. The evaluation of radiological imagery is a specialized activity that requires a radiologist. Artificial intelligence to display radiological images is one of the essential topics. Using a deep machine learning technique to identify morphological differences in the lungs of COVID-19-infected patients could yield promising results on digital images of chest X-rays. Minor differences in digital images that are not detectable or apparent to the human eye may be detected using computer vision algorithms. This paper uses machine learning methods to diagnose COVID-19 on chest X-rays, and the findings have been very promising. The dataset includes COVID-19-enhanced X-ray images for disease detection using chest X-ray images. The data were gathered from two publicly accessible datasets. The feature extractions are done using the gray level co-occurrence matrix methods. ", "journal": "Computational intelligence and neuroscience", "date": "2022-06-01", "authors": ["Firoozeh AbolhasaniZadeh", "Mohammadreza VazifehArdalani", "Ali RezaeiSalehi", "RozaJalali Farahani", "MandanaHashemi", "Adil HusseinMohammed"], "doi": "10.1155/2022/3035426\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1001/jama.2020.4861\n10.5858/arpa.2020-0901-sa\n10.1016/j.compbiomed.2020.103792\n10.1016/j.acra.2020.03.003\n10.1109/RBME.2020.2987975\n10.1101/2020.04.04.20052092v2\n10.1101/2020.04.04.20052092\n10.1136/amiajnl-2012-001145\n10.1038/nrg3208\n10.4137/bii.s31559\n10.1007/s11356-020-11644-9\n10.1016/j.chaos.2020.110170\n10.1007/s11356-021-13249-2\n10.1016/j.compbiomed.2021.104454\n10.1016/j.asoc.2021.107449\n10.1155/2021/9995073\n10.1016/j.asoc.2020.106912\n10.1016/j.compbiomed.2021.104425\n10.1109/access.2021.3058537\n10.1038/nbt.4233\n10.1038/s41591-018-0316-z\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.drudis.2018.01.039\n10.3390/s21155137\n10.1016/j.media.2017.07.005\n10.1109/ICCSRE.2019.8807741\n10.1109/JBHI.2020.2986376\n10.2174/1574893615999200607173829\n10.3390/electronics8030292\n10.1002/jctb.4820\n10.1109/icip.2018.8451355\n10.1109/cvpr.2017.243\n10.1007/978-3-319-24574-4_28\n10.1016/j.ejrad.2020.109041\n10.1080/24749508.2019.1585657\n10.24869/psyd.2020.570\n10.1101/2020.05.04.20082081v1\n10.1038/s41467-020-18685-1\n10.24869/psyd.2020.262\n10.1101/2020.04.01.20049825v1\n10.1109/JIOT.2020.3007518\n10.1007/s00521-020-05687-9\n10.1016/s2468-2667(20)30073-6\n10.21203/rs.3.rs-17715/v1\n10.1111/exsy.12759\n10.1148/radiol.2020200905\n10.1007/s13246-020-00865-4\n10.33889/IJMEMS.2020.5.4.052\n10.1101/2020.04.04.20052092v2\n10.1101/2020.03.30.20047787v1\n10.1097/rli.0000000000000672\n10.1101/2020.04.13.20063461v1\n10.14299/ijser.2020.03.02\n10.1101/2020.04.09.20058594v1\n10.7717/peerj-cs.564\n10.1007/s10044-020-00950-0\n10.3390/s21217286\n10.3390/app11199023\n10.5589/m02-004\n10.1080/00207454.2021.1883602\n10.1109/tsmc.1985.6313426\n10.1016/s0925-2312(03)00431-4\n10.1080/10255842.2021.1921164\n10.1016/j.cell.2018.02.010\n10.1016/j.rxeng.2020.11.002\n10.1016/j.idm.2020.04.001"}
{"title": "Multiclass Classification of Chest X-Ray Images for the Prediction of COVID-19 Using Capsule Network.", "abstract": "It is critical to establish a reliable method for detecting people infected with COVID-19 since the pandemic has numerous harmful consequences worldwide. If the patient is infected with COVID-19, a chest X-ray can be used to determine this. In this work, an X-ray showing a COVID-19 infection is classified by the capsule neural network model we trained to recognise. 6310 chest X-ray pictures were used to train the models, separated into three categories: normal, pneumonia, and COVID-19. This work is considered an improved deep learning model for the classification of COVID-19 disease through X-ray images. Viewpoint invariance, fewer parameters, and better generalisation are some of the advantages of CapsNet compared with the classic convolutional neural network (CNN) models. The proposed model has achieved an accuracy greater than 95% during the model's training, which is better than the other state-of-the-art algorithms. Furthermore, to aid in detecting COVID-19 in a chest X-ray, the model could provide extra information.", "journal": "Computational intelligence and neuroscience", "date": "2022-06-01", "authors": ["MahmoudRagab", "SamahAlshehri", "Nabil AAlhakamy", "Romany FMansour", "DeepikaKoundal"], "doi": "10.1155/2022/6185013\n10.1108/WJE-10-2020-0529\n10.1007/s12195-020-00642-z\n10.4269/ajtmh.20-0280\n10.3390/su12177090\n10.3906/elk-2105-243\n10.1155/2020/1289408\n10.1038/s41598-020-76550-z\n10.1111/exsy.12749\n10.1080/17512433.2020.1832889\n10.3390/info11120548\n10.1016/j.bspc.2022.103778\n10.1016/j.cmpb.2020.105581\n10.1016/j.mehy.2020.109761\n10.1007/s13246-020-00865-4\n10.1016/b978-0-12-824536-1.00003-4\n10.1101/2020.05.10.20097063\n10.1007/s10489-020-01900-3\n10.1038/s41597-021-00900-3\n10.1016/j.cell.2018.02.010\n10.1108/WJE-03-2021-0174\n10.1155/2021/1233166\n10.1016/j.jksuci.2019.09.014"}
{"title": "FedSGDCOVID: Federated SGD COVID-19 Detection under Local Differential Privacy Using Chest X-ray Images and Symptom Information.", "abstract": "Coronavirus (COVID-19) has created an unprecedented global crisis because of its detrimental effect on the global economy and health. COVID-19 cases have been rapidly increasing, with no sign of stopping. As a result, test kits and accurate detection models are in short supply. Early identification of COVID-19 patients will help decrease the infection rate. Thus, developing an automatic algorithm that enables the early detection of COVID-19 is essential. Moreover, patient data are sensitive, and they must be protected to prevent malicious attackers from revealing information through model updates and reconstruction. In this study, we presented a higher privacy-preserving federated learning system for COVID-19 detection without sharing data among data owners. First, we constructed a federated learning system using chest X-ray images and symptom information. The purpose is to develop a decentralized model across multiple hospitals without sharing data. We found that adding the spatial pyramid pooling to a 2D convolutional neural network improves the accuracy of chest X-ray images. Second, we explored that the accuracy of federated learning for COVID-19 identification reduces significantly for non-independent and identically distributed (Non-IID) data. We then proposed a strategy to improve the model's accuracy on Non-IID data by increasing the total number of clients, parallelism (client-fraction), and computation per client. Finally, for our federated learning model, we applied a differential privacy stochastic gradient descent (DP-SGD) to improve the privacy of patient data. We also proposed a strategy to maintain the robustness of federated learning to ensure the security and accuracy of the model.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-05-29", "authors": ["Trang-ThiHo", "Khoa-DangTran", "YennunHuang"], "doi": "10.3390/s22103728\n10.1021/acs.jcim.1c01451\n10.15212/ZOONOSES-2021-0005\n10.1017/S0950268820001430\n10.1136/bmjopen-2020-044154\n10.1016/j.jmii.2020.05.001\n10.1126/science.abc0473\n10.1038/s41591-020-0916-2\n10.2196/23897\n10.1148/radiol.2020200230\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200843\n10.1109/ACCESS.2020.3016780\n10.1109/JIOT.2021.3056185\n10.1038/s41598-021-88807-2\n10.1109/JSEN.2021.3076767\n10.1038/s41598-020-76550-z\n10.1109/TIFS.2017.2787987\n10.1016/j.patrec.2020.09.010\n10.1007/s10489-020-01943-6\n10.1016/j.bspc.2020.102149\n10.1007/s41870-020-00495-9\n10.3390/s21206853\n10.1016/S2589-7500(21)00131-X\n10.1093/jamia/ocaa341\n10.1145/1866739.1866758\n10.1561/0400000042\n10.1016/j.eng.2020.04.010\n10.1007/s10044-021-00984-y\n10.1007/s10489-020-01829-7\n10.1016/j.chaos.2021.110749\n10.1109/TPAMI.2015.2389824\n10.1080/21642583.2020.1824132\n10.1007/s00521-020-05337-0\n10.18280/ts.370620\n10.1109/TASLP.2018.2852502\n10.1007/s11042-019-08547-4\n10.1145/3396237\n10.1007/s00371-020-01794-9\n10.3390/s21237957\n10.3934/publichealth.2021019\n10.1016/j.asoc.2018.02.004\n10.1016/j.rinp.2021.104484\n10.32604/cmc.2021.013228\n10.3390/math9020180\n10.1038/s41598-021-01119-3\n10.3390/life11111118\n10.1016/j.imu.2021.100566\n10.3389/fpubh.2021.744100\n10.1016/j.chaos.2021.110861\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1101/2021.01.07.21249323"}
{"title": "Current Artificial Intelligence (AI) Techniques, Challenges, and Approaches in Controlling and Fighting COVID-19: A Review.", "abstract": "SARS-CoV-2 (COVID-19) has been one of the worst global health crises in the 21st century. The currently available rollout vaccines are not 100% effective for COVID-19 due to the evolving nature of the virus. There is a real need for a concerted effort to fight the virus, and research from diverse fields must contribute. Artificial intelligence-based approaches have proven to be significantly effective in every branch of our daily lives, including healthcare and medical domains. During the early days of this pandemic, artificial intelligence (AI) was utilized in the fight against this virus outbreak and it has played a major role in containing the spread of the virus. It provided innovative opportunities to speed up the development of disease interventions. Several methods, models, AI-based devices, robotics, and technologies have been proposed and utilized for diverse tasks such as surveillance, spread prediction, peak time prediction, classification, hospitalization, healthcare management, heath system capacity, etc. This paper attempts to provide a quick, concise, and precise survey of the state-of-the-art AI-based techniques, technologies, and datasets used in fighting COVID-19. Several domains, including forecasting, surveillance, dynamic times series forecasting, spread prediction, genomics, compute vision, peak time prediction, the classification of medical imaging-including CT and X-ray and how they can be processed-and biological data (genome and protein sequences) have been investigated. An overview of the open-access computational resources and platforms is given and their useful tools are pointed out. The paper presents the potential research areas in AI and will thus encourage researchers to contribute to fighting against the virus and aid global health by slowing down the spread of the virus. This will be a significant contribution to help minimize the high death rate across the globe.", "journal": "International journal of environmental research and public health", "date": "2022-05-29", "authors": ["UmarAlbalawi", "MohammedMustafa"], "doi": "10.3390/ijerph19105901\n10.1148/radiol.2020200642\n10.1016/j.jrid.2020.03.006\n10.3201/eid1103.040675\n10.1016/j.jinf.2005.10.019\n10.1038/s41591-020-0771-1\n10.1016/j.ypmed.2004.11.023\n10.1038/nrmicro3143\n10.1016/S2589-7500(20)30054-6\n10.2217/fvl-2020-0130\n10.1016/j.ebiom.2019.08.024\n10.2196/12383\n10.3390/ijerph17030678\n10.1186/s12874-021-01346-2\n10.1002/sim.7296\n10.1007/s11121-019-01019-z\n10.1007/s11904-020-00490-6\n10.1001/jamacardio.2020.1096\n10.3390/healthcare8010046\n10.1016/j.imu.2022.100862\n10.1007/978-981-16-3783-4_5\n10.1109/ACCESS.2020.3027685\n10.1038/s41598-020-76550-z\n10.2196/27468\n10.1038/s41598-020-76282-0\n10.2214/AJR.20.22954\n10.1148/ryct.2020200025\n10.1148/radiol.2020200905\n10.1109/TCBB.2021.3065361\n10.1016/j.patrec.2020.09.010\n10.1145/3465398\n10.3390/diagnostics11071155\n10.1007/s10489-020-01862-6\n10.1016/j.scs.2020.102589\n10.1080/21505594.2020.1802194\n10.1109/TIM.2020.2977793\n10.1007/s10489-020-02076-6\n10.1101/2020.07.15.205567\n10.1016/j.eswa.2020.114054\n10.14299/ijser.2020.03.02\n10.1016/j.infrared.2012.03.007\n10.1016/S0140-6736(20)30183-5\n10.1155/2020/9121429\n10.1016/j.imu.2020.100378\n10.2196/22845\n10.3390/healthcare8020154\n10.1089/tmj.2020.0114\n10.1183/13993003.00775-2020\n10.1007/s00330-020-06956-w\n10.1101/2020.03.03.972133\n10.4155/fmc-2020-0262\n10.1016/j.compbiomed.2021.104359\n10.3389/fimmu.2020.01581\n10.1101/2020.04.03.20052084\n10.1093/ije/dyaa035\n10.1080/24709360.2021.1913709\n10.1016/j.patter.2020.100145\n10.1016/S0140-6736(20)30460-8\n10.1101/2020.02.27.20028027\n10.1007/s12539-020-00376-6\n10.1007/s00330-021-07715-1"}
{"title": "COVLIAS 1.0", "abstract": "Background: COVID-19 is a disease with multiple variants, and is quickly spreading throughout the world. It is crucial to identify patients who are suspected of having COVID-19 early, because the vaccine is not readily available in certain parts of the world. Methodology: Lung computed tomography (CT) imaging can be used to diagnose COVID-19 as an alternative to the RT-PCR test in some cases. The occurrence of ground-glass opacities in the lung region is a characteristic of COVID-19 in chest CT scans, and these are daunting to locate and segment manually. The proposed study consists of a combination of solo deep learning (DL) and hybrid DL (HDL) models to tackle the lesion location and segmentation more quickly. One DL and four HDL models\u2014namely, PSPNet, VGG-SegNet, ResNet-SegNet, VGG-UNet, and ResNet-UNet\u2014were trained by an expert radiologist. The training scheme adopted a fivefold cross-validation strategy on a cohort of 3000 images selected from a set of 40 COVID-19-positive individuals. Results: The proposed variability study uses tracings from two trained radiologists as part of the validation. Five artificial intelligence (AI) models were benchmarked against MedSeg. The best AI model, ResNet-UNet, was superior to MedSeg by 9% and 15% for Dice and Jaccard, respectively, when compared against MD 1, and by 4% and 8%, respectively, when compared against MD 2. Statistical tests\u2014namely, the Mann\u2212Whitney test, paired t-test, and Wilcoxon test\u2014demonstrated its stability and reliability, with p < 0.0001. The online system for each slice was <1 s. Conclusions: The AI models reliably located and segmented COVID-19 lesions in CT scans. The COVLIAS 1.0Lesion lesion locator passed the intervariability test.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-05-29", "authors": ["Jasjit SSuri", "SushantAgarwal", "Gian LucaChabert", "AlessandroCarriero", "AlessioPasch\u00e8", "Pietro S CDanna", "LucaSaba", "ArminMehmedovi\u0107", "GavinoFaa", "Inder MSingh", "MonikaTurk", "Paramjit SChadha", "Amer MJohri", "Narendra NKhanna", "SophieMavrogeni", "John RLaird", "GyanPareek", "MartinMiner", "David WSobel", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "Athanasios DProtogerou", "Durga PrasannaMisra", "VikasAgarwal", "George DKitas", "Jagjit STeji", "MustafaAl-Maini", "Surinder KDhanjil", "AndrewNicolaides", "AdityaSharma", "VijayRathore", "MostafaFatemi", "AzraAlizad", "Pudukode RKrishnan", "FerencNagy", "ZoltanRuzsa", "Mostafa MFouda", "SubbaramNaidu", "KlaudijaViskovic", "Manudeep KKalra"], "doi": "10.3390/diagnostics12051283\n10.26355/eurrev_202012_24058\n10.1007/s10554-020-02089-9\n10.4239/wjd.v12.i3.215\n10.1016/j.clinimag.2021.05.016\n10.26355/eurrev_202108_26464\n10.1101/gr.6.10.995\n10.1677/jme.1.01755\n10.1148/radiol.2020200432\n10.1016/j.pbiomolbio.2006.07.026\n10.1016/j.clinimag.2020.04.001\n10.1109/TMI.2005.862753\n10.1007/s11547-020-01269-w\n10.4081/jphr.2021.2270\n10.1148/ryct.2020200196\n10.1016/j.ajem.2020.04.016\n10.1148/radiol.2020200230\n10.1016/j.ejrad.2020.109041\n10.1016/j.irbm.2020.05.003\n10.2214/AJR.20.23034\n10.1007/s11604-021-01120-w\n10.1148/radiol.2020200343\n10.13140/RG\n10.1016/j.asoc.2020.106912\n10.1109/TIP.2021.3058783\n10.1016/j.eng.2020.04.010\n10.1016/j.metabol.2017.01.011\n10.1308/147870804290\n10.1007/s10916-010-9645-2\n10.1177/0954411913483637\n10.1016/j.cmpb.2012.09.008\n10.1007/s11517-012-1019-0\n10.1016/j.cmpb.2017.12.016\n10.7785/tcrt.2012.500346\n10.21037/atm-20-7676\n10.1007/s11517-021-02322-0\n10.21037/cdt.2019.09.01\n10.1007/s11517-018-1897-x\n10.1007/s10278-019-00227-x\n10.1109/ACCESS.2017.2788044\n10.1016/j.media.2017.07.005\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.array.2019.100004\n10.1016/j.jormas.2019.06.002\n10.1109/JBHI.2021.3103839\n10.1016/j.compbiomed.2021.104721\n10.1016/j.compbiomed.2021.104803\n10.3390/diagnostics11112025\n10.3390/diagnostics11081405\n10.1007/s11548-021-02317-0\n10.1049/el.2020.2102\n10.1049/el.2018.0989\n10.2174/1573405616666201231100623\n10.1109/TPAMI.2016.2644615\n10.3390/sym11010001\n10.1117/1.JMI.8.S1.014502\n10.1109/TIT.1981.1056373\n10.1007/s10479-005-5724-z\n10.3390/e22010045\n10.1364/BOE.449314\n10.3390/s22072724\n10.1109/TMI.2020.3002417\n10.1093/clinchem/48.5.799\n10.11613/BM.2015.015\n10.1093/clinchem/39.4.561\n10.1177/070674370705200210\n10.20982/tqmp.04.1.p013\n10.1002/sim.4780040112\n10.1016/0169-2607(95)01703-8\n10.1007/s10916-016-0504-7\n10.1148/ryct.2020200034\n10.3389/fmed.2020.00526\n10.1016/j.dsx.2020.03.013\n10.1177/2048872620974605\n10.1080/17476348.2020.1787835\n10.3389/fmed.2020.608525\n10.7717/peerj-cs.368\n10.1155/2021/5544742\n10.1109/TCYB.2021.3123173\n10.1155/2021/5208940\n10.3390/diagnostics11020158\n10.1016/j.cmpb.2021.106406\n10.1109/TNNLS.2021.3054746\n10.3390/diagnostics10110901\n10.1007/s10278-021-00434-5\n10.1016/j.compbiomed.2020.104037\n10.1016/j.acra.2020.09.004\n10.1002/mp.14676\n10.1007/s11042-020-10010-8\n10.1109/TPAMI.2019.2938758\n10.1016/S0031-3203(00)00023-6\n10.1007/s11263-010-0392-0\n10.1109/TMI.2020.2996645\n10.1186/s12880-020-00543-7\n10.1109/TEM.2021.3094544\n10.1088/0031-9155/41/1/009\n10.1007/s00066-013-0464-5\n10.1137/0733060\n10.1109/TIP.2011.2169270\n10.1016/j.ihj.2018.01.024\n10.1016/j.compbiomed.2017.08.014\n10.1007/s10916-015-0214-6\n10.3390/diagnostics11112109\n10.1016/j.cmpb.2012.05.008\n10.1109/TCSVT.2022.3142771\n10.1109/TPAMI.2021.3050918\n10.1007/s00521-015-1916-x\n10.1016/j.patrec.2017.05.018\n10.23736/S0392-9590.21.04771-4\n10.21037/cdt.2020.01.07\n10.1016/j.compbiomed.2020.103804"}
{"title": "Cardiovascular Risk Stratification in Diabetic Retinopathy via Atherosclerotic Pathway in COVID-19/Non-COVID-19 Frameworks Using Artificial Intelligence Paradigm: A Narrative Review.", "abstract": "Diabetes is one of the main causes of the rising cases of blindness in adults. This microvascular complication of diabetes is termed diabetic retinopathy (DR) and is associated with an expanding risk of cardiovascular events in diabetes patients. DR, in its various forms, is seen to be a powerful indicator of atherosclerosis. Further, the macrovascular complication of diabetes leads to coronary artery disease (CAD). Thus, the timely identification of cardiovascular disease (CVD) complications in DR patients is of utmost importance. Since CAD risk assessment is expensive for low-income countries, it is important to look for surrogate biomarkers for risk stratification of CVD in DR patients. Due to the common genetic makeup between the coronary and carotid arteries, low-cost, high-resolution imaging such as carotid B-mode ultrasound (US) can be used for arterial tissue characterization and risk stratification in DR patients. The advent of artificial intelligence (AI) techniques has facilitated the handling of large cohorts in a big data framework to identify atherosclerotic plaque features in arterial ultrasound. This enables timely CVD risk assessment and risk stratification of patients with DR. Thus, this review focuses on understanding the pathophysiology of DR, retinal and CAD imaging, the role of surrogate markers for CVD, and finally, the CVD risk stratification of DR patients. The review shows a step-by-step cyclic activity of how diabetes and atherosclerotic disease cause DR, leading to the worsening of CVD. We propose a solution to how AI can help in the identification of CVD risk. Lastly, we analyze the role of DR/CVD in the COVID-19 framework.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-05-29", "authors": ["SmikshaMunjral", "MaheshMaindarkar", "PuneetAhluwalia", "AnudeepPuvvula", "AnkushJamthikar", "TanayJujaray", "NehaSuri", "SudipPaul", "RajeshPathak", "LucaSaba", "Renoh JohnsonChalakkal", "SuneetGupta", "GavinoFaa", "Inder MSingh", "Paramjit SChadha", "MonikaTurk", "Amer MJohri", "Narendra NKhanna", "KlaudijaViskovic", "SophieMavrogeni", "John RLaird", "GyanPareek", "MartinMiner", "David WSobel", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "AthanasiosProtogerou", "Durga PrasannaMisra", "VikasAgarwal", "George DKitas", "RaghuKolluri", "JagjitTeji", "MustafaAl-Maini", "Surinder KDhanjil", "MeyypanSockalingam", "AjitSaxena", "AdityaSharma", "VijayRathore", "MostafaFatemi", "AzraAlizad", "VijayViswanathan", "Padukode RKrishnan", "TomazOmerzu", "SubbaramNaidu", "AndrewNicolaides", "Mostafa MFouda", "Jasjit SSuri"], "doi": "10.3390/diagnostics12051234\n10.1186/1741-7015-11-117\n10.1097/01.med.0000433056.76699.5d\n10.1016/j.ajo.2010.10.014\n10.1161/CIRCRESAHA.118.311371\n10.4239/wjd.v12.i3.215\n10.2337/dc11-1909\n10.24095/hpcdp.35.6.01\n10.4239/wjd.v6.i6.850\n10.7257/1053-816X.2016.36.1.27\n10.1016/j.arr.2020.101040\n10.3390/ijms19061816\n10.1016/j.preteyeres.2015.08.001\n10.2337/diab.26.1.46\n10.1900/RDS.2007.4.242\n10.12809/hkmj164869\n10.1016/j.diabres.2010.11.005\n10.3389/fendo.2019.00689\n10.1210/jc.2017-01922\n10.1186/s13098-021-00666-z\n10.1111/micc.12702\n10.2337/dc20-3104\n10.1016/j.ophtha.2020.12.019\n10.1016/j.freeradbiomed.2011.05.004\n10.1161/CIRCULATIONAHA.105.535294\n10.1109/RBME.2010.2084567\n10.1259/bjr.20130832\n10.1016/j.neurol.2010.03.024\n10.1186/s12933-015-0196-1\n10.1136/bmjdrc-2019-000845\n10.14740/cr1179\n10.1093/eurheartj/ehw302\n10.1186/s13058-019-1158-4\n10.1016/j.compbiomed.2020.104128\n10.3390/diagnostics11050895\n10.1016/S2589-7500(21)00043-1\n10.1038/s41551-020-00626-4\n10.1016/j.oret.2018.10.014\n10.1007/s13246-021-01012-3\n10.1016/j.preteyeres.2020.100900\n10.1016/j.compbiomed.2019.01.002\n10.1142/S0219519409003115\n10.1016/j.jacc.2010.05.007\n10.1016/j.jacc.2010.06.030\n10.1109/JBHI.2020.3040225\n10.2337/dc10-S062\n10.1056/NEJM199306103282306\n10.4239/wjd.v6.i1.92\n10.1016/j.ijdm.2009.08.001\n10.4239/wjd.v6.i3.489\n10.1016/j.dsx.2014.09.011\n10.1093/fampra/cmh417\n10.1155/2016/6838976\n10.2337/diacare.26.9.2653\n10.2337/diabetes.51.10.3107\n10.1007/978-3-642-12041-1_2\n10.4103/2008-322X.143378\n10.1155/2014/801269\n10.1161/CIRCRESAHA.110.223545\n10.3390/antiox9121244\n10.1155/2018/3420187\n10.1159/000491897\n10.1016/S0161-6420(86)33650-9\n10.1001/jamaophthalmol.2017.0988\n10.4155/fmc.12.206\n10.1186/s12933-018-0706-z\n10.3389/fimmu.2018.00706\n10.2337/dc18-2483\n10.1007/s00125-020-05195-4\n10.1016/j.cjca.2017.12.005\n10.4239/wjd.v6.i5.679\n10.1016/j.cardiores.2004.05.001\n10.1155/2020/5245308\n10.1038/nature00804\n10.1161/CIRCULATIONAHA.106.676890\n10.1111/j.1542-4758.2012.00687.x\n10.1074/jbc.M209649200\n10.1038/s41569-019-0169-2\n10.1016/j.cmet.2010.06.008\n10.1111/joim.12296\n10.2337/diacare.23.9.1310\n10.2337/dc18-0476\n10.1007/s00592-019-01453-z\n10.2337/diacare.28.6.1383\n10.1177/1741826711409324\n10.1089/dia.2014.0141\n10.1016/j.ophtha.2012.08.029\n10.2174/1573399811309020006\n10.2337/dc07-0264\n10.3346/jkms.2016.31.8.1292\n10.1007/s12031-020-01524-9\n10.1016/j.ophtha.2005.10.040\n10.1016/j.athoracsur.2007.07.066\n10.1002/ana.25564\n10.1016/j.jada.2009.05.016\n10.1016/j.acvd.2009.02.008\n10.1093/eurheartj/eht023\n10.1161/CIRCULATIONAHA.116.023425\n10.1590/S0004-27492011000500003\n10.1093/eurheartj/ehx565\n10.1161/01.STR.0000120310.43457.AD\n10.1016/j.amjmed.2009.05.030\n10.1093/eurheartj/ehm244\n10.1185/030079906X148472\n10.1016/j.atherosclerosissup.2018.04.040\n10.1093/eurheartj/ehq431\n10.3791/61865\n10.2337/dc20-0370\n10.1111/dme.14407\n10.1186/s12933-020-01023-6\n10.1089/dia.2019.0251\n10.2147/CLEP.S30621\n10.1186/s12916-014-0130-5\n10.1016/j.ultrasmedbio.2014.12.024\n10.1016/j.cmpb.2016.02.004\n10.1038/s41598-017-00989-w\n10.1586/14779072.5.1.69\n10.1002/jmri.20070\n10.1161/01.STR.0000141426.63959.cc\n10.1007/s11883-018-0736-8\n10.2214/AJR.11.6955\n10.23736/S0392-9590.19.04267-6\n10.1016/j.diabres.2018.07.028\n10.1111/echo.14242\n10.1007/s11517-019-01975-2\n10.21037/cdt.2019.09.03\n10.1109/TUFFC.2010.1522\n10.1109/TIP.2011.2169270\n10.1007/s11517-011-0781-8\n10.1118/1.3670373\n10.1016/j.cmpb.2012.05.008\n10.1093/ehjci/jez070\n10.1016/j.jvssci.2021.03.001\n10.1016/j.ejim.2015.10.018\n10.18295/squmj.2015.15.03.007\n10.1161/01.STR.0000258003.31194.0a\n10.1155/2014/826095\n10.1016/j.atherosclerosis.2015.01.030\n10.1177/0003319720941730\n10.52586/5026\n10.1016/j.jacc.2013.11.005\n10.1136/bmj.39554.624086.AD\n10.3399/bjgp15X685933\n10.1016/j.cjca.2016.07.510\n10.1016/j.jcct.2009.07.003\n10.1161/CIRCULATIONAHA.107.699579\n10.1016/S0195-668X(03)00114-3\n10.1136/bmj.j2099\n10.1007/s10916-020-01675-7\n10.1007/s10554-020-02099-7\n10.1007/s10554-021-02294-0\n10.1371/journal.pone.0174944\n10.1371/journal.pone.0213653\n10.1161/JAHA.118.009476\n10.1016/j.bspc.2016.03.001\n10.1109/ACCESS.2019.2923547\n10.1007/s10916-018-0940-7\n10.1016/j.compbiomed.2017.08.014\n10.1016/j.cmpb.2011.10.001\n10.1177/0954411913483637\n10.1118/1.4725759\n10.1016/j.cmpb.2017.12.016\n10.1016/j.measurement.2017.01.016\n10.31083/j.rcm.2020.04.236\n10.1016/j.compbiomed.2020.104043\n10.1109/MC.2016.339\n10.1109/TIM.2011.2174897\n10.1007/s11517-012-1019-0\n10.1007/s10554-012-0124-3\n10.21037/cdt.2020.01.07\n10.1016/j.ihj.2020.06.004\n10.7785/tcrt.2012.500381\n10.1016/j.ultrasmedbio.2010.07.011\n10.1016/j.cmpb.2015.11.013\n10.1016/j.gaitpost.2021.06.017\n10.1146/annurev-bioeng-071516-044442\n10.1109/JBHI.2016.2631401\n10.1016/j.compbiomed.2018.05.014\n10.1109/72.914517\n10.1016/j.compbiomed.2021.104721\n10.1038/s41467-021-23458-5\n10.1016/j.scitotenv.2020.138882\n10.1016/j.compbiomed.2020.103960\n10.1016/S2213-8587(20)30405-8\n10.1109/JBHI.2021.3103839\n10.1136/heartjnl-2020-317901\n10.1038/s41569-020-0413-9\n10.3390/ijms21186790\n10.1093/cvr/cvaa106\n10.1080/09273948.2020.1772313\n10.1186/s40662-020-00189-0\n10.1016/S1474-4422(21)00182-4\n10.1002/lio2.291\n10.1016/j.diabres.2020.108529\n10.1038/s41598-021-90482-2\n10.1177/1932296820930045\n10.5935/0004-2749.20200070\n10.1155/2020/9036847\n10.3390/ijerph182212209\n10.4103/ijo.IJO_1242_21\n10.1016/j.mvr.2021.104310\n10.1007/s11892-021-01411-6\n10.7759/cureus.14831\n10.7759/cureus.19148\n10.2147/TCRM.S316265\n10.4103/ijo.IJO_1474_21\n10.1016/j.ajo.2021.03.033\n10.4103/ijo.IJO_3798_20\n10.2147/OPTH.S294428\n10.1038/s41433-021-01804-7\n10.1016/j.ejrad.2022.110164\n10.1111/dom.14057\n10.1016/j.diabres.2020.108166\n10.1136/bmj.h5660\n10.1016/j.ijcha.2020.100589\n10.23736/S0392-9590.21.04771-4\n10.21037/cdt-20-561\n10.1016/j.cmpb.2021.106190\n10.3390/metabo12040312\n10.1016/j.clinimag.2021.05.016\n10.1007/s40622-021-00289-3\n10.1201/9780203490907.ch5\n10.3390/diagnostics12010166\n10.1016/S2589-7500(20)30250-8\n10.1038/s41551-018-0195-0\n10.3390/diagnostics11122257"}
{"title": "Towards robust diagnosis of COVID-19 using vision self-attention transformer.", "abstract": "The outbreak of COVID-19, since its appearance, has affected about 200 countries and endangered millions of lives. COVID-19 is extremely contagious disease, and it can quickly incapacitate the healthcare systems if infected cases are not handled timely. Several Conventional Neural Networks (CNN) based techniques have been developed to diagnose the COVID-19. These techniques require a large, labelled dataset to train the algorithm fully, but there are not too many labelled datasets. To mitigate this problem and facilitate the diagnosis of COVID-19, we developed a self-attention transformer-based approach having self-attention mechanism using CT slices. The architecture of transformer can exploit the ample unlabelled datasets using pre-training. The paper aims to compare the performances of self-attention transformer-based approach with CNN and Ensemble classifiers for diagnosis of COVID-19 using binary Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) infection and multi-class Hybrid-learning for UnbiaSed predicTion of COVID-19 (HUST-19) CT scan dataset. To perform this comparison, we have tested Deep learning-based classifiers and ensemble classifiers with proposed approach using CT scan images. Proposed approach is more effective in detection of COVID-19 with an accuracy of 99.7% on multi-class HUST-19, whereas 98% on binary class SARS-CoV-2 dataset. Cross corpus evaluation achieves accuracy of 93% by training the model with Hust19 dataset and testing using Brazilian COVID dataset.", "journal": "Scientific reports", "date": "2022-05-27", "authors": ["FoziaMehboob", "AbdulRauf", "RichardJiang", "Abdul Khader JilaniSaudagar", "Khalid MahmoodMalik", "Muhammad BadruddinKhan", "Mozaherul Hoque AbdulHasnat", "AbdullahAlTameem", "MohammedAlKhathami"], "doi": "10.1038/s41598-022-13039-x\n10.1016/j.bea.2021.100003\n10.2214/AJR.20.22954\n10.1038/s41598-020-79139-8\n10.1148/radiol.2020200905\n10.1101/2020.02.14.20023028\n10.1038/s41598-019-56847-4\n10.1007/s11547-020-01232-9\n10.3389/fbioe.2020.00670\n10.1016/j.cmrp.2020.03.011\n10.1007/s43465-020-00129-z\n10.1016/j.susoc.2021.02.001\n10.1016/j.compbiomed.2020.103795\n10.1109/ACCESS.2021.3058854\n10.1038/s41551-020-00633-5\n10.1016/j.imu.2020.100427"}
{"title": "SC2Net: A Novel Segmentation-Based Classification Network for Detection of COVID-19 in Chest X-Ray Images.", "abstract": "The pandemic of COVID-19 has become a global crisis in public health, which has led to a massive number of deaths and severe economic degradation. To suppress the spread of COVID-19, accurate diagnosis at an early stage is crucial. As the popularly used real-time reverse transcriptase polymerase chain reaction (RT-PCR) swab test can be lengthy and inaccurate, chest screening with radiography imaging is still preferred. However, due to limited image data and the difficulty of the early-stage diagnosis, existing models suffer from ineffective feature extraction and poor network convergence and optimisation. To tackle these issues, a segmentation-based COVID-19 classification network, namely SC2Net, is proposed for effective detection of the COVID-19 from chest x-ray (CXR) images. The SC2Net consists of two subnets: a COVID-19 lung segmentation network (CLSeg), and a spatial attention network (SANet). In order to supress the interference from the background, the CLSeg is first applied to segment the lung region from the CXR. The segmented lung region is then fed to the SANet for classification and diagnosis of the COVID-19. As a shallow yet effective classifier, SANet takes the ResNet-18 as the feature extractor and enhances high-level feature via the proposed spatial attention module. For performance evaluation, the COVIDGR 1.0 dataset is used, which is a high-quality dataset with various severity levels of the COVID-19. Experimental results have shown that, our SC2Net has an average accuracy of 84.23% and an average F1 score of 81.31% in detection of COVID-19, outperforming several state-of-the-art approaches.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-05-26", "authors": ["HuiminZhao", "ZhenyuFang", "JinchangRen", "CalumMacLellan", "YongXia", "ShuoLi", "MeijunSun", "KevinRen"], "doi": "10.1109/JBHI.2022.3177854"}
{"title": "An Interpretable Chest CT Deep Learning Algorithm for Quantification of COVID-19 Lung Disease and Prediction of Inpatient Morbidity and Mortality.", "abstract": "The burden of coronavirus disease 2019 (COVID-19) airspace opacities is time consuming and challenging to quantify on computed tomography. The purpose of this study was to evaluate the ability of a deep convolutional neural network (dCNN) to predict inpatient outcomes associated with COVID-19 pneumonia.\nA previously trained dCNN was tested on an external validation cohort of 241 patients who presented to the emergency department and received a chest computed tomography scan, 93 with COVID-19 and 168 without. Airspace opacity scoring systems were defined by the extent of airspace opacity in each lobe, totaled across the entire lungs. Expert and dCNN scores were concurrently evaluated for interobserver agreement, while both dCNN identified airspace opacity scoring and raw opacity values were used in the prediction of COVID-19 diagnosis and inpatient outcomes.\nInterobserver agreement for airspace opacity scoring was 0.892 (95% CI 0.834-0.930). Probability of each outcome behaved as a logistic function of the opacity scoring (25% intensive care unit admission at score of 13/25, 25% intubation at 17/25, and 25% mortality at 20/25). Length of hospitalization, intensive care unit stay, and intubation were associated with larger airspace opacity score (p\u00a0=\u00a00.032, 0.039, 0.036, respectively).\nThe tested dCNN was highly predictive of inpatient outcomes, performs at a near expert level, and provides added value for clinicians in terms of prognostication and disease severity.", "journal": "Academic radiology", "date": "2022-05-25", "authors": ["Jordan HChamberlin", "GilbertoAquino", "Uwe JosephSchoepf", "SophiaNance", "FrancoGodoy", "LandinCarson", "Vincent MGiovagnoli", "Callum EGill", "Liam JMcGill", "JimO'Doherty", "TilmanEmrich", "Jeremy RBurt", "DhirajBaruah", "AkosVarga-Szemes", "Ismail MKabakus"], "doi": "10.1016/j.acra.2022.03.023"}
{"title": "Novel TB smear microscopy automation system in detecting acid-fast bacilli for tuberculosis - A multi-center double blind study.", "abstract": "Due to COVID-19 pandemic, there is a large global drop in the number of newly diagnosed cases with tuberculosis (TB) worldwide. Actions to mitigate and reverse the impact of the COVID-19 pandemic on TB are urgently needed. Recent development of TB smear microscopy automation systems using artificial intelligence may increase the sensitivity of TB smear microscopy. The objective is to evaluate the performance of an automation system (\u03bc-Scan 2.0, Wellgen Medical) over manual smear microscopy in a multi-center, double-blind trial. Total of 1726 smears were enrolled. Referee medical technician and culture served as primary and secondary gold standards for result discrepancy. Results showed that, compared to manual microscopy, the \u03bc-Scan 2.0's performance of accuracy, sensitivity and specificity were 95.7% (1651/1726), 87.7% (57/65), and 96.0% (1594/1661), respectively. The negative predictive value was 97.8% at prevalence of 8.2%. Manual smear microscopy remains the primary diagnosis of pulmonary tuberculosis (TB). Use of automation system could achieve higher TB smear sensitivity and laboratory efficiency. It can also serve as a screening tool that complements molecular methods to reduce the total cost for TB diagnosis and control. Furthermore, such automation system is capable of remote access by internet connection and can be deployed in area with limited medical resources.", "journal": "Tuberculosis (Edinburgh, Scotland)", "date": "2022-05-25", "authors": ["Hsiao-ChuanHuang", "King-LungKuo", "Mei-HsinLo", "Hsiao-YunChou", "Yusen ELin"], "doi": "10.1016/j.tube.2022.102212"}
{"title": "SPIE Computer-Aided Diagnosis conference anniversary review.", "abstract": "The SPIE Computer-Aided Diagnosis conference has been held for 16 consecutive years at the annual SPIE Medical Imaging symposium. The conference remains vibrant, with a core group of submitters as well as new submitters and attendees each year. Recent developments include a marked shift in submissions relating to the artificial intelligence revolution in medical image analysis. This review describes the topics and trends observed in research presented at the Computer-Aided Diagnosis conference as part of the 50th-anniversary celebration of SPIE Medical Imaging.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2022-05-25", "authors": ["Ronald MSummers", "Maryellen LGiger"], "doi": "10.1117/1.JMI.9.S1.012208\n10.1117/12.911612\n10.1117/12.2254262\n10.1117/12.2293719\n10.1117/12.2083124\n10.1117/12.2216307\n10.1117/12.2293699\n10.1117/12.2293725\n10.1117/12.2293408\n10.1117/12.2277121\n10.1117/12.2254423\n10.1117/12.2580948\n10.1117/12.2582102\n10.1117/12.2582130\n10.1117/12.2581977\n10.1117/12.2581057\n10.1117/12.2582179\n10.1117/12.2580892\n10.1117/12.2581873\n10.1117/12.2582318\n10.1117/12.2580738\n10.1117/12.772298\n10.1117/12.2216747\n10.1117/12.967022\n10.1117/12.713672\n10.1117/12.2081489\n10.1117/12.2082820\n10.1117/12.91090\n10.1117/12.878180\n10.1117/12.911398\n10.1117/12.2253905\n10.1117/12.2083128\n10.1117/12.2217480\n10.1117/12.2008083\n10.1117/12.2216958\n10.1117/12.844932\n10.1117/12.2217752\n10.1117/12.2217587\n10.1117/12.2512208\n10.1117/12.2550868\n10.1117/12.2511787\n10.1117/12.2293140\n10.1117/12.2293140\n10.1117/12.2007822\n10.1117/12.2044333\n10.1117/12.2043737\n10.1117/12.912420\n10.1117/12.2082488\n10.1117/12.713857\n10.1117/12.773016\n10.1117/12.2582115\n10.1117/12.713851\n10.1117/12.844511\n10.1117/12.713640\n10.1117/12.708819\n10.1117/12.910531\n10.1117/12.812971\n10.1117/12.2043343\n10.1117/12.2082691\n10.1117/12.2217775\n10.1117/12.2254128\n10.1117/12.2254516\n10.1117/12.2513561\n10.1117/12.710088\n10.1117/12.2043648\n10.1117/12.811654\n10.1117/12.769824\n10.1117/12.878196\n10.1117/12.844571\n10.1117/12.911708\n10.1117/12.709780\n10.1117/12.844352\n10.1117/12.2217681\n10.1117/12.911177\n10.1117/12.912847\n10.1117/12.2081977\n10.1117/12.2081480\n10.1117/12.2254187\n10.1117/12.2007546\n10.1117/12.2254476\n10.1117/12.2007738\n10.1117/12.2008034\n10.1117/12.2551093\n10.1117/12.2255626\n10.1117/12.2549873\n10.1117/12.2007979\n10.1117/12.2043751\n10.1117/12.771970\n10.1117/12.2007927\n10.1117/12.2217906\n10.1117/12.2082309\n10.1117/12.911836\n10.1117/12.844406\n10.1117/12.2008282\n10.1117/12.2080871\n10.1117/12.2208583\n10.1117/12.2216645\n10.1117/12.2582203\n10.1117/12.2216978\n10.1117/12.811639\n10.1117/12.911216\n10.1117/12.2513134\n10.1117/12.2254136\n10.1117/12.2250910\n10.1117/12.2255247\n10.1117/12.2513228\n10.1117/12.2083596\n10.1117/12.709410\n10.1117/12.845530\n10.1117/12.2007829\n10.1117/12.2081600\n10.1117/12.2217084\n10.1117/12.2255553\n10.1117/12.2251322\n10.1117/12.2254212\n10.1117/12.2293199\n10.1117/12.2295374\n10.1117/12.878055\n10.1117/12.911703\n10.1117/12.911169\n10.1117/12.911700\n10.1117/12.911335\n10.1117/12.2043755\n10.1117/12.2216173\n10.1117/12.713732\n10.1117/12.2007970\n10.1117/12.2548857\n10.1117/12.2293495\n10.1117/12.812968\n10.1117/12.769858\n10.1117/12.812088\n10.1117/12.813468\n10.1117/12.2081521\n10.1117/12.2216917\n10.1117/12.2216929\n10.1117/12.2512584\n10.1117/12.2293207\n10.1117/12.2293661\n10.1117/12.2293764\n10.1117/12.2513567\n10.1117/12.2292573\n10.1117/12.2295178\n10.1117/12.2082811\n10.1117/12.813892\n10.1117/12.2217382\n10.1117/12.911500\n10.1117/12.2082226\n10.1117/12.2043791\n10.1117/12.2293334\n10.1117/12.2293297\n10.1117/12.2082204\n10.1117/12.2292962\n10.1117/12.2042469"}
{"title": "Deep learning based model for classification of COVID -19 images for healthcare research progress.", "abstract": "As imaging technology plays an important role in the diagnosis and evaluation of the new coronavirus pneumonia (COVID-19), COVID-19 related data sets have been published one after another, but there are relatively few data sets and research progress in related literature. To this end, through COVID-19-related journal papers, reports, and related open-source data set websites, organize and analyze the new coronary pneumonia data set and the deep learning models involved, including computed tomography (CT) image data sets and X-ray (CXR) Image dataset. Analyze the characteristics of the medical images presented in these data sets; focus on open-source data sets, as well as classification and segmentation models that perform well on related data sets. Finally, the future development trend of lung imaging technology is discussed.", "journal": "Materials today. Proceedings", "date": "2022-05-24", "authors": ["SarojKumar", "LChandra Sekhar Redd", "SusheelGeorge Joseph", "VinayKumar Sharma", "SabireenH"], "doi": "10.1016/j.matpr.2022.04.884\n10.1109/NSS/MIC42677.2020.9507847\n10.1109/TNNLS.2021.3099165\n10.1109/JBHI.2021.3067465\n10.1109/JBHI.2020.3042523\n10.1109/TBDATA.2020.3035935\n10.1109/ICSPIS51611.2020.9349605\n10.1108/WJE-09-2020-0450\n10.1108/WJE-12-2020-0631\n10.1109/TMI.2020.2995965\n10.1109/ITCA52113.2020.00146\n10.1109/TII.2021.3059023\n10.1007/s11277-021-08565-2\n10.1007/s11277-021-08767-8"}
{"title": "WVALE: Weak variational autoencoder for localisation and enhancement of COVID-19 lung infections.", "abstract": "The COVID-19 pandemic is a major global health crisis of this century. The use of neural networks with CT imaging can potentially improve clinicians' efficiency in diagnosis. Previous studies in this field have primarily focused on classifying the disease on CT images, while few studies targeted the localisation of disease regions. Developing neural networks for automating the latter task is impeded by limited CT images with pixel-level annotations available to the research community.\nThis paper proposes a weakly-supervised framework named \"Weak Variational Autoencoder for Localisation and Enhancement\" (WVALE) to address this challenge for COVID-19 CT images. This framework includes two components: anomaly localisation with a novel WVAE model and enhancement of supervised segmentation models with WVALE.\nThe WVAE model have been shown to produce high-quality post-hoc attention maps with fine borders around infection regions, while weak supervision segmentation shows results comparable to conventional supervised segmentation models. The WVALE framework can enhance the performance of a range of supervised segmentation models, including state-of-art models for the segmentation of COVID-19 lung infection.\nOur study provides a proof-of-concept for weakly supervised segmentation and an alternative approach to alleviate the lack of annotation, while its independence from classification & segmentation frameworks makes it easily integrable with existing systems.", "journal": "Computer methods and programs in biomedicine", "date": "2022-05-22", "authors": ["QinghuaZhou", "ShuihuaWang", "XinZhang", "Yu-DongZhang"], "doi": "10.1016/j.cmpb.2022.106883"}
{"title": "Automatic detection of pneumonia in chest X-ray images using textural features.", "abstract": "Fast and accurate diagnosis is critical for the triage and management of pneumonia, particularly in the current scenario of a COVID-19 pandemic, where this pathology is a major symptom of the infection. With the objective of providing tools for that purpose, this study assesses the potential of three textural image characterisation methods: radiomics, fractal dimension and the recently developed superpixel-based histon, as biomarkers to be used for training Artificial Intelligence (AI) models in order to detect pneumonia in chest X-ray images. Models generated from three different AI algorithms have been studied: K-Nearest Neighbors, Support Vector Machine and Random Forest. Two open-access image datasets were used in this study. In the first one, a dataset composed of paediatric chest X-ray, the best performing generated models achieved an 83.3% accuracy with 89% sensitivity for radiomics, 89.9% accuracy with 93.6% sensitivity for fractal dimension and 91.3% accuracy with 90.5% sensitivity for superpixels based histon. Second, a dataset derived from an image repository developed primarily as a tool for studying COVID-19 was used. For this dataset, the best performing generated models resulted in a 95.3% accuracy with 99.2% sensitivity for radiomics, 99% accuracy with 100% sensitivity for fractal dimension and 99% accuracy with 98.6% sensitivity for superpixel-based histons. The results confirm the validity of the tested methods as reliable and easy-to-implement automatic diagnostic tools for pneumonia.", "journal": "Computers in biology and medicine", "date": "2022-05-20", "authors": ["C\u00e9sarOrtiz-Toro", "AngelGarc\u00eda-Pedrero", "MarioLillo-Saavedra", "ConsueloGonzalo-Mart\u00edn"], "doi": "10.1016/j.compbiomed.2022.105466\n10.1109/NAFIPS.2000.877448\n10.1109/ICCV.2003.1238308"}
{"title": "COVID-19 classification using thermal images.", "abstract": "There is a scarcity of published research on the potential role of thermal imaging in the remote detection of respiratory issues due to coronavirus disease-19 (COVID-19). This is a comprehensive study that explores the potential of this imaging technology resulting from its convenient aspects that make it highly accessible: it is contactless, noninvasive, and devoid of harmful radiation effects, and it does not require a complicated installation process.\nWe aim to investigate the role of thermal imaging, specifically thermal video, for the identification of SARS-CoV-2-infected people using infrared technology and to explore the role of breathing patterns in different parts of the thorax for the identification of possible COVID-19 infection.\nWe used signal moment, signal texture, and shape moment features extracted from five different body regions of interest (whole upper body, chest, face, back, and side) of images obtained from thermal video clips in which optical flow and super-resolution were used. These features were classified into positive and negative COVID-19 using machine learning strategies.\nCOVID-19 detection for male models [receiver operating characteristic (ROC) area under the ROC curve (AUC) = 0.605 95% confidence intervals (CI) 0.58 to 0.64] is more reliable than for female models (ROC AUC = 0.577 95% CI 0.55 to 0.61). Overall, thermal imaging is not very sensitive nor specific in detecting COVID-19; the metrics were below 60% except for the chest view from males.\nWe conclude that, although it may be possible to remotely identify some individuals affected by COVID-19, at this time, the diagnostic performance of current methods for body thermal imaging is not good enough to be used as a mass screening tool.", "journal": "Journal of biomedical optics", "date": "2022-05-20", "authors": ["Martha Rebeca CanalesFiscal", "VictorTrevi\u00f1o", "Luis Javier Ram\u00edrezTrevi\u00f1o", "Rocio OrtizL\u00f3pez", "ServandoCardona Huerta", "VictorJavier Lara-D\u00edaz", "Jos\u00e9 Gerardo TamezPe\u00f1a"], "doi": "10.1117/1.JBO.27.5.056003\n10.1038/s41586-020-2008-3\n10.1056/NEJMoa2002032\n10.1038/s41586-020-2012-7\n10.1016/S0140-6736(20)30183-5\n10.1038/s41591-020-0931-3\n10.1007/s00330-021-07715-1\n10.1080/03091900600711332\n10.1016/j.infrared.2012.03.007\n10.3390/ijerph18063286\n10.1016/j.infrared.2014.06.001\n10.1148/radiol.11091710\n10.1007/s11042-020-09600-3\n10.1111/eci.13474\n10.4104/pcrj.2010.00024\n10.1016/j.jvoice.2020.09.024\n10.1016/j.compbiomed.2021.104944\n10.1186/1475-925X-10-93\n10.1364/BOE.8.004480\n10.1111/j.1469-8986.2010.01167.x\n10.1364/BOE.6.004378\n10.1109/JSYST.2014.2336372\n10.1109/TBME.2009.2032415\n10.1186/s41983-018-0024-0\n10.1164/rccm.201903-0656CI\n10.1109/EEIS.1996.566997\n10.1109/TSMC.1979.4310076\n10.1016/j.neuroimage.2006.01.021\n10.1109/TBME.2012.2186612\n10.1109/TSMC.1973.4309314\n10.1109/ICPR.1994.576366\n10.1093/bioinformatics/17.6.520\n10.1007/BF00994018\n10.1109/ICDAR.1995.598994\n10.2307/2685209\n10.1161/01.CIR.101.23.e215"}
{"title": "Explainable artificial intelligence-based edge fuzzy images for COVID-19 detection and identification.", "abstract": "The COVID-19 pandemic continues to wreak havoc on the world's population's health and well-being. Successful screening of infected patients is a critical step in the fight against it, with radiology examination using chest radiography being one of the most important screening methods. For the definitive diagnosis of COVID-19 disease, reverse-transcriptase polymerase chain reaction remains the gold standard. Currently available lab tests may not be able to detect all infected individuals; new screening methods are required. We propose a Multi-Input Transfer Learning COVID-Net fuzzy convolutional neural network to detect COVID-19 instances from torso X-ray, motivated by the latter and the open-source efforts in this research area. Furthermore, we use an explainability method to investigate several Convolutional Networks COVID-Net forecasts in an effort to not only gain deeper insights into critical factors associated with COVID-19 instances, but also to aid clinicians in improving screening. We show that using transfer learning and pre-trained models, we can detect it with a high degree of accuracy. Using X-ray images, we chose four neural networks to predict its probability. Finally, in order to achieve better results, we considered various methods to verify the techniques proposed here. As a result, we were able to create a model with an AUC of 1.0 and accuracy, precision, and recall of 0.97. The model was quantized for use in Internet of Things devices and maintained a 0.95 percent accuracy.", "journal": "Applied soft computing", "date": "2022-05-19", "authors": ["QinhuaHu", "Francisco Nauber BGois", "RafaelCosta", "LijuanZhang", "LingYin", "NaercioMagaia", "Victor Hugo Cde Albuquerque"], "doi": "10.1016/j.asoc.2022.108966\n10.1109/ACCESS.2020.2994762\n10.1109/ACCESS.2020.2990893\n10.1109/SSIAI.2006.1633722\n10.1109/RBME.2020.2987975\n10.3390/app10020559\n10.1109/tfuzz.2019.2949771\n10.1109/tfuzz.2019.2961350\n10.1109/access.2020.3003810\n10.1101/2020.03.19.20039354\n10.1109/tmi.2020.3000314\n10.1101/2020.03.12.20027185\n10.1016/j.mri.2017.07.016\n10.1109/ICICET.2018.8533865\n10.1109/IIHMSP.2007.4457697\n10.2991/eusflat.2011.73\n10.1109/IndiaCom.2014.6828012\n10.1109/FUZZ-IEEE.2019.8858971\n10.1109/IECBES.2018.8626678\n10.1109/FUZZ-IEEE.2019.8859010\n10.1109/ICACCS.2017.8014648\n10.1109/ICCTCT.2018.8550853\n10.1109/ICCS45141.2019.9065537\n10.1109/ICSEE.2018.8646252\n10.1109/ACCESS.2017.2782884\n10.1109/TFUZZ.2018.2857725\n10.1109/access.2020.3005044\n10.1109/ACCESS.2020.2981561\n10.1109/ISRITI48646.2019.9034624\n10.1109/ACCESS.2020.2971566\n10.1109/SAUPEC/RobMech/PRASA48453.2020.9041139\n10.1109/LifeTech48969.2020.1570619224\n10.1109/ECTI-CON47248.2019.8955345\n10.1109/ELECSYM.2019.8901604\n10.1007/s11548-007-0125-1"}
{"title": "Automated COVID-19 Grading With Convolutional Neural Networks in Computed Tomography Scans: A Systematic Comparison.", "abstract": "Amidst the ongoing pandemic, the assessment of computed tomography (CT) images for COVID-19 presence can exceed the workload capacity of radiologists. Several studies addressed this issue by automating COVID-19 classification and grading from CT images with convolutional neural networks (CNNs). Many of these studies reported initial results of algorithms that were assembled from commonly used components. However, the choice of the components of these algorithms was often pragmatic rather than systematic and systems were not compared to each other across papers in a fair manner. We systematically investigated the effectiveness of using 3-D CNNs instead of 2-D CNNs for seven commonly used architectures, including DenseNet, Inception, and ResNet variants. For the architecture that performed best, we furthermore investigated the effect of initializing the network with pretrained weights, providing automatically computed lesion maps as additional network input, and predicting a continuous instead of a categorical output. A 3-D DenseNet-201 with these components achieved an area under the receiver operating characteristic curve of 0.930 on our test set of 105 CT scans and an AUC of 0.919 on a publicly available set of 742 CT scans, a substantial improvement in comparison with a previously published 2-D CNN. This article provides insights into the performance benefits of various components for COVID-19 classification and grading systems. We have created a challenge on grand-challenge.org to allow for a fair comparison between the results of this and future research.", "journal": "IEEE transactions on artificial intelligence", "date": "2022-05-19", "authors": ["Coende Vente", "Luuk HBoulogne", "Kiran VaidhyaVenkadesh", "CherylSital", "NikolasLessmann", "ColinJacobs", "Clara ISanchez", "Bramvan Ginneken"], "doi": "10.1109/TAI.2021.3115093"}
{"title": "A Deep Learning Framework Integrating the Spectral and Spatial Features for Image-Assisted Medical Diagnostics.", "abstract": "The development of a computer-aided disease detection system to ease the long and arduous manual diagnostic process is an emerging research interest. Living through the recent outbreak of the COVID-19 virus, we propose a machine learning and computer vision algorithms-based automatic diagnostic solution for detecting the COVID-19 infection. Our proposed method applies to chest radiograph that uses readily available infrastructure. No studies in this direction have considered the spatial aspect of the medical images. This motivates us to investigate the role of spectral-domain information of medical images along with the spatial content towards improved disease detection ability. Successful integration of spatial and spectral features is demonstrated on the COVID-19 infection detection task. Our proposed method comprises three stages - Feature extraction, Dimensionality reduction via projection, and prediction. At first, images are transformed into spectral and spatio-spectral domains by using Discrete cosine transform (DCT) and Discrete Wavelet transform (DWT), two powerful image processing algorithms. Next, features from spatial, spectral, and spatio-spectral domains are projected into a lower dimension through the Convolutional Neural Network (CNN), and those three types of projected features are then fed to Multilayer Perceptron (MLP) for final prediction. The combination of the three types of features yielded superior performance than any of the features when used individually. This indicates the presence of complementary information in the spectral domain of the chest radiograph to characterize the considered medical condition. Moreover, saliency maps corresponding to classes representing different medical conditions demonstrate the reliability of the proposed method. The study is further extended to identify different medical conditions using diverse medical image datasets and shows the efficiency of leveraging the combined features. Altogether, the proposed method exhibits potential as a generalized and robust medical image-assisted diagnostic solution.", "journal": "IEEE access : practical innovations, open solutions", "date": "2022-05-19", "authors": ["SusmitaGhosh", "SwagatamDas", "RammohanMallipeddi"], "doi": "10.1109/ACCESS.2021.3133338"}
{"title": "Comment on \"Artificial intelligence in gastroenterology: A state-of-the-art review\".", "abstract": "Colon capsule endoscopy (CCE) was introduced nearly two decades ago. Initially, it was limited by poor image quality and short battery time, but due to technical improvements, it has become an equal diagnostic alternative to optical colonoscopy (OC). Hastened by the coronavirus disease 2019 pandemic, CCE has been introduced in clinical practice to relieve overburdened endoscopy units and move investigations to out-patient clinics. A wider adoption of CCE would be bolstered by positive patient experience, as it offers a diagnostic investigation that is not inferior to other modalities. The shortcomings of CCE include its inability to differentiate adenomatous polyps from hyperplastic polyps. Solving this issue would improve the stratification of patients for polyp removal. Artificial intelligence (AI) has shown promising results in polyp detection and characterization to minimize incomplete CCEs and avoid needless examinations. Onboard AI appears to be a needed application to enable near-real-time decision-making in order to diminish patient waiting times and avoid superfluous subsequent OCs. With this letter, we discuss the potential and role of AI in CCE as a diagnostic tool for the large bowel.", "journal": "World journal of gastroenterology", "date": "2022-05-19", "authors": ["ThomasBj\u00f8rsum-Meyer", "AnastasiosKoulaouzidis", "GunnarBaatrup"], "doi": "10.3748/wjg.v28.i16.1722"}
{"title": "Deep learning model for the automatic classification of COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy: a multi-center retrospective study.", "abstract": "This retrospective study aimed to develop and validate a deep learning model for the classification of coronavirus disease-2019 (COVID-19) pneumonia, non-COVID-19 pneumonia, and the healthy using chest X-ray (CXR) images. One private and two public datasets of CXR images were included. The private dataset included CXR from six hospitals. A total of 14,258 and 11,253 CXR images were included in the 2 public datasets and 455 in the private dataset. A deep learning model based on EfficientNet with noisy student was constructed using the three datasets. The test set of 150 CXR images in the private dataset were evaluated by the deep learning model and six radiologists. Three-category classification accuracy and class-wise area under the curve (AUC) for each of the COVID-19 pneumonia, non-COVID-19 pneumonia, and healthy were calculated. Consensus of the six radiologists was used for calculating class-wise AUC. The three-category classification accuracy of our model was 0.8667, and those of the six radiologists ranged from 0.5667 to 0.7733. For our model and the consensus of the six radiologists, the class-wise AUC of the healthy, non-COVID-19 pneumonia, and COVID-19 pneumonia were 0.9912, 0.9492, and 0.9752 and 0.9656, 0.8654, and 0.8740, respectively. Difference of the class-wise AUC between our model and the consensus of the six radiologists was statistically significant for COVID-19 pneumonia (p value\u2009=\u20090.001334). Thus, an accurate model of deep learning for the three-category classification could be constructed; the diagnostic performance of our model was significantly better than that of the consensus interpretation by the six radiologists for COVID-19 pneumonia.", "journal": "Scientific reports", "date": "2022-05-18", "authors": ["MizuhoNishio", "DaigoKobayashi", "EikoNishioka", "HidetoshiMatsuo", "YasuyoUrase", "KojiOnoue", "ReiichiIshikura", "YuriKitamura", "EiroSakai", "MasaruTomita", "AkihiroHamanaka", "TakamichiMurakami"], "doi": "10.1038/s41598-022-11990-3\n10.1148/radiol.2020200432\n10.1148/radiol.2020200823\n10.1007/s13244-018-0639-9\n10.1016/j.patcog.2020.107700\n10.1109/ACCESS.2021.3058537\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-019-56847-4\n10.1038/s41598-019-56847-4\n10.3389/fmed.2021.629134\n10.1148/radiol.2020203511\n10.1016/S0140-6736(18)31645-3\n10.1038/nature21056\n10.1001/jama.2016.17216\n10.1016/j.compbiomed.2021.104375\n10.1016/j.compbiomed.2020.104181\n10.1007/s11263-019-01228-7\n10.1016/j.media.2020.101797\n10.1038/s41746-021-00399-3\n10.1186/1471-2105-12-77\n10.3348/kjr.2021.0048\n10.1016/S1071-5819(03)00038-7"}
{"title": "The state of the art for artificial intelligence in lung digital pathology.", "abstract": "Lung diseases carry a significant burden of morbidity and mortality worldwide. The advent of digital pathology (DP) and an increase in computational power have led to the development of artificial intelligence (AI)-based tools that can assist pathologists and pulmonologists in improving clinical workflow and patient management. While previous works have explored the advances in computational approaches for breast, prostate, and head and neck cancers, there has been a growing interest in applying these technologies to lung diseases as well. The application of AI tools on radiology images for better characterization of indeterminate lung nodules, fibrotic lung disease, and lung cancer risk stratification has been well documented. In this article, we discuss methodologies used to build AI tools in lung DP, describing the various hand-crafted and deep learning-based unsupervised feature approaches. Next, we review AI tools across a wide spectrum of lung diseases including cancer, tuberculosis, idiopathic pulmonary fibrosis, and COVID-19. We discuss the utility of novel imaging biomarkers for different types of clinical problems including quantification of biomarkers like PD-L1, lung disease diagnosis, risk stratification, and prediction of response to treatments such as immune checkpoint inhibitors. We also look briefly at some emerging applications of AI tools in lung DP such as multimodal data analysis, 3D pathology, and transplant rejection. Lastly, we discuss the future of DP-based AI tools, describing the challenges with regulatory approval, developing reimbursement models, planning clinical deployment, and addressing AI biases. \u00a9 2022 The Authors. The Journal of Pathology published by John Wiley & Sons Ltd on behalf of The Pathological Society of Great Britain and Ireland.", "journal": "The Journal of pathology", "date": "2022-05-18", "authors": ["Vidya SankarViswanathan", "PaulaToro", "Germ\u00e1nCorredor", "SanjayMukhopadhyay", "AnantMadabhushi"], "doi": "10.1002/path.5966\n10.23919/SpliTech.2019.8783041\n10.1117/12.2296646.full\n10.1117/12.2542360.full\n10.1117/12.2293147.full\n10.1109/ICCVW.2017.15\n10.1016/j.semcancer.2021.02.011"}
{"title": "Effective multiscale deep learning model for COVID19 segmentation tasks: A further step towards helping radiologist.", "abstract": "Infection by the SARS-CoV-2 leading to COVID-19 disease is still rising and techniques to either diagnose or evaluate the disease are still thoroughly investigated. The use of CT as a complementary tool to other biological tests is still under scrutiny as the CT scans are prone to many false positives as other lung diseases display similar characteristics on CT scans. However, fully investigating CT images is of tremendous interest to better understand the disease progression and therefore thousands of scans need to be segmented by radiologists to study infected areas. Over the last year, many deep learning models for segmenting CT-lungs were developed. Unfortunately, the lack of large and shared annotated multicentric datasets led to models that were either under-tested (small dataset) or not properly compared (own metrics, none shared dataset), often leading to poor generalization performance. To address, these issues, we developed a model that uses a multiscale and multilevel feature extraction strategy for COVID19 segmentation and extensively validated it on several datasets to assess its generalization capability for other segmentation tasks on similar organs. The proposed model uses a novel encoder and decoder with a proposed kernel-based atrous spatial pyramid pooling module that is used at the bottom of the model to extract small features with a multistage skip connection concatenation approach. The results proved that our proposed model could be applied on a small-scale dataset and still produce generalizable performances on other segmentation tasks. The proposed model produced an efficient Dice score of 90% on a 100 cases dataset, 95% on the NSCLC dataset, 88.49% on the COVID19 dataset, and 97.33 on the StructSeg 2019 dataset as compared to existing state-of-the-art models. The proposed solution could be used for COVID19 segmentation in clinic applications. The source code is publicly available at https://github.com/RespectKnowledge/Mutiscale-based-Covid-_segmentation-usingDeep-Learning-models.", "journal": "Neurocomputing", "date": "2022-05-18", "authors": ["AbdulQayyum", "AlainLalande", "FabriceMeriaudeau"], "doi": "10.1016/j.neucom.2022.05.009\n10.1148/radiol.2020200905\n10.1109/TPAMI.2017.2699184"}
{"title": "A lightweight CNN-based network on COVID-19 detection using X-ray and CT images.", "abstract": "The traditional method of detecting COVID-19 disease mainly rely on the interpretation of computer tomography (CT) or X-ray images (X-ray) by doctors or professional researchers to identify whether it is COVID-19 disease, which is easy to cause identification mistakes. In this study, the technology of convolutional neural network is expected to be able to efficiently and accurately identify the COVID-19 disease.\nThis study uses and fine-tunes seven convolutional neural networks including InceptionV3, ResNet50V2, Xception, DenseNet121, MobileNetV2, EfficientNet-B0, and EfficientNetV2 on COVID-19 detection. In addition, we proposes a lightweight convolutional neural network, LightEfficientNetV2, on small number of chest X-ray and CT images. Five-fold cross-validation was used to evaluate the performance of each model. To confirm the performance of the proposed model, LightEfficientNetV2 was carried out on three different datasets (NIH Chest X-rays, SARS-CoV-2 and COVID-CT).\nOn chest X-ray image dataset, the highest accuracy 96.50% was from InceptionV3 before fine-tuning; and the highest accuracy 97.73% was from EfficientNetV2 after fine-tuning. The accuracy of the LightEfficientNetV2 model proposed in this study is 98.33% on chest X-ray image. On CT images, the best transfer learning model before fine-tuning is MobileNetV2, with an accuracy of 94.46%; the best transfer learning model after fine-tuning is Xception, with an accuracy of 96.78%. The accuracy of the LightEfficientNetV2 model proposed in this study is 97.48% on CT image.\nCompared with the SOTA, LightEfficientNetV2 proposed in this study demonstrates promising performance on chest X-ray images, CT images and three different datasets.", "journal": "Computers in biology and medicine", "date": "2022-05-17", "authors": ["Mei-LingHuang", "Yu-ChiehLiao"], "doi": "10.1016/j.compbiomed.2022.105604\n10.1016/j.imu.2020.100405\n10.1016/j.asoc.2020.106912\n10.1016/j.compmedimag.2019.05.005\n10.1016/j.bspc.2021.103182\n10.1016/j.imu.2020.100505\n10.1016/j.mlwa.2021.100138\n10.1016/j.chaos.2020.110071\n10.1016/j.bbe.2021.09.004\n10.1016/j.imu.2021.100620\n10.1016/j.compbiomed.2022.105244\n10.1016/j.eswa.2021.114883\n10.1007/s13246-020-00865-4\n10.1016/j.asoc.2020.106691\n10.1016/j.asoc.2020.106859\n10.1016/j.imu.2020.100360\n10.1016/j.asoc.2021.107675\n10.1016/j.compbiomed.2021.105134\n10.1016/j.iot.2021.100377\n10.1016/j.compbiomed.2020.103795\n10.1016/j.compbiomed.2021.104857\n10.1016/j.patrec.2021.08.035\n10.1016/j.compbiomed.2021.104575\n10.1016/j.jiph.2021.07.015\n10.1016/j.displa.2022.102150\n10.32604/cmc.2021.018040\n10.1016/j.ibmed.2021.100027\n10.1016/j.asoc.2020.106885\n10.1016/j.compbiomed.2021.104608\n10.1007/s11760-021-01991-6\n10.1016/j.compbiomed.2021.104729\n10.1016/j.patcog.2021.107848\n10.1016/j.bspc.2021.102920\n10.1016/j.bbe.2021.06.011\n10.1016/j.measurement.2021.110289\n10.1016/j.patrec.2021.06.021\n10.1016/j.compbiomed.2021.104742\n10.1016/j.imu.2021.100687\n10.1016/j.ultrasmedbio.2022.01.023\n10.1016/j.compbiomed.2021.105002\n10.1016/j.aej.2021.01.011\n10.1016/j.compbiomed.2021.104348\n10.1016/j.chaos.2020.110495\n10.1016/j.chaos.2020.110190\n10.1016/j.bspc.2021.102987\n10.1016/j.neucom.2021.06.012\n10.1016/j.compbiomed.2021.105014\n10.1016/j.imu.2020.100505\n10.1109/JPROC.2020.3004555\n10.1016/j.compag.2021.106184\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.308\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.195\n10.1109/CVPR.2017.243\n10.1016/j.compbiomed.2021.105014\n10.1016/j.chaos.2020.109944\n10.1016/j.patrec.2018.10.027\n10.1016/j.compbiomed.2021.105127\n10.1101/2020.04.13.20063941\n10.1016/j.patrec.2020.10.001\n10.1109/CVPR.2017.369\n10.1109/CVPR.2018.00865\n10.1016/j.imu.2020.100391\n10.1101/2020.04.24.20078584v3%0Ahttps://www.medrxiv.org/content/10.1101/2020.04.24.20078584v3.abstract\n10.1080/07391102.2020.1788642"}
{"title": "Thoracic imaging tests for the diagnosis of COVID-19.", "abstract": "Our March 2021 edition of this review showed thoracic imaging computed tomography (CT) to be sensitive and moderately specific in diagnosing COVID-19 pneumonia. This new edition is an update of the review.\nOur objectives were to evaluate the diagnostic accuracy of thoracic imaging in people with suspected COVID-19; assess the rate of positive imaging in people who had an initial reverse transcriptase polymerase chain reaction (RT-PCR) negative result and a positive RT-PCR result on follow-up; and evaluate the accuracy of thoracic imaging for screening COVID-19 in asymptomatic individuals. The secondary objective was to assess threshold effects of index test positivity on accuracy.\nWe searched the COVID-19 Living Evidence Database from the University of Bern, the Cochrane COVID-19 Study Register, The Stephen B. Thacker CDC Library, and repositories of COVID-19 publications through to 17 February 2021. We did not apply any language restrictions.\nWe included diagnostic accuracy studies of all designs, except for case-control, that recruited participants of any age group suspected to have COVID-19. Studies had to assess chest CT, chest X-ray, or ultrasound of the lungs for the diagnosis of COVID-19, use a reference standard that included RT-PCR, and report estimates of test accuracy or provide data from which we could compute estimates. We excluded studies that used imaging as part of the reference standard and studies that excluded participants with normal index test results.\nThe review authors independently and in duplicate screened articles, extracted data and assessed risk of bias and applicability concerns using QUADAS-2. We presented sensitivity and specificity per study on paired forest plots, and summarized pooled estimates in tables. We used a bivariate meta-analysis model where appropriate.\nWe included 98 studies in this review. Of these, 94 were included for evaluating the diagnostic accuracy of thoracic imaging in the evaluation of people with suspected COVID-19. Eight studies were included for assessing the rate of positive imaging in individuals with initial RT-PCR negative results and positive RT-PCR results on follow-up, and 10 studies were included for evaluating the accuracy of thoracic imaging for imagining asymptomatic individuals. For all 98 included studies, risk of bias was high or unclear in 52 (53%) studies with respect to participant selection, in 64 (65%) studies with respect to reference standard, in 46 (47%) studies with respect to index test, and in 48 (49%) studies with respect to flow and timing. Concerns about the applicability of the evidence to: participants were high or unclear in eight (8%) studies; index test were high or unclear in seven (7%) studies; and reference standard were high or unclear in seven (7%) studies. Imaging in people with suspected COVID-19 We included 94 studies. Eighty-seven studies evaluated one imaging modality, and seven studies evaluated two imaging modalities. All studies used RT-PCR alone or in combination with other criteria (for example, clinical signs and symptoms, positive contacts) as the reference standard for the diagnosis of COVID-19. For chest CT (69 studies, 28285 participants, 14,342 (51%) cases), sensitivities ranged from 45% to 100%, and specificities from 10% to 99%. The pooled sensitivity of chest CT was 86.9% (95% confidence interval (CI) 83.6 to 89.6), and pooled specificity was 78.3% (95% CI 73.7 to 82.3). Definition for index test positivity was a source of heterogeneity for sensitivity, but not specificity. Reference standard was not a source of heterogeneity. For chest X-ray (17 studies, 8529 participants, 5303 (62%) cases), the sensitivity ranged from 44% to 94% and specificity from 24 to 93%. The pooled sensitivity of chest X-ray was 73.1% (95% CI 64. to -80.5), and pooled specificity was 73.3% (95% CI 61.9 to 82.2). Definition for index test positivity was not found to be a source of heterogeneity. Definition for index test positivity and reference standard were not found to be sources of heterogeneity. For ultrasound of the lungs (15 studies, 2410 participants, 1158 (48%) cases), the sensitivity ranged from 73% to 94% and the specificity ranged from 21% to 98%. The pooled sensitivity of ultrasound was 88.9% (95% CI 84.9 to 92.0), and the pooled specificity was 72.2% (95% CI 58.8 to 82.5). Definition for index test positivity and reference standard were not found to be sources of heterogeneity. Indirect comparisons of modalities evaluated across all 94 studies indicated that chest CT and ultrasound gave higher sensitivity estimates than X-ray (P = 0.0003 and P = 0.001, respectively). Chest CT and ultrasound gave similar sensitivities (P=0.42). All modalities had similar specificities (CT versus X-ray P = 0.36; CT versus ultrasound P = 0.32; X-ray versus ultrasound P = 0.89). Imaging in PCR-negative people who subsequently became positive For rate of positive imaging in individuals with initial RT-PCR negative results, we included 8 studies (7 CT, 1 ultrasound) with a total of 198 participants suspected of having COVID-19, all of whom had a final diagnosis of COVID-19. Most studies (7/8) evaluated CT. Of 177 participants with initially negative RT-PCR who had positive RT-PCR results on follow-up testing, 75.8% (95% CI 45.3 to 92.2) had positive CT findings. Imaging in asymptomatic PCR-positive people For imaging asymptomatic individuals, we included 10 studies (7 CT, 1 X-ray, 2 ultrasound) with a total of 3548 asymptomatic participants, of whom 364 (10%) had a final diagnosis of COVID-19. For chest CT (7 studies, 3134 participants, 315 (10%) cases), the pooled sensitivity was 55.7% (95% CI 35.4 to 74.3) and the pooled specificity was 91.1% (95% CI 82.6 to 95.7).\nChest CT and ultrasound of the lungs are sensitive and moderately specific in diagnosing COVID-19. Chest X-ray is moderately sensitive and moderately specific in diagnosing COVID-19. Thus, chest CT and ultrasound may have more utility for ruling out COVID-19 than for differentiating SARS-CoV-2 infection from other causes of respiratory illness. The uncertainty resulting from high or unclear risk of bias and the heterogeneity of included studies limit our ability to confidently draw conclusions based on our results.", "journal": "The Cochrane database of systematic reviews", "date": "2022-05-17", "authors": ["SanamEbrahimzadeh", "NayaarIslam", "HabenDawit", "Jean-PaulSalameh", "SakibKazi", "NicholasFabiano", "LeeTreanor", "MarissaAbsi", "FarazAhmad", "PaulRooprai", "AhmedAl Khalil", "KellyHarper", "NeilKamra", "Mariska MgLeeflang", "LottyHooft", "Christian Bvan der Pol", "RossPrager", "Samanjit SHare", "CaroleDennie", "Ren\u00e9Spijker", "Jonathan JDeeks", "JacquelineDinnes", "KevinJenniskens", "Dani\u00ebl AKorevaar", "J\u00e9r\u00e9mie FCohen", "AnnVan den Bruel", "YemisiTakwoingi", "Jannekevan de Wijgert", "JunfengWang", "ElenaPena", "SandraSabongui", "Matthew DfMcInnes", "NoneNone"], "doi": "10.1002/14651858.CD013639.pub5\n10.1148/radiol.2020200642\n10.5152/dir.2020.20350\n10.5812/iranjradiol.104950\n10.1590/0100-3984.2020.0040\n10.1007/s00330-020-07273-y\n10.1007/s00330-020-07050-x\n10.1186/s13244-020-00957-5\n10.1101/2020.07.07.20147934\n10.1007/s11739-020-02512-y\n10.1371/journal.pone.0242840\n10.1007/s00330-020-07346-y\n10.1148/radiol.2020201237\n10.1177/1024907920968648\n10.1016/j.ejrad.2020.109344\n10.1016/j.ejrad.2020.109272\n10.1259/bjr.20200994\n10.1007/s00330-020-07126-8\n10.3760/cma.j.cn112149-20200218-00187\n10.1101/2020.05.18.20097444\n10.1002/jmv.26724\n10.4081/monaldi.2020.1446\n10.3390/microorganisms8121929\n10.19193/0393-6384_2020_5_447\n10.1007/s00330-020-07154-4\n10.1016/j.clinimag.2021.01.026\n10.1016/j.ejrad.2020.109192\n10.1186/s12879-021-05829-x\n10.3390/diagnostics10090608\n10.1007/s11547-020-01269-w\n10.1371/journal.pone.0235844\n10.1016/j.bjid.2020.10.002\n10.1259/bjr.20200574\n10.1016/j.ejrad.2020.109209\n10.1007/s11604-020-01061-w\n10.1136/emermed-2020-210125\n10.29271/jcpsp.2021.01.S1\n10.1016/j.rmed.2020.105980\n10.1007/s10140-020-01821-1\n10.1007/s00264-020-04651-5\n10.1148/radiol.2020202568\n10.3348/kjr.2020.0536\n10.1016/j.ejrad.2020.109092\n10.1080/08998280.2020.1834658\n10.1259/bjr.20200643\n10.5152/dir.2020.20270\n10.1016/j.chest.2020.11.026\n10.1183/23120541.00539-2020\n10.1186/s12890-020-1170-6\n10.1007/s12262-020-02626-9\n10.1038/s41591-020-0931-3\n10.1097/RCT.0000000000001054\n10.1007/s11547-020-01327-3\n10.1148/radiol.202020187\n10.1007/s10140-020-01849-3\n10.1007/s00330-020-07345-z\n10.1177/0846537120968919\n10.1016/j.ejrad.2020.109425\n10.1016/j.clinimag.2020.09.00\n10.1016/j.chest.2021.01.053\n10.1007/s11547-020-01302-y\n10.5811/westjem.2020.5.47743\n10.1101/2020.05.26.20114082\n10.1148/radiol.2020203776\n10.1101/2020.04.07.20057315\n10.1016/j.annemergmed.2020.10.008\n10.1097/SLA.0000000000004218\n10.4103/ijri.IJRI_377_20\n10.3390/biology10020089\n10.1097/RCT.0000000000001127\n10.1183/23120541.00357-2020\n10.1007/s42058-021-00075-1\n10.22088/cjim.11.0.527\n10.1148/radiol.2020203465\n10.1186/s12873-020-00389-w\n10.3238/arztebl.2020.0389\n10.4103/ijri.IJRI_405_20\n10.1016/j.ajem.2020.07.058\n10.1093/ofid/ofaa171\n10.1007/s11739-020-02524-\n10.1016/j.ultrasmedbio.2020.12.021\n10.1016/j.radi.2020.06.010\n10.1183/13993003.04188-2020\n10.1259/bjro.20200034\n10.11817/j.issn.1672-7347.2020.200152\n10.1148/radiol.2020203511\n10.16781/j.0258\u2011879x.2020.08.0828\n10.3760/cma.j.cn112137-20200228-00499\n10.1080/14767058.2020.1798398\n10.1016/j.ejrad.2020.109414\n10.1101/2020.04.09.20059352\n10.1101/2020.02.19.20025023\n10.1007/s00330-020-06829-2\n10.5152/ejbh.2020.010420\n10.1148/radiol.2020201433\n10.1016/j.ejrad.2020.109009\n10.1101/2020.04.07.20056762\n10.2214/AJR.20.23035\n10.1016/j.tmaid.2020.101627\n10.1101/2020.02.25.20027763\n10.1056/NEJMc2005073\n10.1101/2020.05.15.20103473\n10.1101/2020.03.22.20040782\n10.1148/radiol.2020200847\n10.2214/AJR.20.23232\n10.1101/2020.03.19.20027078\n10.1002/ijgo.13165\n10.1002/jmv.25904\n10.1007/s00259-020-04720-2\n10.1183/13993003.00407-2020\n10.1148/radiol.2015151516\n10.1001/jamainternmed.2020.8876\n10.1016/j.jclinepi.2006.06.011\n10.1001/jama.2020.22717\n10.1002/14651858.CD013652\n10.1002/14651858.CD013705\n10.1002/14651858.CD013705\n10.2214/AJR.20.22961\n10.1177/1536867X0900900203\n10.1002/jmri.25797\n10.1016/0895-4356(94)00099-c\n10.1148/radiol.2020200241\n10.1183/13993003.01377-2020\n10.7326/M20-1495\n10.1016/S1473-3099(20)30134-1\n10.1002/14651858.CD015013\n10.2214/AJR.20.22954\n10.1002/jmv.25786\n10.1080/22221751.2020.1745095\n10.1016/j.ejrad.2017.05.032\n10.1371/journal.pmed.1000097\n10.1148/ryct.2020200034\n10.1007/s00330-020-06731-x\n10.1007/s00134-020-05996-6\n10.1148/radiol.2020201473\n10.1016/j.jclinepi.2005.02.022\n10.1148/radiol.2020201365\n10.1136/bmj.m2632\n10.2214/AJR.20.23034\n10.1016/S1473-3099(20)30086-4\n10.1148/ryct.2020200152\n10.1002/jum.15285\n10.1002/14651858.CD013787\n10.1002/14651858.CD013665\n10.1007/s00330-020-06801-0\n10.1111/all.14238\n10.2214/AJR.20.22976\n10.1007/s00330-020-06816-7\n10.1002/14651858.CD013639.pub3\n10.1002/14651858.CD013639.pub4\n10.1002/14651858.CD013639\n10.1002/14651858.CD013639.pub2"}
{"title": "A Comprehensive Review of Artificial Intelligence in Prevention and Treatment of COVID-19 Pandemic.", "abstract": "The unprecedented outbreak of the Corona Virus Disease 2019 (COVID-19) pandemic has seriously affected numerous countries in the world from various aspects such as education, economy, social security, public health, etc. Most governments have made great efforts to control the spread of COVID-19, e.g., locking down hard-hit cities and advocating masks for the population. However, some countries and regions have relatively poor medical conditions in terms of insufficient medical equipment, hospital capacity overload, personnel shortage, and other problems, resulting in the large-scale spread of the epidemic. With the unique advantages of Artificial Intelligence (AI), it plays an extremely important role in medical imaging, clinical data, drug development, epidemic prediction, and telemedicine. Therefore, AI is a powerful tool that can help humans solve complex problems, especially in the fight against COVID-19. This study aims to analyze past research results and interpret the role of Artificial Intelligence in the prevention and treatment of COVID-19 from five aspects. In this paper, we also discuss the future development directions in different fields and prove the validity of the models through experiments, which will help researchers develop more efficient models to control the spread of COVID-19.", "journal": "Frontiers in genetics", "date": "2022-05-14", "authors": ["HaishuaiWang", "ShangruJia", "ZhaoLi", "YucongDuan", "GuangyuTao", "ZipingZhao"], "doi": "10.3389/fgene.2022.845305\n10.1007/s11831-020-09472-8\n10.3390/electronics9050827\n10.1016/j.asoc.2021.107161\n10.1016/j.intimp.2021.107516\n10.1126/science.abd5223\n10.1016/j.csbj.2020.03.025\n10.1007/s11042-021-10714-5\n10.1111/tbed.14102\n10.1038/s41746-020-00308-0\n10.1016/j.cmpb.2020.105608\n10.1109/access.2020.2992341\n10.1016/j.eswa.2020.113909\n10.1007/s00330-020-06829-2\n10.1002/jmv.25681\n10.1016/j.chaos.2020.109864\n10.1109/access.2020.3010287\n10.1007/s13246-020-00888-x\n10.1016/j.jbi.2021.103801\n10.1016/j.procs.2021.01.254\n10.1080/07391102.2020.1767212\n10.1016/b978-0-12-802260-3.00004-3\n10.1016/j.psep.2021.03.032\n10.1016/j.bspc.2022.103595\n10.1038/s41467-020-17971-2\n10.1002/jmv.25766\n10.1093/jamia/ocaa156\n10.1109/access.2020.3005510\n10.1109/access.2020.3002445\n10.1109/access.2020.3015102\n10.1016/j.eswa.2020.114054\n10.1016/j.cnsns.2020.105303\n10.1016/j.chaos.2020.110632\n10.1093/cid/ciaa414\n10.3389/fmed.2020.00169\n10.1016/j.cmpb.2020.105581\n10.1093/bioinformatics/btaa645\n10.1093/jamia/ocaa039\n10.1001/jamainternmed.2020.2033\n10.1038/s41467-020-17280-8\n10.1016/j.compbiomed.2020.103869\n10.1016/j.chaos.2020.109889\n10.1111/ggi.13960\n10.1007/s10489-020-01770-9\n10.1007/s10489-021-02359-6\n10.1016/j.chaos.2020.109853\n10.1038/s41467-022-28621-0\n10.1016/j.rinp.2020.103772\n10.1080/17415977.2021.1872563\n10.1038/s41598-020-78652-0\n10.1007/s42979-022-01067-3\n10.1016/j.chaos.2020.109944\n10.1001/jama.2020.0757\n10.1007/s00521-020-05626-8\n10.1109/ACCESS.2021.3079121\n10.1007/s12559-020-09779-5\n10.1007/s12539-020-00403-6\n10.1038/s41746-020-00343-x\n10.1109/access.2021.3063152\n10.1109/access.2020.2997311\n10.3389/fgene.2021.636441\n10.1002/prot.25407\n10.1038/s41586-019-1923-7\n10.1155/2020/8889023\n10.1016/j.ijsu.2020.02.034\n10.1007/s10462-021-09985-z\n10.1017/ice.2020.61\n10.7326/l20-1061\n10.1002/minf.202000028\n10.1093/jamia/ocaa097\n10.1093/jamia/ocaa048\n10.1016/j.mehy.2020.109761\n10.1109/access.2020.3027685\n10.1080/07391102.2020.1788642\n10.1109/access.2020.2994762\n10.1109/JSTSP.2022.3152375\n10.1021/acs.jcim.0c00179\n10.1038/s41598-020-76550-z\n10.1161/HYPERTENSIONAHA.120.15324\n10.1016/j.ijantimicag.2020.105948\n10.1093/jamia/ocaa067\n10.1016/j.cmpb.2021.105996\n10.1016/S2589-7500(20)30217-X\n10.21037/jtd.2020.02.64\n10.1016/j.matcom.2021.01.022\n10.32604/cmc.2020.011391\n10.1002/jmv.26945"}
{"title": "The effect of machine learning explanations on user trust for automated diagnosis of COVID-19.", "abstract": "Recent years have seen deep neural networks (DNN) gain widespread acceptance for a range of computer vision tasks that include medical imaging. Motivated by their performance, multiple studies have focused on designing deep convolutional neural network architectures tailored to detect COVID-19 cases from chest computerized tomography (CT) images. However, a fundamental challenge of DNN models is their inability to explain the reasoning for a diagnosis. Explainability is essential for medical diagnosis, where understanding the reason for a decision is as important as the decision itself. A variety of algorithms have been proposed that generate explanations and strive to enhance users' trust in DNN models. Yet, the influence of the generated machine learning explanations on clinicians' trust for complex decision tasks in healthcare has not been understood. This study evaluates the quality of explanations generated for a deep learning model that detects COVID-19 based on CT images and examines the influence of the quality of these explanations on clinicians' trust. First, we collect radiologist-annotated explanations of the CT images for the diagnosis of COVID-19 to create the ground truth. We then compare ground truth explanations with machine learning explanations. Our evaluation shows that the explanations produced. by different algorithms were often correct (high precision) when compared to the radiologist annotated ground truth but a significant number of explanations were missed (significantly lower recall). We further conduct a controlled experiment to study the influence of machine learning explanations on clinicians' trust for the diagnosis of COVID-19. Our findings show that while the clinicians' trust in automated diagnosis increases with the explanations, their reliance on the diagnosis reduces as clinicians are less likely to rely on algorithms that are not close to human judgement. Clinicians want higher recall of the explanations for a better understanding of an automated diagnosis system.", "journal": "Computers in biology and medicine", "date": "2022-05-14", "authors": ["KanikaGoel", "RenukaSindhgatta", "SumitKalra", "RohanGoel", "PreetiMutreja"], "doi": "10.1016/j.compbiomed.2022.105587\n10.1148/radiol.2020200432\n10.3389/fmed.2020.608525\n10.3390/electronics10050593\n10.1145/2939672.2939778\n10.1109/ICCV.2019.00304\n10.1109/ICCV.2019.00304\n10.3390/jimaging6060052\n10.3390/jimaging6060052\n10.1117/12.2549298\n10.1038/s41598-020-76550-z\n10.1109/re.2019.00032\n10.1145/3290607.3312962\n10.2139/ssrn.3064761\n10.1007/978-3-030-29726-8\\_7\n10.1016/j.cell.2020.04.045\n10.1007/s11263-017-1059-x\n10.1109/CVPR.2015.7298640\n10.1016/j.media.2020.101857\n10.1016/j.inffus.2021.01.008"}
{"title": "SSA-Net: Spatial self-attention network for COVID-19 pneumonia infection segmentation with semi-supervised few-shot learning.", "abstract": "Coronavirus disease (COVID-19) broke out at the end of 2019, and has resulted in an ongoing global pandemic. Segmentation of pneumonia infections from chest computed tomography (CT) scans of COVID-19 patients is significant for accurate diagnosis and quantitative analysis. Deep learning-based methods can be developed for automatic segmentation and offer a great potential to strengthen timely quarantine and medical treatment. Unfortunately, due to the urgent nature of the COVID-19 pandemic, a systematic collection of CT data sets for deep neural network training is quite difficult, especially high-quality annotations of multi-category infections are limited. In addition, it is still a challenge to segment the infected areas from CT slices because of the irregular shapes and fuzzy boundaries. To solve these issues, we propose a novel COVID-19 pneumonia lesion segmentation network, called Spatial Self-Attention network (SSA-Net), to identify infected regions from chest CT images automatically. In our SSA-Net, a self-attention mechanism is utilized to expand the receptive field and enhance the representation learning by distilling useful contextual information from deeper layers without extra training time, and spatial convolution is introduced to strengthen the network and accelerate the training convergence. Furthermore, to alleviate the insufficiency of labeled multi-class data and the long-tailed distribution of training data, we present a semi-supervised few-shot iterative segmentation framework based on re-weighting the loss and selecting prediction values with high confidence, which can accurately classify different kinds of infections with a small number of labeled image data. Experimental results show that SSA-Net outperforms state-of-the-art medical image segmentation networks and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage. Meanwhile, our semi-supervised iterative segmentation model can improve the learning ability in small and unbalanced training set and can achieve higher performance.", "journal": "Medical image analysis", "date": "2022-05-12", "authors": ["XiaoyanWang", "YiwenYuan", "DongyanGuo", "XiaojieHuang", "YingCui", "MingXia", "ZhenhuaWang", "CongBai", "ShengyongChen"], "doi": "10.1016/j.media.2022.102459\n10.1148/radiol.2020200642\n10.1148/radiol.2020200823\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200230\n10.1109/RBME.2020.2990959\n10.1101/2020.04.22.20074948\n10.1148/radiol.2020200432\n10.1016/j.media.2020.101836\n10.1109/TMI.2019.2903562\n10.1109/cvpr.2016.90\n10.1101/2020.04.13.20063941\n10.1186/s41747-020-00173-2\n10.1109/ICCV.2019.00110\n10.1016/S0140-6736(20)30183-5\n10.5281/zenodo.3757476\n10.1109/TMI.2020.2992546\n10.1016/j.media.2020.101851\n10.1109/cvpr42600.2020.00487\n10.1148/radiol.2020200236\n10.1016/j.media.2020.101794\n10.1109/tmi.2020.2993291\n10.1109/isbi45749.2020.9098541\n10.1007/978-3-319-24574-4_28\n10.1148/radiol.2020201365\n10.1109/iccv.2017.74\n10.1002/mp.14609\n10.1109/rbme.2020.2987975\n10.1109/tmi.2020.3000314\n10.1109/tmi.2020.2994908\n10.1109/tmi.2020.2995965\n10.1109/cvpr42600.2020.01229\n10.1148/radiol.2020201160\n10.1007/978-3-030-58548-8_10\n10.1148/radiol.2020200343\n10.1109/cvpr42600.2020.01308\n10.1109/cvpr.2016.319\n10.1109/tmi.2020.3001810"}
{"title": "Study on transfer learning capabilities for pneumonia classification in chest-x-rays images.", "abstract": "over the last year, the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) and its variants have highlighted the importance of screening tools with high diagnostic accuracy for new illnesses such as COVID-19. In that regard, deep learning approaches have proven as effective solutions for pneumonia classification, especially when considering chest-x-rays images. However, this lung infection can also be caused by other viral, bacterial or fungi pathogens. Consequently, efforts are being poured toward distinguishing the infection source to help clinicians to diagnose the correct disease origin. Following this tendency, this study further explores the effectiveness of established neural network architectures on the pneumonia classification task through the transfer learning paradigm.\nto present a comprehensive comparison, 12 well-known ImageNet pre-trained models were fine-tuned and used to discriminate among chest-x-rays of healthy people, and those showing pneumonia symptoms derived from either a viral (i.e., generic or SARS-CoV-2) or bacterial source. Furthermore, since a common public collection distinguishing between such categories is currently not available, two distinct datasets of chest-x-rays images, describing the aforementioned sources, were combined and employed to evaluate the various architectures.\nthe experiments were performed using a total of 6330 images split between train, validation, and test sets. For all models, standard classification metrics were computed (e.g., precision, f1-score), and most architectures obtained significant performances, reaching, among the others, up to 84.46% average f1-score when discriminating the four identified classes. Moreover, execution times, areas under the receiver operating characteristic (AUROC), confusion matrices, activation maps computed via the Grad-CAM algorithm, and additional experiments to assess the robustness of each model using only 50%, 20%, and 10% of the training set were also reported to present an informed discussion on the networks classifications.\nthis paper examines the effectiveness of well-known architectures on a joint collection of chest-x-rays presenting pneumonia cases derived from either viral or bacterial sources, with particular attention to SARS-CoV-2 contagions for viral pathogens; demonstrating that existing architectures can effectively diagnose pneumonia sources and suggesting that the transfer learning paradigm could be a crucial asset in diagnosing future unknown illnesses.", "journal": "Computer methods and programs in biomedicine", "date": "2022-05-11", "authors": ["DaniloAvola", "AndreaBacciu", "LuigiCinque", "AlessioFagioli", "Marco RaoulMarini", "RiccardoTaiello"], "doi": "10.1016/j.cmpb.2022.106833\n10.1001/jamainternmed.2014.4344\n10.1001/jama.2017.9039\n10.1259/bjr/31200593\n10.1016/S0140-6736(20)30183-5\n10.1016/j.scs.2020.102589\n10.1038/d41586-021-00396-2\n10.1016/S0140-6736(21)00370-6\n10.1038/s41591-021-01345-2\n10.1016/j.cmpb.2018.05.034\n10.1016/j.cmpb.2020.105728\n10.1016/j.cmpb.2020.105348\n10.1145/3447243\n10.1016/j.media.2020.101794\n10.1016/j.cmpb.2020.105608\n10.1016/j.asoc.2020.106906\n10.1109/JBHI.2020.3037127\n10.1109/TMI.2020.3040950\n10.1109/TNNLS.2021.3054306\n10.1016/j.asoc.2021.107160\n10.1016/j.compbiomed.2020.103792\n10.1016/j.asoc.2020.106859\n10.1109/TII.2021.3057683\n10.1016/j.asoc.2021.107645\n10.1016/j.asoc.2020.106580\n10.1016/j.cmpb.2020.105581\n10.1109/TMI.2020.2993291\n10.1109/JBHI.2021.3058293\n10.1016/j.compbiomed.2021.104401\n10.1016/j.compbiomed.2020.103869\n10.1016/j.asoc.2020.106744\n10.1016/j.patcog.2021.107826\n10.1016/j.knosys.2020.106647\n10.3390/s21062215\n10.1038/s41591-021-01506-3\n10.1109/JSEN.2021.3076767\n10.1007/978-3-030-78618-2_4\n10.1016/j.cmpb.2021.106004\n10.1016/S2589-7500(20)30102-3\n10.1016/S2589-7500(21)00076-5\n10.1016/j.asoc.2020.106691\n10.1145/3437120.3437300\n10.1007/s13246-020-00865-4\n10.1016/j.asoc.2020.106912\n10.1016/j.irbm.2020.05.003\n10.1109/ACCESS.2020.3016780\n10.1016/j.cell.2018.02.010\n10.1007/s11263-019-01228-7\n10.1109/TKDE.2009.191\n10.1016/j.media.2019.03.009\n10.1145/3065386\n10.1109/CVPR.2017.243\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2019.00293\n10.1109/CVPR.2018.00474\n10.1109/ICCV.2019.00140\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.634\n10.1109/CVPR.2018.00716\n10.1007/s11263-015-0816-y\n10.1109/CVPR.2018.00745\n10.1145/2487575.2487629"}
{"title": "DMDF-Net: Dual multiscale dilated fusion network for accurate segmentation of lesions related to COVID-19 in lung radiographic scans.", "abstract": "The recent disaster of COVID-19 has brought the whole world to the verge of devastation because of its highly transmissible nature. In this pandemic, radiographic imaging modalities, particularly, computed tomography (CT), have shown remarkable performance for the effective diagnosis of this virus. However, the diagnostic assessment of CT data is a human-dependent process that requires sufficient time by expert radiologists. Recent developments in artificial intelligence have substituted several personal diagnostic procedures with computer-aided diagnosis (CAD) methods that can make an effective diagnosis, even in real time. In response to COVID-19, various CAD methods have been developed in the literature, which can detect and localize infectious regions in chest CT images. However, most existing methods do not provide cross-data analysis, which is an essential measure for assessing the generality of a CAD method. A few studies have performed cross-data analysis in their methods. Nevertheless, these methods show limited results in real-world scenarios without addressing generality issues. Therefore, in this study, we attempt to address generality issues and propose a deep learning-based CAD solution for the diagnosis of COVID-19 lesions from chest CT images. We propose a dual multiscale dilated fusion network (DMDF-Net) for the robust segmentation of small lesions in a given CT image. The proposed network mainly utilizes the strength of multiscale deep features fusion inside the encoder and decoder modules in a mutually beneficial manner to achieve superior segmentation performance. Additional pre- and post-processing steps are introduced in the proposed method to address the generality issues and further improve the diagnostic performance. Mainly, the concept of post-region of interest (ROI) fusion is introduced in the post-processing step, which reduces the number of false-positives and provides a way to accurately quantify the infected area of lung. Consequently, the proposed framework outperforms various state-of-the-art methods by accomplishing superior infection segmentation results with an average Dice similarity coefficient of 75.7%, Intersection over Union of 67.22%, Average Precision of 69.92%, Sensitivity of 72.78%, Specificity of 99.79%, Enhance-Alignment Measure of 91.11%, and Mean Absolute Error of 0.026.", "journal": "Expert systems with applications", "date": "2022-05-10", "authors": ["MuhammadOwais", "Na RaeBaek", "Kang RyoungPark"], "doi": "10.1016/j.eswa.2022.117360"}
{"title": "C-COVIDNet: A CNN Model for COVID-19 Detection Using Image Processing.", "abstract": "COVID-19 has become a global disaster that has disturbed the socioeconomic fabric of the world. Efficient and cost-effective diagnosis methods are very much required for better treatment and eliminating false cases for COVID-19. COVID-19 disease is a type of respiratory syndrome, thus lung X-ray analysis has got the attention for an effective diagnosis. Hence, the proposed study introduces an Image processing based COVID-19 detection model C-COVIDNet, which is trained on a dataset of chest X-ray images belonging to three categories: COVID-19, Pneumonia, and Normal person. Image preprocessing pipeline is used for extracting the region of interest (ROI), so that the required features may be present in the input. This lightweight convolution neural network (CNN) based approach has achieved an accuracy of 97.5% and an F1-score of 97.91%. Model input images are generated in batches using a custom data generator. The performance of C-COVIDNet has outperformed the state-of-the-art. The promising results will surely help in accelerating the development of deep learning-based COVID-19 diagnosis tools using radiography.", "journal": "Arabian journal for science and engineering", "date": "2022-05-10", "authors": ["NehaRajawat", "Bharat SinghHada", "MayankMeghawat", "SoniyaLalwani", "RajeshKumar"], "doi": "10.1007/s13369-022-06841-2\n10.1038/s41598-019-56847-4\n10.1007/s10489-020-01829-7\n10.1007/s00138-020-01119-9\n10.1016/j.patrec.2018.08.010\n10.1016/j.patrec.2018.07.026\n10.1016/j.ins.2018.02.060\n10.4018/IJSWIS.2020040101\n10.1109/TIP.2015.2512108\n10.1109/76.915354\n10.1109/JSEN.2018.2828312"}
{"title": "Learning COVID-19 Pneumonia Lesion Segmentation From Imperfect Annotations via Divergence-Aware Selective Training.", "abstract": "Automatic segmentation of COVID-19 pneumonia lesions is critical for quantitative measurement for diagnosis and treatment management. For this task, deep learning is the state-of-the-art method while requires a large set of accurately annotated images for training, which is difficult to obtain due to limited access to experts and the time-consuming annotation process. To address this problem, we aim to train the segmentation network from imperfect annotations, where the training set consists of a small clean set of accurately annotated images by experts and a large noisy set of inaccurate annotations by non-experts. To avoid the labels with different qualities corrupting the segmentation model, we propose a new approach to train segmentation networks to deal with noisy labels. We introduce a dual-branch network to separately learn from the accurate and noisy annotations. To fully exploit the imperfect annotations as well as suppressing the noise, we design a Divergence-Aware Selective Training (DAST) strategy, where a divergence-aware noisiness score is used to identify severely noisy annotations and slightly noisy annotations. For severely noisy samples we use an regularization through dual-branch consistency between predictions from the two branches. We also refine slightly noisy samples and use them as supplementary data for the clean branch to avoid overfitting. Experimental results show that our method achieves a higher performance than standard training process for COVID-19 pneumonia lesion segmentation when learning from imperfect labels, and our framework outperforms the state-of-the-art noise-tolerate methods significantly with various clean label percentages.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-05-07", "authors": ["ShuojueYang", "GuotaiWang", "HuiSun", "XiangdeLuo", "PengSun", "KangLi", "QijunWang", "ShaotingZhang"], "doi": "10.1109/JBHI.2022.3172978"}
{"title": "An externally validated fully automated deep learning algorithm to classify COVID-19 and other pneumonias on chest computed tomography.", "abstract": "In this study, we propose an artificial intelligence (AI) framework based on three-dimensional convolutional neural networks to classify computed tomography (CT) scans of patients with coronavirus disease 2019 (COVID-19), influenza/community-acquired pneumonia (CAP), and no infection, after automatic segmentation of the lungs and lung abnormalities.\nThe AI classification model is based on inflated three-dimensional Inception architecture and was trained and validated on retrospective data of CT images of 667 adult patients (no infection n=188, COVID-19 n=230, influenza/CAP n=249) and 210 adult patients (no infection n=70, COVID-19 n=70, influenza/CAP n=70), respectively. The model's performance was independently evaluated on an internal test set of 273 adult patients (no infection n=55, COVID-19 n= 94, influenza/CAP n=124) and an external validation set from a different centre (305 adult patients: COVID-19 n=169, no infection n=76, influenza/CAP n=60).\nThe model showed excellent performance in the external validation set with area under the curve of 0.90, 0.92 and 0.92 for COVID-19, influenza/CAP and no infection, respectively. The selection of the input slices based on automatic segmentation of the abnormalities in the lung reduces analysis time (56\u2005s per scan) and computational burden of the model. The Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD) score of the proposed model is 47% (15 out of 32 TRIPOD items).\nThis AI solution provides rapid and accurate diagnosis in patients suspected of COVID-19 infection and influenza.", "journal": "ERJ open research", "date": "2022-05-06", "authors": ["AkshayaaVaidyanathan", "JulienGuiot", "FadilaZerka", "FloreBelmans", "IngridVan Peufflik", "LouisDeprez", "DenisDanthine", "GregoryCanivet", "PhilippeLambin", "SeanWalsh", "MariaelenaOcchipinti", "PaulMeunier", "WimVos", "PierreLovinfosse", "Ralph T HLeijenaar"], "doi": "10.1183/23120541.00579-2021\n10.1016/j.ejrad.2009.11.005\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200823\n10.1016/j.media.2017.07.005\n10.1016/j.eng.2020.04.010\n10.1016/j.compbiomed.2020.103795\n10.1056/NEJMsb2005114\n10.1038/s41597-021-00900-3\n10.1016/j.neunet.2018.07.011\n10.5334/jbr-btr.1229\n10.1109/CVPR.2017.502\n10.48550/arXiv.1705.06950\n10.1109/CVPR.2015.7298594\n10.48550/arXiv.1412.6980\n10.1186/s12916-014-0241-z\n10.1016/j.compbiomed.2021.104348\n10.1148/radiol.2020200905\n10.1155/2021/6677314\n10.3389/fcvm.2021.638011\n10.1016/j.ejrad.2021.109602\n10.1183/13993003.00775-2020\n10.1007/s00259-020-05075-4\n10.1186/s12967-020-02683-4\n10.1016/j.cell.2020.04.045\n10.1155/2020/9756518\n10.3390/jimaging6060052\n10.1002/widm.1312\n10.1515/icom-2020-0024\n10.1038/s41467-020-18685-1\n10.1007/s00330-021-07715-1\n10.1109/ACCESS.2018.2877890\n10.1038/s41598-020-70479-z\n10.1007/s11548-020-02286-w\n10.1080/07391102.2020.1767212\n10.1128/microbe.10.354.1\n10.2147/RMHP.S269315\n10.1109/ACCESS.2020.3029445\n10.1200/CCI.19.00047"}
{"title": "Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis.", "abstract": "Ultrasound (US) imaging is widely used for anatomical structure inspection in clinical diagnosis. The training of new sonographers and deep learning based algorithms for US image analysis usually requires a large amount of data. However, obtaining and labeling large-scale US imaging data are not easy tasks, especially for diseases with low incidence. Realistic US image synthesis can alleviate this problem to a great extent. In this paper, we propose a generative adversarial network (GAN) based image synthesis framework. Our main contributions include: (1) we present the first work that can synthesize realistic B-mode US images with high-resolution and customized texture editing features; (2) to enhance structural details of generated images, we propose to introduce auxiliary sketch guidance into a conditional GAN. We superpose the edge sketch onto the object mask and use the composite mask as the network input; (3) to generate high-resolution US images, we adopt a progressive training strategy to gradually generate high-resolution images from low-resolution images. In addition, a feature loss is proposed to minimize the difference of high-level features between the generated and real images, which further improves the quality of generated images; (4) the proposed US image synthesis method is quite universal and can also be generalized to the US images of other anatomical structures besides the three ones tested in our study (lung, hip joint, and ovary); (5) extensive experiments on three large US image datasets are conducted to validate our method. Ablation studies, customized texture editing, user studies, and segmentation tests demonstrate promising results of our method in synthesizing realistic US images.", "journal": "Medical image analysis", "date": "2022-05-06", "authors": ["JiaminLiang", "XinYang", "YuhaoHuang", "HaomingLi", "ShuangchiHe", "XindiHu", "ZejianChen", "WufengXue", "JunCheng", "DongNi"], "doi": "10.1016/j.media.2022.102461"}
{"title": "Multiclass Convolution Neural Network for Classification of COVID-19 CT Images.", "abstract": "In the late December of 2019, a novel coronavirus was discovered in Wuhan, China. In March 2020, WHO announced this epidemic had become a global pandemic and that the novel coronavirus may be mild to most people. However, some people may experience a severe illness that results in hospitalization or maybe death. COVID-19 classification remains challenging due to the ambiguity and similarity with other known respiratory diseases such as SARS, MERS, and other viral pneumonia. The typical symptoms of COVID-19 are fever, cough, chills, shortness of breath, loss of smell and taste, headache, sore throat, chest pains, confusion, and diarrhoea. This research paper suggests the concept of transfer learning using the deterministic algorithm in all binary classification models and evaluates the performance of various CNN architectures. The datasets of 746 CT images of COVID-19 and non-COVID-19 were divided for training, validation, and testing. Various augmentation techniques were applied to increase the number of datasets except for testing images. The images were then pretrained using CNN to obtain a binary class. ResNeXt101 and ResNet152 have the best F1 score of 0.978 and 0.938, whereas GoogleNet has an F1 score of 0.762. ResNeXt101 and ResNet152 have an accuracy of 97.81% and 93.80%. ResNeXt101, DenseNet201, and ResNet152 have 95.71%, 93.81%, and 90% sensitivity, whereas ResNeXt101, ResNet101, and ResNet152 have 100%, 99.58%, and 98.33 specificity, respectively.", "journal": "Computational intelligence and neuroscience", "date": "2022-05-03", "authors": ["Serena LowWoan Ching", "Khin WeeLai", "Joon HuangChuah", "KhairunnisaHasikin", "AziraKhalil", "PengjiangQian", "KaijianXia", "YizhangJiang", "YuanpengZhang", "SamiappanDhanalakshmi"], "doi": "10.1155/2022/9167707\n10.1101/2020.12.19.20248530v2.full\n10.1038/s41586-020-2196-x\n10.9790/0661-1157073\n10.48550/arXiv.1602.07360"}
{"title": "Diagnosis of COVID-19 Pneumonia via a Novel Deep Learning Architecture.", "abstract": "COVID-19 is a contagious infection that has severe effects on the global economy and our daily life. Accurate diagnosis of COVID-19 is of importance for consultants, patients, and radiologists. In this study, we use the deep learning network AlexNet as the backbone, and enhance it with the following two aspects: 1) adding batch normalization to help accelerate the training, reducing the internal covariance shift; 2) replacing the fully connected layer in AlexNet with three classifiers: SNN, ELM, and RVFL. Therefore, we have three novel models from the deep COVID network (DC-Net) framework, which are named DC-Net-S, DC-Net-E, and DC-Net-R, respectively. After comparison, we find the proposed DC-Net-R achieves an average accuracy of 90.91% on a private dataset (available upon email request) comprising of 296 images while the specificity reaches 96.13%, and has the best performance among all three proposed classifiers. In addition, we show that our DC-Net-R also performs much better than other existing algorithms in the literature.\nThe online version contains supplementary material available at 10.1007/s11390-020-0679-8.", "journal": "Journal of computer science and technology", "date": "2022-05-03", "authors": ["XinZhang", "SiyuanLu", "Shui-HuaWang", "XiangYu", "Su-JingWang", "LunYao", "YiPan", "Yu-DongZhang"], "doi": "10.1007/s11390-020-0679-8\n10.1016/S0140-6736(20)30185-9\n10.1001/jama.2020.1585\n10.1166/jmihi.2016.1901\n10.1007/s11042-016-3559-z\n10.2174/1871527315666161019153259\n10.3233/FI-2017-1492\n10.1166/jmihi.2019.2804\n10.3233/FI-2019-1829\n10.1109/TPAMI.2005.159\n10.1148/radiol.2020200230\n10.17818/NM/2020/1.2\n10.1007/s11356-020-07703-w\n10.1162/NECO_a_00892\n10.1016/j.jmapro.2020.03.006\n10.1016/0925-2312(94)90053-1\n10.1016/S0165-0114(02)00521-3\n10.1038/s41591-019-0447-x\n10.1016/j.acra.2019.05.018\n10.1148/rg.2018170048"}
{"title": "COVID-19 prognosis using limited chest X-ray images.", "abstract": "The COrona VIrus Disease 2019 (COVID-19) pandemic is an ongoing global pandemic that has claimed millions of lives till date. Detecting COVID-19 and isolating affected patients at an early stage is crucial to contain its rapid spread. Although accurate, the primary viral test 'Reverse Transcription Polymerase Chain Reaction' (RT-PCR) for COVID-19 diagnosis has an elaborate test kit, and the turnaround time is high. This has motivated the research community to develop CXR based automated COVID-19 diagnostic methodologies. However, COVID-19 being a novel disease, there is no annotated large-scale CXR dataset for this particular disease. To address the issue of limited data, we propose to exploit a large-scale CXR dataset collected in the pre-COVID era and train a deep neural network in a self-supervised fashion to extract CXR specific features. Further, we compute attention maps between the global and the local features of the backbone convolutional network while finetuning using a limited COVID-19 CXR dataset. We empirically demonstrate the effectiveness of the proposed method. We provide a thorough ablation study to understand the effect of each proposed component. Finally, we provide visualizations highlighting the critical patches instrumental to the predictive decision made by our model. These saliency maps are not only a stepping stone towards explainable AI but also aids radiologists in localizing the infected area.", "journal": "Applied soft computing", "date": "2022-05-03", "authors": ["Arnab KumarMondal"], "doi": "10.1016/j.asoc.2022.108867\n10.1145/3293353.3293408\n10.1016/j.asoc.2020.107052\n10.1109/JBHI.2021.3069798\n10.1016/j.asoc.2021.107323\n10.1016/j.cmpb.2020.105581\n10.1016/j.asoc.2020.106859\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.asoc.2020.106742\n10.1148/radiol.2020202944\n10.1109/JBHI.2020.3037127\n10.1016/j.asoc.2021.107160\n10.1016/j.asoc.2021.107645\n10.1109/JBHI.2021.3058293\n10.1109/JBHI.2021.3074893\n10.1016/j.patcog.2020.107613\n10.1016/j.asoc.2021.107330\n10.1016/j.asoc.2021.107522\n10.1016/j.chaos.2020.110122\n10.1109/CVPR.2018.00745\n10.1016/j.neucom.2020.07.144\n10.1007/s11263-015-0816-y"}
{"title": "Diagnosis of COVID-19 patients by adapting hyper parametertuned deep belief network using hosted cuckoo optimization algorithm.", "abstract": "COVID-19 is an infection caused by recently discovered corona virus. The symptoms of COVID-19 are fever, cough and dumpiness of breathing. A quick and accurate identification is essential for an efficient fight against COVID-19. A machine learning technique is initiated for categorizing the chest x-ray images into two cases: COVID-19 positive case or negative case. In this manuscript, the categorization of COVID-19 can be determined by hyper parameter tuned deep belief network using hosted cuckoo optimization algorithm. At first, the input chest x-ray images are pre-processed for removing noises. In this manuscript, the deep belief network method is enhanced by hosted cuckoo optimization approach for getting optimum hyper tuning parameters. By this, exact categorization of COVID-19 is attained effectively. The proposed methodology is stimulated at MATLAB. The proposed approach attains 28.3% and 23.5% higher accuracy for Normal and 32.3% and 31.5% higher accuracy for COVID-19, 19.3% and 28.5% higher precision for Normal and 45.3% and 28.5% higher precision for COVID-19, 20.3% and 21.5% higher F-score for Normal and 40.3% and 21.5% higher F-score for COVID-19. The proposed methodology is analyzed using two existing methodologies, as Convolutional Neural Network with Social Mimic Optimization (CNN-SMO) and Support Vector Machine classifier using Bayesian Optimization algorithm (SVM-BOA).", "journal": "Electromagnetic biology and medicine", "date": "2022-05-03", "authors": ["VeerrajuGampala", "KarunyaRathan", "Christalin NelsonS", "Francis HShajin", "PRajesh"], "doi": "10.1080/15368378.2022.2065679"}
{"title": "Does imbalance in chest X-ray datasets produce biased deep learning approaches for COVID-19 screening?", "abstract": "The health crisis resulting from the global COVID-19 pandemic highlighted more than ever the need for rapid, reliable and safe methods of diagnosis and monitoring of respiratory diseases. To study pulmonary involvement in detail, one of the most common resources is the use of different lung imaging modalities (like chest radiography) to explore the possible affected areas.\nThe study of patient characteristics like sex and age in pathologies of this type is crucial for gaining knowledge of the disease and for avoiding biases due to the clear scarcity of data when developing representative systems. In this work, we performed an analysis of these factors in chest X-ray images to identify biases. Specifically, 11 imbalance scenarios were defined with female and male COVID-19 patients present in different proportions for the sex analysis, and 6 scenarios where only one specific age range was used for training for the age factor. In each study, 3 different approaches for automatic COVID-19 screening were used: Normal vs COVID-19, Pneumonia vs COVID-19 and Non-COVID-19 vs COVID-19. The study was validated using two public chest X-ray datasets, allowing a reliable analysis to support the clinical decision-making process.\nThe results for the sex-related analysis indicate this factor slightly affects the system in the Normal VS COVID-19 and Pneumonia VS COVID-19 approaches, although the identified differences are not relevant enough to worsen considerably the system. Regarding the age-related analysis, this factor was observed to be influencing the system in a more consistent way than the sex factor, as it was present in all considered scenarios. However, this worsening does not represent a major factor, as it is not of great magnitude.\nMultiple studies have been conducted in other fields in order to determine if certain patient characteristics such as sex or age influenced these deep learning systems. However, to the best of our knowledge, this study has not been done for COVID-19 despite the urgency and lack of COVID-19 chest x-ray images. The presented results evidenced that the proposed methodology and tested approaches allow a robust and reliable analysis to support the clinical decision-making process in this pandemic scenario.", "journal": "BMC medical research methodology", "date": "2022-04-29", "authors": ["Lorena\u00c1lvarez-Rodr\u00edguez", "Joaquim deMoura", "JorgeNovo", "MarcosOrtega"], "doi": "10.1186/s12874-022-01578-w\n10.3389/fcvm.2021.638011\n10.1038/s41598-020-76550-z\n10.1109/TMI.2020.3040950\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.06.21.20136598\n10.3389/fmed.2020.00427\n10.1016/j.eswa.2020.114054\n10.1109/ACCESS.2020.2994762\n10.1016/j.eswa.2021.115681\n10.1109/ACCESS.2020.3033762\n10.1073/pnas.1919012117\n10.1016/j.eswa.2021.114677\n10.1016/j.patcog.2020.107613\n10.1016/j.media.2020.101794\n10.1109/ACCESS.2020.3044858\n10.1016/j.compbiomed.2021.104210"}
{"title": "External COVID-19 Deep Learning Model Validation on ACR AI-LAB: It's a Brave New World.", "abstract": "Deploying external artificial intelligence (AI) models locally can be logistically challenging. We aimed to use the ACR AI-LAB software platform for local testing of a chest radiograph (CXR) algorithm for COVID-19 lung disease severity assessment.\nAn externally developed deep learning model for COVID-19 radiographic lung disease severity assessment was loaded into the AI-LAB platform at an independent academic medical center, which was separate from the institution in which the model was trained. The data set consisted of CXR images from 141 patients with reverse transcription-polymerase chain reaction-confirmed COVID-19, which were routed to AI-LAB for model inference. The model calculated a Pulmonary X-ray Severity (PXS) score for each image. This score was correlated with the average of a radiologist-based assessment of severity, the modified Radiographic Assessment of Lung Edema score, independently interpreted by three radiologists. The associations between the PXS score and patient admission and intubation or death were assessed.\nThe PXS score deployed in AI-LAB correlated with the radiologist-determined modified Radiographic Assessment of Lung Edema score (r\u00a0= 0.80). PXS score was significantly higher in patients who were admitted (4.0 versus 1.3, P < .001) or intubated or died within 3 days (5.5 versus 3.3, P\u00a0= .001).\nAI-LAB was successfully used to test an external COVID-19 CXR AI algorithm on local data with relative ease, showing generalizability of the PXS score model. For AI models to scale and be clinically useful, software tools that facilitate the local testing process, like the freely available AI-LAB, will be important to cross the AI implementation gap in health care systems.", "journal": "Journal of the American College of Radiology : JACR", "date": "2022-04-29", "authors": ["AliArdestani", "Matthew DLi", "PauleyChea", "Jeremy RWortman", "AdamMedina", "JayashreeKalpathy-Cramer", "ChristophWald"], "doi": "10.1016/j.jacr.2022.03.013\n10.1007/s00330-020-07269-8\n10.1101/2020.09.15.20195453"}
{"title": "Explainability of radiomics through formal methods.", "abstract": "Artificial Intelligence has proven to be effective in radiomics. The main problem in using Artificial Intelligence is that researchers and practitioners are not able to know how the predictions are generated. This is currently an open issue because results' explainability is advantageous in understanding the reasoning behind the model, both for patients than for implementing a feedback mechanism for medical specialists using decision support systems.\nAddressing transparency issues related to the Artificial Intelligence field, the innovative technique of Formal methods use a mathematical logic reasoning to produce an automatic, quick and reliable diagnosis. In this paper we analyze results given by the adoption of Formal methods for the diagnosis of the Coronavirus disease: specifically, we want to analyse and understand, in a more medical way, the meaning of some radiomic features to connect them with clinical or radiological evidences.\nIn particular, the usage of Formal methods allows the authors to do statistical analysis on the feature value distributions, to do pattern recognition on disease models, to generalize the model of a disease and to reach high performances of results and interpretation of them. A further step for explainability can be accounted by the localization and selection of the most important slices in a multi-slice approach.\nIn conclusion, we confirmed the clinical significance of some First order features as Skewness and Kurtosis. On the other hand, we suggest to decline the use of the Minimum feature because of its intrinsic connection with the Computational Tomography exam of the lung.", "journal": "Computer methods and programs in biomedicine", "date": "2022-04-29", "authors": ["GiuliaVarriano", "PasqualeGuerriero", "AntonellaSantone", "FrancescoMercaldo", "LucaBrunese"], "doi": "10.1016/j.cmpb.2022.106824"}
{"title": "Structures of Omicron spike complexes and implications for neutralizing antibody development.", "abstract": "The emergence of the SARS-CoV-2 Omicron variant is dominant in many countries worldwide. The high number of spike mutations is responsible for the broad immune evasion from existing vaccines and antibody drugs. To understand this, we first present the cryo-electron microscopy structure of ACE2-bound SARS-CoV-2 Omicron spike. Comparison to previous spike antibody structures explains how Omicron escapes these therapeutics. Secondly, we report structures of Omicron, Delta, and wild-type spikes bound to a patient-derived Fab antibody fragment (510A5), which provides direct evidence where antibody binding is greatly attenuated by the Omicron mutations, freeing spike to bind ACE2. Together with biochemical binding and 510A5 neutralization assays, our work establishes principles of binding required for neutralization and clearly illustrates how the mutations lead to antibody evasion yet retain strong ACE2 interactions. Structural information on spike with both bound and unbound antibodies collectively elucidates potential strategies for generation of therapeutic antibodies.", "journal": "Cell reports", "date": "2022-04-28", "authors": ["HangtianGuo", "YanGao", "TinghanLi", "TingtingLi", "YuchiLu", "LeZheng", "YueLiu", "TingtingYang", "FeiyangLuo", "ShuyiSong", "WeiWang", "XiunaYang", "Henry CNguyen", "HongkaiZhang", "AilongHuang", "AishunJin", "HaitaoYang", "ZiheRao", "XiaoyunJi"], "doi": "10.1016/j.celrep.2022.110770\n10.1038/s41586-020-2852-1\n10.1126/science.abe2402\n10.1016/j.celrep.2022.110368\n10.1126/science.abc5902\n10.1038/d41586-021-03825-4\n10.1038/s41586-021-04386-2\n10.1038/d41586-021-03796-6\n10.1038/d41586-021-03846-z\n10.1016/j.ultramic.2013.06.004\n10.1101/2021.12.30.21268560\n10.1016/j.cell.2022.01.019\n10.1016/j.cell.2021.12.046\n10.1001/jama.2021.14811\n10.1038/s41467-021-25331-x\n10.1107/S0907444910007493\n10.1016/j.cell.2021.12.033\n10.1101/2021.12.20.21268130\n10.1016/j.chom.2020.11.007\n10.1038/s41586-022-04441-6\n10.1038/s41467-021-26401-w\n10.1038/s41579-021-00573-0\n10.1016/j.cell.2021.12.032\n10.1016/j.chom.2020.03.025\n10.1038/nprot.2015.053\n10.1038/s41467-020-20602-5\n10.1038/s41598-021-99827-3\n10.1038/s41467-021-26539-7\n10.1107/S2059798319011471\n10.1038/d41586-021-03826-3\n10.1038/s41586-020-2571-7\n10.1038/s41586-021-04245-0\n10.1126/science.abn7760\n10.1001/jama.2021.24868\n10.1016/j.jsb.2005.07.007\n10.1126/science.abn8652\n10.1101/2022.01.02.474743\n10.1101/2021.12.27.474250\n10.1101/2021.12.31.474653\n10.1002/jcc.20084\n10.1002/pro.3943\n10.2210/pdb7jxe/pdb\n10.1038/s41586-020-2349-y\n10.1038/d41586-021-03827-2\n10.1038/nmeth.4169\n10.1016/j.jmb.2003.07.013\n10.1016/j.jsb.2012.09.006\n10.46234/ccdcw2021.255\n10.1038/s41586-022-04442-5\n10.1038/s41586-021-03807-6\n10.1016/j.xcrm.2021.100255\n10.1038/s41586-022-04462-1\n10.3390/ijerph18157752\n10.1038/s41576-021-00408-x\n10.1126/science.abe3354\n10.1038/s41586-021-03817-4\n10.1038/d41586-021-03832-5\n10.1016/j.cell.2020.02.058\n10.1038/s41467-020-16256-y\n10.1016/j.immuni.2021.06.003\n10.1038/s41422-020-00460-y\n10.1038/s41586-021-03324-6\n10.1126/science.abc7424\n10.1016/j.chom.2020.11.012\n10.1016/j.celrep.2021.109822\n10.1038/s41586-020-2008-3\n10.1126/science.abb7269\n10.1038/nmeth.4193\n10.1038/s41586-021-03361-1\n10.1038/s41586-020-2012-7\n10.1038/s41586-020-2548-6"}
{"title": "6G and Artificial Intelligence Technologies for Dementia Care: Literature Review and Practical Analysis.", "abstract": "The dementia epidemic is progressing fast. As the world's older population keeps skyrocketing, the traditional incompetent, time-consuming, and laborious interventions are becoming increasingly insufficient to address dementia patients' health care needs. This is particularly true amid COVID-19. Instead, efficient, cost-effective, and technology-based strategies, such as sixth-generation communication solutions (6G) and artificial intelligence (AI)-empowered health solutions, might be the key to successfully managing the dementia epidemic until a cure becomes available. However, while 6G and AI technologies hold great promise, no research has examined how 6G and AI applications can effectively and efficiently address dementia patients' health care needs and improve their quality of life.\nThis study aims to investigate ways in which 6G and AI technologies could elevate dementia care to address this study gap.\nA literature review was conducted in databases such as PubMed, Scopus, and PsycINFO. The search focused on three themes: dementia, 6G, and AI technologies. The initial search was conducted on April 25, 2021, complemented by relevant articles identified via a follow-up search on November 11, 2021, and Google Scholar alerts.\nThe findings of the study were analyzed in terms of the interplay between people with dementia's unique health challenges and the promising capabilities of health technologies, with in-depth and comprehensive analyses of advanced technology-based solutions that could address key dementia care needs, ranging from impairments in memory (eg, Egocentric Live 4D Perception), speech (eg, Project Relate), motor (eg, Avatar Robot Caf\u00e9), cognitive (eg, Affectiva), to social interactions (eg, social robots).\nTo live is to grow old. Yet dementia is neither a proper way to live nor a natural aging process. By identifying advanced health solutions powered by 6G and AI opportunities, our study sheds light on the imperative of leveraging the potential of advanced technologies to elevate dementia patients' will to live, enrich their daily activities, and help them engage in societies across shapes and forms.", "journal": "Journal of medical Internet research", "date": "2022-04-28", "authors": ["ZhaohuiSu", "Barry LBentley", "DeanMcDonnell", "JunaidAhmad", "JiguangHe", "FengShi", "KazuakiTakeuchi", "AliCheshmehzangi", "Claudimar Pereirada Veiga"], "doi": "10.2196/30503\n10.1038/475s2a\n10.1038/d41586-021-01546-2\n10.1097/00002093-199603000-00011\n10.1038/s41574-018-0048-7\n10.1017/s1041610220001593\n10.1111/jgs.16215\n10.1111/jgs.12549\n10.1111/jocn.13909\n10.1007/s40273-019-00785-6\n10.1097/JCMA.0000000000000504\n10.1080/08946566.2020.1741054\n10.1159/000512238\n10.1002/gps.5020\n10.1017/s1041610218002296\n10.3233/jad-180275\n10.1016/j.jagp.2019.10.006\n10.3233/jad-180531\n10.1016/j.socscimed.2016.11.041\n10.1186/s12916-020-01841-1\n10.1186/s12916-020-01841-1\n10.1109/mic.2018.112102418\n10.2196/15122\n10.1037/0003-066x.32.7.513\n10.1159/000518194\n10.1177/1471301216659797\n10.5455/msm.2018.30.221-224\n10.1145/3371382.3380737\n10.1016/j.jamda.2015.05.002\n10.3233/jad-160703\n10.1111/jan.13076\n10.1016/j.jamda.2017.05.019\n10.1016/j.jamda.2017.03.018\n10.2147/cia.s152561\n10.1016/j.clinthera.2020.01.001\n10.1111/ggi.13890\n10.3389/fpsyg.2017.00950\n10.3389/fpsyg.2017.00950\n10.1109/jbhi.2017.2754861\n10.3389/fpsyg.2016.01066\n10.3389/fpsyg.2016.01066\n10.1109/ICMRE49073.2020.9065021\n10.1016/j.jamda.2020.05.036\n10.1109/mnet.001.1900287\n10.1016/j.dcan.2020.05.003\n10.14569/ijacsa.2020.0110281\n10.3390/sym12040676\n10.1038/s41928-019-0355-6\n10.1109/ojcoms.2020.3010270\n10.1109/mcom.2019.1900271\n10.1109/access.2020.3015289\n10.1109/mnet.001.1900652\n10.1016/j.comcom.2020.07.035\n10.1109/mvt.2019.2921162\n10.3390/s20205796\n10.1007/s00415-019-09216-0\n10.1016/j.jagp.2016.10.013\n10.1109/jproc.2012.2200559\n10.2196/25340\n10.1177/1471301219894141?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dpubmed\n10.1177/1471301219894141\n10.2196/13203\n10.1093/geront/gny046\n10.1080/08098131.2021.1963315\n10.1016/S1474-4422(18)30403-4\n10.1212/wnl.0000000000010022\n10.3233/jad-191092\n10.1080/13607863.2020.1787336\n10.1111/hsc.12937"}
{"title": "Mining intrinsic information of convalescent patients after suffering coronavirus disease 2019 in Wuhan.", "abstract": "To summarize the potential characteristics of convalescent patients with coronavirus disease 2019 (COVID-19) in China based on emerging clinical tongue data and guide the treatment and recovery of COVID-19 patients from the perspective of Traditional Chinese Medicine tongue diagnosis.\nIn this study, we developed and validated radiomics-based and lab-based methods as a novel approach to provide individualized pretreatment evaluation by analyzing different features to mine the orderliness behind tongue data of convalescent patients. In addition, this study analyzed the tongue features of convalescent patients from clinical tongue qualitative values, including thick and thin, fur, peeling, fat and lean, tooth marks and cracked, and greasy and putrid fur.\nWe included 2164 tongue images in total (34% from day 0, 35.4% from day 14 and 30.6% from day 28) from convalescent patients. The significance results are shown as follows. Firstly, as the recovery time prolongs, the L average values of tongue and coat decrease from 60.21 to 57.18 and from 60.06 to 57.03 respectively. Secondly, the decrease of abnormality rate of tongue coat, included greasy tongue fur, putrid fur, teeth-mark, thick-thin fur, are of significant statistical difference ( < 0.05). Thirdly, the average value of gray-level co-occurrence matrices increases from 0.173 to 0.194, the average value of entropy increases from 0.606 to 0.665, the average value of inverse difference normalized decrease from 0.981 to 0.979, and the average value of dissimilarity decrease from 0.1576 to 0.1828. The details of other radiomics features are describe in results section.\nOur experiment shows that patients in different recovery periods have a relationship with quantitative values of tongue images, including L color space of the tongue and coat radiomics features analysis. This relationship can help clinical doctors master the recovery and health of patients as soon as possible and improve their understanding of the potential mechanisms underlying the dynamic changes and mechanisms underlying COVID-19.", "journal": "Journal of traditional Chinese medicine = Chung i tsa chih ying wen pan", "date": "2022-04-28", "authors": ["YanShixing", "LiuZiqing", "RenMeng", "H EHaiyang", "XiaoLi", "GuoFeng", "PengMiao", "L IXiaoxia", "WangYong", "X UXi", "YangTao", "ShaoZuoyu", "HuangJingjing", "XiaoMingzhong"], "doi": "10.19852/j.cnki.jtcm.20220225.001"}
{"title": "Trends in inflammatory bowel disease infections and vaccinations in the past four decades: A high-level text mining analysis of PubMed publications.", "abstract": "We aimed at assessing the published literature on different prophylactic screening and vaccination options in inflammatory bowel disease (IBD) patients between 1980 and 2020. Special attention was attributed to latest data assessing covid-19 vaccinations.\nWe have queried PubMed for all available IBD-related entries published during 1980-2020. The following data were extracted for each entry: PubMed unique article ID (PMID), title, publishing journal, abstract text, keywords (if any), and authors' affiliations. Two gastrointestinal specialists decided by consensus on a list of terms to classify entries. The terms belonged to four treatment groups: opportunistic infections, prophylactic screening, prophylactic vaccinations/treatment, and routine vaccines. Annual trends of publications for the years 1980-2020 were plotted for different screening, vaccinations and infection types. Slopes of publication trends were calculated by fitting regression lines to the annual number of publications.\nOverall, 98,339 IBD entries were published between 1980 and 2020. Of those, 7773 entries belonged to the investigated groups. Entries concerning opportunistic infections showed the sharpest rise, with 19 entries and 1980 to 423 entries in 2020 (slope 11.3, p\u2009<\u2009.001). Entries concerning prophylactic screening rose from 10 entries in 1980 to 204 entries in 2020 (slope 5.4, p\u2009<\u2009.001). Both entries concerning prophylactic vaccinations/treatments and routine vaccines did not show a significant rise (slope 0.33 and slope 0.92, respectively). During the COVID 19 pandemic, a total of 44 publications were identified. Of them, 37 were relevant to vaccines and immune reaction. Nineteen publications (51%) were guidelines/recommendations, and 14 (38%) assessed immune reaction to vaccination, most of them (11, 61%) to mRNA vaccines.\nDuring the past two decades, along with a rapid increase in biologic therapy, publications regarding opportunistic infections and prophylactic screening increased in a steep slope compared to the two decades in the pre-biologic area. During the COVID-19 pandemic, most publications included vaccination recommendations and guidelines and only 38% included real-world data assessing reaction to vaccinations. More research is needed.", "journal": "Human vaccines & immunotherapeutics", "date": "2022-04-27", "authors": ["EyalKlang", "ShellySoffer", "EyalShachar", "AdiLahat"], "doi": "10.1080/21645515.2022.2065814\n10.1097/00004836-199201000-00005\n10.1097/01.MIB.0000159661.55028.56\n10.1111/j.1365-2036.2004.1873.x\n10.1097/00042737-200105000-00017\n10.1111/j.1365-2036.2006.02753.x\n10.3748/wjg.v18.i29.3790\n10.1097/01.mog.0000231808.10773.8e\n10.1146/annurev-immunol-030409-101225\n10.1155/2019/7247238\n10.1002/ueg2.12138\n10.1080/17474124.2018.1530983\n10.1007/s11192-012-0900-9\n10.1016/j.crohns.2013.12.013\n10.1080/17843286.1999.11754245\n10.1053/j.gastro.2020.12.031\n10.1016/j.cgh.2020.02.009\n10.1053/j.gastro.2018.04.012\n10.1007/s11894-019-0705-6\n10.1056/NEJMoa011110\n10.1136/gutjnl-2011-301668\n10.1136/gutjnl-2019-318484\n10.1136/gut.2010.221127\n10.1128/microbiolspec.MCHD-0022-2015"}
{"title": "Deep learning representations to support COVID-19 diagnosis on CT slices.", "abstract": "The coronavirus disease 2019 (COVID-19) has become a significant public health problem worldwide. In this context, CT-scan automatic analysis has emerged as a COVID-19 complementary diagnosis tool allowing for radiological finding characterization, patient categorization, and disease follow-up. However, this analysis depends on the radiologist's expertise, which may result in subjective evaluations.\nTo explore deep learning representations, trained from thoracic CT-slices, to automatically distinguish COVID-19 disease from control samples.\nTwo datasets were used: SARS-CoV-2 CT Scan (Set-1) and FOSCAL clinic's dataset (Set-2). The deep representations took advantage of supervised learning models previously trained on the natural image domain, which were adjusted following a transfer learning scheme. The deep classification was carried out: (a) via an end-to-end deep learning approach and (b) via random forest and support vector machine classifiers by feeding the deep representation embedding vectors into these classifiers.\nThe end-to-end classification achieved an average accuracy of 92.33% (89.70% precision) for Set-1 and 96.99% (96.62% precision) for Set-2. The deep feature embedding with a support vector machine achieved an average accuracy of 91.40% (95.77% precision) and 96.00% (94.74% precision) for Set-1 and Set-2, respectively.\nDeep representations have achieved outstanding performance in the identification of COVID-19 cases on CT scans demonstrating good characterization of the COVID-19 radiological patterns. These representations could potentially support the COVID-19 diagnosis in clinical settings.\nIntroducci\u00f3n. La enfermedad por coronavirus (COVID-19) es actualmente el principal problema de salud p\u00fablica en el mundo. En este contexto, el an\u00e1lisis autom\u00e1tico de tomograf\u00edas computarizadas (TC) surge como una herramienta diagn\u00f3stica complementaria que permite caracterizar hallazgos radiol\u00f3gicos, y categorizar y hacer el seguimiento de pacientes con COVID-19. Sin embargo, este an\u00e1lisis depende de la experiencia de los radi\u00f3logos, por lo que las valoraciones pueden ser subjetivas. Objetivo. Explorar representaciones de aprendizaje profundo entrenadas con cortes de TC tor\u00e1cica para diferenciar autom\u00e1ticamente entre los casos de COVID-19 y personas no infectadas. Materiales y m\u00e9todos. Se usaron dos conjuntos de datos de TC: de SARS-CoV-2 CT (conjunto 1) y de la cl\u00ednica FOSCAL (conjunto 2). Los modelos de aprendizaje supervisados y previamente entrenados en im\u00e1genes naturales, se ajustaron usando aprendizaje por transferencia. La clasificaci\u00f3n se llev\u00f3 a cabo mediante aprendizaje de extremo a extremo y clasificadores tales como los \u00e1rboles de decisiones y las m\u00e1quinas de soporte vectorial, alimentados por la representaci\u00f3n profunda previamente aprendida. Resultados. El enfoque de extremo a extremo alcanz\u00f3 una exactitud promedio de 92,33 % (89,70 % de precisi\u00f3n) para el conjunto 1 y de 96,99 % (96,62 % de precisi\u00f3n) para el conjunto-2. La m\u00e1quina de soporte vectorial alcanz\u00f3 una exactitud promedio de 91,40 % (precisi\u00f3n del 95,77 %) para el conjunto-1 y del 96,00 % (precisi\u00f3n del 94,74 %) para el conjunto 2. Conclusi\u00f3n. Las representaciones profundas lograron resultados sobresalientes al caracterizar patrones radiol\u00f3gicos usados en la detecci\u00f3n de casos de COVID-19 a partir de estudios de TC y demostraron ser una potencial herramienta de apoyo del diagn\u00f3stico.\nLa enfermedad por coronavirus (COVID-19) es actualmente el principal problema de salud p\u00fablica en el mundo. En este contexto, el an\u00e1lisis autom\u00e1tico de tomograf\u00edas computarizadas (TC) surge como una herramienta diagn\u00f3stica complementaria que permite caracterizar hallazgos radiol\u00f3gicos, y categorizar y hacer el seguimiento de pacientes con COVID-19. Sin embargo, este an\u00e1lisis depende de la experiencia de los radi\u00f3logos, por lo que las valoraciones pueden ser subjetivas.\nExplorar representaciones de aprendizaje profundo entrenadas con cortes de TC tor\u00e1cica para diferenciar autom\u00e1ticamente entre los casos de COVID-19 y personas no infectadas.\nSe usaron dos conjuntos de datos de TC: de SARS-CoV-2 CT (conjunto 1) y de la cl\u00ednica FOSCAL (conjunto 2). Los modelos de aprendizaje supervisados y previamente entrenados en im\u00e1genes naturales, se ajustaron usando aprendizaje por transferencia. La clasificaci\u00f3n se llev\u00f3 a cabo mediante aprendizaje de extremo a extremo y clasificadores tales como los \u00e1rboles de decisiones y las m\u00e1quinas de soporte vectorial, alimentados por la representaci\u00f3n profunda previamente aprendida.\nEl enfoque de extremo a extremo alcanz\u00f3 una exactitud promedio de 92,33 % (89,70 % de precisi\u00f3n) para el conjunto 1 y de 96,99 % (96,62 % de precisi\u00f3n) para el conjunto-2. La m\u00e1quina de soporte vectorial alcanz\u00f3 una exactitud promedio de 91,40 % (precisi\u00f3n del 95,77 %) para el conjunto-1 y del 96,00 % (precisi\u00f3n del 94,74 %) para el conjunto 2.\nLas representaciones profundas lograron resultados sobresalientes al caracterizar patrones radiol\u00f3gicos usados en la detecci\u00f3n de casos de COVID-19 a partir de estudios de TC y demostraron ser una potencial herramienta de apoyo del diagn\u00f3stico.", "journal": "Biomedica : revista del Instituto Nacional de Salud", "date": "2022-04-27", "authors": ["Josu\u00e9Ruano", "JohnArcila", "DavidRomo-Bucheli", "CarlosVargas", "JeffersonRodr\u00edguez", "\u00d3scarMendoza", "MiguelPlazas", "LolaBautista", "JorgeVillamizar", "GabrielPedraza", "AlejandraMoreno", "DianaValenzuela", "LinaV\u00e1zquez", "CarolinaValenzuela-Santos", "PaulCamacho", "DanielMantilla", "FabioMart\u00ednez Carrillo"], "doi": "10.7705/biomedica.5927\n10.1093/ajcp/aqaa029\n10.1016/j.cca.2020.03.009\n10.1001/jama.2020.12839\n10.1148/radiol.2020200905\n10.1038/s41562-020-0931-9\n10.1001/jama.2020.3786\n10.7326/M20-1495\n10.1371/journal.pone.0251661\n10.1038/s41598-020-68862-x\n10.1148/ryct.2020200110\n10.1177/0846537120913033\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30728-5\n10.1148/radiol.2020200823\n10.1002/jmv.25855\n10.1007/s00247-019-04593-0\n10.1016/j.cell.2018.02.010\n10.1016/j.imu.2020.100427\n10.7717/peerj-cs.306\n10.1109/CVPR.2009.5206848\n10.1101/2020.04.24.20078584\n10.1148/radiol.2462070712\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1148/radiol.2020202504\n10.1109/CVPR.2016.90\n10.1109/CVPR.2016.308\n10.1007/978-3-030-01424-7_27\n10.1109/GlobalSIP.2017.8309150\n10.1038/s41598-020-74164-z\n10.1109/SCORED.2019.8896277\n10.1023/A:1010933404324\n10.1023/A:1010933404324\n10.3390/s19235219\n10.48550/arXiv.1507.06020"}
{"title": "Artificial intelligence at the time of COVID-19: who does the lion's share?", "abstract": "The development and use of artificial intelligence (AI) methodologies, especially machine learning (ML) and deep learning (DL), have been considerably fostered during the ongoing coronavirus disease 2019 (COVID-19) pandemic. Several models and algorithms have been developed and applied for both identifying COVID-19 cases and for assessing and predicting the risk of developing unfavourable outcomes. Our aim was to summarize how AI is being currently applied to COVID-19.\nWe conducted a PubMed search using as query MeSH major terms \"Artificial Intelligence\" AND \"COVID-19\", searching for articles published until December 31, 2021, which explored the possible role of AI in COVID-19. The dataset origin (internal dataset or public datasets available online) and data used for training and testing the proposed ML/DL model(s) were retrieved.\nOur analysis finally identified 292 articles in PubMed. These studies displayed large heterogeneity in terms of imaging test, laboratory parameters and clinical-demographic data included. Most models were based on imaging data, in particular CT scans or chest X-rays images. C-Reactive protein, leukocyte count, creatinine, lactate dehydrogenase, lymphocytes and platelets counts were found to be the laboratory biomarkers most frequently included in COVID-19 related AI models.\nThe lion's share of AI applied to COVID-19 seems to be played by diagnostic imaging. However, AI in laboratory medicine is also gaining momentum, especially with digital tools characterized by low cost and widespread applicability.", "journal": "Clinical chemistry and laboratory medicine", "date": "2022-04-27", "authors": ["DavideNegrini", "ElisaDanese", "Brandon MHenry", "GiuseppeLippi", "MartinaMontagnana"], "doi": "10.1515/cclm-2022-0306"}
{"title": "COVID-opt-aiNet: A clinical decision support system for COVID-19 detection.", "abstract": "Coronavirus disease (COVID-19) has had a major and sometimes lethal effect on global public health. COVID-19 detection is a difficult task that necessitates the use of intelligent diagnosis algorithms. Numerous studies have suggested the use of artificial intelligence (AI) and machine learning (ML) techniques to detect COVID-19 infection in patients through chest X-ray image analysis. The use of medical imaging with different modalities for COVID-19 detection has become an important means of containing the spread of this disease. However, medical images are not sufficiently adequate for routine clinical use; there is, therefore, an increasing need for AI to be applied to improve the diagnostic performance of medical image analysis. Regrettably, due to the evolving nature of the COVID-19 global epidemic, the systematic collection of a large data set for deep neural network (DNN)/ML training is problematic. Inspired by these studies, and to aid in the medical diagnosis and control of this contagious disease, we suggest a novel approach that ensembles the feature selection capability of the optimized artificial immune networks (opt-aiNet) algorithm with deep learning (DL) and ML techniques for better prediction of the disease. In this article, we experimented with a DNN, a convolutional neural network (CNN), bidirectional long-short-term memory, a support vector machine (SVM), and logistic regression for the effective detection of COVID-19 in patients. We illustrate the effectiveness of this proposed technique by using COVID-19 image datasets with a variety of modalities. An empirical study using the COVID-19 image dataset demonstrates that the proposed hybrid approaches, named COVID-opt-aiNet, improve classification accuracy by up to 98%-99% for SVM, 96%-97% for DNN, and 70.85%-71% for CNN, to name a few examples. Furthermore, statistical analysis ensures the validity of our proposed algorithms. The source code can be downloaded from Github: https://github.com/faizakhan1925/COVID-opt-aiNet.", "journal": "International journal of imaging systems and technology", "date": "2022-04-26", "authors": ["SummrinaKanwal", "FaizaKhan", "SultanAlamri", "KiaDashtipur", "MandarGogate"], "doi": "10.1002/ima.22695"}
{"title": "Can laboratory parameters be an alternative to CT and RT-PCR in the diagnosis of COVID-19? A machine learning approach.", "abstract": "In this study, a machine learning-based decision support system that uses routine laboratory parameters has been proposed in order to increase the diagnostic success in COVID-19. The main goal of the proposed method was to reduce the number of misdiagnoses in the RT-PCR and CT scans and to reduce the cost of testing. In this study, we retrospectively reviewed the files of patients who presented to the coronavirus outpatient. The demographic, thoracic CT, and laboratory data of the individuals without any symptoms of the disease, who had negative RT-PCR test and who had positive RT-PCR test were analyzed. CT images were classified using hybrid CNN methods to show the superiority of the decision support system using laboratory parameters. Detection of COVID-19 from CT images achieved an accuracy of 97.56% with the AlexNet-SVM hybrid method, while COVID-19 was classified with an accuracy of 97.86% with the proposed method using laboratory parameters.", "journal": "International journal of imaging systems and technology", "date": "2022-04-26", "authors": ["MehmetKalayc\u0131", "HakanAyy\u0131ld\u0131z", "Seda ArslanTuncer", "Pinar GundoganBozdag", "Gulden EserKarlidag"], "doi": "10.1002/ima.22705"}
{"title": "Multimodal covid network: Multimodal bespoke convolutional neural network architectures for COVID-19 detection from chest X-ray's and computerized tomography scans.", "abstract": "AI-based tools were developed in the existing works, which focused on one type of image data; either CXR's or computerized tomography (CT) scans for COVID-19 prediction. There is a need for an AI-based tool that predicts COVID-19 detection from chest images such as Chest X-ray (CXR) and CT scans given as inputs. This research gap is considered the core objective of the proposed work. In the proposed work, multimodal CNN architectures were developed based on the parameters and hyperparameters of neural networks. Nine experiments evaluate optimizers, learning rates, and the number of epochs. Based on the experimental results, suitable parameters are fixed for multimodal architecture development for COVID-19 detection. We have constructed a bespoke convolutional neural network (CNN) architecture named multimodal covid network (MMCOVID-NET) by varying the number of layers from two to seven, which can predict covid or normal images from both CXR's and CT scans. In the proposed work, we have experimented by constructing 24 models for COVID-19 prediction. Among them, four models named\u00a0MMCOVID-NET-I, MMCOVID-NET-II, MMCOVID-NET-III, and MMCOVID-NET-IV performed well by producing an accuracy of 100%. We obtained these results from a small dataset. So we repeated these experiments in a larger dataset. We inferred that MMCOVID-NET-III outperformed all the state-of-the-art methods by producing an accuracy of 99.75%. The experiments carried out in this work conclude that the parameters and hyperparameters play a vital role in increasing or decreasing the model's performance.", "journal": "International journal of imaging systems and technology", "date": "2022-04-26", "authors": ["ThiyagarajanPadmapriya", "ThiruvenkatamKalaiselvi", "VenugopalPriyadharshini"], "doi": "10.1002/ima.22712"}
{"title": "Automatic classification of severity of COVID-19 patients using texture feature and random forest based on computed tomography images.", "abstract": "Severity assessment of the novel Coronavirus (COVID-19) using chest computed tomography (CT) scan is crucial for the effective administration of the right therapeutic drugs and also for monitoring the progression of the disease. However, determining the severity of COVID-19 needs a highly expert radiologist by visual assessment, which is time-consuming, boring, and subjective. This article introduces an advanced machine learning tool to determine the severity of COVID-19 to mild, moderate, and severe from the lung CT images. We have used a set of quantitative first- and second-order statistical texture features from each image. The first-order texture features extracted from the image histogram are variance, skewness, and kurtosis. The second-order texture features extraction methods are gray-level co-occurrence matrix, gray-level run length matrix, and gray-level size zone matrix. Finally, using the extracted features, CT images of each person are classified using random forest (RF) as an ensemble method based on majority voting of the decision trees outputs to four classes. We have used a dataset of CT scans labeled as being normal (231), mild (563), moderate (120), and severe (42) determined by expert radiologists. The experimental results indicate the combination of all feature extraction methods, and RF achieves the highest result compared with the other strategies in detecting the four classes of severity of COVID-19 from CT images with an accuracy of 90.95%. This proposed system can work well and can be used as an assistant diagnostic tool for quantification of lung involvement of COVID-19 to monitor the progression of the disease.", "journal": "International journal of imaging systems and technology", "date": "2022-04-26", "authors": ["NasrinAmini", "AhmadShalbaf"], "doi": "10.1002/ima.22679"}
{"title": "Leveraging deep learning for COVID-19 diagnosis through chest imaging.", "abstract": "COVID-19 has taken a toll on the entire world, rendering serious illness and high mortality rate. In the present day, when the globe is hit by a pandemic, those suspected to be infected by the virus need to confirm its presence to seek immediate medical attention to avoid adverse outcomes and also to prevent further transmission of the virus in their close contacts by ensuring timely isolation. The most reliable laboratory testing currently available is the reverse transcription-polymerase chain reaction (RT-PCR) test. Although the test is considered gold standard, 20-25% of results can still be false negatives, which has lately led physicians to recommend medical imaging in specific cases. Our research examines the aspect of chest imaging as a method to diagnose COVID-19. This work is not directed to establish an alternative to RT-PCR, but to aid physicians in determining the presence of virus in medical images. As the disease presents lung involvement, it provides a basis to explore computer vision for classification in radiographic images. In this paper, authors compare the performance of various models, namely ResNet-50, EfficientNetB0, VGG-16 and a custom convolutional neural network (CNN) for detecting the presence of virus in chest computed tomography (CT) scan and chest X-ray images. The most promising results have been derived by using ResNet-50 on CT scans with an accuracy of 98.9% and ResNet-50 on X-rays with an accuracy of 98.7%, which offer an opportunity to further explore these methods for prospective use.", "journal": "Neural computing & applications", "date": "2022-04-26", "authors": ["YashikaKhurana", "UmangSoni"], "doi": "10.1007/s00521-022-07250-0\n10.1038/s41579-020-00459-7\n10.1371/journal.pone.0249090\n10.1371/journal.pone.0242958\n10.1148/radiol.2020200642\n10.1148/radiol.2020200463\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1148/radiol.2020203173\n10.1038/s41467-020-20657-4\n10.1371/journal.pone.0250952\n10.1183/13993003.00775-2020\n10.1007/s10489-020-01902-1\n10.3389/fmed.2020.00427\n10.1007/s00521-020-05410-8"}
{"title": "Author Correction: Federated deep learning for detecting COVID-19 lung abnormalities in CT: a privacy-preserving multinational validation study.", "abstract": null, "journal": "NPJ digital medicine", "date": "2022-04-26", "authors": ["QiDou", "Tiffany YSo", "MeiruiJiang", "QuandeLiu", "VarutVardhanabhuti", "GeorgiosKaissis", "ZejuLi", "WeixinSi", "Heather H CLee", "KevinYu", "ZuxinFeng", "LiDong", "EgonBurian", "FriederikeJungmann", "RickmerBraren", "MarcusMakowski", "BernhardKainz", "DanielRueckert", "BenGlocker", "Simon C HYu", "Pheng AnnHeng"], "doi": "10.1038/s41746-022-00600-1"}
{"title": "Non-targeted metallomics through synchrotron radiation X-ray fluorescence with machine learning for cancer screening using blood samples.", "abstract": "Cancer is the leading cause of death in many countries. The development of new methods for early screening of cancers is highly desired. Targeted metallomics has been successfully applied in the screening of cancers through quantification of elements in the matrix, which is time consuming and requires combined techniques for the quantification due to the large elemental difference in the matrix. This work proposed a non-targeted metallomics (NTM) approach through synchrotron radiation based X-ray fluorescence (SRXRF) and machine learning algorithms (MLAs) for the screening of cancers. One hundred serum samples were collected from cancer patients who were confirmed by pathological examination with 100 matched serum samples from healthy volunteers. The serum samples were studied with SRXRF and the spectra from both groups were directly clarified through MLAs, which did not require the quantification of elements. The NTM approach through SRXRF and MLAs is fast (5s for data collection for one sample) and accurate (over 96% accuracy) for cancer screening. Besides, this approach can also identify the most affected elements in cancer samples like Ca, Zn and Ti as we found, which may shed lights on the drug development for cancer treatment. This NTM approach can also be applied through commercially available XRF instruments or ICP-TOF-MS with MLAs. It has the potential for the screening and prediction of other diseases like COVID-19 and neurodegenerative diseases in a high throughput and least invasive way.", "journal": "Talanta", "date": "2022-04-25", "authors": ["LinaHe", "YaoLu", "ChaoLi", "HongxinXie", "JiatingZhao", "YatingWang", "LimingWang", "XinWang", "WeiWang", "DongliangChen", "YuxiGao", "BaiLi", "Yu-FengLi"], "doi": "10.1016/j.talanta.2022.123486"}
{"title": "QUCoughScope: An Intelligent Application to Detect COVID-19 Patients Using Cough and Breath Sounds.", "abstract": "Problem-Since the outbreak of the COVID-19 pandemic, mass testing has become essential to reduce the spread of the virus. Several recent studies suggest that a significant number of COVID-19 patients display no physical symptoms whatsoever. Therefore, it is unlikely that these patients will undergo COVID-19 testing, which increases their chances of unintentionally spreading the virus. Currently, the primary diagnostic tool to detect COVID-19 is a reverse-transcription polymerase chain reaction (RT-PCR) test from the respiratory specimens of the suspected patient, which is invasive and a resource-dependent technique. It is evident from recent researches that asymptomatic COVID-19 patients cough and breathe in a different way than healthy people. Aim-This paper aims to use a novel machine learning approach to detect COVID-19 (symptomatic and asymptomatic) patients from the convenience of their homes so that they do not overburden the healthcare system and also do not spread the virus unknowingly by continuously monitoring themselves. Method-A Cambridge University research group shared such a dataset of cough and breath sound samples from 582 healthy and 141 COVID-19 patients. Among the COVID-19 patients, 87 were asymptomatic while 54 were symptomatic (had a dry or wet cough). In addition to the available dataset, the proposed work deployed a real-time deep learning-based backend server with a web application to crowdsource cough and breath datasets and also screen for COVID-19 infection from the comfort of the user's home. The collected dataset includes data from 245 healthy individuals and 78 asymptomatic and 18 symptomatic COVID-19 patients. Users can simply use the application from any web browser without installation and enter their symptoms, record audio clips of their cough and breath sounds, and upload the data anonymously. Two different pipelines for screening were developed based on the symptoms reported by the users: asymptomatic and symptomatic. An innovative and novel stacking CNN model was developed using three base learners from of eight state-of-the-art deep learning CNN algorithms. The stacking CNN model is based on a logistic regression classifier meta-learner that uses the spectrograms generated from the breath and cough sounds of symptomatic and asymptomatic patients as input using the combined (Cambridge and collected) dataset. Results-The stacking model outperformed the other eight CNN networks with the best classification performance for binary classification using cough sound spectrogram images. The accuracy, sensitivity, and specificity for symptomatic and asymptomatic patients were 96.5%, 96.42%, and 95.47% and 98.85%, 97.01%, and 99.6%, respectively. For breath sound spectrogram images, the metrics for binary classification of symptomatic and asymptomatic patients were 91.03%, 88.9%, and 91.5% and 80.01%, 72.04%, and 82.67%, respectively. Conclusion-The web-application QUCoughScope records coughing and breathing sounds, converts them to a spectrogram, and applies the best-performing machine learning model to classify the COVID-19 patients and healthy subjects. The result is then reported back to the test user in the application interface. Therefore, this novel system can be used by patients in their premises as a pre-screening method to aid COVID-19 diagnosis by prioritizing the patients for RT-PCR testing and thereby reducing the risk of spreading of the disease.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-04-24", "authors": ["TawsifurRahman", "NabilIbtehaz", "AmithKhandakar", "Md Sakib AbrarHossain", "Yosra Magdi SalihMekki", "MaymounaEzeddin", "Enamul HaqueBhuiyan", "Mohamed ArseleneAyari", "AnasTahir", "YazanQiblawey", "SakibMahmud", "Susu MZughaier", "TariqAbbas", "SomayaAl-Maadeed", "Muhammad E HChowdhury"], "doi": "10.3390/diagnostics12040920\n10.1002/rmv.2112\n10.1016/S2665-9913(20)30212-5\n10.1016/j.dsx.2020.06.060\n10.1136/bmj.n1315\n10.1016/j.dsx.2020.06.067\n10.1016/j.jcv.2020.104455\n10.3390/jcm10163493\n10.1016/j.cmi.2020.11.004\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1007/s12559-021-09955-1\n10.1016/j.compbiomed.2021.105002\n10.3390/diagnostics11050893\n10.1101/2020.04.13.20063941\n10.1007/s13755-021-00169-1\n10.1016/j.rinp.2021.105045\n10.1111/exsy.12759\n10.32604/cmc.2021.012955\n10.1109/JIOT.2021.3050775\n10.1088/1361-6579/ac1d59\n10.3390/s19122781\n10.1016/S0020-7373(86)80012-8\n10.1007/s10044-020-00921-5\n10.3390/s17010171\n10.1109/JSEN.2016.2585039\n10.1007/s00702-017-1676-0\n10.1371/journal.pone.0182428\n10.1016/j.mayocp.2017.12.025\n10.1007/s10115-019-01337-2\n10.1038/tp.2016.123\n10.1101/2020.04.07.20051060\n10.1016/j.imu.2020.100378\n10.1109/ACCESS.2020.3018028\n10.1007/s42979-020-00422-6\n10.1016/j.compbiomed.2021.104765\n10.1016/j.compbiomed.2021.104572\n10.1109/OJEMB.2020.3026928\n10.3389/fmed.2021.585578\n10.1145/3421725\n10.1136/bmjinnov-2021-000668\n10.1038/s41597-021-00937-4\n10.1109/ACCESS.2020.3031384\n10.3390/app10093233\n10.1007/s12559-020-09812-7\n10.1016/j.compbiomed.2021.104838\n10.1016/j.compbiomed.2021.104944\n10.1016/j.bea.2022.100025"}
{"title": "A Literature Review on the Use of Artificial Intelligence for the Diagnosis of COVID-19 on CT and Chest X-ray.", "abstract": "A COVID-19 diagnosis is primarily determined by RT-PCR or rapid lateral-flow testing, although chest imaging has been shown to detect manifestations of the virus. This article reviews the role of imaging (CT and X-ray), in the diagnosis of COVID-19, focusing on the published studies that have applied artificial intelligence with the purpose of detecting COVID-19 or reaching a differential diagnosis between various respiratory infections. In this study, ArXiv, MedRxiv, PubMed, and Google Scholar were searched for studies using the criteria terms 'deep learning', 'artificial intelligence', 'medical imaging', 'COVID-19' and 'SARS-CoV-2'. The identified studies were assessed using a modified version of the Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis (TRIPOD). Twenty studies fulfilled the inclusion criteria for this review. Out of those selected, 11 papers evaluated the use of artificial intelligence (AI) for chest X-ray and 12 for CT. The size of datasets ranged from 239 to 19,250 images, with sensitivities, specificities and AUCs ranging from 0.789-1.00, 0.843-1.00 and 0.850-1.00. While AI demonstrates excellent diagnostic potential, broader application of this method is hindered by the lack of relevant comparators in studies, sufficiently sized datasets, and independent testing.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-04-24", "authors": ["CiaraMulrenan", "KawalRhode", "Barbara MaleneFischer"], "doi": "10.3390/diagnostics12040869\n10.1038/s41579-018-0118-9\n10.1016/S0140-6736(03)14630-2\n10.1016/S0140-6736(20)30211-7\n10.7326/M20-1495\n10.1136/bmj.m4469\n10.1145/3466690\n10.1007/s42979-021-00690-w\n10.1016/j.imu.2020.100405\n10.1186/s41747-018-0061-6\n10.1016/j.neunet.2014.09.003\n10.1186/s12916-014-0241-z\n10.1136/bmj.m689\n10.1053/j.semnuclmed.2020.09.001\n10.12788/fp.0045\n10.1007/s00330-021-08050-1\n10.1101/2020.08.31.20175828\n10.1155/2021/8828404\n10.7717/peerj.10309\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.05.16.20103408\n10.1101/2020.06.08.20125963\n10.1101/2020.08.14.20170290\n10.1101/2020.03.12.20027185\n10.1038/s41598-021-83424-5\n10.3390/diagnostics11010041\n10.2196/19569\n10.1038/s41467-020-17971-2\n10.1038/s41591-020-0931-3\n10.1038/s41467-020-18685-1\n10.1016/S2589-7500(20)30186-2\n10.1038/s42256-021-00307-0"}
{"title": "COVID-19 pneumonia chest radiographic severity score: variability assessment among experienced and in-training radiologists and creation of a multireader composite score database for artificial intelligence algorithm development.", "abstract": "The purpose was to evaluate reader variability between experienced and in-training radiologists of COVID-19 pneumonia severity on chest radiograph (CXR), and to create a multireader database suitable for AI development.\nIn this study, CXRs from polymerase chain reaction positive COVID-19 patients were reviewed. Six experienced cardiothoracic radiologists and two residents classified each CXR according to severity. One radiologist performed the classification twice to assess intraobserver variability. Severity classification was assessed using a 4-class system: normal (0), mild (1), moderate (2), and severe (3). A median severity score (Rad Med) for each CXR was determined for the six radiologists for development of a multireader database (XCOMS). Kendal Tau correlation and percentage of disagreement were calculated to assess variability.\nA total of 397 patients (1208 CXRs) were included (mean age, 60 years SD \u00b1 1), 189 men). Interobserver variability between the radiologists ranges between 0.67 and 0.78. Compared to the Rad Med score, the radiologists show good correlation between 0.79-0.88. Residents show slightly lower interobserver agreement of 0.66 with each other and between 0.69 and 0.71 with experienced radiologists. Intraobserver agreement was high with a correlation coefficient of 0.77. In 220 (18%), 707 (59%), 259 (21%) and 22 (2%) CXRs there was a 0, 1, 2 or 3 class-difference. In 594 (50%) CXRs the median scores of the residents and the radiologists were similar, in 578 (48%) and 36 (3%) CXRs there was a 1 and 2 class-difference.\nExperienced and in-training radiologists demonstrate good inter- and intraobserver agreement in COVID-19 pneumonia severity classification. A higher percentage of disagreement was observed in moderate cases, which may affect training of AI algorithms.\nMost AI algorithms are trained on data labeled by a single expert. This study shows that for COVID-19 X-ray severity classification there is significant variability and disagreement between radiologist and between residents.", "journal": "The British journal of radiology", "date": "2022-04-23", "authors": ["Marlyvan Assen", "MohammadrezaZandehshahvar", "HosseinMaleki", "YasharKiarashi", "TimothyArleo", "Arthur EStillman", "PeterFilev", "Amir HDavarpanah", "Eugene ABerkowitz", "StefanTigges", "Scott JLee", "Brianna LVey", "AliAdibi", "Carlo NDe Cecco"], "doi": "10.1259/bjr.20211028"}
{"title": "Assessing clinical applicability of COVID-19 detection in chest radiography with deep learning.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has impacted healthcare systems across the world. Chest radiography (CXR) can be used as a complementary method for diagnosing/following COVID-19 patients. However, experience level and workload of technicians and radiologists may affect the decision process. Recent studies suggest that deep learning can be used to assess CXRs, providing an important second opinion for radiologists and technicians in the decision process, and super-human performance in detection of COVID-19 has been reported in multiple studies. In this study, the clinical applicability of deep learning systems for COVID-19 screening was assessed by testing the performance of deep learning systems for the detection of COVID-19. Specifically, four datasets were used: (1) a collection of multiple public datasets (284.793 CXRs); (2) BIMCV dataset (16.631 CXRs); (3) COVIDGR (852 CXRs) and 4) a private dataset (6.361 CXRs). All datasets were collected retrospectively and consist of only frontal CXR views. A ResNet-18 was trained on each of the datasets for the detection of COVID-19. It is shown that a high dataset bias was present, leading to high performance in intradataset train-test scenarios (area under the curve 0.55-0.84 on the collection of public datasets). Significantly lower performances were obtained in interdataset train-test scenarios however (area under the curve > 0.98). A subset of the data was then assessed by radiologists for comparison to the automatic systems. Finetuning with radiologist annotations significantly increased performance across datasets (area under the curve 0.61-0.88) and improved the attention on clinical findings in positive COVID-19 CXRs. Nevertheless, tests on CXRs from different hospital services indicate that the screening performance of CXR and automatic systems is limited (area under the curve < 0.6 on emergency service CXRs). However, COVID-19 manifestations can be accurately detected when present, motivating the use of these tools for evaluating disease progression on mild to severe COVID-19 patients.", "journal": "Scientific reports", "date": "2022-04-23", "authors": ["Jo\u00e3oPedrosa", "GuilhermeAresta", "CarlosFerreira", "CatarinaCarvalho", "JoanaSilva", "PedroSousa", "LucasRibeiro", "Ana MariaMendon\u00e7a", "Aur\u00e9lioCampilho"], "doi": "10.1038/s41598-022-10568-3\n10.1001/jama.2020.12458\n10.1186/s12879-021-06528-3\n10.1016/j.tmaid.2020.101623\n10.1148/radiol.2020201365\n10.1016/j.radi.2014.02.007\n10.1109/RBME.2020.2987975\n10.1038/s41598-019-56847-4\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1148/ryct.2020200337\n10.1038/s42256-021-00338-7\n10.1016/j.media.2020.101797\n10.11613/BM.2012.031\n10.1007/BF02289261\n10.1109/TMI.2020.3006437\n10.1111/j.0006-341X.2000.01134.x\n10.1080/01621459.1961.10482090"}
{"title": "MS-ResNet: disease-specific survival prediction using longitudinal CT images and clinical data.", "abstract": "Medical imaging data of lung cancer in different stages contain a large amount of time information related to its evolution (emergence, development, or extinction). We try to explore the evolution process of lung images in time dimension to improve the prediction of lung cancer survival by using longitudinal CT images and clinical data jointly.\nIn this paper, we propose an innovative multi-branch spatiotemporal residual network (MS-ResNet) for disease-specific survival (DSS) prediction by integrating the longitudinal computed tomography (CT) images at different times and clinical data. Specifically, we first extract the deep features from the multi-period CT images by an improved residual network. Then, the feature selection algorithm is used to select the most relevant feature subset from the clinical data. Finally, we integrate the deep features and feature subsets to take full advantage of the complementarity between the two types of data to generate the final prediction results.\nThe experimental results demonstrate that our MS-ResNet model is superior to other methods, achieving a promising 86.78% accuracy in the classification of short-survivor, med-survivor, and long-survivor.\nIn computer-aided prognostic analysis of cancer, the time dimension features of the course of disease and the integration of patient clinical data and CT data can effectively improve the prediction accuracy.", "journal": "International journal of computer assisted radiology and surgery", "date": "2022-04-22", "authors": ["JiahaoHan", "NingXiao", "WantingYang", "ShichaoLuo", "JunZhao", "YanQiang", "SumanChaudhary", "JuanjuanZhao"], "doi": "10.1007/s11548-022-02625-z\n10.1016/j.compbiomed.2021.104294\n10.2174/0929867327666200204141952\n10.1371/journal.pone.0123694\n10.1109/TNB.2019.2936398\n10.1016/j.ijmedinf.2020.104371\n10.1007/s10151-019-01997-w\n10.7150/jca.43268\n10.1016/j.compbiomed.2020.104037\n10.1109/JTEHM.2019.2955458\n10.1049/iet-ipr.2020.0496\n10.21037/jtd.2017.12.123\n10.3389/fnins.2019.00966\n10.3389/fnins.2019.00810\n10.3390/cancers11081140\n10.3390/s20092649\n10.1016/j.inffus.2020.05.005\n10.1007/s11227-020-03367-y\n10.1016/j.ebiom.2018.12.028\n10.1158/1078-0432.CCR-17-2236\n10.1056/NEJMoa1208962\n10.1016/j.compbiomed.2021.104348\n10.1088/1361-6560/ab6e51"}
{"title": "Think positive: An interpretable neural network for image recognition.", "abstract": "The COVID-19 pandemic is an ongoing pandemic and is placing additional burden on healthcare systems around the world. Timely and effectively detecting the virus can help to reduce the spread of the disease. Although, RT-PCR is still a gold standard for COVID-19 testing, deep learning models to identify the virus from medical images can also be helpful in certain circumstances. In particular, in situations when patients undergo routine X-rays and/or CT-scans tests but within a few days of such tests they develop respiratory complications. Deep learning models can also be used for pre-screening prior to RT-PCR testing. However, the transparency/interpretability of the reasoning process of predictions made by such deep learning models is essential. In this paper, we propose an interpretable deep learning model that uses positive reasoning process to make predictions. We trained and tested our model over the dataset of chest CT-scan images of COVID-19 patients, normal people and pneumonia patients. Our model gives the accuracy, precision, recall and F-score equal to 99.48%, 0.99, 0.99 and 0.99, respectively.", "journal": "Neural networks : the official journal of the International Neural Network Society", "date": "2022-04-20", "authors": ["GurmailSingh"], "doi": "10.1016/j.neunet.2022.03.034\n10.1007/s00500-020-05424-3\n10.32604/cmc.2021.012955\n10.1016/j.scs.2020.102589\n10.1109/HEALTHCOM49281.2021.9398980\n10.1155/2020/8828855\n10.1155/2020/8889023\n10.7759/cureus.9448\n10.1007/s00500-020-05275-y\n10.1007/s11063-019-10043-7\n10.1109/CVPR.2014.81\n10.3389/fmed.2020.608525\n10.3389/fmed.2020.608525\n10.1109/CVPR.2016.90\n10.1007/978-3-642-35289-8_32\n10.1109/CVPR.2017.243\n10.1007/s10489-020-01902-1\n10.1016/j.bbe.2020.08.008\n10.1101/2020.04.13.20063461\n10.1145/1553374.1553453\n10.1109/ICCKE.2017.8167877\n10.1016/j.compbiomed.2020.103792\n10.1007/s00357-003-0003-7\n10.1371/journal.pone.0242301\n10.1109/ic-ETITE47903.2020.235\n10.1109/ICCV.2015.136\n10.1109/ACCESS.2021.3087583\n10.3390/diagnostics11091732\n10.1109/ACCESS.2021.3064838\n10.1007/s11263-013-0620-5\n10.1109/TPAMI.2020.2975798\n10.1109/TCSVT.2021.3067449\n10.1145/3404374\n10.1145/3472810\n10.1145/3468872\n10.1007/s10489-020-01867-1\n10.1007/978-3-319-10590-1_53\n10.1007/978-3-319-10590-1_54\n10.1109/ICCV.2017.557\n10.1109/CVPR.2016.319"}
{"title": "Diagnosis of Lumbar Spondylolisthesis Using Optimized Pretrained CNN Models.", "abstract": "Spondylolisthesis refers to the slippage of one vertebral body over the adjacent one. It is a chronic condition that requires early detection to prevent unpleasant surgery. The paper presents an optimized deep learning model for detecting spondylolisthesis in X-ray radiographs. The dataset contains a total of 299 X-ray radiographs from which 156 images are showing the spine with spondylolisthesis and 143 images are of the normal spine. Image augmentation technique is used to increase the data samples. In this study, VGG16 and InceptionV3 models were used for the image classification task. The developed model is optimized by utilizing the TFLite model optimization technique. The experimental result shows that the VGG16 model has achieved a 98% accuracy rate, which is higher than InceptionV3's 96% accuracy rate. The size of the implemented model is reduced up to four times so it can be used on small devices. The compressed VGG16 and InceptionV3 models have achieved 100% and 96% accuracy rate, respectively. Our finding shows that the implemented models were outperformed in the diagnosis of lumbar spondylolisthesis as compared to the model suggested by Varcin et al. (which had a maximum of 93% accuracy rate). Also, the developed quantized model has achieved higher accuracy rate than Zebin and Rezvy's (VGG16\u2009+\u2009TFLite) model with 90% accuracy. Furthermore, by evaluating the model's performance on other publicly available datasets, we have generalised our approach on the public platform.", "journal": "Computational intelligence and neuroscience", "date": "2022-04-19", "authors": ["DeepikaSaravagi", "ShwetaAgrawal", "ManishaSaravagi", "Jyotir MoyChatterjee", "MohitAgarwal"], "doi": "10.1155/2022/7459260\n10.1002/jsp2.1044\n10.5435/00124635-200607000-00004\n10.1155/2014/182956\n10.1007/978-3-030-40850-3_11\n10.1038/s41591-018-0307-0\n10.1038/s41568-018-0016-5\n10.1504/IJESMS.2021.115534\n10.12928/TELKOMNIKA.v18i3.14753\n10.1016/j.ejrad.2019.02.038\n10.1016/j.imu.2020.100391\n10.1016/j.cmpb.2016.10.007\n10.1109/CVPR.2016.90\n10.14419/ijet.v7i2.7.10930\n10.1109/TKDE.2009.191\n10.1016/j.tice.2019.02.001\n10.1109/IDAP.2019.8875988\n10.1109/EHB50910.2020.9280227\n10.17632/rscbjbr9sj.2\n10.1007/s42600-021-00163-2\n10.1007/S10489-020-01867-1/FIGURES/8\n10.1109/ICESC51422.2021.9532711\n10.1109/ICIMTech.2019.8843844\n10.1109/CCWC.2018.8301729\n10.1038/s41598-020-59108-x\n10.1007/s42979-020-0114-9\n10.1016/j.imu.2020.100505\n10.1109/EMBC.2018.8512750\n10.1109/CIBEC.2018.8641815\n10.1016/j.neuroimage.2016.08.055\n10.1109/SMARTCOMP50058.2020.00027\n10.1177/2192568218770769\n10.1016/j.measurement.2015.09.013\n10.1016/j.patrec.2020.07.042\n10.1504/IJESMS.2021.115532"}
{"title": "MA-Net:Mutex attention network for COVID-19 diagnosis on CT images.", "abstract": "COVID-19 is an infectious pneumonia caused by 2019-nCoV. The number of newly confirmed cases and confirmed deaths continues to remain at a high level. RT-PCR is the gold standard for the COVID-19 diagnosis, but the computed tomography (CT) imaging technique is an important auxiliary diagnostic tool. In this paper, a deep learning network mutex attention network (MA-Net) is proposed for COVID-19 auxiliary diagnosis on CT images. Using positive and negative samples as mutex inputs, the proposed network combines mutex attention block (MAB) and fusion attention block (FAB) for the diagnosis of COVID-19. MAB uses the distance between mutex inputs as a weight to make features more distinguishable for preferable diagnostic results. FAB acts to fuse features to obtain more representative features. Particularly, an adaptive weight multiloss function is proposed for better effect. The accuracy, specificity and sensitivity were reported to be as high as 98.17%, 97.25% and 98.79% on the COVID-19 dataset-A provided by the Affiliated Medical College of Qingdao University, respectively. State-of-the-art results have also been achieved on three other public COVID-19 datasets. The results show that compared with other methods, the proposed network can provide effective auxiliary information for the diagnosis of COVID-19 on CT images.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2022-04-19", "authors": ["BingBingZheng", "YuZhu", "QinShi", "DaweiYang", "YanmeiShao", "TaoXu"], "doi": "10.1007/s10489-022-03431-5\n10.1016/S0140-6736(20)30795-9\n10.15585/mmwr.mm7003e2\n10.1080/14737159.2020.1757437\n10.1007/s00330-020-06801-0\n10.1016/j.ejrad.2020.108961\n10.1038/s41591-020-0931-3\n10.1016/j.neucom.2020.09.068\n10.1038/s41467-020-17280-8\n10.1155/2020/9756518\n10.1007/s10489-020-01770-9\n10.1016/j.neucom.2021.03.122\n10.1109/TMI.2020.3000314\n10.1016/j.media.2020.101794\n10.1007/s10489-020-01826-w\n10.1109/ACCESS.2020.3001973\n10.1007/s10489-020-01829-7\n10.1109/JBHI.2020.3030853\n10.1016/j.media.2020.101836\n10.1016/j.ijleo.2021.167100\n10.1148/radiol.2020200905\n10.1007/s10489-020-02122-3\n10.1109/JBHI.2020.3023246\n10.1109/TIP.2021.3109518"}
{"title": "CapsNet-COVID19: Lung CT image classification method based on CapsNet model.", "abstract": "The outbreak of the Corona Virus Disease 2019 (COVID-19) has posed a serious threat to human health and life around the world. As the number of COVID-19 cases continues to increase, many countries are facing problems such as errors in nucleic acid testing (RT-PCR), shortage of testing reagents, and lack of testing personnel. In order to solve such problems, it is necessary to propose a more accurate and efficient method as a supplement to the detection and diagnosis of COVID-19. This research uses a deep network model to classify some of the COVID-19, general pneumonia, and normal lung CT images in the 2019 Novel Coronavirus Information Database. The first level of the model uses convolutional neural networks to locate lung regions in lung CT images. The second level of the model uses the capsule network to classify and predict the segmented images. The accuracy of our method is 84.291% on the test set and 100% on the training set. Experiment shows that our classification method is suitable for medical image classification with complex background, low recognition rate, blurred boundaries and large image noise. We believe that this classification method is of great value for monitoring and controlling the growth of patients in COVID-19 infected areas.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2022-04-19", "authors": ["XiaoQingZhang", "GuangYuWang", "Shu-GuangZhao"], "doi": "10.3934/mbe.2022236"}
{"title": "COV-DLS: Prediction of COVID-19 from X-Rays Using Enhanced Deep Transfer Learning Techniques.", "abstract": "In this paper, modifications in neoteric architectures such as VGG16, VGG19, ResNet50, and InceptionV3 are proposed for the classification of COVID-19 using chest X-rays. The proposed architectures termed \"COV-DLS\" consist of two phases: heading model construction and classification. The heading model construction phase utilizes four modified deep learning architectures, namely Modified-VGG16, Modified-VGG19, Modified-ResNet50, and Modified-InceptionV3. An attempt is made to modify these neoteric architectures by incorporating the average pooling and dense layers. The dropout layer is also added to prevent the overfitting problem. Two dense layers with different activation functions are also added. Thereafter, the output of these modified models is applied during the classification phase, when COV-DLS are applied on a COVID-19 chest X-ray image data set. Classification accuracy of 98.61% is achieved by Modified-VGG16, 97.22% by Modified-VGG19, 95.13% by Modified-ResNet50, and 99.31% by Modified-InceptionV3. COV-DLS outperforms existing deep learning models in terms of accuracy and F1-score.", "journal": "Journal of healthcare engineering", "date": "2022-04-16", "authors": ["VijayKumar", "AnisZarrad", "RahulGupta", "OmarCheikhrouhou"], "doi": "10.1155/2022/6216273\n10.3390/su132413642\n10.7717/peerj-cs.655\n10.1007/s10044-021-00984-y\n10.33889/ijmems.2020.5.4.052\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1007/s10096-020-03901-z\n10.1016/j.cmpb.2020.105581\n10.3390/jimaging7050081\n10.1016/j.mehy.2020.109761\n10.1016/b978-0-12-824536-1.00003-4\n10.1016/j.patcog.2020.107613\n10.1016/j.patcog.2020.107747\n10.1016/j.patcog.2021.108341\n10.1016/j.asoc.2021.107947\n10.1109/access.2021.3120717\n10.1155/2021/6621607\n10.1109/tkde.2009.191\n10.1162/neco_a_00990\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.09.010"}
{"title": "Evaluation of AI-Based Segmentation Tools for COVID-19 Lung Lesions on Conventional and Ultra-low Dose CT Scans.", "abstract": "A reliable diagnosis and accurate monitoring are pivotal steps for treatment and prevention of COVID-19. Chest computed tomography (CT) has been considered a crucial diagnostic imaging technique for the injury assessment of the viral pneumonia. Furthermore, the automatization of the segmentation methods for lung alterations helps to speed up the diagnosis and lighten radiologists' workload. Considering the assiduous pathology monitoring, ultra-low dose (ULD) chest CT protocols have been implemented to drastically reduce the radiation burden. Unfortunately, the available AI technologies have not been trained on ULD-CT data and validated and their applicability deserves careful evaluation. Therefore, this work aims to compare the results of available AI tools (BCUnet, CORADS AI, NVIDIA CLARA Train SDK and CT Pneumonia Analysis) on a dataset of 73 CT examinations acquired both with conventional dose (CD) and ULD protocols. COVID-19 volume percentage, resulting from each tool, was statistically compared. This study demonstrated high comparability of the results on CD-CT and ULD-CT data among the four AI tools, with high correlation between the results obtained on both protocols (R > .68, P < .001, for all AI tools).", "journal": "Dose-response : a publication of International Hormesis Society", "date": "2022-04-16", "authors": ["MarcoAiello", "DarioBaldi", "GiuseppinaEsposito", "MarikaValentino", "MarcoRandon", "MarcoSalvatore", "CarloCavaliere"], "doi": "10.1177/15593258221082896\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020200230\n10.2214/AJR.20.23078\n10.1148/radiol.2020201102\n10.1148/radiol.2020201365\n10.2214/ajr.176.2.1760289\n10.1007/s003300050062\n10.1007/s00247-002-0678-7\n10.2214/ajr.177.2.1770289\n10.1148/radiology.213.1.r99oc29289\n10.1007/s003300050114\n10.2214/ajr.164.3.7863879\n10.1148/radiology.175.3.2343122\n10.1148/radiology.209.1.9769838\n10.1148/radiology.210.3.r99mr05645\n10.1177/1559325820973131\n10.1371/journal.pone.0168979\n10.1148/ryct.2020200196\n10.1038/s41746-021-00438-z\n10.1016/j.zemedi.2018.11.002\n10.1007/s11548-021-02317-0\n10.1016/j.cell.2018.02.010\n10.3390/app8101715\n10.1148/radiol.2020200905\n10.1038/s41467-020-17971-2\n10.1007/s00330-017-4800-5\n10.21203/rs.3.rs-571332/v1\n10.1148/radiol.2020202439\n10.1148/ryai.2020200048\n10.2307/2987937\n10.1109/ISBI.2019.8759468\n10.1097/MD.0000000000026034\n10.21037/qims-20-1176\n10.1148/rg.2021200196\n10.3390/app11062456\n10.1093/rpd/ncy212\n10.3348/kjr.2020.0237\n10.1186/s41747-021-00210-8\n10.1007/s10278-017-9988-z\n10.1016/j.ejrad.2019.01.028\n10.1159/000503996\n10.1038/s41597-020-00715-8"}
{"title": "Using artificial intelligence to improve the diagnostic efficiency of pulmonologists in differentiating COVID-19 pneumonia from community-acquired pneumonia.", "abstract": "Coronavirus disease 2019 (COVID-19) has quickly turned into a global health problem. Computed tomography (CT) findings of COVID-19 pneumonia and community-acquired pneumonia (CAP) may be similar. Artificial intelligence (AI) is a popular topic among medical imaging techniques and has caused significant developments in diagnostic techniques. This retrospective study aims to analyze the contribution of AI to the diagnostic performance of pulmonologists in distinguishing COVID-19 pneumonia from CAP using CT scans. A deep learning-based AI model was created to be utilized in the detection of COVID-19, which extracted visual data from volumetric CT scans. The final data set covered a total of 2496 scans (887 patients), which included 1428 (57.2%) from the COVID-19 group and 1068 (42.8%) from the CAP group. CT slices were classified into training, validation, and test datasets in an 8:1:1. The independent test data set was analyzed by comparing the performance of four pulmonologists in differentiating COVID-19 pneumonia both with and without the help of the AI. The accuracy, sensitivity, and specificity values of the proposed AI model for determining COVID-19 in the independent test data set were 93.2%, 85.8%, and 99.3%, respectively, with the area under the receiver operating characteristic curve of 0.984. With the assistance of the AI, the pulmonologists accomplished a higher mean accuracy (88.9% vs. 79.9%, p\u2009<\u20090.001), sensitivity (79.1% vs. 70%, p\u2009<\u20090.001), and specificity (96.5% vs. 87.5%, p\u2009<\u20090.001). AI support significantly increases the diagnostic efficiency of pulmonologists in the diagnosis of COVID-19 via CT. Studies in the future should focus on real-time applications of AI to fight the COVID-19 infection.", "journal": "Journal of medical virology", "date": "2022-04-15", "authors": ["Erdal\u0130n", "Ay\u015feg\u00fcl AGe\u00e7kil", "G\u00fcrkanKavuran", "Mahmut\u015eahin", "Nurcan KBerber", "MutluKulu\u00f6zt\u00fcrk"], "doi": "10.1002/jmv.27777\n10.1101/2020.03.20.20039834\n10.1101/2020.02.14.20023028"}
{"title": "Deep learning of chest X-rays can predict mechanical ventilation outcome in ICU-admitted COVID-19 patients.", "abstract": "The COVID-19 pandemic repeatedly overwhelms healthcare systems capacity and forced the development and implementation of triage guidelines in ICU for scarce resources (e.g. mechanical ventilation). These guidelines were often based on known risk factors for COVID-19. It is proposed that image data, specifically bedside computed X-ray (CXR), provide additional predictive information on mortality following mechanical ventilation that can be incorporated in the guidelines. Deep transfer learning was used to extract convolutional features from a systematically collected, multi-institutional dataset of COVID-19 ICU patients. A model predicting outcome of mechanical ventilation (remission or mortality) was trained on the extracted features and compared to a model based on known, aggregated risk factors. The model reached a 0.702 area under the curve (95% CI 0.707-0.694) at predicting mechanical ventilation outcome from pre-intubation CXRs, higher than the risk factor model. Combining imaging data and risk factors increased model performance to 0.743 AUC (95% CI 0.746-0.732). Additionally, a post-hoc analysis showed an increase performance on high-quality than low-quality CXRs, suggesting that using only high-quality images would result in an even stronger model.", "journal": "Scientific reports", "date": "2022-04-15", "authors": ["DanielGourdeau", "OlivierPotvin", "Jason HenryBiem", "FlorenceCloutier", "LynaAbrougui", "PatrickArchambault", "CarlChartrand-Lefebvre", "LouisDieumegarde", "ChristianGagn\u00e9", "LouisGagnon", "RaphaelleGigu\u00e8re", "AlexandreHains", "HuyLe", "SimonLemieux", "Marie-H\u00e9l\u00e8neL\u00e9vesque", "SimonNepveu", "LorneRosenbloom", "AnTang", "IssacYang", "NathalieDuchesne", "SimonDuchesne"], "doi": "10.1038/s41598-022-10136-9\n10.1001/jama.2020.4031\n10.1111/bioe.12836\n10.1016/j.media.2020.101860\n10.1016/S1473-3099(20)30134-1\n10.1001/jama.2020.1585\n10.2214/AJR.20.22976\n10.1016/j.crad.2020.03.003\n10.1016/j.ijid.2020.05.021\n10.1007/s11547-020-01200-3\n10.1038/s41597-019-0322-0\n10.1038/s42256-021-00307-0\n10.1038/s41598-022-09356-w\n10.1371/journal.pone.0236621\n10.1373/clinchem.2015.246280\n10.1038/s41586-020-2521-4\n10.1186/s12931-019-1261-1\n10.1038/s41746-021-00453-0"}
{"title": "Detection of COVID-19 from CT and Chest X-ray Images Using Deep Learning Models.", "abstract": "Coronavirus 2019 (COVID-19) is a highly transmissible and pathogenic virus caused by severe respiratory syndrome coronavirus 2 (SARS-CoV-2), which first appeared in Wuhan, China, and has since spread in the whole world. This pathology has caused a major health crisis in the world. However, the early detection of this anomaly is a key task to minimize their spread. Artificial intelligence is one of the approaches commonly used by researchers to discover the problems it causes and provide solutions. These estimates would help enable health systems to take the necessary steps to diagnose and track cases of COVID. In this review, we intend to offer a novel method of automatic detection of COVID-19 using tomographic images (CT) and radiographic images (Chest X-ray). In order to improve the performance of the detection system for this outbreak, we used two deep learning models: the VGG and ResNet. The results of the experiments show that our proposed models achieved the best accuracy of 99.35 and 96.77% respectively for VGG19 and ResNet50 with all the chest X-ray images.", "journal": "Annals of biomedical engineering", "date": "2022-04-14", "authors": ["WassimZouch", "DhouhaSagga", "AmiraEchtioui", "RafikKhemakhem", "MohamedGhorbel", "ChokriMhiri", "Ahmed BenHamida"], "doi": "10.1007/s10439-022-02958-5\n10.1007/s10439-020-02636-4\n10.1007/s10439-020-02648-0\n10.1007/s10439-020-02676-w\n10.1007/s10439-02002580-3\n10.1007/s13246-020-00865-4\n10.1016/j.envres.2020.109819\n10.1016/j.crad.2018.12.015\n10.1007/s10439-020-02636-4\n10.1148/radiol.2020200905\n10.1007/s10439-020-02599-6\n10.1016/j.compbiomed.2020.103792\n10.1007/s10439-018-02190-0\n10.3390/app10103641\n10.1148/radiol.2020200642\n10.1016/j.eng.2020.04.010"}
{"title": "Automated detection of COVID-19 cases from chest X-ray images using deep neural network and XGBoost.", "abstract": "In late 2019 and after the COVID-19 pandemic in the world, many researchers and scholars tried to provide methods for detecting COVID-19 cases. Accordingly, this study focused on identifying patients with COVID-19 from chest X-ray images.\nIn this paper, a method for diagnosing coronavirus disease from X-ray images was developed. In this method, DenseNet169 Deep Neural Network (DNN) was used to extract the features of X-ray images taken from the patients' chests. The extracted features were then given as input to the Extreme Gradient Boosting (XGBoost) algorithm to perform the classification task.\nEvaluation of the proposed approach and its comparison with the methods presented in recent years revealed that this method was more accurate and faster than the existing ones and had an acceptable performance for detecting COVID-19 cases from X-ray images. The experiments showed 98.23% and 89.70% accuracy, 99.78% and 100% specificity, 92.08% and 95.20% sensitivity in two and three-class problems, respectively.\nThis study aimed to detect people with COVID-19, focusing on non-clinical approaches. The developed method could be employed as an initial detection tool to assist the radiologists in more accurate and faster diagnosing the disease.\nThe proposed method's simple implementation, along with its acceptable accuracy, allows it to be used in COVID-19 diagnosis. Moreover, the gradient-based class activation mapping (Grad-CAM) can be used to represent the deep neural network's decision area on a heatmap. Radiologists might use this heatmap to evaluate the chest area more accurately.", "journal": "Radiography (London, England : 1995)", "date": "2022-04-13", "authors": ["HNasiri", "SHasani"], "doi": "10.1016/j.radi.2022.03.011"}
{"title": "Artificial Intelligence for COVID-19 Detection in Medical Imaging-Diagnostic Measures and Wasting-A Systematic Umbrella Review.", "abstract": "The COVID-19 pandemic has sparked a barrage of primary research and reviews. We investigated the publishing process, time and resource wasting, and assessed the methodological quality of the reviews on artificial intelligence techniques to diagnose COVID-19 in medical images. We searched nine databases from inception until 1 September 2020. Two independent reviewers did all steps of identification, extraction, and methodological credibility assessment of records. Out of 725 records, 22 reviews analysing 165 primary studies met the inclusion criteria. This review covers 174,277 participants in total, including 19,170 diagnosed with COVID-19. The methodological credibility of all eligible studies was rated as ", "journal": "Journal of clinical medicine", "date": "2022-04-13", "authors": ["Pawe\u0142Jemio\u0142o", "DawidStorman", "PatrykOrzechowski"], "doi": "10.3390/jcm11072054\n10.3390/jcm10061260\n10.1038/s41598-021-82384-0\n10.15585/mmwr.mm6913e1\n10.1056/NEJMoa2002032\n10.1101/2021.01.27.21249817\n10.1136/bmj.m3026\n10.1186/s13613-020-00650-2\n10.1002/14651858.CD013639.pub4\n10.1001/jama.2020.3786\n10.1148/ryct.2020200280\n10.1136/bmj.m2426\n10.1007/s00330-020-07347-x\n10.1016/S2213-2600(20)30132-6\n10.1148/radiol.2020200642\n10.1016/j.ultrasmedbio.2020.05.012\n10.1098/rsif.2017.0387\n10.1038/s41568-018-0016-5\n10.1016/j.acra.2018.02.018\n10.1136/gutjnl-2018-317500\n10.1016/S2589-7500(19)30123-2\n10.1016/j.ophtha.2019.12.015\n10.1613/jair.1.12162\n10.1016/j.jclinepi.2021.02.021\n10.1038/d41586-020-00694-1\n10.1126/science.abc1731\n10.1186/s12916-021-01920-x\n10.1136/bmj.m2197\n10.1111/eci.13222\n10.1056/NEJMp058068\n10.1038/s41564-020-0695-z\n10.1186/s13643-016-0384-4\n10.1016/j.ijantimicag.2020.105948\n10.1136/bmj.j4008\n10.1016/j.jclinepi.2019.05.028\n10.1001/jama.2017.19163\n10.1186/2046-4053-3-58\n10.1136/bmj.n160\n10.1109/RBME.2020.2987975\n10.1109/RBME.2020.2990959\n10.5152/dir.2019.20294\n10.1016/j.dsx.2020.05.008\n10.1016/j.bios.2020.112349\n10.1016/j.diii.2020.06.001\n10.1016/j.matpr.2020.06.245\n10.1007/s00138-020-01101-5\n10.1067/j.cpradiol.2020.06.009\n10.1136/bmj.m1328\n10.1145/3465398\n10.1109/ACCESS.2020.3009328\n10.1007/s10489-020-01770-9\n10.1109/ACCESS.2021.3058537\n10.1109/ACCESS.2020.3027685\n10.1038/s41467-021-21220-5\n10.1002/jmv.25901\n10.1007/s11845-021-02710-3\n10.1136/bmj.m689\n10.1111/obr.12994\n10.1016/j.jclinepi.2019.10.005\n10.1016/j.jclinepi.2020.09.046\n10.1016/j.jksuci.2021.07.010\n10.1093/humrep/dey056\n10.1016/j.jmpt.2006.07.001\n10.1136/jme.29.2.109\n10.1007/s00146-020-00978-0\n10.1038/nbt.3780\n10.1016/j.patter.2021.100269\n10.3390/ijerph18094749\n10.2196/23811\n10.1016/S2589-7500(20)30274-0\n10.1016/S2589-7500(20)30295-8\n10.1148/ryai.2020200029\n10.1007/s10916-020-01582-x\n10.1038/s41598-020-70479-z\n10.1109/access.2020.2992341\n10.3390/ijerph17093176\n10.1259/bjr.20200538\n10.1007/s10916-020-01617-3\n10.1016/j.ibmed.2020.100005\n10.1016/j.procbio.2020.08.016"}
{"title": "Reduced Chest Computed Tomography Scan Length for Patients Positive for Coronavirus Disease 2019: Dose Reduction and Impact on Diagnostic Utility.", "abstract": "This study used the Personalized Rapid Estimation of Dose in CT (PREDICT) tool to estimate patient-specific organ doses from CT image data. The PREDICT is a research tool that combines a linear Boltzmann transport equation solver for radiation dose map generation with deep learning algorithms for organ contouring. Computed tomography images from 74 subjects in the Medical Imaging Data Resource Center-RSNA International COVID-19 Open Radiology Database data set (chest CT of adult patients positive for COVID-19), which included expert annotations including \"infectious opacities,\" were analyzed. First, the full z-scan length of the CT image data set was evaluated. Next, the z-scan length was reduced from the left hemidiaphragm to the top of the aortic arch. Generic dose reduction based on dose length product (DLP) and patient-specific organ dose reductions were calculated. The percentage of infectious opacities excluded from the reduced z-scan length was used to quantify the effect on diagnostic utility.\nGeneric dose reduction, based on DLP, was 69%. The organ dose reduction ranged from approximately equal to 18% (breasts) to approximately equal to 64% (bone surface and bone marrow). On average, 12.4% of the infectious opacities were not included in the reduced z-coverage, per patient, of which 5.1% were above the top of the arch and 7.5% below the left hemidiaphragm.\nLimiting z-scan length of chest CTs reduced radiation dose without significantly compromising diagnostic utility in COVID-19 patients. The PREDICT demonstrated that patient-specific organ dose reductions varied from generic dose reduction based on DLP.", "journal": "Journal of computer assisted tomography", "date": "2022-04-12", "authors": ["SaraPrincipi", "StacyO'Connor", "LubaFrank", "Taly GilatSchmidt"], "doi": "10.1097/RCT.0000000000001312\n10.1016/j.chest.2020.04.003\n10.1183/16000617.0076-2018\n10.1007/s00330-020-07034-x\n10.1148/radiol.2020203453\n10.1542/peds.2007-1910\n10.1001/jama.298.3.317\n10.1111/j.1526-4610.2008.01071.x\n10.1007/s10140-015-1340-7\n10.7937/VTW4-X588\n10.1148/radiol.2021203957\n10.1007/s10278-013-9622-7\n10.1118/1.4824918\n10.1118/1.4933197\n10.1002/mp.13305\n10.1002/mp.14494\n10.1002/mp.15485\n10.1002/mp.15301\n10.1097/RCT.0b013e318198cd18\n10.1007/s11547-020-01237-4\n10.1148/ryct.2020209004\n10.17226/11340\n10.7937/91ah-v663\n10.1002/mp.13141\n10.7937/K9/TCIA.2017.3r3fvz08\n10.1002/acm2.12505\n10.1118/1.3298015"}
{"title": "Deep Learning-Based Automatic CT Quantification of Coronavirus Disease 2019 Pneumonia: An International Collaborative Study.", "abstract": "We aimed to develop and validate the automatic quantification of coronavirus disease 2019 (COVID-19) pneumonia on computed tomography (CT) images.\nThis retrospective study included 176 chest CT scans of 131 COVID-19 patients from 14 Korean and Chinese institutions from January 23 to March 15, 2020. Two experienced radiologists semiautomatically drew pneumonia masks on CT images to develop the 2D U-Net for segmenting pneumonia. External validation was performed using Japanese (n = 101), Italian (n = 99), Radiopaedia (n = 9), and Chinese data sets (n = 10). The primary measures for the system's performance were correlation coefficients for extent (%) and weight (g) of pneumonia in comparison with visual CT scores or human-derived segmentation. Multivariable logistic regression analyses were performed to evaluate the association of the extent and weight with symptoms in the Japanese data set and composite outcome (respiratory failure and death) in the Spanish data set (n = 115).\nIn the internal test data set, the intraclass correlation coefficients between U-Net outputs and references for the extent and weight were 0.990 and 0.993. In the Japanese data set, the Pearson correlation coefficients between U-Net outputs and visual CT scores were 0.908 and 0.899. In the other external data sets, intraclass correlation coefficients were between 0.949-0.965 (extent) and between 0.978-0.993 (weight). Extent and weight in the top quartile were independently associated with symptoms (odds ratio, 5.523 and 10.561; P = 0.041 and 0.016) and the composite outcome (odds ratio, 9.365 and 7.085; P = 0.021 and P = 0.035).\nAutomatically quantified CT extent and weight of COVID-19 pneumonia were well correlated with human-derived references and independently associated with symptoms and prognosis in multinational external data sets.", "journal": "Journal of computer assisted tomography", "date": "2022-04-12", "authors": ["Seung-JinYoo", "XiaolongQi", "ShoheiInui", "HyungjinKim", "Yeon JooJeong", "Kyung HeeLee", "Young KyungLee", "Bae YoungLee", "Jin YongKim", "Kwang NamJin", "Jae-KwangLim", "Yun-HyeonKim", "Ki BeomKim", "ZichengJiang", "ChuxiaoShao", "JunqiangLei", "ShengqiangZou", "HongqiuPan", "YeGu", "GuoZhang", "Jin MoGoo", "Soon HoYoon"], "doi": "10.1097/RCT.0000000000001303"}
{"title": "Cardiovascular signatures of COVID-19 predict mortality and identify barrier stabilizing therapies.", "abstract": "Endothelial cell (EC) activation, endotheliitis, vascular permeability, and thrombosis have been observed in patients with severe coronavirus disease 2019 (COVID-19), indicating that the vasculature is affected during the acute stages of SARS-CoV-2 infection. It remains unknown whether circulating vascular markers are sufficient to predict clinical outcomes, are unique to COVID-19, and if vascular permeability can be therapeutically targeted.\nProspectively evaluating the prevalence of circulating inflammatory, cardiac, and EC activation markers as well as developing a microRNA atlas in 241 unvaccinated patients with suspected SARS-CoV-2 infection allowed for prognostic value assessment using a Random Forest model machine learning approach. Subsequent ex vivo experiments assessed EC permeability responses to patient plasma and were used to uncover modulated gene regulatory networks from which rational therapeutic design was inferred.\nMultiple inflammatory and EC activation biomarkers were associated with mortality in COVID-19 patients and in severity-matched SARS-CoV-2-negative patients, while dysregulation of specific microRNAs at presentation was specific for poor COVID-19-related outcomes and revealed disease-relevant pathways. Integrating the datasets using a machine learning approach further enhanced clinical risk prediction for in-hospital mortality. Exposure of ECs to COVID-19 patient plasma resulted in severity-specific gene expression responses and EC barrier dysfunction, which was ameliorated using angiopoietin-1 mimetic or recombinant Slit2-N.\nIntegration of multi-omics data identified microRNA and vascular biomarkers prognostic of in-hospital mortality in COVID-19 patients and revealed that vascular stabilizing therapies should be explored as a treatment for endothelial dysfunction in COVID-19, and other severe diseases where endothelial dysfunction has a central role in pathogenesis.\nThis work was directly supported by grant funding from the Ted Rogers Center for Heart Research, Toronto, Ontario, Canada and the Peter Munk Cardiac Center, Toronto, Ontario, Canada.", "journal": "EBioMedicine", "date": "2022-04-12", "authors": ["DakotaGustafson", "MichelleNgai", "RuilinWu", "HuayunHou", "Alice CarvalhalSchoffel", "ClaraErice", "SerenaMandla", "FilioBillia", "Michael DWilson", "MilicaRadisic", "EddyFan", "UrielTrahtemberg", "AndrewBaker", "ChrisMcIntosh", "Chun-Po SFan", "Claudia CDos Santos", "Kevin CKain", "KateHanneman", "PaaladineshThavendiranathan", "Jason EFish", "Kathryn LHowe"], "doi": "10.1016/j.ebiom.2022.103982"}
{"title": "A novel explainable COVID-19 diagnosis method by integration of feature selection with random forest.", "abstract": "Several Artificial Intelligence-based models have been developed for COVID-19 disease diagnosis. In spite of the promise of artificial intelligence, there are very few models which bridge the gap between traditional human-centered diagnosis and the potential future of machine-centered disease diagnosis. Under the concept of human-computer interaction design, this study proposes a new explainable artificial intelligence method that exploits graph analysis for feature visualization and optimization for the purpose of COVID-19 diagnosis from blood test samples. In this developed model, an explainable decision forest classifier is employed to COVID-19 classification based on routinely available patient blood test data. The approach enables the clinician to use the decision tree and feature visualization to guide the explainability and interpretability of the prediction model. By utilizing this novel feature selection phase, the proposed diagnosis model will not only improve diagnosis accuracy but decrease the execution time as well.", "journal": "Informatics in medicine unlocked", "date": "2022-04-12", "authors": ["MehrdadRostami", "MouradOussalah"], "doi": "10.1016/j.imu.2022.100941\n10.1016/j.isatra.2021.07.003\n10.1016/j.jointm.2021.04.001"}
{"title": "Newly developed artificial intelligence algorithm for COVID-19 pneumonia: utility of quantitative CT texture analysis for prediction of favipiravir treatment effect.", "abstract": "Using CT findings from a prospective, randomized, open-label multicenter trial of favipiravir treatment of COVID-19 patients, the purpose of this study was to compare the utility of machine learning (ML)-based algorithm with that of CT-determined disease severity score and time from disease onset to CT (i.e., time until CT) in this setting.\nFrom March to May 2020, 32 COVID-19 patients underwent initial chest CT before enrollment were evaluated in this study. Eighteen patients were randomized to start favipiravir on day 1 (early treatment group), and 14 patients on day 6 of study participation (late treatment group). In this study, percentages of ground-glass opacity\u00a0(GGO), reticulation, consolidation, emphysema, honeycomb, and nodular lesion volumes were calculated as quantitative indexes by means of the software, while CT-determined disease severity was also visually scored. Next, univariate and stepwise regression analyses were performed to determine relationships between quantitative indexes and time until CT. Moreover, patient outcomes determined as viral clearance in the first 6\u00a0days and duration of fever were compared for those who started therapy within 4, 5, or 6\u00a0days as time until CT and those who started later by means of the Kaplan-Meier method followed by Wilcoxon's signed-rank test.\n% GGO and % consolidation showed significant correlations with time until CT (p\u2009<\u20090.05), and stepwise regression analyses identified both indexes as significant descriptors for time until CT (p\u2009<\u20090.05). When divided all patients between time until CT of 4\u00a0days and that of more than 4\u00a0days, accuracy of the combined quantitative method (87.5%) was significantly higher than that of the CT disease severity score (62.5%, p\u2009=\u20090.008).\nML-based CT texture analysis is equally or more useful for predicting time until CT for favipiravir treatment on COVID-19 patients than CT disease severity score.", "journal": "Japanese journal of radiology", "date": "2022-04-10", "authors": ["YoshiharuOhno", "KotaAoyagi", "KazumasaArakita", "YoheiDoi", "MasashiKondo", "SumiBanno", "KeiKasahara", "TakuOgawa", "HideakiKato", "RyotaHase", "FumihiroKashizaki", "KoichiNishi", "TadashiKamio", "KeikoMitamura", "NobuhiroIkeda", "AtsushiNakagawa", "YasukoFujisawa", "AkiraTaniguchi", "HirotakaIkeda", "HidekazuHattori", "KazuhiroMurayama", "HiroshiToyama"], "doi": "10.1007/s11604-022-01270-5\n10.1056/NEJMoa2002032\n10.1016/S2213-2600(20)30079-5\n10.1164/rccm.202002-0445OC\n10.1109/RBME.2020.2987975\n10.1016/S0140-6736(20)31022-9\n10.1056/NEJMoa2007016\n10.1016/j.antiviral.2018.03.003\n10.1016/j.antiviral.2013.09.015\n10.1016/j.pharmthera.2020.107512\n10.1093/cid/ciaa1176\n10.1016/j.ijid.2020.11.142\n10.1016/j.ejrad.2020.109410\n10.1128/AAC.01897-20\n10.1097/00000421-198212000-00014\n10.7883/yoken.JJID.2020.061\n10.1177/02841851211044973\n10.1148/radiol.2462070712\n10.1007/s10994-006-6226-1\n10.1109/TVCG.2012.136\n10.1148/radiol.2020200370\n10.1148/radiology.214.1.r00ja1473\n10.1148/radiol.2242010992\n10.1148/radiol.2313030103\n10.1148/radiol.2020200274\n10.2214/AJR.20.22976\n10.1148/radiol.2020200463\n10.1016/j.ejrad.2020.109009\n10.2214/AJR.20.23078\n10.1148/radiol.2020200843\n10.1007/s00330-020-07268-9\n10.1007/s00330-020-07271-0\n10.1007/s00330-021-07938-2"}
{"title": "COVID-CCD-Net: COVID-19 and colon cancer diagnosis system with optimized CNN hyperparameters using gradient-based optimizer.", "abstract": "Coronavirus disease-2019 (COVID-19) is a new types of coronavirus which have turned into a pandemic within a short time. Reverse transcription-polymerase chain reaction (RT-PCR) test is used for the diagnosis of COVID-19 in national healthcare centers. Because the number of PCR test kits is often limited, it is sometimes difficult to diagnose the disease at an early stage. However, X-ray technology is accessible nearly all over the world, and it succeeds in detecting symptoms of COVID-19 more successfully. Another disease which affects people's lives to a great extent is colorectal cancer. Tissue microarray (TMA) is a technological method which is widely used for its high performance in the analysis of colorectal cancer. Computer-assisted approaches which can classify colorectal cancer in TMA images are also needed. In this respect, the present study proposes a convolutional neural network (CNN) classification approach with optimized parameters using gradient-based optimizer (GBO) algorithm. Thanks to the proposed approach, COVID-19, normal, and viral pneumonia in various chest X-ray images can be classified accurately. Additionally, other types such as epithelial and stromal regions in epidermal growth factor receptor (EFGR) colon in TMAs can also be classified. The proposed approach was called COVID-CCD-Net. AlexNet, DarkNet-19, Inception-v3, MobileNet, ResNet-18, and ShuffleNet architectures were used in COVID-CCD-Net, and the hyperparameters of this architecture was optimized for the proposed approach. Two different medical image classification datasets, namely, COVID-19 and Epistroma, were used in the present study. The experimental findings demonstrated that proposed approach increased the classification performance of the non-optimized CNN architectures significantly and displayed a very high classification performance even in very low value of epoch.", "journal": "Medical & biological engineering & computing", "date": "2022-04-10", "authors": ["SonerKiziloluk", "EserSert"], "doi": "10.1007/s11517-022-02553-9\n10.1038/s41564-020-0695-z\n10.1080/07391102.2020.1767212\n10.1016/j.bspc.2020.102365\n10.1148/radiol.2020200527\n10.1016/j.compbiomed.2020.104181\n10.1016/j.chaos.2020.110245\n10.1016/j.media.2020.101794\n10.1016/j.chaos.2020.110495\n10.1016/j.asoc.2020.106859\n10.1038/nm0798-844\n10.1093/annonc/mdi006\n10.1016/j.ins.2020.06.037\n10.1016/j.envpol.2020.115618\n10.1016/j.swevo.2019.100643\n10.1016/j.eswa.2020.113506\n10.1016/j.irbm.2020.10.006\n10.1016/j.ijleo.2018.07.044\n10.1016/j.isatra.2020.10.052\n10.1016/j.compag.2020.105456\n10.1016/j.procs.2020.09.075\n10.5555/2188385.2188395\n10.1016/j.swevo.2019.06.002\n10.1049/iet-its.2018.5127\n10.1111/coin.12350\n10.1016/j.eswa.2021.115525\n10.1109/TEVC.2021.3060833\n10.1109/JPROC.2015.2494218\n10.1109/ACCESS.2021.3091729\n10.1016/j.compbiomed.2019.03.017\n10.1016/j.cmpb.2020.105608\n10.1016/j.chaos.2020.109944\n10.1007/s10096-020-03901-z\n10.1007/s13246-020-00865-4\n10.1109/RBME.2020.2987975\n10.1186/s41747-020-00203-z\n10.1016/j.cmpb.2020.105581\n10.1016/j.neucom.2016.01.034\n10.1186/1746-1596-7-22\n10.1016/j.autcon.2018.07.008\n10.1007/s13244-018-0639-9\n10.1155/2020/2616510\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2020.103792\n10.1007/s00330-021-07715-1\n10.1007/s00330-021-08050-1\n10.1007/s12539-020-00393-5\n10.1007/s10489-020-01904-z\n10.1109/JBHI.2017.2691738\n10.1117/1.JEI.27.1.011002"}
{"title": "Generalizability assessment of COVID-19 3D CT data for deep learning-based disease detection.", "abstract": "Artificial intelligence technologies in classification/detection of COVID-19 positive cases suffer from generalizability. Moreover, accessing and preparing another large dataset is not always feasible and time-consuming. Several studies have combined smaller COVID-19 CT datasets into \"supersets\" to maximize the number of training samples. This study aims to assess generalizability by splitting datasets into different portions based on 3D CT images using deep learning.\nTwo large datasets, including 1110 3D CT images, were split into five segments of 20% each. Each dataset's first 20% segment was separated as a holdout test set. 3D-CNN training was performed with the remaining 80% from each dataset. Two small external datasets were also used to independently evaluate the trained models.\nThe total combination of 80% of each dataset has an accuracy of 91% on Iranmehr and 83% on Moscow holdout test datasets. Results indicated that 80% of the primary datasets are adequate for fully training a model. The additional fine-tuning using 40% of a secondary dataset helps the model generalize to a third, unseen dataset. The highest accuracy achieved through transfer learning was 85% on LDCT dataset and 83% on Iranmehr holdout test sets when retrained on 80% of Iranmehr dataset.\nWhile the total combination of both datasets produced the best results, different combinations and transfer learning still produced generalizable results. Adopting the proposed methodology may help to obtain satisfactory results in the case of limited external datasets.", "journal": "Computers in biology and medicine", "date": "2022-04-08", "authors": ["MaryamFallahpoor", "SubrataChakraborty", "Mohammad TavakoliHeshejin", "HosseinChegeni", "Michael JamesHorry", "BiswajeetPradhan"], "doi": "10.1016/j.compbiomed.2022.105464\n10.1056/NEJMp2006141\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2108891\n10.1002/jmv.27515\n10.1016/j.ijsu.2020.02.034\n10.1016/j.jmii.2020.02.012\n10.1016/S2213-2600(20)30076\n10.1016/j.jpha.2020.02.010\n10.1148/radiol.2020200432\n10.1155/2021/5528144\n10.1109/ACCESS.2020.3027685\n10.22114/ajem.v4i2s.451\n10.1007/s10489-020-01826-w\n10.1109/ACCESS.2020.3005510\n10.1016/S1473-3099(20)30241-3\n10.1148/radiol.2020200343\n10.1016/j.jacr.2019.05.036\n10.1007/s12525-021-00475-2\n10.1186/s40537-021-00444-8\n10.1038/s41467-020-17971-2\n10.1016/j.compbiomed.2020.104037\n10.1016/j.compbiomed.2020.103795\n10.1136/bmjopen-2020-042946\n10.1016/j.cell.2020.04.045\n10.1109/RBME.2020.2987975\n10.1148/radiol.2020200905\n10.1038/s41746-021-00399-3\n10.2196/19569\n10.1007/s10140-020-01886-y\n10.1080/07391102.2020.1788642\n10.1109/TMI.2020.2996256\n10.1016/S0140-6736(20)32589-7\n10.1109/ACCESS.2021.3079716\n10.1016/j.imu.2020.100427\n10.1016/j.patrec.2021.09.012\n10.3390/s21238045\n10.1109/TMI.2020.2995965\n10.1016/j.inffus.2021.04.008\n10.3934/mbe.2021456\n10.1016/j.bspc.2021.102588\n10.17816/DD46826\n10.1016/j.patcog.2021.107848\n10.1016/j.patcog.2021.108135\n10.1109/JSEN.2021.3076767\n10.1101/2020.04.24.20078584\n10.21227/mxb3-7j48\n10.1186/s41747-020-00173-2\n10.3390/s22020506\n10.1109/ICASSP39728.2021.9414007\n10.1371/journal.pone.0258214\n10.1148/radiol.2020200905\n10.1017/S1481803500013336\n10.1148/radiol.2020192224\n10.1109/ACCESS.2020.3016780\n10.1109/CVPR.2009.5206848"}
{"title": "FDG PET/CT radiomics as a tool to differentiate between reactive axillary lymphadenopathy following COVID-19 vaccination and metastatic breast cancer axillary lymphadenopathy: a pilot study.", "abstract": "To evaluate if radiomics with machine learning can differentiate between F-18-fluorodeoxyglucose (FDG)-avid breast cancer metastatic lymphadenopathy and FDG-avid COVID-19 mRNA vaccine-related axillary lymphadenopathy.\nWe retrospectively analyzed FDG-positive, pathology-proven, metastatic axillary lymph nodes in 53 breast cancer patients who had PET/CT for follow-up or staging, and FDG-positive axillary lymph nodes in 46 patients who were vaccinated with the COVID-19 mRNA vaccine. Radiomics features (110 features classified into 7 groups) were extracted from all segmented lymph nodes. Analysis was performed on PET, CT, and combined PET/CT inputs. Lymph nodes were randomly assigned to a training (n = 132) and validation cohort (n = 33) by 5-fold cross-validation. K-nearest neighbors (KNN) and random forest (RF) machine learning models were used. Performance was evaluated using an area under the receiver-operator characteristic curve (AUC-ROC) score.\nAxillary lymph nodes from breast cancer patients (n = 85) and COVID-19-vaccinated individuals (n = 80) were analyzed. Analysis of first-order features showed statistically significant differences (p < 0.05) in all combined PET/CT features, most PET features, and half of the CT features. The KNN model showed the best performance score for combined PET/CT and PET input with 0.98 (\u00b1 0.03) and 0.88 (\u00b1 0.07) validation AUC, and 96% (\u00b1 4%) and 85% (\u00b1 9%) validation accuracy, respectively. The RF model showed the best result for CT input with 0.96 (\u00b1 0.04) validation AUC and 90% (\u00b1 6%) validation accuracy.\nRadiomics features can differentiate between FDG-avid breast cancer metastatic and FDG-avid COVID-19 vaccine-related axillary lymphadenopathy. Such a model may have a role in differentiating benign nodes from malignant ones.\n\u2022 Patients who were vaccinated with the COVID-19 mRNA vaccine have shown FDG-avid reactive axillary lymph nodes in PET-CT scans. \u2022 We evaluated if radiomics and machine learning can distinguish between FDG-avid metastatic axillary lymphadenopathy in breast cancer patients and FDG-avid reactive axillary lymph nodes. \u2022 Combined PET and CT radiomics data showed good test AUC (0.98) for distinguishing between metastatic axillary lymphadenopathy and post-COVID-19 vaccine-associated axillary lymphadenopathy. Therefore, the use of radiomics may have a role in differentiating between benign from malignant FDG-avid nodes.", "journal": "European radiology", "date": "2022-04-08", "authors": ["MichalEifer", "HodayaPinian", "EyalKlang", "YousefAlhoubani", "NayrozKanana", "NoamTau", "TimaDavidson", "EliKonen", "Onofrio ACatalano", "YaelEshet", "LiranDomachevsky"], "doi": "10.1007/s00330-022-08725-3\n10.1056/NEJMoa2113017\n10.1056/NEJMoa2034577\n10.1001/jamaoncol.2021.3127\n10.2214/AJR.21.25604\n10.3322/caac.21492\n10.1148/radiol.2021210436\n10.1148/radiol.2015151169\n10.2967/jnumed.118.222893\n10.1186/s12885-020-07053-3\n10.23736/S1824-4785.19.03137-6\n10.1155/2019/4507694\n10.1016/j.crad.2016.09.013\n10.3389/fmolb.2020.613918\n10.1148/radiol.2018171820\n10.1109/TRPMS.2019.2893860\n10.1007/s00259-011-1729-9\n10.1023/A:1010933404324\n10.1158/0008-5472.CAN-17-0339\n10.1007/s00259-021-05314-2\n10.1097/RLU.0000000000003648\n10.1148/radiol.2021210886\n10.1155/2020/3959236\n10.1016/j.ejca.2021.06.023\n10.1007/s11307-012-0562-2"}
{"title": "Automatic COVID-19 detection mechanisms and approaches from medical images: a systematic review.", "abstract": "Since early 2020, Coronavirus Disease 2019 (COVID-19) has spread widely around the world. COVID-19 infects the lungs, leading to breathing difficulties. Early detection of COVID-19 is important for the prevention and treatment of pandemic. Numerous sources of medical images (e.g., Chest X-Rays (CXR), Computed Tomography (CT), and Magnetic Resonance Imaging (MRI)) are regarded as a desirable technique for diagnosing COVID-19 cases. Medical images of coronavirus patients show that the lungs are filled with sticky mucus that prevents them from inhaling. Today, Artificial Intelligence (AI) based algorithms have made a significant shift in the computer aided diagnosis due to their effective feature extraction capabilities. In this survey, a complete and systematic review of the application of Machine Learning (ML) methods for the detection of COVID-19 is presented, focused on works that used medical images. We aimed to evaluate various ML-based techniques in detecting COVID-19 using medical imaging. A total of 26 papers were extracted from ACM, ScienceDirect, Springerlink, Tech Science Press, and IEEExplore. Five different ML categories to review these mechanisms are considered, which are supervised learning-based, deep learning-based, active learning-based, transfer learning-based, and evolutionary learning-based mechanisms. A number of articles are investigated in each group. Also, some directions for further research are discussed to improve the detection of COVID-19 using ML techniques in the future. In most articles, deep learning is used as the ML method. Also, most of the researchers used CXR images to diagnose COVID-19. Most articles reported accuracy of the models to evaluate model performance. The accuracy of the studied models ranged from 0.84 to 0.99. The studies demonstrated the current status of AI techniques in using AI potentials in the fight against COVID-19.", "journal": "Multimedia tools and applications", "date": "2022-04-07", "authors": ["Amir MasoudRahmani", "ElhamAzhir", "MortezaNaserbakht", "MokhtarMohammadi", "Adil Hussein MohammedAldalwie", "Mohammed KamalMajeed", "Sarkhel HTaher Karim", "MehdiHosseinzadeh"], "doi": "10.1007/s11042-022-12952-7\n10.1007/s10489-020-01829-7\n10.1109/ACCESS.2020.2990893\n10.1109/JIOT.2021.3050775\n10.2196/19104\n10.32604/cmc.2021.014265\n10.1016/j.jiph.2020.06.028\n10.1016/j.radi.2020.09.010\n10.32604/cmc.2021.012955\n10.1007/s13246-020-00865-4\n10.1186/s41824-020-00086-8\n10.1016/j.scs.2020.102589\n10.24086/cuesj.v6n1y2022.pp1-6\n10.1016/j.cmpb.2020.105608\n10.1016/j.eswa.2020.113909\n10.1145/3465398\n10.1109/RBME.2020.2990959\n10.1109/ACCESS.2020.3028012\n10.1016/j.asoc.2020.106859\n10.1016/j.jiph.2020.03.019\n10.1109/ACCESS.2020.3016780\n10.1109/ACCESS.2020.3005510\n10.1016/j.artmed.2020.101981\n10.1109/ACCESS.2019.2945338\n10.5812/archcid.103232\n10.1016/j.chaos.2020.110059\n10.1038/nature14539\n10.1016/j.artmed.2020.101985\n10.1016/j.asoc.2020.106691\n10.1109/ACCESS.2020.2995597\n10.32604/cmc.2021.012874\n10.1155/2020/9756518\n10.1016/j.radi.2020.10.018\n10.1016/j.chaos.2020.109944\n10.1016/j.imu.2020.100360\n10.1007/s10916-020-01562-1\n10.1109/RBME.2020.2987975\n10.1007/s10489-020-01862-6\n10.1016/j.imu.2020.100427\n10.1007/s13198-019-00863-0\n10.1016/j.dsx.2020.04.012\n10.1016/j.ins.2020.09.041\n10.1186/s40537-016-0043-6\n10.1016/j.media.2020.101913"}
{"title": "Pre-processing methods in chest X-ray image classification.", "abstract": "The SARS-CoV-2 pandemic began in early 2020, paralyzing human life all over the world and threatening our security. Thus, the need for an effective, novel approach to diagnosing, preventing, and treating COVID-19 infections became paramount.\nThis article proposes a machine learning-based method for the classification of chest X-ray images. We also examined some of the pre-processing methods such as thresholding, blurring, and histogram equalization.\nWe found the F1-score results rose to 97%, 96%, and 99% for the three analyzed classes: healthy, COVID-19, and pneumonia, respectively.\nOur research provides proof that machine learning can be used to support medics in chest X-ray classification and improving pre-processing leads to improvements in accuracy, precision, recall, and F1-scores.", "journal": "PloS one", "date": "2022-04-06", "authors": ["AgataGie\u0142czyk", "AnnaMarciniak", "MartynaTarczewska", "ZbigniewLutowski"], "doi": "10.1371/journal.pone.0265949\n10.1148/ryct.2020200034\n10.1016/j.future.2020.04.013\n10.1109/ACCESS.2020.3007656\n10.3390/e23010090\n10.1186/s12880-020-00529-5\n10.1016/j.media.2020.101693\n10.32604/cmc.2021.012955\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.mehy.2020.109761\n10.1016/j.cmpb.2018.04.025\n10.1016/j.measurement.2019.05.076\n10.1016/j.imu.2020.100391\n10.1016/j.compbiomed.2020.103795\n10.1016/j.eswa.2020.114054\n10.1007/s10489-020-01902-1\n10.1016/j.chaos.2020.110495\n10.1016/j.compbiomed.2021.104425\n10.1007/s42979-021-00762-x\n10.1016/j.asoc.2021.107238\n10.1016/j.cmpb.2020.105608\n10.1007/s00521-020-05636-6\n10.1038/s41598-021-95680-6"}
{"title": "Tracking and predicting COVID-19 radiological trajectory on chest X-rays using deep learning.", "abstract": "Radiological findings on chest X-ray (CXR) have shown to be essential for the proper management of COVID-19 patients as the maximum severity over the course of the disease is closely linked to the outcome. As such, evaluation of future severity from current CXR would be highly desirable. We trained a repurposed deep learning algorithm on the CheXnet open dataset (224,316 chest X-ray images of 65,240 unique patients) to extract features that mapped to radiological labels. We collected CXRs of COVID-19-positive patients from an open-source dataset (COVID-19 image data collection) and from a multi-institutional local ICU dataset. The data was grouped into pairs of sequential CXRs and were categorized into three categories: 'Worse', 'Stable', or 'Improved' on the basis of radiological evolution ascertained from images and reports. Classical machine-learning algorithms were trained on the deep learning extracted features to perform immediate severity evaluation and prediction of future radiological trajectory. Receiver operating characteristic analyses and Mann-Whitney tests were performed. Deep learning predictions between \"Worse\" and \"Improved\" outcome categories and for severity stratification were significantly different for three radiological signs and one diagnostic ('Consolidation', 'Lung Lesion', 'Pleural effusion' and 'Pneumonia'; all P\u00a0<\u00a00.05). Features from the first CXR of each pair could correctly predict the outcome category between 'Worse' and 'Improved' cases with a 0.81 (0.74-0.83 95% CI) AUC in the open-access dataset and with a 0.66 (0.67-0.64 95% CI) AUC in the ICU dataset. Features extracted from the CXR could predict disease severity with a 52.3% accuracy in a 4-way classification. Severity evaluation trained on the COVID-19 image data collection had good out-of-distribution generalization when testing on the local dataset, with 81.6% of intubated ICU patients being classified as critically ill, and the predicted severity was correlated with the clinical outcome with a 0.639 AUC. CXR deep learning features show promise for classifying disease severity and trajectory. Once validated in studies incorporating clinical data and with larger sample sizes, this information may be considered to inform triage decisions.", "journal": "Scientific reports", "date": "2022-04-06", "authors": ["DanielGourdeau", "OlivierPotvin", "PatrickArchambault", "CarlChartrand-Lefebvre", "LouisDieumegarde", "RezaForghani", "ChristianGagn\u00e9", "AlexandreHains", "DavidHornstein", "HuyLe", "SimonLemieux", "Marie-H\u00e9l\u00e8neL\u00e9vesque", "DiegoMartin", "LorneRosenbloom", "AnTang", "FabrizioVecchio", "IssacYang", "NathalieDuchesne", "SimonDuchesne"], "doi": "10.1038/s41598-022-09356-w\n10.1503/cmaj.200465\n10.1056/NEJMe2005477\n10.1016/S1473-3099(20)30134-1\n10.2214/AJR.20.23034\n10.1001/jama.2020.1585\n10.2214/AJR.20.22976\n10.1148/radiol.2020201160\n10.1148/ryct.2020200028\n10.1016/j.crad.2020.03.003\n10.1038/s41598-020-79139-8\n10.1016/j.clinimag.2020.11.004\n10.1007/s00330-020-07270-1\n10.1016/j.ijid.2020.05.021\n10.1056/NEJMp2005689\n10.1016/j.cell.2020.04.045\n10.1038/s41591-020-0931-3\n10.1186/s43055-021-00524-y\n10.3233/XST-200831\n10.1016/j.cmpb.2020.105581\n10.1007/s13246-020-00865-4\n10.1371/journal.pone.0236621\n10.5152/dir.2020.20205\n10.1136/bmjinnov-2020-000593\n10.1373/clinchem.2015.246280\n10.1086/589754\n10.1038/s42256-021-00307-0\n10.1136/bmj.m1328\n10.1038/s41586-020-2521-4"}
{"title": "Low-Dose COVID-19 CT Image Denoising Using CNN and its Method Noise Thresholding.", "abstract": "Noise in computed tomography (CT) images may occur due to low radiation dose. Hence, the main aim of this paper is to reduce the noise from low dose CT images so that the risk of high radiation dose can be reduced.\nThe novel corona virus outbreak has ushered in different new areas of research in medical instrumentation and technology. Medical diagnostics and imaging are one of the ways in which the area and level of infection can be detected.\nThe COVID-19 attacks people who have less immunity, so infants, kids, and pregnant women are more vulnerable to the infection. So they need to undergo CT scanning to find the infection level. But the high radiation diagnostic is also fatal for them, so the intensity of radiation needs to be reduced significantly, which may generate the noise in the CT images.\nIn this paper, a new denoising technique for such low dose Covid-19 CT images has been introduced using a convolution neural network (CNN) and the method noise-based thresholding. The major concern of the methodology for reducing the risk associated with radiation while diagnosing.\nThe results are evaluated visually and also by using standard performance metrics. From comparative analysis, it was observed that proposed works gives better outcomes.\nThe proposed low-dose COVID-19 CT image denoising model is therefore concluded to have a better potential to be effective in various pragmatic medical image processing applications in terms of noise suppression and clinical edge preservation.", "journal": "Current medical imaging", "date": "2022-04-06", "authors": ["ManojDiwakar", "Neeraj KumarPandey", "RavinderSingh", "DilipSisodia", "ChandrakalaArya", "PrabhishekSingh", "ChinmayChakraborty"], "doi": "10.2174/1573405618666220404162241"}
{"title": "COVID-19 prognostic modeling using CT radiomic features and machine learning algorithms: Analysis of a multi-institutional dataset of 14,339 patients.", "abstract": "We aimed to analyze the prognostic power of CT-based radiomics models using data of 14,339 COVID-19 patients.\nWhole lung segmentations were performed automatically using a deep learning-based model to extract 107 intensity and texture radiomics features. We used four feature selection algorithms and seven classifiers. We evaluated the models using ten different splitting and cross-validation strategies, including non-harmonized and ComBat-harmonized datasets. The sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) were reported.\nIn the test dataset (4,301) consisting of CT and/or RT-PCR positive cases, AUC, sensitivity, and specificity of 0.83\u00a0\u00b1\u00a00.01 (CI95%: 0.81-0.85), 0.81, and 0.72, respectively, were obtained by ANOVA feature selector\u00a0+\u00a0Random Forest (RF) classifier. Similar results were achieved in RT-PCR-only positive test sets (3,644). In ComBat harmonized dataset, Relief feature selector\u00a0+\u00a0RF classifier resulted in the highest performance of AUC, reaching 0.83\u00a0\u00b1\u00a00.01 (CI95%: 0.81-0.85), with a sensitivity and specificity of 0.77 and 0.74, respectively. ComBat harmonization did not depict statistically significant improvement compared to a non-harmonized dataset. In leave-one-center-out, the combination of ANOVA feature selector and RF classifier resulted in the highest performance.\nLung CT radiomics features can be used for robust prognostic modeling of COVID-19. The predictive power of the proposed CT radiomics model is more reliable when using a large multicentric heterogeneous dataset, and may be used prospectively in clinical setting to manage COVID-19 patients.", "journal": "Computers in biology and medicine", "date": "2022-04-05", "authors": ["IsaacShiri", "YazdanSalimi", "MasoumehPakbin", "GhasemHajianfar", "Atlas HaddadiAvval", "AmirhosseinSanaat", "ShayanMostafaei", "AzadehAkhavanallaf", "AbdollahSaberi", "ZahraMansouri", "DariushAskari", "MohammadrezaGhasemian", "EhsanSharifipour", "SalehSandoughdaran", "AhmadSohrabi", "ElhamSadati", "SomayehLivani", "PooyaIranpour", "ShahriarKolahi", "MaziarKhateri", "SalarBijari", "Mohammad RezaAtashzar", "Sajad PShayesteh", "BardiaKhosravi", "Mohammad RezaBabaei", "ElnazJenabi", "MohammadHasanian", "AlirezaShahhamzeh", "Seyaed YaserForoghi Ghomi", "AbolfazlMozafari", "ArashTeimouri", "FatemehMovaseghi", "AzinAhmari", "NedaGoharpey", "RamaBozorgmehr", "HesamaddinShirzad-Aski", "RoozbehMortazavi", "JalalKarimi", "NazaninMortazavi", "SimaBesharat", "MandanaAfsharpad", "HamidAbdollahi", "ParhamGeramifar", "Amir RezaRadmard", "HosseinArabi", "KiaraRezaei-Kalantari", "MehrdadOveisi", "ArmanRahmim", "HabibZaidi"], "doi": "10.1016/j.compbiomed.2022.105467"}
{"title": "COVID-WideNet-A capsule network for COVID-19 detection.", "abstract": "Ever since the outbreak of COVID-19, the entire world is grappling with panic over its rapid spread. Consequently, it is of utmost importance to detect its presence. Timely diagnostic testing leads to the quick identification, treatment and isolation of infected people. A number of deep learning classifiers have been proved to provide encouraging results with higher accuracy as compared to the conventional method of RT-PCR testing. Chest radiography, particularly using X-ray images, is a prime imaging modality for detecting the suspected COVID-19 patients. However, the performance of these approaches still needs to be improved. In this paper, we propose a capsule network called COVID-WideNet for diagnosing COVID-19 cases using Chest X-ray (CXR) images. Experimental results have demonstrated that a discriminative trained, multi-layer capsule network achieves state-of-the-art performance on the ", "journal": "Applied soft computing", "date": "2022-04-05", "authors": ["P KGupta", "Mohammad KhubebSiddiqui", "XiaodiHuang", "RubenMorales-Menendez", "HarshPawar", "HugoTerashima-Marin", "Mohammad SaifWajid"], "doi": "10.1016/j.asoc.2022.108780\n10.1007/s00330-020-06748-2\n10.1016/j.chaos.2020.110122\n10.1590/1678-4324-2020190736\n10.3923/jas.2013.416.422\n10.1016/j.chaos.2020.109944"}
{"title": "Automated System for Identifying COVID-19 Infections in Computed Tomography Images Using Deep Learning Models.", "abstract": "Coronavirus disease 2019 (COVID-19) is a novel disease that affects healthcare on a global scale and cannot be ignored because of its high fatality rate. Computed tomography (CT) images are presently being employed to assist doctors in detecting COVID-19 in its early stages. In several scenarios, a combination of epidemiological criteria (contact during the incubation period), the existence of clinical symptoms, laboratory tests (nucleic acid amplification tests), and clinical imaging-based tests are used to diagnose COVID-19. This method can miss patients and cause more complications. Deep learning is one of the techniques that has been proven to be prominent and reliable in several diagnostic domains involving medical imaging. This study utilizes a convolutional neural network (CNN), stacked autoencoder, and deep neural network to develop a COVID-19 diagnostic system. In this system, classification undergoes some modification before applying the three CT image techniques to determine normal and COVID-19 cases. A large-scale and challenging CT image dataset was used in the training process of the employed deep learning model and reporting their final performance. Experimental outcomes show that the highest accuracy rate was achieved using the CNN model with an accuracy of 88.30%, a sensitivity of 87.65%, and a specificity of 87.97%. Furthermore, the proposed system has outperformed the current existing state-of-the-art models in detecting the COVID-19 virus using CT images.", "journal": "Journal of healthcare engineering", "date": "2022-04-05", "authors": ["Karrar HameedAbdulkareem", "Salama AMostafa", "Zainab NAl-Qudsy", "Mazin AbedMohammed", "Alaa SAl-Waisy", "SeifedineKadry", "JinseokLee", "YunyoungNam"], "doi": "10.1155/2022/5329014\n10.1016/j.bspc.2021.103128\n10.1016/j.bspc.2021.103182\n10.1016/j.idm.2020.02.002\n10.2196/21788\n10.1259/bjr.20210759\n10.32604/cmc.2021.012955\n10.1007/978-3-030-79753-9_4\n10.22059/jitm.2020.79187\n10.1111/exsy.12759\n10.1016/j.imu.2020.100427\n10.3390/biology11010043\n10.1016/j.rinp.2021.105045\n10.1007/978-981-16-1342-5_23\n10.1007/s00521-021-05820-2\n10.1016/j.bspc.2021.103326\n10.1007/s00330-021-07715-1\n10.1007/978-3-030-55258-9_17\n10.1002/mp.14609\n10.1148/radiol.2020200905\n10.1016/j.eng.2020.04.010\n10.1109/TCBB.2021.3065361\n10.3390/e22050517\n10.1111/all.14238\n10.1148/radiol.2020200432\n10.1007/s11042-021-11153-y\n10.1148/rg.246045065\n10.1148/rg.2015140232\n10.2147/OTT.S80733\n10.1038/srep46479\n10.1007/s13246-020-00865-4\n10.1007/s10489-020-01829-7\n10.3389/fpubh.2021.744100\n10.3390/covid1010034\n10.1109/ACCESS.2020.3010287\n10.3390/healthcare9121614\n10.1007/978-3-642-21735-7_7\n10.1155/2017/5218247\n10.1038/s41597-021-00900-3\n10.24018/ejeng.2021.6.5.2485\n10.1016/j.bbe.2021.05.013\n10.1016/j.compbiomed.2020.104037\n10.1016/j.imu.2020.100427"}
{"title": "Lung Disease Classification in CXR Images Using Hybrid Inception-ResNet-v2 Model and Edge Computing.", "abstract": "Chest X-ray (CXR) imaging is one of the most widely used and economical tests to diagnose a wide range of diseases. However, even for expert radiologists, it is a challenge to accurately diagnose diseases from CXR samples. Furthermore, there remains an acute shortage of trained radiologists worldwide. In the present study, a range of machine learning (ML), deep learning (DL), and transfer learning (TL) approaches have been evaluated to classify diseases in an openly available CXR image dataset. A combination of the synthetic minority over-sampling technique (SMOTE) and weighted class balancing is used to alleviate the effects of class imbalance. A hybrid Inception-ResNet-v2 transfer learning model coupled with data augmentation and image enhancement gives the best accuracy. The model is deployed in an edge environment using Amazon IoT Core to automate the task of disease detection in CXR images with three categories, namely pneumonia, COVID-19, and normal. Comparative analysis has been given in various metrics such as precision, recall, accuracy, AUC-ROC score, etc. The proposed technique gives an average accuracy of 98.66%. The accuracies of other TL models, namely SqueezeNet, VGG19, ResNet50, and MobileNetV2 are 97.33%, 91.66%, 90.33%, and 76.00%, respectively. Further, a DL model, trained from scratch, gives an accuracy of 92.43%. Two feature-based ML classification techniques, namely support vector machine with local binary pattern (SVM\u2009+\u2009LBP) and decision tree with histogram of oriented gradients (DT\u2009+\u2009HOG) yield an accuracy of 87.98% and 86.87%, respectively.", "journal": "Journal of healthcare engineering", "date": "2022-04-05", "authors": ["Chandra ManiSharma", "LakshayGoyal", "Vijayaraghavan MChariar", "NavelSharma"], "doi": "10.1155/2022/9036457\n10.3390/sym12071146\n10.3390/diagnostics11122208\n10.1109/ssci.2018.8628869\n10.3390/diagnostics11112025\n10.1016/j.compbiomed.2021.104319\n10.1109/access.2020.3010287\n10.1016/j.sysarc.2020.101830\n10.1007/s00500-021-06514-6\n10.1016/j.compbiomed.2021.104401\n10.1155/2021/8828404\n10.1155/2021/9437538\n10.1016/j.bspc.2019.04.031\n10.1016/j.eswa.2020.114054\n10.1016/j.chaos.2020.110495\n10.1016/j.patrec.2019.11.013\n10.2174/1573405616666200604163954\n10.1016/j.cmpb.2019.06.005\n10.1007/s10489-020-01829-7\n10.1016/j.cmpb.2020.105532\n10.1016/j.irbm.2019.10.006\n10.1007/978-981-15-1624-5_9\n10.3390/s21217116\n10.1109/jbhi.2021.3110805\n10.1109/cvpr.2018.00943\n10.1016/j.media.2020.101839\n10.7717/peerj-cs.495\n10.1038/s41598-019-42294-8\n10.1016/j.asoc.2021.107692\n10.1007/s12553-021-00520-2\n10.3892/etm.2020.8797\n10.1002/ett.3710\n10.1109/access.2020.3021983\n10.1109/iotm.0001.2000138\n10.1109/jiot.2021.3051844\n10.1613/jair.953\n10.1109/ACCESS.2019.2961511\n10.1109/iccerec.2016.7814989\n10.1007/978-3-319-23192-1_50\n10.1016/j.imu.2021.100642"}
{"title": "Machine Learning with Quantum Seagull Optimization Model for COVID-19 Chest X-Ray Image Classification.", "abstract": "Early and accurate detection of COVID-19 is an essential process to curb the spread of this deadly disease and its mortality rate. Chest radiology scan is a significant tool for early management and diagnosis of COVID-19 since the virus targets the respiratory system. Chest X-ray (CXR) images are highly useful in the effective detection of COVID-19, thanks to its availability, cost-effective means, and rapid outcomes. In addition, Artificial Intelligence (AI) techniques such as deep learning (DL) models play a significant role in designing automated diagnostic processes using CXR images. With this motivation, the current study presents a new Quantum Seagull Optimization Algorithm with DL-based COVID-19 diagnosis model, named QSGOA-DL technique. The proposed QSGOA-DL technique intends to detect and classify COVID-19 with the help of CXR images. In this regard, the QSGOA-DL technique involves the design of EfficientNet-B4 as a feature extractor, whereas hyperparameter optimization is carried out with the help of QSGOA technique. Moreover, the classification process is performed by a multilayer extreme learning machine (MELM) model. The novelty of the study lies in the designing of QSGOA for hyperparameter optimization of the EfficientNet-B4 model. An extensive series of simulations was carried out on the benchmark test CXR dataset, and the results were assessed under different aspects. The simulation results demonstrate the promising performance of the proposed QSGOA-DL technique compared to recent approaches.", "journal": "Journal of healthcare engineering", "date": "2022-04-05", "authors": ["MahmoudRagab", "SamahAlshehri", "Nabil AAlhakamy", "WafaaAlsaggaf", "Hani AAlhadrami", "JaberAlyami"], "doi": "10.1155/2022/6074538\n10.1109/access.2021.3058537\n10.1155/2021/6799202\n10.1155/2021/3514821\n10.1155/2021/5528441\n10.1148/radiol.2020201160\n10.1016/j.compbiomed.2020.103792\n10.1109/tmi.2020.2995965\n10.1038/s41591-019-0447-x\n10.1109/tmi.2020.2994459\n10.1109/access.2020.3040245\n10.1109/access.2020.3025010\n10.1109/jtehm.2021.3077142\n10.26599/bdma.2020.9020012\n10.1109/jbhi.2020.3018181\n10.1109/tnnls.2021.3054306\n10.1109/tip.2021.3058783\n10.1109/tmi.2020.2996256\n10.1016/b978-0-12-811318-9.00020-x\n10.1016/j.compag.2020.105652\n10.1155/2021/6639671\n10.3390/app9081707\n10.3390/rs10122036\n10.1016/j.compbiomed.2021.104816"}
{"title": "Efficient COVID-19 CT Scan Image Segmentation by Automatic Clustering Algorithm.", "abstract": "This article addresses automated segmentation and classification of COVID-19 and normal chest CT scan images. Segmentation is the preprocessing step for classification, and 12 DWT-PCA-based texture features extracted from the segmented image are utilized as input for the random forest machine-learning algorithm to classify COVID-19/non-COVID-19 disease. Diagnosing COVID-19 disease through an RT-PCR test is a time-consuming process. Sometimes, the RT-PCR test result is not accurate; that is, it has a false negative, which can cause a threat to the person's life due to delay in starting the specified treatment. At this moment, there is an urgent need to develop a reliable automatic COVID-19 detection tool that can detect COVID-19 disease from chest CT scan images within a shorter period and can help doctors to start COVID-19 treatment at the earliest. In this article, a variant of the whale optimization algorithm named improved whale optimization algorithm (IWOA) is introduced. The efficiency of the IWOA is tested for unimodal (F1-F7), multimodal (F8-F13), and fixed-dimension multimodal (F14-F23) benchmark functions and is compared with the whale optimization algorithm (WOA), salp swarm optimization (SSA), and sine cosine algorithm (SCA). The experiment is carried out in 30 trials and population size, and iterations are set as 30 and 100 under each trial. IWOA achieves faster convergence than WOA, SSA, and SCA and enhances the exploitation and exploration phases of WOA, avoiding local entrapment. IWOA, WOA, SSA, and SCA utilized Otsu's maximum between-class variance criteria as fitness function to compute optimal threshold values for multilevel medical CT scan image segmentation. Evaluation measures such as accuracy, specificity, precision, recall, ", "journal": "Journal of healthcare engineering", "date": "2022-04-05", "authors": ["Basu DevShivahare", "S KGupta"], "doi": "10.1155/2022/9009406\n10.1002/ppul.24718\n10.3390/jcm9030674\n10.3390/ijerph17103520\n10.1148/radiol.2020200330\n10.1016/j.procs.2020.03.179\n10.1111/exsy.12749\n10.1007/s11042-019-08048-4\n10.1186/s12880-020-00529-5\n10.1007/s42979-021-00496-w\n10.1155/2017/9749108\n10.1049/iet-ipr.2019.0772\n10.1016/j.bspc.2020.102365\n10.2174/1573405617999210112195450\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.knosys.2015.12.022\n10.2174/1872212114999200730163151\n10.25046/aj050303\n10.17485/ijst/2016/v9i19/90440\n10.32604/cmc.2020.09519\n10.1007/978-3-319-48944-5_6\n10.1142/S0218001421510046\n10.11591/ijeecs.v18.i1\n10.1007/s11042-017-4638-5\n10.1371/journalpone.0244416\n10.1155/2021/8829829\n10.1002/eng2.12149\n10.1155/2021/5528441"}
{"title": "A privacy-aware method for COVID-19 detection in chest CT images using lightweight deep conventional neural network and blockchain.", "abstract": "With the global spread of the COVID-19 epidemic, a reliable method is required for identifying COVID-19 victims. The biggest issue in detecting the virus is a lack of testing kits that are both reliable and affordable. Due to the virus's rapid dissemination, medical professionals have trouble finding positive patients. However, the next real-life issue is sharing data with hospitals around the world while considering the organizations' privacy concerns. The primary worries for training a global Deep Learning (DL) model are creating a collaborative platform and personal confidentiality. Another challenge is exchanging data with health care institutions while protecting the organizations' confidentiality. The primary concerns for training a universal DL model are creating a collaborative platform and preserving privacy. This paper provides a model that receives a small quantity of data from various sources, like organizations or sections of hospitals, and trains a global DL model utilizing blockchain-based Convolutional Neural Networks (CNNs). In addition, we use the Transfer Learning (TL) technique to initialize layers rather than initialize randomly and discover which layers should be removed before selection. Besides, the blockchain system verifies the data, and the DL method trains the model globally while keeping the institution's confidentiality. Furthermore, we gather the actual and novel COVID-19 patients. Finally, we run extensive experiments utilizing Python and its libraries, such as Scikit-Learn and TensorFlow, to assess the proposed method. We evaluated works using five different datasets, including Boukan Dr. Shahid Gholipour hospital, Tabriz Emam Reza hospital, Mahabad Emam Khomeini hospital, Maragheh Dr.Beheshti hospital, and Miandoab Abbasi hospital datasets, and our technique outperform state-of-the-art methods on average in terms of precision (2.7%), recall (3.1%), F1 (2.9%), and accuracy (2.8%).", "journal": "Computers in biology and medicine", "date": "2022-04-03", "authors": ["ArashHeidari", "ShivaToumaj", "Nima JafariNavimipour", "MehmetUnal"], "doi": "10.1016/j.compbiomed.2022.105461"}
{"title": "Improved-Mask R-CNN: Towards an accurate generic MSK MRI instance segmentation platform (data from the Osteoarthritis Initiative).", "abstract": "Objective assessment of osteoarthritis (OA) Magnetic Resonance Imaging (MRI) scans can address the limitations of the current OA assessment approaches. Detecting and extracting bone, cartilage, and joint fluid is a necessary component for the objective assessment of OA, which helps to quantify tissue characteristics such as volume and thickness. Many algorithms, based on Artificial Intelligence (AI), have been proposed over recent years for segmenting bone and soft tissues. Most of these segmentation methods suffer from the class imbalance problem, can't differentiate between the same anatomic structure, or do not support segmenting different rang of tissue sizes. Mask R-CNN is an instance segmentation framework, meaning it segments and distinct each object of interest like different anatomical structures (e.g. bone and cartilage) using a single model. In this study, the Mask R-CNN architecture was deployed to address the need for a segmentation method that is applicable to use for different tissue scales, pathologies, and MRI sequences associated with OA, without having a problem with imbalanced classes. In addition, we modified the Mask R-CNN to improve segmentation accuracy around instance edges.\nA total of 500 adult knee MRI scans from the publicly available Osteoarthritis Initiative (OAI), and 97 hip MRI scans from adults with symptomatic hip OA, evaluated by two readers, were used for training and validating the network. Three specific modifications to Mask R-CNN yielded the improved-Mask R-CNN (iMaskRCNN): an additional ROIAligned block, an extra decoder block in the segmentation header, and connecting them using a skip connection. The results were evaluated using Hausdorff distance, dice score for bone and cartilage segmentation, and differences in detected volume, dice score, and coefficients of variation (CoV) for effusion segmentation.\nThe iMaskRCNN led to improved bone and cartilage segmentation compared to Mask RCNN as indicated with the increase in dice score from 95% to 98% for the femur, 95-97% for the tibia, 71-80% for the femoral cartilage, and 81-82% for the tibial cartilage. For the effusion detection, the dice score improved with iMaskRCNN 72% versus Mask R-CNN 71%. The CoV values for effusion detection between Reader1 and Mask R-CNN (0.33), Reader1 and iMaskRCNN (0.34), Reader2 and Mask R-CNN (0.22), Reader2 and iMaskRCNN (0.29) are close to CoV between two readers (0.21), indicating a high agreement between the human readers and both Mask R-CNN and iMaskRCNN.\nMask R-CNN and iMaskRCNN can reliably and simultaneously extract different scale articular tissues involved in OA, forming the foundation for automated assessment of OA. The iMaskRCNN results show that the modification improved the network performance around the edges.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2022-04-02", "authors": ["BanafsheFelfeliyan", "AbhilashHareendranathan", "GregorKuntze", "Jacob LJaremko", "Janet LRonsky"], "doi": "10.1016/j.compmedimag.2022.102056"}
{"title": "Facilitating standardized COVID-19 suspicion prediction based on computed tomography radiomics in a multi-demographic setting.", "abstract": "To develop an automatic COVID-19 Reporting and Data System (CO-RADS)-based classification in a multi-demographic setting.\nThis multi-institutional review boards-approved retrospective study included 2720 chest CT scans (mean age, 58 years [range 18-100 years]) from Italian and Russian patients. Three board-certified radiologists from three countries assessed randomly selected subcohorts from each population and provided CO-RADS-based annotations. CT radiomic features were extracted from the selected subcohorts after preprocessing steps like lung lobe segmentation and automatic noise reduction. We compared three machine learning models, logistic regression (LR), multilayer perceptron (MLP), and random forest (RF) for the automated CO-RADS classification. Model evaluation was carried out in two scenarios, first, training on a mixed multi-demographic subcohort and testing on an independent hold-out dataset. In the second scenario, training was done on a single demography and externally validated on the other demography.\nThe overall inter-observer agreement for the CO-RADS scoring between the radiologists was substantial (k = 0.80). Irrespective of the type of validation test scenario, suspected COVID-19 CT scans were identified with an accuracy of 84%. SHapley Additive exPlanations (SHAP) interpretation showed that the \"wavelet_(LH)_GLCM_Imc1\" feature had a positive impact on COVID prediction both with and without noise reduction. The application of noise reduction improved the overall performance between the classifiers for all types.\nUsing an automated model based on the COVID-19 Reporting and Data System (CO-RADS), we achieved clinically acceptable performance in a multi-demographic setting. This approach can serve as a standardized tool for automated COVID-19 assessment.\n\u2022 Automatic CO-RADS scoring of large-scale multi-demographic chest CTs with mean AUC of 0.93 \u00b1 0.04. \u2022 Validation procedure resembles TRIPOD 2b and 3 categories, enhancing the quality of experimental design to test the cross-dataset domain shift between institutions aiding clinical integration. \u2022 Identification of COVID-19 pneumonia in the presence of community-acquired pneumonia and other comorbidities with an AUC of 0.92.", "journal": "European radiology", "date": "2022-04-02", "authors": ["YeshaswiniNagaraj", "Gondade Jonge", "AnnaAndreychenko", "GabrielePresti", "Matthias AFink", "NikolayPavlov", "Carlo CQuattrocchi", "SergeyMorozov", "RaymondVeldhuis", "MatthijsOudkerk", "Peter M Avan Ooijen"], "doi": "10.1007/s00330-022-08730-6\n10.1177/0846537120938328\n10.1109/RBME.2020.2990959\n10.1097/IM9.0000000000000022\n10.1148/radiol.2020201473\n10.1148/ryct.2020200152\n10.5152/dir.2021.201032\n10.1148/radiol.2020202439\n10.1038/s42256-021-00307-0\n10.1259/bjr.20160665\n10.1007/s00330-020-06939-x\n10.1038/nrclinonc.2017.141\n10.17816/DD46826\n10.21037/qims-20-782\n10.1007/s11604-018-0798-0\n10.1259/bjr.20181019\n10.1259/bjr.20200677\n10.1158/0008-5472.CAN-17-0339\n10.1038/srep37241\n10.1613/jair.953\n10.1038/s41551-018-0304-0\n10.15212/bioi-2020-0015\n10.1007/s00330-020-07032-z\n10.1148/ryct.2020200322\n10.1007/s00259-020-05075-4\n10.1038/s41598-021-83237-6\n10.1186/s12916-014-0241-z\n10.1186/s12967-020-02692-3\n10.1038/s41746-020-00369-1\n10.1148/radiol.2020200905\n10.1007/s00330-020-06956-w\n10.3390/diagnostics11010041\n10.1186/s12879-021-06331-0\n10.1016/j.media.2020.101844\n10.1002/mp.15178"}
{"title": "Applications of Artificial Intelligence in Myopia: Current and Future Directions.", "abstract": "With the continuous development of computer technology, big data acquisition and imaging methods, the application of artificial intelligence (AI) in medical fields is expanding. The use of machine learning and deep learning in the diagnosis and treatment of ophthalmic diseases is becoming more widespread. As one of the main causes of visual impairment, myopia has a high global prevalence. Early screening or diagnosis of myopia, combined with other effective therapeutic interventions, is very important to maintain a patient's visual function and quality of life. Through the training of fundus photography, optical coherence tomography, and slit lamp images and through platforms provided by telemedicine, AI shows great application potential in the detection, diagnosis, progression prediction and treatment of myopia. In addition, AI models and wearable devices based on other forms of data also perform well in the behavioral intervention of myopia patients. Admittedly, there are still some challenges in the practical application of AI in myopia, such as the standardization of datasets; acceptance attitudes of users; and ethical, legal and regulatory issues. This paper reviews the clinical application status, potential challenges and future directions of AI in myopia and proposes that the establishment of an AI-integrated telemedicine platform will be a new direction for myopia management in the post-COVID-19 period.", "journal": "Frontiers in medicine", "date": "2022-04-02", "authors": ["ChenchenZhang", "JingZhao", "ZheZhu", "YanxiaLi", "KeLi", "YuanpingWang", "YajuanZheng"], "doi": "10.3389/fmed.2022.840498\n10.1016/j.preteyeres.2019.04.003\n10.1016/j.ejrad.2019.02.038\n10.1016/j.jacc.2018.12.054\n10.3322/caac.21552\n10.1001/jama.2017.18152\n10.1001/jama.2016.17216\n10.1016/j.ophtha.2017.02.008\n10.1167/iovs.16-19964\n10.1001/jamaophthalmol.2017.3782\n10.1007/s00417-017-3850-3\n10.1016/j.ophtha.2017.10.031\n10.1016/j.ajo.2018.10.007\n10.1038/s41598-018-33013-w\n10.1016/j.ophtha.2018.04.020\n10.1016/j.ophtha.2016.05.029\n10.1167/iovs.18-23887\n10.21037/atm.2019.12.39\n10.1097/apo.0000000000000394\n10.1038/s41572-020-00231-4\n10.1609/aimag.v27i4.1904\n10.3390/s21134412\n10.1109/tpami.2013.50\n10.21037/atm-20-976\n10.1038/nature14539\n10.1016/j.ejca.2019.06.012\n10.1016/j.ejca.2019.06.013\n10.1016/j.ebiom.2019.04.055\n10.1007/978-3-030-33128-3_4\n10.1161/circulationaha.115.001593\n10.1016/j.preteyeres.2017.09.004\n10.1016/j.ophtha.2016.01.006\n10.1016/j.ajo.2020.07.034\n10.1001/jamaophthalmol.2020.6239\n10.1038/s41598-017-14507-5\n10.1016/j.preteyeres.2020.100900\n10.1109/iembs.2009.5333517\n10.1371/journal.pone.0065736\n10.1016/j.cmpb.2020.105920\n10.1016/j.oret.2021.02.006\n10.1016/s2589-7500(21)00055-8\n10.1136/bjophthalmol-2020-317825\n10.1371/journal.pone.0227240\n10.1136/bjophthalmol-2021-319129\n10.1371/journal.pmed.1002674\n10.3390/ijerph17020463\n10.3928/1081-597X-19980501-15\n10.1080/08820538.2019.1569075\n10.1001/jamaophthalmol.2020.0507\n10.1167/tvst.9.2.8\n10.1016/j.ajo.2019.10.015\n10.1159/000453528\n10.3390/bios11060182\n10.1097/apo.0000000000000293\n10.1016/j.ophtha.2017.08.027\n10.7717/peerj.7202\n10.1016/j.ajo.2019.04.019\n10.1136/bjophthalmol-2020-316193\n10.1016/j.jcrs.2016.12.021\n10.1016/j.jcrs.2019.08.014\n10.1136/bmjophth-2018-000251\n10.1016/j.ophtha.2007.12.019\n10.1016/j.ophtha.2012.04.020\n10.1167/tvst.6.3.20\n10.1007/s00417-016-3440-9\n10.1155/2018/9781987\n10.1097/md.0000000000017992\n10.1167/tvst.8.6.15\n10.1038/ng.2554\n10.1371/journal.pgen.1002753\n10.1016/j.exer.2019.107778\n10.1152/physiolgenomics.00119.2017\n10.1007/s00439-019-01970-5\n10.1186/s13073-019-0689-8\n10.1097/icu.0000000000000791\n10.1007/s11882-018-0808-4\n10.1056/NEJMp2003539\n10.1097/icl.0000000000000051\n10.1038/s41433-020-1085-8\n10.1136/bjophthalmol-2019-314729\n10.1001/jamanetworkopen.2018.5474\n10.1117/1.Jmi.7.1.012703\n10.1088/1361-6560/aada6d\n10.3390/s19102361\n10.1109/tmi.2018.2827462\n10.1016/j.media.2019.101552\n10.3348/kjr.2017.18.4.570\n10.1007/s10384-019-00659-6\n10.1001/jama.2018.11029\n10.1097/icu.0000000000000694\n10.1177/1120672120934405\n10.1001/amajethics.2019.160"}
{"title": "Deep-Precognitive Diagnosis: Preventing Future Pandemics by Novel Disease Detection With Biologically-Inspired Conv-Fuzzy Network.", "abstract": "Deep learning-based Computer-Aided Diagnosis has gained immense attention in recent years due to its capability to enhance diagnostic performance and elucidate complex clinical tasks. However, conventional supervised deep learning models are incapable of recognizing novel diseases that do not exist in the training dataset. Automated early-stage detection of novel infectious diseases can be vital in controlling their rapid spread. Moreover, the development of a conventional CAD model is only possible after disease outbreaks and datasets become available for training (viz. COVID-19 outbreak). Since novel diseases are unknown and cannot be included in training data, it is challenging to recognize them through existing supervised deep learning models. Even after data becomes available, recognizing new classes with conventional models requires a complete extensive re-training. The present study is the ", "journal": "IEEE access : practical innovations, open solutions", "date": "2022-04-02", "authors": ["AviralChharia", "RahulUpadhyay", "VinayKumar", "ChaoCheng", "JingZhang", "TianyangWang", "MinXu"], "doi": "10.1109/access.2022.3153059\n10.1007/s00500-020-05275-y\n10.17632/rscbjbr9sj.2\n10.1101/2020.05.10.20097063\n10.1109/TEM.2021.3059664"}
{"title": "DeBoNet: A deep bone suppression model ensemble to improve disease detection in chest radiographs.", "abstract": "Automatic detection of some pulmonary abnormalities using chest X-rays may be impacted adversely due to obscuring by bony structures like the ribs and the clavicles. Automated bone suppression methods would increase soft tissue visibility and enhance automated disease detection. We evaluate this hypothesis using a custom ensemble of convolutional neural network models, which we call DeBoNet, that suppresses bones in frontal CXRs. First, we train and evaluate variants of U-Nets, Feature Pyramid Networks, and other proposed custom models using a private collection of CXR images and their bone-suppressed counterparts. The DeBoNet, constructed using the top-3 performing models, outperformed the individual models in terms of peak signal-to-noise ratio (PSNR) (36.7977\u00b11.6207), multi-scale structural similarity index measure (MS-SSIM) (0.9848\u00b10.0073), and other metrics. Next, the best-performing bone-suppression model is applied to CXR images that are pooled from several sources, showing no abnormality and other findings consistent with COVID-19. The impact of bone suppression is demonstrated by evaluating the gain in performance in detecting pulmonary abnormality consistent with COVID-19 disease. We observe that the model trained on bone-suppressed CXRs (MCC: 0.9645, 95% confidence interval (0.9510, 0.9780)) significantly outperformed (p < 0.05) the model trained on non-bone-suppressed images (MCC: 0.7961, 95% confidence interval (0.7667, 0.8255)) in detecting findings consistent with COVID-19 indicating benefits derived from automatic bone suppression on disease classification. The code is available at https://github.com/sivaramakrishnan-rajaraman/Bone-Suppresion-Ensemble.", "journal": "PloS one", "date": "2022-04-01", "authors": ["SivaramakrishnanRajaraman", "GreggCohen", "LillianSpear", "LesFolio", "SameerAntani"], "doi": "10.1371/journal.pone.0265691\n10.1109/TMI.2017.2775636\n10.1148/radiol.2261011924\n10.3978/j.issn.2223-4292.2015.10.09\n10.1148/rg.261055034\n10.2214/AJR.10.4816\n10.1016/j.clinimag.2018.05.021\n10.1007/s13246-019-00822-w\n10.1016/j.media.2016.08.004\n10.1109/TMI.2006.871549\n10.1007/978-3-319-11776-8_47\n10.1148/radiol.11100153\n10.2214/AJR.09.2431\n10.1148/radiol.11110192\n10.1007/s00330-012-2550-y\n10.3390/diagnostics11050840\n10.1007/3-540-45014-9\n10.1371/journal.pone.0242301\n10.1016/j.scs.2020.102589\n10.1007/978-3-319-24574-4_28\n10.1007/978-3-030-03341-5_17\n10.3390/diagnostics9020038\n10.1148/ryai.2019180041\n10.1109/CVPRW.2017.151\n10.1109/CVPR.2016.90\n10.1109/CVPR.2018.00745\n10.1109/CVPR.2017.243\n10.1002/2014GB005021\n10.1109/CVPR.2018.00474\n10.1109/EMBC.2019.8856715"}
{"title": "Comparison and ensemble of 2D and 3D approaches for COVID-19 detection in CT images.", "abstract": "Detecting COVID-19 in computed tomography (CT) or radiography images has been proposed as a supplement to the RT-PCR test. We compare slice-based (2D) and volume-based (3D) approaches to this problem and propose a deep learning ensemble, called IST-CovNet, combining the best 2D and 3D systems with novel preprocessing and attention modules and the use of a bidirectional Long Short-Term Memory model for combining slice-level decisions. The proposed ensemble obtains 90.80% accuracy and 0.95 AUC score overall on the newly collected IST-C dataset in detecting COVID-19 among normal controls and other types of lung pathologies; and 93.69% accuracy and 0.99 AUC score on the publicly available MosMedData dataset that consists of COVID-19 scans and normal controls only. The system also obtains state-of-art results (90.16% accuracy and 0.94 AUC) on the COVID-CT-MD dataset which is only used for testing. The system is deployed at Istanbul University Cerrahpa\u015fa School of Medicine where it is used to automatically screen CT scans of patients, while waiting for RT-PCR tests or radiologist evaluation.", "journal": "Neurocomputing", "date": "2022-03-30", "authors": ["Sara AtitoAli Ahmed", "Mehmet CanYavuz", "Mehmet Umut\u015een", "FatihG\u00fcl\u015fen", "OnurTutar", "BoraKorkmazer", "CesurSamanc\u0131", "Sabri\u015eirolu", "RaufHamid", "Ali ErgunEry\u00fcrekli", "ToghrulMammadov", "BerrinYanikoglu"], "doi": "10.1016/j.neucom.2022.02.018"}
{"title": "Trends and hot topics in radiology, nuclear medicine and medical imaging from 2011-2021: a bibliometric analysis of highly cited papers.", "abstract": "To spotlight the trends and hot topics looming from the highly cited papers in the subject category of Radiology, Nuclear Medicine & Medical Imaging with bibliometric analysis.\nBased on the Essential Science Indicators, this study employed a bibliometric method to examine the highly cited papers in the subject category of Radiology, Nuclear Medicine & Medical Imaging in Web of Science (WoS) Categories, both quantitatively and qualitatively. In total, 1325 highly cited papers were retrieved and assessed spanning from the years of 2011 to 2021. In particular, the bibliometric information of the highly cited papers based on WoS database such as the main publication venues, the most productive countries, and the top cited publications was presented. An Abstract corpus was built to help identify the most frequently explored topics. VoSviewer was used to visualize the co-occurrence networks of author keywords.\nThe top three active journals are Neuroimage, Radiology and IEEE T Med Imaging. The United States, Germany and England have the most influential publications. The top cited publications unrelated to COVID-19 can be grouped in three categories: recommendations or guidelines, processing software, and analysis methods. The top cited publications on COVID-19 are dominantly in China. The most frequently explored topics based on the Abstract corpus and the author keywords with the great link strengths overlap to a great extent. Specifically, phrases such as magnetic resonance imaging, deep learning, prostate cancer, chest CT, computed tomography, CT images, coronavirus disease, convolutional neural network(s) are among the most frequently mentioned.\nThe bibliometric analysis of the highly cited papers provided the most updated trends and hot topics which may provide insights and research directions for medical researchers and healthcare practitioners in the future.", "journal": "Japanese journal of radiology", "date": "2022-03-29", "authors": ["ShengYan", "HuitingZhang", "JunWang"], "doi": "10.1007/s11604-022-01268-z\n10.1023/B:SCIE.0000018529.58334.eb\n10.1002/asi.21454\n10.1007/s11192-007-1859-9\n10.1007/s11192-011-0416-8\n10.1007/s11192-007-1913-7\n10.1007/s11192-015-1699-y\n10.1093/reseval/rvu002\n10.1016/j.joi.2018.09.006\n10.1038/514561a\n10.3152/147154403781776645\n10.1209/0295-5075/105/28002\n10.1209/0295-5075/86/68001\n10.1177/0165551519877049\n10.1007/s11192-007-2068-x\n10.1016/j.joi.2015.05.007\n10.1177/0266666912458515\n10.1093/applin/amy003\n10.1016/j.omega.2018.11.005\n10.1177/0165551518761013\n10.1186/s40537-017-0088-1\n10.1186/s12911-018-0594-x\n10.3390/su12156058\n10.1371/journal.pbio.1002541\n10.1016/j.ijpe.2015.01.003\n10.1016/j.joi.2015.08.001\n10.1093/ehjci/jev014\n10.1186/s13000-021-01085-4\n10.1001/jama.2016.17216\n10.1001/jama.2017.14580\n10.1148/radiol.2020200905\n10.1371/journal.pone.0252573"}
{"title": "Transfer learning with fine-tuned deep CNN ResNet50 model for classifying COVID-19 from chest X-ray images.", "abstract": "COVID-19 cases are putting pressure on healthcare systems all around the world. Due to the lack of available testing kits, it is impractical for screening every patient with a respiratory ailment using traditional methods (RT-PCR). In addition, the tests have a high turn-around time and low sensitivity. Detecting suspected COVID-19 infections from the chest X-ray might help isolate high-risk people before the RT-PCR test. Most healthcare systems already have X-ray equipment, and because most current X-ray systems have already been computerized, there is no need to transfer the samples. The use of a chest X-ray to prioritize the selection of patients for subsequent RT-PCR testing is the motivation of this work. Transfer learning (TL) with fine-tuning on deep convolutional neural network-based ResNet50 model has been proposed in this work to classify COVID-19 patients from the COVID-19 Radiography Database. Ten distinct pre-trained weights, trained on varieties of large-scale datasets using various approaches such as supervised learning, self-supervised learning, and others, have been utilized in this work. Our proposed ", "journal": "Informatics in medicine unlocked", "date": "2022-03-29", "authors": ["Md BelalHossain", "S M Hasan SazzadIqbal", "Md MonirulIslam", "Md NasimAkhtar", "Iqbal HSarker"], "doi": "10.1016/j.imu.2022.100916"}
{"title": "A deep learning-based framework for detecting COVID-19 patients using chest X-rays.", "abstract": "Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) has caused outbreaks of new coronavirus disease (COVID-19) around the world. Rapid and accurate detection of COVID-19 coronavirus is an important step in limiting the spread of the COVID-19 epidemic. To solve this problem, radiography techniques (such as chest X-rays and computed tomography (CT)) can play an important role in the early prediction of COVID-19 patients, which will help to treat patients in a timely manner. We aimed to quickly develop a highly efficient lightweight CNN architecture for detecting COVID-19-infected patients. The purpose of this paper is to propose a robust deep learning-based system for reliably detecting COVID-19 from chest X-ray images. First, we evaluate the performance of various pre-trained deep learning models (InceptionV3, Xception, MobileNetV2, NasNet and DenseNet201) recently proposed for medical image classification. Second, a lightweight shallow convolutional neural network (CNN) architecture is proposed for classifying X-ray images of a patient with a low false-negative rate. The data set used in this work contains 2,541 chest X-rays from two different public databases, which have confirmed COVID-19 positive and healthy cases. The performance of the proposed model is compared with the performance of pre-trained deep learning models. The results show that the proposed shallow CNN provides a maximum accuracy of 99.68% and more importantly sensitivity, specificity and AUC of 99.66%, 99.70% and 99.98%. The proposed model has fewer parameters and low complexity compared to other deep learning models. The experimental results of our proposed method show that it is superior to the existing state-of-the-art methods. We believe that this model can help healthcare professionals to treat COVID-19 patients through improved and faster patient screening.", "journal": "Multimedia systems", "date": "2022-03-29", "authors": ["SohaibAsif", "MingZhao", "FengxiaoTang", "YusenZhu"], "doi": "10.1007/s00530-022-00917-7\n10.1001/jama.2020.2648\n10.1016/j.bj.2020.05.016\n10.1016/S2213-2600(20)30167-3\n10.1016/S0140-6736(20)30211-7\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1148/radiol.2020200343\n10.1109/RBME.2020.2990959\n10.1148/radiol.2020200527\n10.1109/ACCESS.2017.2762703\n10.1007/s11548-017-1696-0\n10.3390/app10020559\n10.1148/radiol.2017162326\n10.1038/s41598-020-76550-z\n10.1007/s13246-020-00865-4\n10.1016/j.chaos.2020.109944\n10.1016/j.patrec.2020.09.010\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1109/ACCESS.2020.2994762\n10.1016/j.cmpb.2020.105581\n10.1016/j.mehy.2020.109761\n10.1080/07391102.2020.1767212\n10.1007/s42979-021-00695-5\n10.1007/s10044-021-00984-y\n10.1016/j.irbm.2020.07.001\n10.1007/s00330-021-07715-1\n10.3390/v12070769\n10.32604/cmc.2022.020140\n10.1016/j.patrec.2021.06.021\n10.1109/TKDE.2009.191\n10.1109/ACCESS.2020.3010287\n10.1016/j.fss.2007.12.023\n10.1109/ACCESS.2020.3016780\n10.1016/j.imu.2020.100360\n10.1016/j.compbiomed.2020.103805\n10.1016/j.chaos.2020.110122\n10.1007/s10489-020-01943-6\n10.1371/journal.pone.0242535\n10.1016/j.bspc.2021.102490\n10.1016/j.chaos.2021.110713"}
{"title": "Reimagining the status quo: How close are we to rapid sputum-free tuberculosis diagnostics for all?", "abstract": "Rapid, accurate, sputum-free tests for tuberculosis (TB) triage and confirmation are urgently needed to close the widening diagnostic gap. We summarise key technologies and review programmatic, systems, and resource issues that could affect the impact of diagnostics. Mid-to-early-stage technologies like artificial intelligence-based automated digital chest X-radiography and capillary blood point-of-care assays are particularly promising. Pitfalls in the diagnostic pipeline, included a lack of community-based tools. We outline how these technologies may complement one another within the context of the TB care cascade, help overturn current paradigms (eg, reducing syndromic triage reliance, permitting subclinical TB to be diagnosed), and expand options for extra-pulmonary TB. We review challenges such as the difficulty of detecting paucibacillary TB and the limitations of current reference standards, and discuss how researchers and developers can better design and evaluate assays to optimise programmatic uptake. Finally, we outline how leveraging the urgency and innovation applied to COVID-19 is critical to improving TB patients' diagnostic quality-of-care.", "journal": "EBioMedicine", "date": "2022-03-28", "authors": ["Ruvandhi RNathavitharana", "Alberto LGarcia-Basteiro", "MortenRuhwald", "FrankCobelens", "GrantTheron"], "doi": "10.1016/j.ebiom.2022.103939"}
{"title": "Comparison of CO-RADS Scores Based on Visual and Artificial Intelligence Assessments in a Non-Endemic Area.", "abstract": "In this study, we first developed an artificial intelligence (AI)-based algorithm for classifying chest computed tomography (CT) images using the coronavirus disease 2019 Reporting and Data System (CO-RADS). Subsequently, we evaluated its accuracy by comparing the calculated scores with those assigned by radiologists with varying levels of experience. This study included patients with suspected SARS-CoV-2 infection who underwent chest CT imaging between February and October 2020 in Japan, a non-endemic area. For each chest CT, the CO-RADS scores, determined by consensus among three experienced chest radiologists, were used as the gold standard. Images from 412 patients were used to train the model, whereas images from 83 patients were tested to obtain AI-based CO-RADS scores for each image. Six independent raters (one medical student, two residents, and three board-certified radiologists) evaluated the test images. Intraclass correlation coefficients (ICC) and weighted kappa values were calculated to determine the inter-rater agreement with the gold standard. The mean ICC and weighted kappa were 0.754 and 0.752 for the medical student and residents (taken together), 0.851 and 0.850 for the diagnostic radiologists, and 0.913 and 0.912 for AI, respectively. The CO-RADS scores calculated using our AI-based algorithm were comparable to those assigned by radiologists, indicating the accuracy and high reproducibility of our model. Our study findings would enable accurate reading, particularly in areas where radiologists are unavailable, and contribute to improvements in patient management and workflow.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-03-26", "authors": ["YoshinobuIshiwata", "KentaroMiura", "MayukoKishimoto", "KoichiroNomura", "ShungoSawamura", "ShigeruMagami", "MizukiIkawa", "TsuneoYamashiro", "DaisukeUtsunomiya"], "doi": "10.3390/diagnostics12030738\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.2214/AJR.20.23418\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200490\n10.1007/s00330-020-06928-0\n10.1148/radiol.2020201473\n10.1148/radiol.2020202708\n10.1016/j.chest.2020.11.026\n10.1007/s00330-020-07273-y\n10.3390/diagnostics10090608\n10.1007/s11604-020-01009-0\n10.1007/s11604-020-00986-6\n10.1007/s11604-020-01070-9\n10.1148/radiol.2020200905\n10.1038/s41591-020-0931-3\n10.1109/ACCESS.2021.3120717\n10.1097/MD.0000000000026161\n10.2196/19569\n10.1007/s00330-020-07044-9\n10.1148/radiol.2020202439\n10.1038/bmt.2012.244\n10.1259/bjro.20200053\n10.3390/ijerph18189804\n10.1038/s41467-020-18786-x\n10.1002/rmv.2146\n10.1038/s41598-020-80061-2"}
{"title": "The Role of 3D CT Imaging in the Accurate Diagnosis of Lung Function in Coronavirus Patients.", "abstract": "Early grading of coronavirus disease 2019 (COVID-19), as well as ventilator support machines, are prime ways to help the world fight this virus and reduce the mortality rate. To reduce the burden on physicians, we developed an automatic Computer-Aided Diagnostic (CAD) system to grade COVID-19 from Computed Tomography (CT) images. This system segments the lung region from chest CT scans using an unsupervised approach based on an appearance model, followed by 3D rotation invariant Markov-Gibbs Random Field (MGRF)-based morphological constraints. This system analyzes the segmented lung and generates precise, analytical imaging markers by estimating the MGRF-based analytical potentials. Three Gibbs energy markers were extracted from each CT scan by tuning the MGRF parameters on each lesion separately. The latter were healthy/mild, moderate, and severe lesions. To represent these markers more reliably, a Cumulative Distribution Function (CDF) was generated, then statistical markers were extracted from it, namely, 10th through 90th CDF percentiles with 10% increments. Subsequently, the three extracted markers were combined together and fed into a backpropagation neural network to make the diagnosis. The developed system was assessed on 76 COVID-19-infected patients using two metrics, namely, accuracy and Kappa. In this paper, the proposed system was trained and tested by three approaches. In the first approach, the MGRF model was trained and tested on the lungs. This approach achieved 95.83% accuracy and 93.39% kappa. In the second approach, we trained the MGRF model on the lesions and tested it on the lungs. This approach achieved 91.67% accuracy and 86.67% kappa. Finally, we trained and tested the MGRF model on lesions. It achieved 100% accuracy and 100% kappa. The results reported in this paper show the ability of the developed system to accurately grade COVID-19 lesions compared to other machine learning classifiers, such as k-Nearest Neighbor (KNN), decision tree, na\u00efve Bayes, and random forest.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-03-26", "authors": ["Ibrahim ShawkyFarahat", "AhmedSharafeldeen", "MohamedElsharkawy", "AhmedSoliman", "AliMahmoud", "MohammedGhazal", "FatmaTaher", "MahaBilal", "Ahmed Abdel KhalekAbdel Razek", "WaleedAladrousy", "SamirElmougy", "Ahmed ElsaidTolba", "MoumenEl-Melegy", "AymanEl-Baz"], "doi": "10.3390/diagnostics12030696\n10.1016/S0140-6736(20)30185-9\n10.1002/ctm2.158\n10.1038/s41598-021-91305-0\n10.1148/radiol.2020200490\n10.1148/radiol.2020200463\n10.1016/j.acra.2020.03.003\n10.1148/radiol.2020200642\n10.1007/s00330-020-07087-y\n10.1007/s00330-020-07044-9\n10.1016/j.media.2021.102054\n10.1016/j.ijleo.2021.167199\n10.1145/3065386\n10.1101/2020.03.12.20027185\n10.1016/j.asoc.2020.106897\n10.1007/978-3-030-00889-5_1\n10.3390/s21165482\n10.1038/s41598-021-83735-7\n10.3390/diagnostics12020461\n10.1007/s11749-016-0481-7\n10.1002/widm.8\n10.1038/nbt1206-1565\n10.1001/jamainternmed.2020.0994\n10.1001/jama.2020.4326\n10.1001/jama.2020.6775\n10.32604/cmc.2020.010691\n10.1371/journal.pone.0240200\n10.1007/978-3-319-24574-4_28\n10.1016/j.media.2020.101913\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2020.103795\n10.1109/42.816070\n10.1109/TCBB.2021.3065361\n10.1007/978-3-030-01264-9_26\n10.1016/j.ejrad.2020.109041"}
{"title": "Four Types of Multiclass Frameworks for Pneumonia Classification and Its Validation in X-ray Scans Using Seven Types of Deep Learning Artificial Intelligence Models.", "abstract": "Background and Motivation: The novel coronavirus causing COVID-19 is exceptionally contagious, highly mutative, decimating human health and life, as well as the global economy, by consistent evolution of new pernicious variants and outbreaks. The reverse transcriptase polymerase chain reaction currently used for diagnosis has major limitations. Furthermore, the multiclass lung classification X-ray systems having viral, bacterial, and tubercular classes\u2014including COVID-19\u2014are not reliable. Thus, there is a need for a robust, fast, cost-effective, and easily available diagnostic method. Method: Artificial intelligence (AI) has been shown to revolutionize all walks of life, particularly medical imaging. This study proposes a deep learning AI-based automatic multiclass detection and classification of pneumonia from chest X-ray images that are readily available and highly cost-effective. The study has designed and applied seven highly efficient pre-trained convolutional neural networks\u2014namely, VGG16, VGG19, DenseNet201, Xception, InceptionV3, NasnetMobile, and ResNet152\u2014for classification of up to five classes of pneumonia. Results: The database consisted of 18,603 scans with two, three, and five classes. The best results were using DenseNet201, VGG16, and VGG16, respectively having accuracies of 99.84%, 96.7%, 92.67%; sensitivity of 99.84%, 96.63%, 92.70%; specificity of 99.84, 96.63%, 92.41%; and AUC of 1.0, 0.97, 0.92 (p < 0.0001 for all), respectively. Our system outperformed existing methods by 1.2% for the five-class model. The online system takes <1 s while demonstrating reliability and stability. Conclusions: Deep learning AI is a powerful paradigm for multiclass pneumonia classification.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-03-26", "authors": ["NoneNillmani", "Pankaj KJain", "NeerajSharma", "Mannudeep KKalra", "KlaudijaViskovic", "LucaSaba", "Jasjit SSuri"], "doi": "10.3390/diagnostics12030652\n10.1111/cns.13372\n10.1056/NEJMoa2001017\n10.23750/abm.v91i1.9397\n10.1038/s41579-020-00468-6\n10.7759/cureus.7423\n10.1007/s10554-020-02089-9\n10.4239/wjd.v12.i3.215\n10.1016/j.clinimag.2021.05.016\n10.1001/jama.2020.25381\n10.1001/jama.2020.11787\n10.1001/jamainternmed.2020.2306\n10.1136/bmj.n230\n10.1093/pubmed/fdaa165\n10.1001/jama.2020.3786\n10.4081/jphr.2021.2270\n10.2807/1560-7917.ES.2020.25.50.2000568\n10.1002/jmv.25786\n10.3390/diagnostics10030165\n10.1148/ryct.2020200034\n10.1016/j.jinf.2020.03.007\n10.1148/radiol.2020200230\n10.1097/RTI.0000000000000404\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(20)30183-5\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1093/clinchem/hvaa029\n10.1002/jmv.25674\n10.2214/AJR.20.22969\n10.2214/AJR.20.23034\n10.2741/4725\n10.1016/j.ejrad.2019.02.038\n10.1016/j.compbiomed.2021.104803\n10.1016/j.irbm.2018.08.002\n10.1016/j.bbe.2020.07.001\n10.1016/j.bbe.2016.10.006\n10.1109/JBHI.2017.2715078\n10.1007/978-3-319-97982-3_16\n10.1109/TIP.2017.2725580\n10.1109/TIP.2018.2809606\n10.1109/TCYB.2020.2983860\n10.1016/j.compbiomed.2022.105273\n10.1016/j.ecoinf.2020.101093\n10.1109/TPAMI.2015.2491929\n10.1109/ACCESS.2019.2927169\n10.1016/j.neucom.2016.09.010\n10.1016/j.isprsjprs.2017.07.014\n10.1109/TPAMI.2019.2950923\n10.1016/j.compbiomed.2020.103804\n10.3390/cancers11010111\n10.1080/21681163.2020.1818628\n10.1016/j.compmedimag.2018.09.004\n10.1016/j.patrec.2019.03.022\n10.1109/ACCESS.2019.2913847\n10.1007/s00296-021-05062-4\n10.1007/s11883-018-0736-8\n10.3390/diagnostics11122257\n10.1016/j.compbiomed.2021.104721\n10.1007/s11517-019-02099-3\n10.1007/s10916-017-0745-0\n10.1016/j.compbiomed.2016.11.011\n10.1016/j.measurement.2019.05.076\n10.1007/s11548-021-02317-0\n10.1016/j.eswa.2017.11.028\n10.1007/s00138-020-01069-2\n10.1145/3331453.3361658\n10.1109/LGRS.2018.2876378\n10.3390/rs11111374\n10.1109/TGRS.2018.2868851\n10.1109/ACCESS.2020.3010287\n10.1016/j.chaos.2020.110495\n10.1007/s10489-020-01902-1\n10.1101/2020.03.30.20047787\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.09.010\n10.1038/s41598-021-99015-3\n10.1016/j.bspc.2020.102365\n10.1016/j.bspc.2021.103182\n10.1016/j.bspc.2021.103126\n10.1007/s13755-021-00166-4\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2021.104319\n10.1016/j.cell.2018.02.010\n10.1109/ACCESS.2020.3031384\n10.1016/j.compbiomed.2020.103958\n10.1007/s10554-020-02124-9\n10.1007/s11517-021-02322-0\n10.1186/s40537-019-0197-0\n10.1016/j.knosys.2021.107517\n10.1080/17476348.2021.1826315\n10.1109/ACCESS.2021.3085240\n10.3390/biology11010125\n10.1016/j.bspc.2016.03.001\n10.3390/diagnostics11112109\n10.1117/1.JMI.8.S1.014001\n10.1038/s42003-020-01535-7\n10.1007/s10916-017-0797-1\n10.1109/ACCESS.2020.3003810\n10.1016/j.ibmed.2021.100034\n10.1038/s41598-021-90411-3"}
{"title": "Automatic Deep-Learning Segmentation of Epicardial Adipose Tissue from Low-Dose Chest CT and Prognosis Impact on COVID-19.", "abstract": "Background: To develop a deep-learning (DL) pipeline that allowed an automated segmentation of epicardial adipose tissue (EAT) from low-dose computed tomography (LDCT) and investigate the link between EAT and COVID-19 clinical outcomes. Methods: This monocentric retrospective study included 353 patients: 95 for training, 20 for testing, and 238 for prognosis evaluation. EAT segmentation was obtained after thresholding on a manually segmented pericardial volume. The model was evaluated with Dice coefficient (DSC), inter-and intraobserver reproducibility, and clinical measures. Uni-and multi-variate analyzes were conducted to assess the prognosis value of the EAT volume, EAT extent, and lung lesion extent on clinical outcomes, including hospitalization, oxygen therapy, intensive care unit admission and death. Results: The mean DSC for EAT volumes was 0.85 \u00b1 0.05. For EAT volume, the mean absolute error was 11.7 \u00b1 8.1 cm3 with a non-significant bias of \u22124.0 \u00b1 13.9 cm3 and a correlation of 0.963 with the manual measures (p < 0.01). The multivariate model providing the higher AUC to predict adverse outcome include both EAT extent and lung lesion extent (AUC = 0.805). Conclusions: A DL algorithm was developed and evaluated to obtain reproducible and precise EAT segmentation on LDCT. EAT extent in association with lung lesion extent was associated with adverse clinical outcomes with an AUC = 0.805.", "journal": "Cells", "date": "2022-03-26", "authors": ["AxelBartoli", "JorisFournel", "L\u00e9aAit-Yahia", "FarahCadour", "FaroukTradi", "BadihGhattas", "S\u00e9bastienCortaredona", "MatthieuMillion", "Ad\u00e8leLasbleiz", "AnneDutour", "B\u00e9n\u00e9dicteGaborit", "AlexisJacquier"], "doi": "10.3390/cells11061034\n10.5935/abc.20130138\n10.1002/cphy.c160034\n10.14797/mdcj-13-1-20\n10.1016/j.jacc.2012.11.062\n10.1016/j.amjcard.2008.04.002\n10.1016/j.numecd.2009.10.010\n10.1016/j.atherosclerosis.2012.02.029\n10.1016/j.atherosclerosis.2011.09.041\n10.1093/eurheartj/ehaa471\n10.1172/JCI137647\n10.3389/fendo.2021.726967\n10.18087/cardio.2021.8.n1638\n10.1080/00015385.2021.2010009\n10.1016/j.metabol.2020.154436\n10.1016/j.intimp.2020.107174\n10.1016/j.acra.2020.09.012\n10.1016/j.numecd.2021.04.020\n10.1016/j.media.2017.07.005\n10.2174/1874431101004010126\n10.1016/j.atherosclerosis.2009.08.032\n10.3390/jcm10235650\n10.1016/j.mri.2012.05.001\n10.1016/j.diii.2021.10.001\n10.1007/s13139-012-0175-3\n10.1109/TMI.2018.2804799\n10.1016/j.cmpb.2020.105395\n10.1007/s10554-021-02276-2\n10.1161/JAHA.117.006379\n10.1118/1.4927375\n10.1016/j.compbiomed.2019.103424\n10.1148/ryai.2019190045\n10.1016/j.metabol.2020.154319\n10.1186/s12933-021-01327-1\n10.1016/j.recesp.2021.07.005\n10.2337/db15-0399\n10.1111/obr.13225\n10.1161/CIRCULATIONAHA.120.052009"}
{"title": "Review of Machine Learning in Lung Ultrasound in COVID-19 Pandemic.", "abstract": "Ultrasound imaging of the lung has played an important role in managing patients with COVID-19-associated pneumonia and acute respiratory distress syndrome (ARDS). During the COVID-19 pandemic, lung ultrasound (LUS) or point-of-care ultrasound (POCUS) has been a popular diagnostic tool due to its unique imaging capability and logistical advantages over chest X-ray and CT. Pneumonia/ARDS is associated with the sonographic appearances of pleural line irregularities and B-line artefacts, which are caused by interstitial thickening and inflammation, and increase in number with severity. Artificial intelligence (AI), particularly machine learning, is increasingly used as a critical tool that assists clinicians in LUS image reading and COVID-19 decision making. We conducted a systematic review from academic databases (PubMed and Google Scholar) and preprints on arXiv or TechRxiv of the state-of-the-art machine learning technologies for LUS images in COVID-19 diagnosis. Openly accessible LUS datasets are listed. Various machine learning architectures have been employed to evaluate LUS and showed high performance. This paper will summarize the current development of AI for COVID-19 management and the outlook for emerging trends of combining AI-based LUS with robotics, telehealth, and other techniques.", "journal": "Journal of imaging", "date": "2022-03-25", "authors": ["JingWang", "XiaofengYang", "BoranZhou", "James JSohn", "JunZhou", "Jesse TJacob", "Kristin AHiggins", "Jeffrey DBradley", "TianLiu"], "doi": "10.3390/jimaging8030065\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30183-5\n10.1002/jum.15417\n10.15585/mmwr.mm6924e2\n10.1109/RBME.2020.2990959\n10.1515/dx-2020-0058\n10.1186/s13089-020-00171-w\n10.1136/postgradmedj-2020-138137\n10.6061/clinics/2020/e2027\n10.2196/19673\n10.1109/TUFFC.2020.3020055\n10.3389/fmed.2020.00375\n10.1016/j.ultrasmedbio.2020.05.012\n10.1186/2110-5820-4-1\n10.5644/ama2006-124.162\n10.1136/bmj.m4944\n10.1136/bmj.n158\n10.1136/bmj.m4857\n10.1016/j.ejro.2020.100231\n10.1016/j.ultrasmedbio.2020.04.026\n10.2214/AJR.20.23513\n10.21203/rs.2.24369/v1\n10.1016/j.ultrasmedbio.2020.09.014\n10.4269/ajtmh.20-0280\n10.14366/usg.20084\n10.1186/s13089-020-00198-z\n10.1002/jum.15284\n10.1016/j.ultrasmedbio.2020.05.006\n10.1097/CCM.0b013e31824e68ae\n10.1007/s11547-008-0247-8\n10.1002/uog.22028\n10.1002/jum.15285\n10.1183/20734735.004717\n10.1007/s13089-011-0066-3\n10.1378/chest.07-2800\n10.7861/clinmed.2020-0123\n10.1080/17476348.2019.1565997\n10.1109/JBHI.2019.2936151\n10.1111/anae.15082\n10.1016/j.advms.2020.06.005\n10.15557/JoU.2020.0025\n10.1213/ANE.0000000000004929\n10.1093/ehjci/jeaa163\n10.1007/s00134-020-05996-6\n10.1002/jum.15508\n10.1002/uog.22034\n10.1007/s00134-020-06048-9\n10.1016/j.acra.2020.07.002\n10.1007/s00134-020-06212-1\n10.1136/bmj.m1328\n10.1038/s41598-020-76550-z\n10.3390/app11020672\n10.1109/TMI.2020.2994459\n10.1177/15533506211018671\n10.1109/TUFFC.2020.3005512\n10.1109/TMI.2009.2024415\n10.1109/TGRS.2016.2616949\n10.1109/TUFFC.2021.3107598\n10.1371/journal.pone.0255886\n10.1016/0165-0114(95)00133-6\n10.1016/j.ejmp.2021.02.023\n10.1121/10.0004855\n10.1002/jum.15902\n10.1016/j.imu.2021.100687\n10.1121/10.0007272\n10.36227/techrxiv.17912387.v2\n10.1109/TUFFC.2021.3068190\n10.3390/s21165486\n10.1016/j.compbiomed.2021.104296\n10.1016/j.inffus.2021.02.013\n10.1007/s13755-021-00154-8\n10.1109/TUFFC.2020.3002249\n10.1136/bmjopen-2020-045120\n10.1016/j.media.2021.101975\n10.1016/j.inffus.2021.05.015\n10.1186/s12938-021-00863-x\n10.1109/ACCESS.2020.3016780\n10.1016/j.compbiomed.2021.104742\n10.7150/ijbs.58855\n10.1111/exsy.12759\n10.21037/atm-20-3043\n10.1016/j.acra.2020.04.032\n10.1016/j.jcrc.2015.08.021\n10.1002/jum.15765\n10.3389/frobt.2021.610677\n10.3389/frobt.2021.645756\n10.1002/jum.15406\n10.3389/frobt.2021.645424\n10.1007/s11227-021-04166-9\n10.1007/s00521-021-06396-7"}
{"title": "Applying Machine Learning with Localized Surface Plasmon Resonance Sensors to Detect SARS-CoV-2 Particles.", "abstract": "The sudden outbreak of COVID-19 rapidly developed into a global pandemic, which caused tens of millions of infections and millions of deaths. Although SARS-CoV-2 is known to cause COVID-19, effective approaches to detect SARS-CoV-2 using a convenient, rapid, accurate, and low-cost method are lacking. To date, most of the diagnostic methods for patients with early infections are limited to the detection of viral nucleic acids via polymerase chain reaction (PCR), or antigens, using an enzyme-linked immunosorbent assay or a chemiluminescence immunoassay. This study developed a novel method that uses localized surface plasmon resonance (LSPR) sensors, optical imaging, and artificial intelligence methods to directly detect the SARS-CoV-2 virus particles without any sample preparation. The virus concentration can be qualitatively and quantitatively detected in the range of 125.28 to 106 vp/mL through a few steps within 12 min with a limit of detection (LOD) of 100 vp/mL. The accuracy of the SARS-CoV-2 positive or negative assessment was found to be greater than 97%, and this was demonstrated by establishing a regression machine learning model for the virus concentration prediction (R2 > 0.95).", "journal": "Biosensors", "date": "2022-03-25", "authors": ["JiaweiLiang", "WeiZhang", "YuQin", "YingLi", "Gang LoganLiu", "WenjunHu"], "doi": "10.3390/bios12030173\n10.1038/s41591-020-0796-5\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1080/14737159.2020.1757437\n10.1016/j.cmi.2020.04.001\n10.2807/ese.17.49.20334-en\n10.7150/thno.48076\n10.1016/j.bios.2020.112686\n10.1080/22221751.2020.1745095\n10.1021/ac1000338\n10.1016/j.bios.2020.112685\n10.1016/j.bios.2021.113669\n10.1021/acs.analchem.0c05300\n10.1021/acsnano.1c00957\n10.1021/acsnano.0c02439\n10.1038/s41598-022-05036-x\n10.1038/lsa.2014.3\n10.3390/s16060870\n10.1038/srep06789\n10.3390/s18010098\n10.1016/j.bios.2005.09.010\n10.1016/j.talanta.2018.09.096\n10.1016/j.talanta.2020.121627\n10.1166/jbn.2019.2769\n10.1088/0957-4484/22/36/365203\n10.1021/acs.analchem.6b02484\n10.1016/j.snb.2021.130711\n10.3390/bios7020023\n10.1186/1471-2164-7-252\n10.1093/nar/gkv007\n10.1161/CIRCULATIONAHA.115.001593\n10.1139/gen-2020-0131\n10.1016/0893-6080(96)00038-X\n10.1016/j.neunet.2009.01.012\n10.1016/S0004-3702(97)00063-5\n10.1016/S0893-6080(03)00169-2"}
{"title": "Innovation in Gastroenterology-Can We Do Better?", "abstract": "The health system can reap significant benefits by adopting and implementing innovative measures, as was recently demonstrated and emphasized during the COVID-19 pandemic. Herein, we present our bird's eye view of gastroenterology's innovative technologies via utilizing a text-mining technique. We analyzed five research fields that comply with innovation: artificial intelligence (AI), virtual reality (VR), telemedicine, the microbiome, and advanced endoscopy. According to gastroenterology literature, the two most innovative fields were the microbiome and advanced endoscopy. Though artificial intelligence (AI), virtual reality (VR), and telemedicine trailed behind, the number of AI publications in gastroenterology has shown an exponential trend in the last couple of years. While VR and telemedicine are neglected compared to other fields, their implementation could improve physician and patient training, patient access to care, cost reduction, and patient outcomes.", "journal": "Biomimetics (Basel, Switzerland)", "date": "2022-03-25", "authors": ["EyalKlang", "ShellySoffer", "AbrahamTsur", "EyalShachar", "AdiLahat"], "doi": "10.3390/biomimetics7010033\n10.1016/j.jss.2013.01.013\n10.1007/s11192-012-0900-9\n10.1038/s41579-019-0213-6\n10.1053/j.gastro.2020.09.056\n10.1016/j.jcmgh.2021.08.013\n10.1126/science.abb5920\n10.1002/lsm.23102\n10.1016/j.gie.2008.12.049\n10.1016/j.cgh.2014.08.004\n10.5009/gnl19392\n10.1016/j.gie.2017.10.028\n10.1055/a-0632-1927\n10.1038/s43588-021-00114-y\n10.1038/s43588-021-00124-w\n10.1038/s41467-021-24470-5\n10.1136/gutjnl-2019-318343\n10.1093/ibd/izx007\n10.1093/ecco-jcc/jjx014\n10.1001/jamanetworkopen.2019.3721\n10.5946/ce.2020.082\n10.1136/gutjnl-2021-324471\n10.2147/CEG.S292857\n10.1016/j.gie.2020.04.039\n10.1016/j.gie.2019.11.012\n10.1053/j.gastro.2019.08.058\n10.1177/1756284821989178\n10.14309/ajg.0000000000000767\n10.1038/s41395-018-0272-8\n10.1002/hep.30074\n10.1177/1357633X16674087\n10.2196/jmir.3117\n10.1097/00000658-200210000-00008\n10.1016/j.suronc.2011.04.005\n10.2196/25499\n10.5694/mja17.00540\n10.1093/pm/pnx109\n10.1212/WNL.0000000000004585\n10.1055/a-0894-4400\n10.3748/wjg.v24.i48.5439"}
{"title": "The inherent flexibility of receptor binding domains in SARS-CoV-2 spike protein.", "abstract": "Spike (S) protein is the primary antigenic target for neutralization and vaccine development for the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It decorates the virus surface and undergoes large motions of its receptor binding domains (RBDs) to enter the host cell. Here, we observe Down, one-Up, one-Open, and two-Up-like structures in enhanced molecular dynamics simulations, and characterize the transition pathways via inter-domain interactions. Transient salt-bridges between RBD", "journal": "eLife", "date": "2022-03-25", "authors": ["Hisham MDokainish", "SuyongRe", "TakaharuMori", "ChigusaKobayashi", "JaewoonJung", "YujiSugita"], "doi": "10.7554/eLife.75720\n10.1039/D1ME00119A\n10.1016/0021-9991(83)90014-1\n10.1038/s41467-021-23328-0\n10.1038/s41586-020-2852-1\n10.1016/j.cell.2020.06.025\n10.1039/d1sc00244a\n10.1063/1.2408420\n10.1063/1.3073889\n10.1111/bcpt.13537\n10.1126/science.abd4251\n10.1021/acs.jctc.1c00552\n10.1038/s41586-021-04385-3\n10.1021/acscentsci.0c01056\n10.1021/acs.jctc.0c01144\n10.1016/j.tibs.2010.04.009\n10.1007/s10822-020-00356-4\n10.3390/ijms22010270\n10.1038/s41594-020-00547-5\n10.1063/1.470117\n10.1038/s41467-020-17371-6\n10.1038/s41586-020-2739-1\n10.1126/science.abi6226\n10.1038/s41598-020-71748-7\n10.1063/5.0011141\n10.1021/ct200328p\n10.1126/sciadv.abh3032\n10.1038/s41579-021-00573-0\n10.1038/s41594-020-0479-4\n10.1101/2020.06.26.173765\n10.1126/science.abd0826\n10.1038/s41579-020-00459-7\n10.1038/nmeth.4067\n10.1016/0263-7855(96)00018-5\n10.1021/ci049714+\n10.1063/1.5008438\n10.1063/5.0027873\n10.1002/jcc.26450\n10.1038/s41467-020-20321-x\n10.1063/1.5016222\n10.1038/s41586-020-2665-2\n10.1002/jcc.24874\n10.1186/s13321-018-0285-8\n10.1021/acs.jctc.5b00935\n10.1016/j.cell.2020.07.012\n10.1021/acscentsci.1c00120\n10.1016/j.chom.2020.11.001\n10.1038/s41467-021-25020-9\n10.1038/s41594-020-0483-8\n10.1126/science.abi7994\n10.1002/jcc.540130805\n10.1016/j.bpj.2021.01.012\n10.1002/jcc.21256\n10.1038/s41598-020-75762-7\n10.1101/2021.08.12.456168\n10.1021/acs.jpclett.1c01494\n10.1038/s41586-020-2349-y\n10.1016/j.str.2021.04.006\n10.3390/v13030494\n10.1021/acs.jpcb.0c11600\n10.1016/0021-9991(77)90098-5\n10.1006/jmbi.1993.1626\n10.1073/pnas.2003138117\n10.1063/1.2978177\n10.1038/ncomms4397\n10.1371/journal.pcbi.1008790\n10.26434/chemrxiv.13502646\n10.1038/s41557-021-00758-3\n10.1126/science.abd3255\n10.1002/jcc.21334\n10.3389/fphar.2021.685308\n10.1063/1.463137\n10.1126/science.abd5223\n10.1021/acscentsci.1c00216\n10.3389/fmolb.2021.637378\n10.1038/s41579-020-00468-6\n10.1016/j.cell.2020.02.058\n10.1016/j.cell.2020.03.045\n10.1038/s41586-021-03398-2\n10.1126/science.abb9983\n10.1021/acs.jpcb.0c04553\n10.1126/science.abb2507\n10.1038/s41594-020-0468-7\n10.1126/sciadv.abe5575\n10.1126/science.abb2762\n10.1093/bioinformatics/btp023\n10.1038/ncomms15092\n10.1016/j.cell.2020.09.032\n10.1126/science.abf2303\n10.1038/s41467-020-20465-w\n10.1038/s41557-021-00707-0"}
{"title": "Better, Faster, Cheaper: Recent Advances in Cryo-Electron Microscopy.", "abstract": "Cryo-electron microscopy (cryo-EM) continues its remarkable growth as a method for visualizing biological objects, which has been driven by advances across the entire pipeline. Developments in both single-particle analysis and in situ tomography have enabled more structures to be imaged and determined to better resolutions, at faster speeds, and with more scientists having improved access. This review highlights recent advances at each stageof the cryo-EM pipeline and provides examples of how these techniques have been used to investigate real-world problems, including antibody development against the SARS-CoV-2 spike during the recent COVID-19 pandemic.", "journal": "Annual review of biochemistry", "date": "2022-03-24", "authors": ["Eugene Y DChua", "Joshua HMendez", "MicahRapp", "Serban LIlca", "Yong ZiTan", "KashyapMaruthi", "HuihuiKuang", "Christina MZimanyi", "AnchiCheng", "Edward TEng", "Alex JNoble", "Clinton SPotter", "BridgetCarragher"], "doi": "10.1146/annurev-biochem-032620-110705"}
{"title": "Deep Learning-Based Classification of Reduced Lung Ultrasound Data From COVID-19 Patients.", "abstract": "The application of lung ultrasound (LUS) imaging for the diagnosis of lung diseases has recently captured significant interest within the research community. With the ongoing COVID-19 pandemic, many efforts have been made to evaluate LUS data. A four-level scoring system has been introduced to semiquantitatively assess the state of the lung, classifying the patients. Various deep learning (DL) algorithms supported with clinical validations have been proposed to automate the stratification process. However, no work has been done to evaluate the impact on the automated decision by varying pixel resolution and bit depth, leading to the reduction in size of overall data. This article evaluates the performance of DL algorithm over LUS data with varying pixel and gray-level resolution. The algorithm is evaluated over a dataset of 448 LUS videos captured from 34 examinations of 20 patients. All videos are resampled by a factor of 2, 3, and 4 of original resolution, and quantized to 128, 64, and 32 levels, followed by score prediction. The results indicate that the automated scoring shows negligible variation in accuracy when it comes to the quantization of intensity levels only. Combined effect of intensity quantization with spatial down-sampling resulted in a prognostic agreement ranging from 73.5% to 82.3%.These results also suggest that such level of prognostic agreement can be achieved over evaluation of data reduced to 32 times of its original size. Thus, laying foundation to efficient processing of data in resource constrained environments.", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2022-03-24", "authors": ["UmairKhan", "FedericoMento", "LucreziaNicolussi Giacomaz", "RiccardoTrevisan", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone", "LibertarioDemi"], "doi": "10.1109/TUFFC.2022.3161716"}
{"title": "Human-level COVID-19 diagnosis from low-dose CT scans using a two-stage time-distributed capsule network.", "abstract": "Reverse transcription-polymerase chain reaction is currently the gold standard in COVID-19 diagnosis. It can, however, take days to provide the diagnosis, and false negative rate is relatively high. Imaging, in particular chest computed tomography (CT), can assist with diagnosis and assessment of this disease. Nevertheless, it is shown that standard dose CT scan gives significant radiation burden to patients, especially those in need of multiple scans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT) scan protocols that reduce the radiation exposure close to that of a single X-ray, while maintaining an acceptable resolution for diagnosis purposes. Since thoracic radiology expertise may not be widely available during the pandemic, we develop an Artificial Intelligence (AI)-based framework using a collected dataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can provide human-level performance. The AI model uses a two stage capsule network architecture and can rapidly classify COVID-19, community acquired pneumonia (CAP), and normal cases, using LDCT/ULDCT scans. Based on a cross validation, the AI model achieves COVID-19 sensitivity of [Formula: see text], CAP sensitivity of [Formula: see text], normal cases sensitivity (specificity) of [Formula: see text], and accuracy of [Formula: see text]. By incorporating clinical data (demographic and symptoms), the performance further improves to COVID-19 sensitivity of [Formula: see text], CAP sensitivity of [Formula: see text], normal cases sensitivity (specificity) of [Formula: see text] , and accuracy of [Formula: see text]. The proposed AI model achieves human-level diagnosis based on the LDCT/ULDCT scans with reduced radiation exposure. We believe that the proposed AI model has the potential to assist the radiologists to accurately and promptly diagnose COVID-19 infection and help control the transmission chain during the pandemic.", "journal": "Scientific reports", "date": "2022-03-24", "authors": ["ParnianAfshar", "Moezedin JavadRafiee", "FarnooshNaderkhani", "ShahinHeidarian", "NastaranEnshaei", "AnastasiaOikonomou", "FaranakBabaki Fard", "ReutAnconina", "KeyvanFarahani", "Konstantinos NPlataniotis", "ArashMohammadi"], "doi": "10.1038/s41598-022-08796-8\n10.1001/jama.2020.2648\n10.1148/radiol.2020200432\n10.1109/MSP.2021.3090674\n10.2214/AJR.20.23034\n10.1016/j.radcr.2021.01.011\n10.1001/jamapediatrics.2013.311\n10.2214/ajr.144.4.805\n10.1016/S0140-6736(12)60815-0\n10.1016/j.clinimag.2020.12.041\n10.1148/radiol.2020190389\n10.1259/bjr.20150654\n10.1148/ryct.2020200196\n10.1007/s10140-020-01784-3\n10.1038/s41591-019-0447-x\n10.3390/jcm9123860\n10.1007/s00330-020-07225-6\n10.1038/s41598-020-64824-5\n10.1038/s41591-020-0931-3\n10.1038/s41597-021-00900-3\n10.1148/radiol.2020201473\n10.1007/s00330-020-07018-x\n10.1001/jama.2021.0377\n10.1118/1.3528204\n10.3390/jcm10102196\n10.1038/s41591-021-01303-y\n10.1038/s41568-020-00327-9\n10.21037/qims-20-564\n10.1007/s42399-020-00341-w\n10.1007/s00330-020-07033-y\n10.1007/s00330-010-1990-5\n10.1186/s41747-020-00173-2\n10.1016/j.jksuci.2019.09.014\n10.1038/srep34181"}
{"title": "Development of portable and robust cataract detection and grading system by analyzing multiple texture features for Tele-Ophthalmology.", "abstract": "This paper presents a low cost, robust, portable and automated cataract detection system which can detect the presence of cataract from the colored digital eye images and grade their severity. Ophthalmologists detect cataract through visual screening using ophthalmoscope and slit lamps. Conventionally a patient has to visit an ophthalmologist for eye screening and treatment follows the course. Developing countries lack the proper health infrastructure and face huge scarcity of trained medical professionals as well as technicians. The condition is not very satisfactory with the rural and remote areas of developed nations. To bridge this barrier between the patient and the availability of resources, current work focuses on the development of portable low-cost, robust cataract screening and grading system. Similar works use fundus and retinal images which use costly imaging modules and image based detection algorithms which use much complex neural network models. Current work derives its benefit from the advancements in digital image processing techniques. A set of preprocessing has been done on the colored eye image and later texture information in form of mean intensity, uniformity, standard deviation and randomness has been calculated and mapped with the diagnostic opinion of doctor for cataract screening of over 200 patients. For different grades of cataract severity edge pixel count was calculated as per doctor's opinion and later these data are used for calculating the thresholds using hybrid k-means algorithm, for giving a decision on the presence of cataract and grade its severity. Low value of uniformity and high value of other texture parameters confirm the presence of cataract as clouding in eye lens causes the uniformity function to take lower value due to presence of coarse texture. Higher the edge pixel count value, this confirms the presence of starting of cataract as solidified regions in lens are nonuniform. Lower value corresponds to fully solidified region or matured cataract. Proposed algorithm was initially developed on MATLAB, and tested on over 300 patients in an eye camp. The system has shown more than 98% accuracy in detection and grading of cataract. Later a cloud based system was developed with 3D printed image acquisition module to manifest an automated, portable and efficient cataract detection system for Tele-Ophthalmology. The proposed system uses a very simple and efficient technique by mapping the diagnostic opinion of the doctor as well, giving very promising results which suggest its potential use in teleophthalmology applications to reduce the cost of delivering eye care services and increasing its reach effectively. Developed system is simple in design and easy to operate and suitable for mass screening of cataracts. Due to non-invasive and non-mydriatic and mountable nature of device, in person screening is not required. Hence, social distancing norms are easy to follow and device is very useful in COVID-19 like situation.", "journal": "Multimedia tools and applications", "date": "2022-03-24", "authors": ["ShashwatPathak", "RahulRaj", "KartikSingh", "Pawan KumarVerma", "BasantKumar"], "doi": "10.1007/s11042-022-12544-5\n10.4103/0301-4738.100531\n10.1016/j.phpro.2015.08.263\n10.1109/TPAMI.2002.1017616\n10.1016/S0262-8856(99)00053-0\n10.3390/electronics5030057\n10.1117/1.JMI.1.1.014502\n10.1109/TFUZZ.2016.2551289\n10.1016/j.proeng.2011.08.563\n10.1016/j.cmpb.2015.10.007"}
{"title": "Deep Learning and Medical Image Analysis for COVID-19 Diagnosis and Prediction.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has imposed dramatic challenges to health-care organizations worldwide. To combat the global crisis, the use of thoracic imaging has played a major role in the diagnosis, prediction, and management of COVID-19 patients with moderate to severe symptoms or with evidence of worsening respiratory status. In response, the medical image analysis community acted quickly to develop and disseminate deep learning models and tools to meet the urgent need of managing and interpreting large amounts of COVID-19 imaging data. This review aims to not only summarize existing deep learning and medical image analysis methods but also offer in-depth discussions and recommendations for future investigations. We believe that the wide availability of high-quality, curated, and benchmarked COVID-19 imaging data sets offers the great promise of a transformative test bed to develop, validate, and disseminate novel deep learning methods in the frontiers of data science and artificial intelligence.", "journal": "Annual review of biomedical engineering", "date": "2022-03-23", "authors": ["TianmingLiu", "EliotSiegel", "DinggangShen"], "doi": "10.1146/annurev-bioeng-110220-012203"}
{"title": "EDNC: Ensemble Deep Neural Network for COVID-19 Recognition.", "abstract": "The automatic recognition of COVID-19 diseases is critical in the present pandemic since it relieves healthcare staff of the burden of screening for infection with COVID-19. Previous studies have proven that deep learning algorithms can be utilized to aid in the diagnosis of patients with potential COVID-19 infection. However, the accuracy of current COVID-19 recognition models is relatively low. Motivated by this fact, we propose three deep learning architectures, F-EDNC, FC-EDNC, and O-EDNC, to quickly and accurately detect COVID-19 infections from chest computed tomography (CT) images. Sixteen deep learning neural networks have been modified and trained to recognize COVID-19 patients using transfer learning and 2458 CT chest images. The proposed EDNC has then been developed using three of sixteen modified pre-trained models to improve the performance of COVID-19 recognition. The results suggested that the F-EDNC method significantly enhanced the recognition of COVID-19 infections with 97.75% accuracy, followed by FC-EDNC and O-EDNC (97.55% and 96.12%, respectively), which is superior to most of the current COVID-19 recognition models. Furthermore, a localhost web application has been built that enables users to easily upload their chest CT scans and obtain their COVID-19 results automatically. This accurate, fast, and automatic COVID-19 recognition system will relieve the stress of medical professionals for screening COVID-19 infections.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-03-23", "authors": ["LinYang", "Shui-HuaWang", "Yu-DongZhang"], "doi": "10.3390/tomography8020071\n10.1503/cmaj.211248\n10.3390/ijms21083004\n10.2214/AJR.20.23418\n10.2147/JMDH.S293601\n10.1007/s00330-020-06915-5\n10.1038/s41746-020-00376-2\n10.1016/S2589-7500(19)30123-2\n10.7717/peerj.7702\n10.4236/jbise.2020.137014\n10.1007/s00521-020-05437-x\n10.1016/j.patrec.2020.10.001\n10.2196/19569\n10.1016/j.bspc.2021.102588\n10.1155/2021/6633755\n10.1016/j.inffus.2020.11.005\n10.1016/j.inffus.2020.10.004\n10.1007/s12559-020-09776-8\n10.1186/s40537-019-0197-0\n10.1109/TKDE.2009.191\n10.1109/JPROC.2020.3004555\n10.1186/s40537-016-0043-6\n10.1016/S0933-3657(01)00077-X\n10.7150/thno.38065\n10.1186/s40537-021-00444-8\n10.1109/TNNLS.2021.3084827\n10.1117/1.OE.58.4.040901\n10.1109/ACCESS.2020.3003810\n10.1016/j.chaos.2020.110190\n10.1155/2020/8843664\n10.1016/j.compbiomed.2021.104575\n10.3390/s21020455\n10.1186/s12938-020-00807-x\n10.1080/07391102.2020.1788642\n10.1007/s00259-020-04929-1\n10.20944/preprints202005.0151.v3\n10.1183/13993003.00775-2020\n10.1007/s00330-021-07715-1\n10.1148/radiol.2020200905"}
{"title": "A proposed artificial intelligence workflow to address application challenges leveraged on algorithm uncertainty.", "abstract": "Artificial Intelligence (AI) has achieved state-of-the-art performance in medical imaging. However, most algorithms focused exclusively on improving the accuracy of classification while neglecting the major challenges in a real-world application. The opacity of algorithms prevents users from knowing when the algorithms might fail. And the natural gap between training datasets and the in-reality data may lead to unexpected AI system malfunction. Knowing the underlying uncertainty is essential for improving system reliability. Therefore, we developed a COVID-19 AI system, utilizing a Bayesian neural network to calculate uncertainties in classification and reliability intervals of datasets. Validated with four multi-region datasets simulating different scenarios, our approach was proved to be effective to suggest the system failing possibility and give the decision power to human experts in time. Leveraging on the complementary strengths of AI and health professionals, our present method has the potential to improve the practicability of AI systems in clinical application.", "journal": "iScience", "date": "2022-03-22", "authors": ["DantongLi", "LiantingHu", "XiaotingPeng", "NingXiao", "HongZhao", "GuangjianLiu", "HongshengLiu", "KuanrongLi", "BinAi", "HuiminXia", "LongLu", "YunfeiGao", "JianWu", "HuiyingLiang"], "doi": "10.1016/j.isci.2022.103961\n10.1016/j.media.2020.101724\n10.1038/s41467-020-17971-2\n10.1038/s41467-020-18685-1\n10.1148/radiol.2020190283\n10.1148/ryai.2020190146\n10.1101/2020.04.24.20079012\n10.1038/s41591-019-0548-6"}
{"title": "CHS-Net: A Deep Learning Approach for Hierarchical Segmentation of COVID-19 via CT Images.", "abstract": "The pandemic of novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) also known as COVID-19 has been spreading worldwide, causing rampant loss of lives. Medical imaging such as computed tomography (CT), X-ray, etc., plays a significant role in diagnosing the patients by presenting the visual representation of the functioning of the organs. However, for any radiologist analyzing such scans is a tedious and time-consuming task. The emerging deep learning technologies have displayed its strength in analyzing such scans to aid in the faster diagnosis of the diseases and viruses such as COVID-19. In the present article, an automated deep learning based model, COVID-19 hierarchical segmentation network (CHS-Net) is proposed that functions as a semantic hierarchical segmenter to identify the COVID-19 infected regions from lungs contour via CT medical imaging using two cascaded residual attention inception U-Net (RAIU-Net) models. RAIU-Net comprises of a residual inception U-Net model with spectral spatial and depth attention network (SSD) that is developed with the contraction and expansion phases of depthwise separable convolutions and hybrid pooling (max and spectral pooling) to efficiently encode and decode the semantic and varying resolution information. The CHS-Net is trained with the segmentation loss function that is the defined as the average of binary cross entropy loss and dice loss to penalize false negative and false positive predictions. The approach is compared with the recently proposed approaches and evaluated using the standard metrics like accuracy, precision, specificity, recall, dice coefficient and Jaccard similarity along with the visualized interpretation of the model prediction with GradCam++ and uncertainty maps. With extensive trials, it is observed that the proposed approach outperformed the recently proposed approaches and effectively segments the COVID-19 infected regions in the lungs.", "journal": "Neural processing letters", "date": "2022-03-22", "authors": ["Narinder SinghPunn", "SonaliAgarwal"], "doi": "10.1007/s11063-022-10785-x\n10.1007/s11517-019-01965-4\n10.1148/radiol.2019181960\n10.1016/j.cell.2018.02.010\n10.1007/s00330-020-06817-6\n10.1007/s10096-020-03901-z\n10.1109/TMI.2020.2996645\n10.2214/AJR.20.22954\n10.1016/j.ejrad.2020.109009\n10.1016/j.jinf.2020.04.004\n10.1109/RBME.2020.2987975\n10.1109/ACCESS.2020.3005510\n10.1016/j.patcog.2020.107747\n10.1016/j.patcog.2021.108168\n10.1016/j.asoc.2021.107947\n10.1016/j.inffus.2021.05.008\n10.1007/s00521-019-04296-5\n10.1109/TPAMI.2016.2644615"}
{"title": "COVID-19 Identification System Using Transfer Learning Technique With Mobile-NetV2 and Chest X-Ray Images.", "abstract": "Diagnosis is a crucial precautionary step in research studies of the coronavirus disease, which shows indications similar to those of various pneumonia types. The COVID-19 pandemic has caused a significant outbreak in more than 150 nations and has significantly affected the wellness and lives of many individuals globally. Particularly, discovering the patients infected with COVID-19 early and providing them with treatment is an important way of fighting the pandemic. Radiography and radiology could be the fastest techniques for recognizing infected individuals. Artificial intelligence strategies have the potential to overcome this difficulty. Particularly, transfer learning MobileNetV2 is a convolutional neural network architecture that can perform well on mobile devices. In this study, we used MobileNetV2 with transfer learning and augmentation data techniques as a classifier to recognize the coronavirus disease. Two datasets were used: the first consisted of 309 chest X-ray images (102 with COVID-19 and 207 were normal), and the second consisted of 516 chest X-ray images (102 with COVID-19 and 414 were normal). We assessed the model based on its sensitivity rate, specificity rate, confusion matrix, and F1-measure. Additionally, we present a receiver operating characteristic curve. The numerical simulation reveals that the model accuracy is 95.8% and 100% at dropouts of 0.3 and 0.4, respectively. The model was implemented using Keras and Python programming.", "journal": "Frontiers in public health", "date": "2022-03-22", "authors": ["MahmoudRagab", "SamahAlshehri", "Gamil AbdelAzim", "Hibah MAldawsari", "AdeebNoor", "JaberAlyami", "SAbdel-Khalek"], "doi": "10.3389/fpubh.2022.819156\n10.1016/j.ijantimicag.2020.105924\n10.1016/S0140-6736(20)30185-9\n10.1148/radiol.2020201160\n10.1016/j.ejmp.2020.01.004\n10.1148/radiol.2020200642\n10.1148/radiol.2020200527\n10.1038/s41598-020-76550-z\n10.1186/s13244-019-0832-5\n10.1109/ACCESS.2020.3007336\n10.1109/MSP.2017.2749125\n10.1109/ACCESS.2020.2982906\n10.1109/ACCESS.2019.2962284\n10.1007/s13244-018-0639-9\n10.1109/5.726791\n10.1109/TPAMI.2016.2644615\n10.1007/978-3-319-10593-2_13\n10.1016/j.media.2017.07.005\n10.1109/CVPRW.2014.131\n10.1109/ISCAS.2018.8351550\n10.1109/CVPR.2016.90\n10.1109/CVPR.2015.7298594\n10.1016/j.scs.2020.102589\n10.1109/GCWkshps50303.2020.9367469\n10.1016/j.cell.2018.02.010\n10.3390/app8101715\n10.1007/s00330-021-07715-1\n10.1007/s13246-020-00865-4\n10.1007/s40747-020-00199-4\n10.1016/j.cmpb.2020.105581\n10.1186/s13007-020-00624-2\n10.1145/3331453.3361658\n10.1109/OJEMB.2021.3066097\n10.1186/s13640-021-00554-6\n10.3390/biology11010043\n10.1109/CVPR.2017.195\n10.1088/1742-6596/2071/1/012003\n10.7717/peerj-cs.390"}
{"title": "A Machine Learning Pipeline for Accurate COVID-19 Health Outcome Prediction using Longitudinal Electronic Health Records.", "abstract": "Current COVID-19 predictive models primarily focus on predicting the risk of mortality, and rely on COVID-19 specific medical data such as chest imaging after COVID-19 diagnosis. In this project, we developed an innovative supervised machine learning pipeline using longitudinal Electronic Health Records (EHR) to accurately predict COVID-19 related health outcomes including mortality, ventilation, days in hospital or ICU. In particular, we developed unique and effective data processing algorithms, including data cleaning, initial feature screening, vector representation. Then we trained models using state-of-the-art machine learning strategies combined with different parameter settings. Based on routinely collected EHR, our machine learning pipeline not only consistently outperformed those developed by other research groups using the same set of data, but also achieved similar accuracy as those trained on medical data that were only available after COVID-19 diagnosis. In addition, top risk factors for COVID-19 were identified, and are consistent with epidemiologic findings.", "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium", "date": "2022-03-22", "authors": ["AliceFeng"], "doi": null}
{"title": "Predictive and Causal Analysis of No-Shows for Medical Exams During COVID-19: A Case Study of Breast Imaging in a Nationwide Israeli Health Organization.", "abstract": "\"No-shows\", defined as missed appointments or late cancellations, is a central problem in healthcare systems. It has appeared to intensify during the COVID-19 pandemic and the nonpharmaceutical interventions, such as closures, taken to slow its spread. No-shows interfere with patients' continuous care, lead to inefficient utilization of medical resources, and increase healthcare costs. We present a comprehensive analysis of no-shows for breast imaging appointments made during 2020 in a large medical network in Israel. We applied advanced machine learning methods to provide insights into novel and known predictors. Additionally, we employed causal inference methodology to infer the effect of closures on no-shows, after accounting for confounding biases, and demonstrate the superiority of adversarial balancing over inverse probability weighting in correcting these biases. Our results imply that a patient's perceived risk of cancer and the COVID-19 time-based factors are major predictors. Further, we reveal that closures impact patients over 60, but not patients undergoing advanced diagnostic examinations.", "journal": "AMIA ... Annual Symposium proceedings. AMIA Symposium", "date": "2022-03-22", "authors": ["MichalOzery-Flato", "OraPinchasov", "MielDabush-Kasa", "EfratHexter", "GabrielChodick", "MichalGuindy", "MichalRosen-Zvi"], "doi": null}
{"title": "MultiR-Net: A Novel Joint Learning Network for COVID-19 segmentation and classification.", "abstract": "The outbreak of COVID-19 has caused a severe shortage of healthcare resources. Ground Glass Opacity (GGO) and consolidation of chest CT scans have been an essential basis for imaging diagnosis since 2020. The similarity of imaging features between COVID-19 and other pneumonia makes it challenging to distinguish between them and affects radiologists' diagnosis. Recently, deep learning in COVID-19 has been mainly divided into disease classification and lesion segmentation, yet little work has focused on the feature correlation between the two tasks. To address these issues, in this study, we propose MultiR-Net, a 3D deep learning model for combined COVID-19 classification and lesion segmentation, to achieve real-time and interpretable COVID-19 chest CT diagnosis. Precisely, the proposed network consists of two subnets: a multi-scale feature fusion UNet-like subnet for lesion segmentation and a classification subnet for disease diagnosis. The features between the two subnets are fused by the reverse attention mechanism and the iterable training strategy. Meanwhile, we proposed a loss function to enhance the interaction between the two subnets. Individual metrics can not wholly reflect network effectiveness. Thus we quantify the segmentation results with various evaluation metrics such as average surface distance, volume Dice, and test on the dataset. We employ a dataset containing 275 3D CT scans for classifying COVID-19, Community-acquired Pneumonia (CAP), and healthy people and segmented lesions in pneumonia patients. We split the dataset into 70% and 30% for training and testing. Extensive experiments showed that our multi-task model framework obtained an average recall of 93.323%, an average precision of 94.005% on the classification test set, and a 69.95% Volume Dice score on the segmentation test set of our dataset.", "journal": "Computers in biology and medicine", "date": "2022-03-20", "authors": ["Cheng-FanLi", "Yi-DuoXu", "Xue-HaiDing", "Jun-JuanZhao", "Rui-QiDu", "Li-ZhongWu", "Wen-PingSun"], "doi": "10.1016/j.compbiomed.2022.105340"}
{"title": "COVID-19 image classification using deep learning: Advances, challenges and opportunities.", "abstract": "Corona Virus Disease-2019 (COVID-19), caused by Severe Acute Respiratory Syndrome-Corona Virus-2 (SARS-CoV-2), is a highly contagious disease that has affected the lives of millions around the world. Chest X-Ray (CXR) and Computed Tomography (CT) imaging modalities are widely used to obtain a fast and accurate diagnosis of COVID-19. However, manual identification of the infection through radio images is extremely challenging because it is time-consuming and highly prone to human errors. Artificial Intelligence (AI)-techniques have shown potential and are being exploited further in the development of automated and accurate solutions for COVID-19 detection. Among AI methodologies, Deep Learning (DL) algorithms, particularly Convolutional Neural Networks (CNN), have gained significant popularity for the classification of COVID-19. This paper summarizes and reviews a number of significant research publications on the DL-based classification of COVID-19 through CXR and CT images. We also present an outline of the current state-of-the-art advances and a critical discussion of open challenges. We conclude our study by enumerating some future directions of research in COVID-19 imaging classification.", "journal": "Computers in biology and medicine", "date": "2022-03-20", "authors": ["PriyaAggarwal", "Narendra KumarMishra", "BinishFatimah", "PushpendraSingh", "AnubhaGupta", "Shiv DuttJoshi"], "doi": "10.1016/j.compbiomed.2022.105350\n10.1007/s00500-021-06137-x\n10.5281/zenodo.3757476"}
{"title": "INASNET: Automatic identification of coronavirus disease (COVID-19) based on chest X-ray using deep neural network.", "abstract": "Testing is one of the important methodologies used by various countries in order to fight against COVID-19 infection. The infection is considered as one of the deadliest ones although the mortality rate is not very high. COVID-19 infection is being caused by SARS-CoV2 which is termed as severe acute respiratory syndrome coronavirus 2 virus. To prevent the community, transfer among the masses, testing plays an important role. Efficient and quicker testing techniques helps in identification of infected person which makes it easier for to isolate the patient. Deep learning methods have proved their presence and effectiveness in medical image analysis and in the identification of some of the diseases like pneumonia. Authors have been proposed a deep learning mechanism and system to identify the COVID-19 infected patient on analyzing the X-ray images. Symptoms in the COVID-19 infection is well similar to the symptoms occurring in the influenza and pneumonia. The proposed model Inception Nasnet (INASNET) is being able to separate out and classify the X-ray images in the corresponding normal, COVID-19 infected or pneumonia infected classes. This testing method will be a boom for the doctors and for the state as it is a way cheaper method as compared to the other testing kits used by the healthcare workers for the diagnosis of the disease. Continuous analysis by convolutional neural network and regular evaluation will result in better accuracy and helps in eliminating the false-negative results. INASNET is based on the combined platform of InceptionNet and Neural network architecture search which will result in having higher and faster predictions. Regular testing, faster results, economically viable testing using X-ray images will help the front line workers to make a win over COVID-19.", "journal": "ISA transactions", "date": "2022-03-19", "authors": ["MurukessanPerumal", "AkshayNayak", "R PraneethaSree", "MSrinivas"], "doi": "10.1016/j.isatra.2022.02.033"}
{"title": "Cytotoxic T lymphocytes targeting a conserved SARS-CoV-2 spike epitope are efficient serial killers.", "abstract": "Understanding immune response to infections and vaccines lags understanding humoral responses. While neutralizing antibody responses wane over time, T cells are instrumental in long-term immunity. We\u00a0apply machine learning and time-lapse imaging microscopy in nanowell grids (TIMING)\u00a0to study thousands of videos of T cells with specificity for SARS-CoV-2\u00a0eliminating targets bearing spike protein as a surrogate for viral infection. The data on effector functions, including cytokine secretion and cytotoxicity, provide the first direct evidence that cytotoxic T lymphocytes from a convalescent patient targeting an epitope conserved across all known variants of concern are serial killers capable of eliminating multiple infected target cells. These data have implications for vaccine development and for the recovery and monitoring of infected individuals.", "journal": "BioTechniques", "date": "2022-03-18", "authors": ["MohsenFathi", "LindseyCharley", "Laurence JnCooper", "NavinVaradarajan", "Daniel DMeyer"], "doi": "10.2144/btn-2022-0016"}
{"title": "Metaheuristics based COVID-19 detection using medical images: A review.", "abstract": "Many countries in the world have been facing the rapid spread of COVID-19 since February 2020. There is a dire need for efficient and cheap automated diagnosis systems that can reduce the pressure on healthcare systems. Extensive research is being done on the use of image classification for the detection of COVID-19 through X-ray and CT-scan images of patients. Deep learning has been the most popular technique for image classification during the last decade. However, the performance of deep learning-based methods heavily depends on the architecture of the deep neural network. Over the last few years, metaheuristics have gained popularity for optimizing the architecture of deep neural networks. Metaheuristics have been widely used to solve different complex non-linear optimization problems due to their flexibility, simplicity, and problem independence. This paper aims to study the different image classification techniques for chest images, including the applications of metaheuristics for optimization and feature selection of deep learning and machine learning models. The motivation of this study is to focus on applications of different types of metaheuristics for COVID-19 detection and to shed some light on future challenges in COVID-19 detection from medical images. The aim is to inspire researchers to focus their research on overlooked aspects of COVID-19 detection.", "journal": "Computers in biology and medicine", "date": "2022-03-17", "authors": ["MamoonaRiaz", "MaryamBashir", "IrfanYounas"], "doi": "10.1016/j.compbiomed.2022.105344\n10.1155/2021/8829829\n10.1155/2015/232193"}
{"title": "Recognition of Peripheral Lung Cancer and Focal Pneumonia on Chest Computed Tomography Images Based on Convolutional Neural Network.", "abstract": "", "journal": "Technology in cancer research & treatment", "date": "2022-03-17", "authors": ["XiaoyueCheng", "HeWen", "HaoYou", "LiHua", "WuXiaohua", "CaoQiuting", "LiuJiabao"], "doi": "10.1177/15330338221085375\n10.1117/12.2513631\n10.1109/ijcnn.2018.8489345\n10.1155/2018/1382309\n10.1371/journal.pmed.1002711\n10.1371/journal.pmed.1002697\n10.1371/journal.pmed.1002683\n10.1007/978-3-319-46723-8_49\n10.1117/1.JMI.4.4.041308"}
{"title": "Efficacy of Transfer Learning-based ResNet models in Chest X-ray image classification for detecting COVID-19 Pneumonia.", "abstract": "Because of COVID-19's effect on pulmonary tissues, Chest X-ray(CXR) and Computed Tomography (CT) images have become the preferred imaging modality for detecting COVID-19 infections at the early diagnosis stages, particularly when the symptoms are not specific. A significant fraction of individuals with COVID-19 have negative polymerase chain reaction (PCR) test results; therefore, imaging studies coupled with epidemiological, clinical, and laboratory data assist in the decision making. With the newer variants of COVID-19 emerging, the burden on diagnostic laboratories has increased manifold. Therefore, it is important to employ beyond laboratory measures to solve complex CXR image classification problems. One such tool is Convolutional Neural Network (CNN), one of the most dominant Deep Learning (DL) architectures. DL entails training a CNN for a task such as classification using extensive datasets. However, the labelled data for COVID-19 is scarce, proving to be a prime impediment to applying DL-assisted analysis. The available datasets are either scarce or too diversified to learn effective feature representations; therefore Transfer Learning (TL) approach is utilized. TL-based ResNet architecture has a powerful representational ability, making it popular in Computer Vision. The aim of this study is two-fold- firstly, to assess the performance of ResNet models for classifying Pneumonia cases from CXR images and secondly, to build a customized ResNet model and evaluate its contribution to the performance improvement. The global accuracies achieved by the five models i.e., ResNet18_v1, ResNet34_v1, ResNet50_v1, ResNet101_v1, ResNet152_v1 are 91.35%, 90.87%, 92.63%, 92.95%, and 92.95% respectively. ResNet50_v1 displayed the highest sensitivity of 97.18%, ResNet101_v1 showed the specificity of 94.02%, and ResNet18_v1 had the highest precision of 93.53%. The findings are encouraging, demonstrating the effectiveness of ResNet in the automatic detection of Pneumonia for COVID-19 diagnosis. The customized ResNet model presented in this study achieved 95% global accuracy, 95.65% precision, 92.74% specificity, and 95.9% sensitivity, thereby allowing a reliable analysis of CXR images to facilitate the clinical decision-making process. All simulations were carried in PyTorch utilizing Quadro 4000 GPU with Intel(R) Xeon(R) CPU E5-1650 v4 @ 3.60\u00a0\u200bGHz processor and 63.9\u00a0\u200bGB useable RAM.", "journal": "Chemometrics and intelligent laboratory systems : an international journal sponsored by the Chemometrics Society", "date": "2022-03-17", "authors": ["SadiaShowkat", "ShaimaQureshi"], "doi": "10.1016/j.chemolab.2022.104534\n10.3201/eid2701.201543\n10.18051/UnivMed.2021.v40.77-78\n10.22270/jddt.v11i2-S.4644\n10.1016/S1473-3099(21)00146-8\n10.1016/j.rxeng.2020.11.003\n10.1097/RTI.0000000000000347\n10.1177/0846537120924606\n10.1016/j.ijtb.2020.09.023\n10.1109/ICEngTechnol.2017.8308186\n10.1002/9781119711278.ch11\n10.1109/ACCESS.2021.3058537\n10.1101/2020.05.04.20090803\n10.1186/s40537-016-0043-6\n10.1109/JPROC.2020.3004555\n10.1109/TMI.2016.2535302\n10.1021/acs.jmedchem.9b02147\n10.1016/j.compag.2020.105393\n10.1016/j.compag.2019.01.041\n10.1109/TII.2018.2864759\n10.18653/v1/N19-5004\n10.1117/12.2243849\n10.1109/WACV45572.2020.9093290\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2017.195\n10.1016/j.irbm.2020.05.003\n10.1007/s00330-021-07715-1\n10.1007/s13246-020-00865-4\n10.1101/2020.05.01.20088211\n10.1109/ACCESS.2020.3033762\n10.1109/72.279181\n10.1142/S0218488598000094\n10.1007/978-3-319-46493-0_38\n10.1109/ICTAI.2018.00017\n10.1007/978-3-319-54184-6_12\n10.1007/978-3-319-61316-1_6\n10.1016/j.media.2021.101985\n10.1109/ACCESS.2021.3054484"}
{"title": "An automated diagnosis and classification of COVID-19 from chest CT images using a transfer learning-based convolutional neural network.", "abstract": "Researchers have developed more intelligent, highly responsive, and efficient detection methods owing to the COVID-19 demands for more widespread diagnosis. The work done deals with developing an AI-based framework that can help radiologists and other healthcare professionals diagnose COVID-19 cases at a high level of accuracy. However, in the absence of publicly available CT datasets, the development of such AI tools can prove challenging. Therefore, an algorithm for performing automatic and accurate COVID-19 classification using Convolutional Neural Network (CNN), pre-trained model, and Sparrow search algorithm (SSA) on CT lung images was proposed. The pre-trained CNN models used are SeresNext50, SeresNext101, SeNet154, MobileNet, MobileNetV2, MobileNetV3Small, and MobileNetV3Large. In addition, the SSA will be used to optimize the different CNN and transfer learning(TL) hyperparameters to find the best configuration for the pre-trained model used and enhance its performance. Two datasets are used in the experiments. There are two classes in the first dataset, while three in the second. The authors combined two publicly available COVID-19 datasets as the first dataset, namely the COVID-19 Lung CT Scans and COVID-19 CT Scan Dataset. In total, 14,486 images were included in this study. The authors analyzed the Large COVID-19 CT scan slice dataset in the second dataset, which utilized 17,104 images. Compared to other pre-trained models on both classes datasets, MobileNetV3Large pre-trained is the best model. As far as the three-classes dataset is concerned, a model trained on SeNet154 is the best available. Results show that, when compared to other CNN models like LeNet-5 CNN, COVID faster R-CNN, Light CNN, Fuzzy\u00a0+\u00a0CNN, Dynamic CNN, CNN and Optimized CNN, the proposed Framework achieves the best accuracy of 99.74% (two classes) and 98% (three classes).", "journal": "Computers in biology and medicine", "date": "2022-03-16", "authors": ["Nadiah ABaghdadi", "AmerMalki", "Sally FAbdelaliem", "HossamMagdy Balaha", "MahmoudBadawy", "MostafaElhosseini"], "doi": "10.1016/j.compbiomed.2022.105383"}
{"title": "Quantitative Chest CT Assessment of Small Airways Disease in Post-Acute SARS-CoV-2 Infection.", "abstract": "Background The long-term effects of SARS-CoV-2 infection on pulmonary structure and function remain incompletely characterized. Purpose To test whether SARS-CoV-2 infection leads to small airways disease in patients with persistent symptoms. Materials and Methods In this single-center study at a university teaching hospital, adults with confirmed COVID-19 who remained symptomatic more than 30 days following diagnosis were prospectively enrolled from June to December 2020 and compared with healthy participants (controls) prospectively enrolled from March to August 2018. Participants with post-acute sequelae of COVID-19 (PASC) were classified as ambulatory, hospitalized, or having required the intensive care unit (ICU) based on the highest level of care received during acute infection. Symptoms, pulmonary function tests, and chest CT images were collected. Quantitative CT analysis was performed using supervised machine learning to measure regional ground-glass opacity (GGO) and using inspiratory and expiratory image-matching to measure regional air trapping. Univariable analyses and multivariable linear regression were used to compare groups. Results Overall, 100 participants with PASC (median age, 48 years; 66 women) were evaluated and compared with 106 matched healthy controls; 67% (67 of 100) of the participants with PASC were classified as ambulatory, 17% (17 of 100) were hospitalized, and 16% (16 of 100) required the ICU. In the hospitalized and ICU groups, the mean percentage of total lung classified as GGO was 13.2% and 28.7%, respectively, and was higher than that in the ambulatory group (3.7%, ", "journal": "Radiology", "date": "2022-03-16", "authors": ["Josalyn LCho", "RaulVillacreses", "PrashantNagpal", "JunfengGuo", "Alejandro APezzulo", "Andrew LThurman", "Nabeel YHamzeh", "Robert JBlount", "SpyridonFortis", "Eric AHoffman", "JosephZabner", "Alejandro PComellas"], "doi": "10.1148/radiol.212170"}
{"title": "A hybrid machine learning/deep learning COVID-19 severity predictive model from CT images and clinical data.", "abstract": "COVID-19 clinical presentation and prognosis are highly variable, ranging from asymptomatic and paucisymptomatic cases to acute respiratory distress syndrome and multi-organ involvement. We developed a hybrid machine learning/deep learning model to classify patients in two outcome categories, non-ICU and ICU (intensive care admission or death), using 558 patients admitted in a northern Italy hospital in February/May of 2020. A fully 3D patient-level CNN classifier on baseline CT images is used as feature extractor. Features extracted, alongside with laboratory and clinical data, are fed for selection in a Boruta algorithm with SHAP game theoretical values. A classifier is built on the reduced feature space using CatBoost gradient boosting algorithm and reaching a probabilistic AUC of 0.949 on holdout test set. The model aims to provide clinical decision support to medical doctors, with the probability score of belonging to an outcome class and with case-based SHAP interpretation of features importance.", "journal": "Scientific reports", "date": "2022-03-16", "authors": ["MatteoChieregato", "FabioFrangiamore", "MauroMorassi", "ClaudiaBaresi", "StefaniaNici", "ChiaraBassetti", "ClaudioBn\u00e0", "MarcoGalelli"], "doi": "10.1038/s41598-022-07890-1"}
{"title": "Supervised and weakly supervised deep learning models for COVID-19 CT diagnosis: A systematic review.", "abstract": "Artificial intelligence (AI) and computer vision (CV) methods become reliable to extract features from radiological images, aiding COVID-19 diagnosis ahead of the pathogenic tests and saving critical time for disease management and control. Thus, this review article focuses on cascading numerous deep learning-based COVID-19 computerized tomography (CT) imaging diagnosis research, providing a baseline for future research. Compared to previous review articles on the topic, this study pigeon-holes the collected literature very differently (i.e., its multi-level arrangement). For this purpose, 71 relevant studies were found using a variety of trustworthy databases and search engines, including Google Scholar, IEEE Xplore, Web of Science, PubMed, Science Direct, and Scopus. We classify the selected literature in multi-level machine learning groups, such as supervised and weakly supervised learning. Our review article reveals that weak supervision has been adopted extensively for COVID-19 CT diagnosis compared to supervised learning. Weakly supervised (conventional transfer learning) techniques can be utilized effectively for real-time clinical practices by reusing the sophisticated features rather than over-parameterizing the standard models. Few-shot and self-supervised learning are the recent trends to address data scarcity and model efficacy. The deep learning (artificial intelligence) based models are mainly utilized for disease management and control. Therefore, it is more appropriate for readers to comprehend the related perceptive of deep learning approaches for the in-progress COVID-19 CT diagnosis research.", "journal": "Computer methods and programs in biomedicine", "date": "2022-03-15", "authors": ["HaseebHassan", "ZhaoyuRen", "ChengminZhou", "Muazzam AKhan", "YiPan", "JianZhao", "BingdingHuang"], "doi": "10.1016/j.cmpb.2022.106731"}
{"title": "Multi-task semantic segmentation of CT images for COVID-19 infections using DeepLabV3+ based on dilated residual network.", "abstract": "COVID-19 is a deadly outbreak that has been declared a public health emergency of international concern. The massive damage of the disease to public health, social life, and the global economy increases the importance of alternative rapid diagnosis and follow-up methods. RT-PCR assay, which is considered the gold standard in diagnosing the disease, is complicated, expensive, time-consuming, prone to contamination, and may give false-negative results. These drawbacks reinforce the trend toward medical imaging techniques such as computed tomography (CT). Typical visual signs such as ground-glass opacity (GGO) and consolidation of CT images allow for quantitative assessment of the disease. In this context, it is aimed at the segmentation of the infected lung CT images with the residual network-based DeepLabV3+, which is a redesigned convolutional neural network (CNN) model. In order to evaluate the robustness of the proposed model, three different segmentation tasks as Task-1, Task-2, and Task-3 were applied. Task-1 represents binary segmentation as lung (infected and non-infected tissues) and background. Task-2 represents multi-class segmentation as lung (non-infected tissue), COVID (GGO, consolidation, and pleural effusion irregularities are gathered under a single roof), and background. Finally, the segmentation in which each lesion type is considered as a separate class is defined as Task-3. COVID-19 imaging data for each segmentation task consists of 100 CT single-slice scans from over 40 diagnosed patients. The performance of the model was evaluated using Dice similarity coefficient (DSC), intersection over union (IoU), sensitivity, specificity, and accuracy by performing five-fold cross-validation. The average DSC performance for three different segmentation tasks was obtained as 0.98, 0.858, and 0.616, respectively. The experimental results demonstrate that the proposed method has robust performance and great potential in evaluating COVID-19 infection.", "journal": "Physical and engineering sciences in medicine", "date": "2022-03-15", "authors": ["HasanPolat"], "doi": "10.1007/s13246-022-01110-w\n10.1016/j.compbiomed.2020.103805\n10.1016/j.iot.2021.100377\n10.1016/j.measurement.2020.108288\n10.1016/j.eng.2020.04.010\n10.1016/j.bbe.2021.04.006\n10.1016/j.ibmed.2020.100013\n10.1016/j.jrid.2020.04.001\n10.1016/j.compbiomed.2020.104037\n10.1016/j.clinimag.2021.01.019\n10.1186/s12880-020-00529-5\n10.1016/j.patrec.2020.07.029\n10.1007/978-3-030-01234-2_49\n10.33889/IJMEMS.2020.5.4.052\n10.1002/ima.22558\n10.1016/j.bspc.2021.102987\n10.1016/j.bbe.2021.05.013\n10.1109/JAS.2020.1003393\n10.1016/j.knosys.2021.106849\n10.1007/978-3-319-24574-4_28\n10.1109/TPAMI.2016.2644615\n10.1109/TPAMI.2016.2572683\n10.1016/j.procs.2021.01.025\n10.1007/978-3-319-10578-9_23\n10.1002/mp.14676\n10.1109/ACCESS.2021.3067047\n10.3390/s20113183"}
{"title": "AI-driven quantification of ground glass opacities in lungs of COVID-19 patients using 3D computed tomography imaging.", "abstract": "Ground-glass opacity (GGO)-a hazy, gray appearing density on computed tomography (CT) of lungs-is one of the hallmark features of SARS-CoV-2 in COVID-19 patients. This AI-driven study is focused on segmentation, morphology, and distribution patterns of GGOs.\nWe use an AI-driven unsupervised machine learning approach called PointNet++ to detect and quantify GGOs in CT scans of COVID-19 patients and to assess the severity of the disease. We have conducted our study on the \"MosMedData\", which contains CT lung scans of 1110 patients with or without COVID-19 infections. We quantify the morphologies of GGOs using Minkowski tensors and compute the abnormality score of individual regions of segmented lung and GGOs.\nPointNet++ detects GGOs with the highest evaluation accuracy (98%), average class accuracy (95%), and intersection over union (92%) using only a fraction of 3D data. On average, the shapes of GGOs in the COVID-19 datasets deviate from sphericity by 15% and anisotropies in GGOs are dominated by dipole and hexapole components. These anisotropies may help to quantitatively delineate GGOs of COVID-19 from other lung diseases.\nThe PointNet++ and the Minkowski tensor based morphological approach together with abnormality analysis will provide radiologists and clinicians with a valuable set of tools when interpreting CT lung scans of COVID-19 patients. Implementation would be particularly useful in countries severely devastated by COVID-19 such as India, where the number of cases has outstripped available resources creating delays or even breakdowns in patient care. This AI-driven approach synthesizes both the unique GGO distribution pattern and severity of the disease to allow for more efficient diagnosis, triaging and conservation of limited resources.", "journal": "PloS one", "date": "2022-03-15", "authors": ["MonjoySaha", "Sagar BAmin", "AshishSharma", "T K SatishKumar", "Rajiv KKalia"], "doi": "10.1371/journal.pone.0263916\n10.1016/j.clim.2020.108427\n10.1017/S000711452000330X\n10.3348/kjr.2020.0146\n10.1148/radiol.2020201365\n10.1148/radiol.2020202504\n10.1007/s00330-020-06975-7\n10.1016/j.artmed.2020.101792\n10.1016/j.compmedimag.2020.101817\n10.1016/j.cmpb.2020.105864\n10.1038/s41598-020-79139-8\n10.1109/TMI.2020.2996645\n10.2214/AJR.12.9640\n10.1007/s13312-020-1816-8\n10.1109/ACCESS.2020.3027738\n10.1016/j.eswa.2021.114848\n10.1038/s41467-019-13993-7\n10.1016/j.cmpb.2020.105798\n10.1038/s41467-019-13993-7\n10.1073/pnas.1717139115\n10.1002/adma.201100562\n10.1097/RCT.0b013e31815e6291"}
{"title": "Automated assessment of hyoid movement during normal swallow using ultrasound.", "abstract": "The potential for using ultrasound by speech and language therapists (SLTs) as an adjunct clinical tool to assess swallowing function has received increased attention during the COVID-19 pandemic, with a recent review highlighting the need for further research on normative data, objective measurement, elicitation protocol and training. The dynamic movement of the hyoid, visible in ultrasound, is crucial in facilitating bolus transition and protection of the airway during a swallow and has shown promise as a biomarker of swallowing function.\nTo examine the kinematics of the hyoid during a swallow using ultrasound imaging and to relate the patterns to the different stages of a normal swallow. To evaluate the accuracy and robustness of two different automatic hyoid tracking methods relative to manual hyoid position estimation.\nUltrasound data recorded from 15 healthy participants swallowing a 10 ml water bolus delivered by cup or spoon were analysed. The movement of the hyoid was tracked using manually marked frame-to-frame positions, automated hyoid shadow tracking and deep neural net (DNN) tracking. Hyoid displacement along the horizontal image axis (HxD) was charted throughout a swallow, and the maximum horizontal displacement (HxD max) and maximum hyoid velocity (HxV max) along the same axis were automatically calculated.\nThe HxD and HxV of 10 ml swallows are similar to values reported in the literature. The trajectory of the hyoid movement and its location at significant swallow event time points showed increased hyoid displacement towards the peak of the swallow. Using an interclass correlation coefficient, HxD max and HxV max values derived from the DNN tracker and shadow tracker are shown to be in high agreement and moderate agreement, respectively, when compared with values derived from manual tracking.\nThe similarity of the hyoid tracking results using ultrasound to previous reports based on different instrumental tools supports the possibility of using hyoid movement as a measure of swallowing function in ultrasound. The use of machine learning to automatically track the hyoid movement potentially provides a reliable and efficient way to quantify swallowing function. These findings contribute towards improving the clinical utility of ultrasound as a swallowing assessment tool. Further research on both normative and clinical populations is needed to validate hyoid movement metrics as a means of differentiating normal and abnormal swallows and to verify the reliability of automatic tracking.\nWhat is already known on this subject There is growing interest in the use of ultrasound as an adjunct tool for assessing swallowing function. However, there is currently insufficient knowledge about the patterning and timing of lingual and hyoid movement in a typical swallow. We know that movement of the hyoid plays an essential role in bolus transition and airway protection. However, manual tracking of hyoid movement is time-consuming and restricts the extent of large-scale normative studies. What this study adds We show that hyoid movement can be tracked automatically, providing measurable continuous positional data. Measurements derived from this objective data are comparable with similar measures previously reported using videofluoroscopy and of the two automatic trackers assessed, the DNN approach demonstrates better robustness and higher agreement with manually derived measures. Using this kinematic data, hyoid movement can be related to different stages of swallowing. Clinical implications of this study This study contributes towards our understanding of the kinematics of a typical swallow by evaluating an automated hyoid tracking method, paving the way for future studies of typical and disordered swallow. The challenges of image acquisition highlight issues to be considered when establishing clinical protocols. The application of machine learning enhances the utility of ultrasound swallowing assessment by reducing the labour required and permitting a wider range of hyoid measurements. Further research in normative and clinical populations is facilitated by automatic data extraction allowing the validity of prospective hyoid measures in differentiating different types of swallows to be rigorously assessed.", "journal": "International journal of language & communication disorders", "date": "2022-03-15", "authors": ["Joan K-YMa", "Alan AWrench"], "doi": "10.1111/1460-6984.12712\n10.1155/2014/738971\n10.1186/s12877-020-01832-0\n10.1186/s12938-017-0412-1\n10.1038/s41598-020-80871-4"}
{"title": "Integrated model for COVID-19 diagnosis based on computed tomography artificial intelligence, and clinical features: a multicenter cohort study.", "abstract": "We developed and validated a machine learning diagnostic model for the novel coronavirus (COVID-19) disease, integrating artificial-intelligence-based computed tomography (CT) imaging and clinical features.\nWe conducted a retrospective cohort study in 11 Japanese tertiary care facilities that treated COVID-19 patients. Participants were tested using both real-time reverse transcription polymerase chain reaction (RT-PCR) and chest CTs between January 1 and May 30, 2020. We chronologically split the dataset in each hospital into training and test sets, containing patients in a 7:3 ratio. A Light Gradient Boosting Machine model was used for the analysis.\nA total of 703 patients were included, and two models-the full model and the A-blood model-were developed for their diagnosis. The A-blood model included eight variables (the Ali-M3 confidence, along with seven clinical features of blood counts and biochemistry markers). The areas under the receiver-operator curve of both models [0.91, 95% confidence interval (CI): 0.86 to 0.95 for the full model and 0.90, 95% CI: 0.86 to 0.94 for the A-blood model] were better than that of the Ali-M3 confidence (0.78, 95% CI: 0.71 to 0.83) in the test set.\nThe A-blood model, a COVID-19 diagnostic model developed in this study, combines machine-learning and CT evaluation with blood test data and performs better than the Ali-M3 framework existing for this purpose. This would significantly aid physicians in making a quicker diagnosis of COVID-19.", "journal": "Annals of translational medicine", "date": "2022-03-15", "authors": ["YukiKataoka", "YuyaKimura", "TatsuyoshiIkenoue", "YoshinoriMatsuoka", "JunichiMatsumoto", "JunjiKumasawa", "KentaroTochitatni", "HirakuFunakoshi", "TomohiroHosoda", "AikoKugimiya", "MichinoriShirano", "FumikoHamabe", "SachiyoIwata", "ShingoFukuma", "NoneNone"], "doi": "10.21037/atm-21-5571\n10.1038/s41577-021-00522-1\n10.1056/NEJMoa2035389\n10.1056/NEJMoa2034577\n10.1056/NEJMoa2109072\n10.1136/bmj.m1808\n10.1371/journal.pone.0251661\n10.4103/ijri.IJRI_739_20\n10.1136/bmj.m1328\n10.1038/s42256-021-00307-0\n10.1371/journal.pone.0258760\n10.7326/M14-0697\n10.7326/M20-1495\n10.1097/JTO.0b013e3181ec173d\n10.1097/EDE.0b013e3181c30fb2\n10.3995/jstroke.10781\n10.3389/fneur.2020.611504\n10.1136/bmj.n2231\n10.1177/0272989X09336144"}
{"title": "Impact of Age and Sex on COVID-19 Severity Assessed From Radiologic and Clinical Findings.", "abstract": "Data on the epidemiological characteristics and clinical features of COVID-19 in patients of different ages and sex are limited. Existing studies have mainly focused on the pediatric and elderly population.\nAssess whether age and sex interact with other risk factors to influence the severity of SARS-CoV-2 infection.\nThe study sample included all consecutive patients who satisfied the inclusion criteria and who were treated from 24 February to 1 July 2020 in Dubai Mediclinic Parkview (560 cases) and Al Ain Hospital (605 cases), United Arab Emirates. We compared disease severity estimated from the radiological findings among patients of different age groups and sex. To analyze factors associated with an increased risk of severe disease, we conducted uni- and multivariate regression analyses. Specifically, age, sex, laboratory findings, and personal risk factors were used to predict moderate and severe COVID-19 with conventional machine learning methods.\nNeed for \nAge and sex were important predictors of disease severity in the set of data typically collected on admission. Sexual dissimilarities reduced with age. Age disparities were more pronounced if studied with the clinical markers of disease severity than with the radiological markers. The impact of sex on the clinical markers was more evident than that of age in our study.", "journal": "Frontiers in cellular and infection microbiology", "date": "2022-03-15", "authors": ["YauhenStatsenko", "FatmahAl Zahmi", "TetianaHabuza", "Taleb MAlmansoori", "DaryaSmetanina", "Gillian LylianSimiyu", "KlausNeidl-Van Gorkom", "MilosLjubisavljevic", "RashaAwawdeh", "HossamElshekhali", "MartinLee", "NassimSalamin", "RuhinaSajid", "DhanyaKiran", "SanjayNihalani", "TomLoney", "AntonyBedson", "AlirezaDehdashtian", "JamalAl Koteesh"], "doi": "10.3389/fcimb.2021.777070\n10.22037/aaem.v9i1.1363\n10.1016/j.diagmicrobio.2019.114876\n10.1186/s12879-017-2712-2\n10.1016/j.mayocp.2020.05.014\n10.3386/w26947\n10.3389/fcimb.2021.773141\n10.1371/journal.pone.0243262\n10.1016/j.ijid.2020.05.021\n10.1007/s11547-020-01202-1\n10.1016/j.ejrad.2020.108972\n10.3390/jcm10010038\n10.1148/radiol.2020201433\n10.23812/Editorial-Conti-3\n10.1002/iid3.404\n10.1111/ggi.13960\n10.1017/S095026880600690X\n10.1016/j.crad.2020.04.005\n10.1186/s43055-021-00517-x\n10.1155/2021/6695707\n10.1093/cid/ciaa354\n10.1007/s11604-020-00973-x\n10.1016/j.jcv.2020.104362\n10.1136/emermed-2020-210528\n10.1183/13993003.00547-2020\n10.1007/s10096-021-04260-z\n10.1109/IIT50501.2020.9299110\n10.1016/j.imu.2021.100596\n10.1016/j.ajem.2020.12.076\n10.2139/ssrn.3618215\n10.1186/s41747-020-00173-2\n10.15585/mmwr.mm6927a3\n10.1093/eurpub/ckaa150\n10.1016/j.neuroimage.2011.09.015\n10.3389/fpubh.2020.00152\n10.1126/science.abi5273\n10.1038/s43856-021-00006-2\n10.1002/bies.201200099\n10.1016/j.jmii.2020.02.011\n10.1007/s41999-020-00356-5\n10.1016/j.jinf.2020.03.005\n10.4103/ijmm.IJMM_20_133\n10.1016/j.intimp.2020.107343\n10.1136/bmjopen-2020-039887\n10.1056/NEJMc2005073\n10.1097/RLI.0000000000000689\n10.1371/journal.pone.0244171\n10.1148/ryct.2020200248\n10.1016/j.coviro.2019.02.009\n10.1016/j.jiac.2020.09.003\n10.3389/fendo.2020.567824\n10.1038/s41467-020-19741-6\n10.1016/j.jointm.2021.05.002\n10.1002/JLB.3COVA0820-410RRR\n10.1111/apha.13538\n10.1136/bmjresp-2020-000644\n10.26719/emhj.20.105\n10.1086/650575\n10.3389/fmed.2021.607059\n10.1016/S1473-3099(20)30086-4\n10.1136/bmjopen-2020-044500\n10.1038/s41586-020-2700-3\n10.1177/097206340200400204\n10.1148/radiol.2020201754\n10.3389/fcimb.2021.680422\n10.1186/s12879-020-05154-9\n10.1126/science.283.5406.1277\n10.1038/s41586-020-2521-4\n10.3389/fphys.2021.627260\n10.1016/j.mayocp.2020.04.006\n10.1016/j.exger.2021.111423\n10.1016/S2213-2600(20)30079-5\n10.18632/aging.103298\n10.1016/S0140-6736(20)30566-3\n10.1097/RTI.0000000000000513"}
{"title": "Analytical mapping of information and communication technology in emerging infectious diseases using CiteSpace.", "abstract": "The prevalence of severe infectious diseases has become a major global health concern. Currently, the COVID-19 outbreak has spread across the world and has created an unprecedented humanitarian crisis. The proliferation of novel viruses has put traditional health systems under immense pressure and posed several serious issues. Henceforth, early detection, identification, rapid testing, and advanced surveillance systems are required to address public health emergencies. However, Information and Communication Technology (ICT) tackles several issues raised by this pandemic and significantly improves the quality of services in the health care sector. This paper presents an ICT-assisted scientometric analysis of infectious diseases, namely, airborne, food & waterborne, fomite-borne, sexually transmitted illnesses, and vector-borne illnesses. It assesses the international research status of this field in terms of citation structure, prolific journals, and country contributions. It has used the CiteSpace tool to address the visualization needs and in-depth insights of scientific literature to pinpoint core hotspots, research frontiers, emerging research areas, and ICT trends. The research finding reveals that mobile apps, telemedicine, and artificial intelligence technologies have greater scope to reduce the threats of infectious diseases. COVID-19, influenza, HIV, and malaria viruses have been identified as research hotspots whereas COVID-19, contact tracing applications, security and privacy concerns about users' data are the recent challenges in this field that need to address. The United States has produced higher research output in all domains of infectious diseases. Furthermore, it explores the co-occurrence network analysis and intellectual landscape of each domain of infectious diseases. It provides potential research directions and insightful clues to researchers and the academic fraternity for further research.", "journal": "Telematics and informatics", "date": "2022-03-15", "authors": ["Sandeep KumarSood", "Keshav SinghRawat", "DheerajKumar"], "doi": "10.1016/j.tele.2022.101796\n10.1073/pnas.1701410114\n10.1016/j.tele.2021.101755\n10.1016/j.tele.2020.101547\n10.1016/j.tele.2020.101475\n10.1016/j.tele.2021.101676\n10.1016/j.tele.2021.101712\n10.1111/exsy.12700\n10.1007/s11135-020-01061-y\n10.3390/ijerph18168578\n10.1080/15323269.2018.1471914\n10.1007/s00521-020-05626-8\n10.1109/access.2020.3005638\n10.1007/s40139-020-00213-x\n10.1007/s11069-021-04512-3\n10.1055/s-0040-1715798\n10.1080/07421222.2003.11045748\n10.1016/j.telpol.2021.102154\n10.2196/23250\n10.1007/s13205-020-02382-3\n10.2196/22098\n10.1142/s2424862220300057\n10.2196/12273\n10.1038/nature07634\n10.2105/ajph.2015.302696\n10.1016/s2589-7500(20)30054-6\n10.1056/nejmp2003539\n10.1056/nejmoa2001017\n10.2196/11303\n10.1016/j.cmpb.2015.08.003\n10.2196/mhealth.6436\n10.2196/mhealth.6436\n10.1002/jia2.25439\n10.1007/s10461-020-03054-2\n10.1016/j.jbi.2018.08.012\n10.3109/17518423.2014.880526\n10.1097/coh.0000000000000401\n10.1097/qad.0b013e32834380c1\n10.1080/09540121.2014.911811\n10.1007/s10461-014-0760-9\n10.1136/sextrans-2013-051494\n10.1007/s10508-016-0709-3\n10.1186/s13071-015-1166-x\n10.1186/s13071-017-2044-5\n10.1016/j.parepi.2017.12.001\n10.1186/1475-2875-8-271\n10.1016/j.trstmh.2007.09.003\n10.1016/s1473-3099(04)00973-9\n10.1371/journal.pntd.0001484\n10.1292/jvms.19-0654\n10.1186/s40249-018-0474-8"}
{"title": "Diagnosis of COVID-19 using chest X-ray images based on modified DarkCovidNet model.", "abstract": "Coronavirus disease, also known as COVID-19, is an infectious disease caused by SARS-CoV-2. It has a direct impact on the upper and lower respiratory tract and threatened the health of many people around the world. The latest statistics show that the number of people diagnosed with COVID-19 is growing exponentially. Diagnosing positive cases of COVID-19 is important for preventing further spread of the disease. Currently, Coronavirus is a serious threat to scientists, medical experts and researchers around the world from its detection to its treatment. It is currently detected using reverse transcription polymerase chain reaction (RT-PCR) analysis at the most test centers around the world. Yet, knowing the reliability of a deep learning based medical diagnosis is important for doctors to build confidence in the technology and improve treatment. The goal of this study is to develop a model that automatically identifies COVID-19 by using chest X-ray images. To achieve this, we modified the DarkCovidNet model which is based on a convolutional neural network (CNN) and plotted the experimental results for two scenarios: binary classification (COVID-19 versus No-findings) and multi-class classification (COVID-19 versus pneumonia versus No-findings). The model is trained on more than 10 thousand X-ray images and achieved an average accuracy of 99.53% and 94.18% for binary and multi-class classification, respectively. Therefore, the proposed method demonstrates the effectiveness of COVID-19 detection using X-ray images. Our model can be used to test the patient via cloud and also be used in situations where RT-PCR tests and other options aren't available.", "journal": "Evolutionary intelligence", "date": "2022-03-15", "authors": ["Dawit KirosRedie", "Abdulhakim EdaoSirko", "Tensaie MelkamuDemissie", "Semagn SisayTeferi", "Vimal KumarShrivastava", "Om PrakashVerma", "Tarun KumarSharma"], "doi": "10.1007/s12065-021-00679-7\n10.1056/NEJMoa2002032\n10.1016/S1473-3099(20)30237-1\n10.1016/j.compbiomed.2020.103792\n10.1016/j.heliyon.2018.e00938\n10.1007/s12065-020-00493-7\n10.1080/08839514.2020.1792034\n10.1049/iet-ipr.2019.0561\n10.1038/s41598-019-56847-4"}
{"title": "Classification of COVID-19 and Influenza Patients Using Deep Learning.", "abstract": "Coronavirus (COVID-19) is a deadly virus that initially starts with flu-like symptoms. COVID-19 emerged in China and quickly spread around the globe, resulting in the coronavirus epidemic of 2019-22. As this virus is very similar to influenza in its early stages, its accurate detection is challenging. Several techniques for detecting the virus in its early stages are being developed. Deep learning techniques are a handy tool for detecting various diseases. For the classification of COVID-19 and influenza, we proposed tailored deep learning models. A publicly available dataset of X-ray images was used to develop proposed models. According to test results, deep learning models can accurately diagnose normal, influenza, and COVID-19 cases. Our proposed long short-term memory (LSTM) technique outperformed the CNN model in the evaluation phase on chest X-ray images, achieving 98% accuracy.", "journal": "Contrast media & molecular imaging", "date": "2022-03-15", "authors": ["MuhammadAftab", "RashidAmin", "DeepikaKoundal", "HamzaAldabbas", "BaderAlouffi", "ZeshanIqbal"], "doi": "10.1155/2022/8549707\n10.1109/iccca49541.2020.9250907\n10.2807/1560-7917.ES.2020.25.47.2001943\n10.1016/j.jcmg.2020.10.023\n10.1109/iscv49265.2020.9204043\n10.1016/j.gene.2020.145145\n10.1007/s00330-021-07715-1\n10.1016/j.jcv.2020.104543\n10.1016/s0169-5347(02)02502-8\n10.1128/mr.56.1.152-179.1992\n10.1016/s0140-6736(99)01241-6\n10.1086/515616\n10.1016/j.bbe.2021.05.013\n10.1515/cclm-2020-1294\n10.1016/j.eswa.2020.114054\n10.36548/jismac.2021.2.006\n10.1016/j.cmpb.2020.105608\n10.1073/pnas.95.17.10224\n10.32604/cmc.2020.012148\n10.1142/s0219720020400028\n10.18632/aging.104132\n10.1007/s00592-020-01522-8\n10.1177/1756286420917830\n10.1002/jmv.26125\n10.1016/j.vaccine.2020.07.058\n10.1001/archinte.160.21.3243\n10.1086/529211\n10.1016/j.earlhumdev.2020.105116\n10.1186/s12916-020-01816-2\n10.1016/j.micinf.2020.05.016\n10.1038/s41598-021-83967-7\n10.1093/emph/eoaa050\n10.1109/access.2019.2927169\n10.1109/icarcv.2014.7064414\n10.1007/s11042-020-10165-4\n10.1016/j.isprsjprs.2019.01.015"}
{"title": "Ensemble Deep Learning and Internet of Things-Based Automated COVID-19 Diagnosis Framework.", "abstract": "Coronavirus disease (COVID-19) is a viral infection caused by SARS-CoV-2. The modalities such as computed tomography (CT) have been successfully utilized for the early stage diagnosis of COVID-19 infected patients. Recently, many researchers have utilized deep learning models for the automated screening of COVID-19 suspected cases. An ensemble deep learning and Internet of Things (IoT) based framework is proposed for screening of COVID-19 suspected cases. Three well-known pretrained deep learning models are ensembled. The medical IoT devices are utilized to collect the CT scans, and automated diagnoses are performed on IoT servers. The proposed framework is compared with thirteen competitive models over a four-class dataset. Experimental results reveal that the proposed ensembled deep learning model yielded 98.98% accuracy. Moreover, the model outperforms all competitive models in terms of other performance metrics achieving 98.56% precision, 98.58% recall, 98.75% F-score, and 98.57% AUC. Therefore, the proposed framework can improve the acceleration of COVID-19 diagnosis.", "journal": "Contrast media & molecular imaging", "date": "2022-03-15", "authors": ["Anita SKini", "A NandaGopal Reddy", "ManjitKaur", "SSatheesh", "JagendraSingh", "ThomasMartinetz", "HammamAlshazly"], "doi": "10.1155/2022/7377502\n10.1109/jbhi.2020.3001216\n10.3390/su132413642\n10.1007/s12652-020-02669-6\n10.1109/tmi.2020.2995965\n10.1109/tmi.2020.2996256\n10.1142/s0218001421510046\n10.1109/JIOT.2020.3034074\n10.1109/access.2020.3010287\n10.7717/peerj-cs.564\n10.1007/s10462-021-09985-z\n10.1109/access.2020.3001973\n10.1007/978-3-030-55258-9_17\n10.1002/ima.22469\n10.7717/peerj-cs.306\n10.1109/tmi.2020.2993291\n10.1109/jbhi.2020.3023246\n10.1109/access.2020.3005510\n10.1109/access.2020.3003810\n10.1109/access.2020.3033762\n10.1109/access.2020.3025010\n10.1007/s10489-020-02149-6\n10.1109/tmi.2020.2994908\n10.1109/access.2020.2994762\n10.1109/jbhi.2020.3019505\n10.1109/access.2020.3025164\n10.3390/s21020455\n10.7717/peerj-cs.655\n10.3390/app11157004\n10.1155/2021/8829829\n10.2196/23811\n10.1109/access.2021.3120717\n10.1049/trit.2018.1006\n10.37965/jait.2021.0017\n10.1049/trit.2019.0028\n10.37965/jait.2020.0037\n10.1049/trit.2019.0051\n10.37965/jait.2020.0051\n10.1109/access.2020.3024116\n10.1109/access.2021.3109441\n10.3233/xst-200715\n10.1109/34.58871\n10.3390/s19194139\n10.1142/S0129065716500258\n10.1016/j.bspc.2021.103009\n10.1109/access.2021.3101142\n10.1016/j.asoc.2020.106885\n10.1038/s41467-020-17971-2\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2020.104037"}
{"title": "Evaluation of Pulmonary Edema Using Ultrasound Imaging in Patients With COVID-19 Pneumonia Based on a Non-local Channel Attention ResNet.", "abstract": "Recent research has revealed that COVID-19 pneumonia is often accompanied by pulmonary edema. Pulmonary edema is a manifestation of acute lung injury (ALI), and may progress to hypoxemia and potentially acute respiratory distress syndrome (ARDS), which have higher mortality. Precise classification of the degree of pulmonary edema in patients is of great significance in choosing a treatment plan and improving the chance of survival. Here we propose a deep learning neural network named Non-local Channel Attention ResNet to analyze the lung ultrasound images and automatically score the degree of pulmonary edema of patients with COVID-19 pneumonia. The proposed method was designed by combining the ResNet with the non-local module and the channel attention mechanism. The non-local module was used to extract the information on characteristics of A-lines and B-lines, on the basis of which the degree of pulmonary edema could be defined. The channel attention mechanism was used to assign weights to decisive channels. The data set contains 2220 lung ultrasound images provided by Huoshenshan Hospital, Wuhan, China, of which 2062 effective images with accurate scores assigned by two experienced clinicians were used in the experiment. The experimental results indicated that our method achieved high accuracy in classifying the degree of pulmonary edema in patients with COVID-19 pneumonia by comparison with previous deep learning methods, indicating its potential to monitor patients with COVID-19 pneumonia.", "journal": "Ultrasound in medicine & biology", "date": "2022-03-13", "authors": ["QinghuaHuang", "YeLei", "WenyuXing", "ChaoHe", "GaofengWei", "ZhaojiMiao", "YifanHao", "GuannanLi", "YanWang", "QingliLi", "XuelongLi", "WenfangLi", "JiangangChen"], "doi": "10.1016/j.ultrasmedbio.2022.01.023"}
{"title": "Study of Different Deep Learning Methods for Coronavirus (COVID-19) Pandemic: Taxonomy, Survey and Insights.", "abstract": "COVID-19 has evolved into one of the most severe and acute illnesses. The number of deaths continues to climb despite the development of vaccines and new strains of the virus have appeared. The early and precise recognition of COVID-19 are key in viably treating patients and containing the pandemic on the whole. Deep learning technology has been shown to be a significant tool in diagnosing COVID-19 and in assisting radiologists to detect anomalies and numerous diseases during this epidemic. This research seeks to provide an overview of novel deep learning-based applications for medical imaging modalities, computer tomography (CT) and chest X-rays (CXR), for the detection and classification COVID-19. First, we give an overview of the taxonomy of medical imaging and present a summary of types of deep learning (DL) methods. Then, utilizing deep learning techniques, we present an overview of systems created for COVID-19 detection and classification. We also give a rundown of the most well-known databases used to train these networks. Finally, we explore the challenges of using deep learning algorithms to detect COVID-19, as well as future research prospects in this field.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-03-11", "authors": ["LamiaAwassa", "ImenJdey", "HabibDhahri", "GhazalaHcini", "AwaisMahmood", "EsamOthman", "MuhammadHaneef"], "doi": "10.3390/s22051890\n10.7717/peerj.9725\n10.1016/j.diii.2020.03.014\n10.1109/ACCESS.2020.3010287\n10.47419/bjbabs.v2i01.25\n10.15585/mmwr.mm7003e2\n10.1101/2021.02.10.21251247\n10.1016/j.cmi.2021.05.022\n10.1101/2021.01.03.21249169\n10.1101/2020.12.30.20249034\n10.32604/cmc.2020.013232\n10.1097/RTI.0000000000000533\n10.1038/npre.2009.3267.2\n10.3174/ajnr.A4967\n10.1016/j.ejrad.2020.109151\n10.1148/radiol.2020202568\n10.1088/1742-6596/1228/1/012045\n10.1016/j.media.2017.07.005\n10.1007/s00259-020-04953-1\n10.1038/s41746-021-00399-3\n10.1007/s42600-021-00151-6\n10.1016/j.irbm.2020.05.003\n10.1007/s11548-020-02286-w\n10.1155/2021/5527923\n10.3390/app10165683\n10.1109/ACCESS.2020.3025010\n10.32604/cmc.2021.018449\n10.4249/scholarpedia.32832\n10.1016/j.imu.2020.100412\n10.1016/j.jbi.2020.103627\n10.4249/scholarpedia.5947\n10.1016/j.cie.2022.107960\n10.1186/s12880-020-00529-5\n10.1109/TNNLS.2021.3054746\n10.1002/mp.14609\n10.1186/s12967-021-02992-2\n10.3390/diagnostics11081405\n10.1016/j.patcog.2021.108452\n10.11591/ijece.v11i1.pp844-850\n10.11591/ijece.v11i1.pp365-374\n10.1007/s00138-020-01119-9\n10.3390/computation9010003\n10.1007/s12539-020-00408-1\n10.32604/cmc.2020.012585\n10.1007/s00330-021-07715-1\n10.1016/j.bspc.2021.102588\n10.1080/07391102.2021.1875049\n10.1016/j.ipm.2020.102411\n10.1148/radiol.2020203511\n10.1007/s00521-020-05410-8\n10.1016/j.imu.2020.100505\n10.1016/j.bbe.2020.08.008\n10.1101/2020.03.12.20027185\n10.1007/s10140-020-01886-y\n10.1007/s10278-013-9622-7\n10.1038/s41597-020-00741-6\n10.3390/sym13010113\n10.1007/s40747-020-00199-4\n10.1007/s10489-020-01867-1\n10.1007/s12559-020-09787-5\n10.1007/s10489-020-01902-1\n10.1080/07391102.2020.1767212\n10.1038/s42256-021-00338-7\n10.1038/s41598-020-76550-z\n10.1016/j.asoc.2020.106885\n10.1097/RLI.0000000000000748\n10.1016/j.patcog.2020.107613\n10.1145/3431804\n10.1016/j.asoc.2020.106912\n10.1016/j.asoc.2020.106859\n10.1016/j.asoc.2020.106744\n10.1016/j.compbiomed.2020.104181\n10.1016/j.bspc.2020.102257\n10.1016/j.chaos.2020.110495\n10.1088/1757-899X/1051/1/012007\n10.1007/s10489-020-01978-9\n10.1016/j.asoc.2021.107160\n10.1038/s42003-020-01535-7\n10.1016/j.neucom.2021.03.034\n10.1016/j.compbiomed.2020.103805\n10.1109/TMI.2020.3040950\n10.1016/j.chaos.2020.110245\n10.1016/j.inffus.2021.04.008\n10.1117/1.JMI.3.4.044506\n10.1016/j.media.2017.06.015\n10.1109/ACCESS.2021.3058537\n10.1186/s40537-019-0197-0"}
{"title": "Trends in Illness Severity, Hospitalization, and Mortality for Community-Onset Pneumonia at 118 US Veterans Affairs Medical Centers.", "abstract": "Deaths from pneumonia were decreasing globally prior to the COVID-19 pandemic, but it is unclear whether this was due to changes in patient populations, illness severity, diagnosis, hospitalization thresholds, or treatment. Using clinical data from the electronic health record among a national cohort of patients initially diagnosed with pneumonia, we examined temporal trends in severity of illness, hospitalization, and short- and long-term deaths.\nRetrospective cohort PARTICIPANTS: All patients >18 years presenting to emergency departments (EDs) at 118 VA Medical Centers between 1/1/2006 and 12/31/2016 with an initial clinical diagnosis of pneumonia and confirmed by chest imaging report.\nYear of encounter.\nHospitalization and 30-day and 90-day mortality. Illness severity was defined as the probability of each outcome predicted by machine learning predictive models using age, sex, comorbidities, vital signs, and laboratory data from encounters during years 2006-2007, and similar models trained on encounters from years 2015 to 2016. We estimated the changes in hospitalizations and 30-day and 90-day mortality between the first and the last 2 years of the study period accounted for by illness severity using time covariate decompositions with model estimates.\nAmong 196,899 encounters across the study period, hospitalization decreased from 71 to 63%, 30-day mortality 10 to 7%, 90-day mortality 16 to 12%, and 1-year mortality 29 to 24%. Comorbidity risk increased, but illness severity decreased. Decreases in illness severity accounted for 21-31% of the decrease in hospitalizations, and 45-47%, 32-24%, and 17-19% of the decrease in 30-day, 90-day, and 1-year mortality. Findings were similar among underrepresented patients and those with only hospital discharge diagnosis codes.\nOutcomes for community-onset pneumonia have improved across the VA healthcare system after accounting for illness severity, despite an increase in cases and comorbidity burden.", "journal": "Journal of general internal medicine", "date": "2022-03-11", "authors": ["Barbara EJones", "JianYing", "Mckenna RNevers", "Patrick RAlba", "Olga VPatterson", "Kelly SPeterson", "ElizabethRutter", "Matthew AChristensen", "SarahStern", "Makoto MJones", "AdiGundlapalli", "Nathan CDean", "Matthew CSamore", "TomeGreene"], "doi": "10.1007/s11606-022-07413-8\n10.1177/2049936120969607\n10.1097/Mlr.0b013e3181f38006\n10.1001/Jama.2012.384\n10.1177/1062860605280358\n10.1016/J.Jclinepi.2006.10.018\n10.1177/0885713x9701200404\n10.1016/J.Amjmed.2004.03.029\n10.1055/S-0038-1626725\n10.1001/Jama.2016.0287\n10.1086/589754\n10.1007/Bf01709751\n10.1097/00003246-200107000-00002\n10.1056/Nejm199701233360402\n10.1378/Chest.101.6.1644\n10.1016/J.Jbi.2019.103255\n10.1378/Chest.101.6.1644\n10.1186/S12931-018-0781-4\n10.1016/J.Vaccine.2015.12.003\n10.1016/J.Ejim.2016.12.010\n10.1378/Chest.09-2702\n10.5888/Pcd14.170230\n10.1159/000486647\n10.7205/Milmed-D-15-00035\n10.1371/Journal.Pone.0193996\n10.1111/Resp.13233\n10.1016/J.Ajem.2019.158423\n10.1001/Jamanetworkopen.2020.8120\n10.1056/Nejmoa1209165\n10.2105/Ajph.2006.099440\n10.1056/Nejmoa022139\n10.1097/Ccm.0b013e3181cb0cdc\n10.1016/J.Ahj.2017.05.018\n10.1097/Mcp.0000000000000155\n10.1016/J.Amjmed.2005.01.006\n10.1111/J.1532-5415.2008.02006.X\n10.1097/01.Nhh.0000269965.16119.E5\n10.1186/S12913-015-0964-3"}
{"title": "A radiomics-boosted deep-learning model for COVID-19 and non-COVID-19 pneumonia classification using chest x-ray images.", "abstract": "To develop a deep learning model design that integrates radiomics analysis for enhanced performance of COVID-19 and non-COVID-19 pneumonia detection using chest x-ray images.\nAs a novel radiomics approach, a 2D sliding kernel was implemented to map the impulse response of radiomic features throughout the entire chest x-ray image; thus, each feature is rendered as a 2D map in the same dimension as the x-ray image. Based on each of the three investigated deep neural network architectures, including VGG-16, VGG-19, and DenseNet-121, a pilot model was trained using x-ray images only. Subsequently, two radiomic feature maps (RFMs) were selected based on cross-correlation analysis in reference to the pilot model saliency map results. The radiomics-boosted model was then trained based on the same deep neural network architecture using x-ray images plus the selected RFMs as input. The proposed radiomics-boosted design was developed using 812 chest x-ray images with 262/288/262 COVID-19/non-COVID-19 pneumonia/healthy cases, and 649/163 cases were assigned as training-validation/independent test sets. For each model, 50 runs were trained with random assignments of training/validation cases following the 7:1 ratio in the training-validation set. Sensitivity, specificity, accuracy, and ROC curves together with area-under-the-curve (AUC) from all three deep neural network architectures were evaluated.\nAfter radiomics-boosted implementation, all three investigated deep neural network architectures demonstrated improved sensitivity, specificity, accuracy, and ROC AUC results in COVID-19 and healthy individual classifications. VGG-16 showed the largest improvement in COVID-19 classification ROC (AUC from 0.963 to 0.993), and DenseNet-121 showed the largest improvement in healthy individual classification ROC (AUC from 0.962 to 0.989). The reduced variations suggested improved robustness of the model to data partition. For the challenging non-COVID-19 pneumonia classification task, radiomics-boosted implementation of VGG-16 (AUC from 0.918 to 0.969) and VGG-19 (AUC from 0.964 to 0.970) improved ROC results, while DenseNet-121 showed a slight yet insignificant ROC performance reduction (AUC from 0.963 to 0.949). The achieved highest accuracy of COVID-19/non-COVID-19 pneumonia/healthy individual classifications were 0.973 (VGG-19)/0.936 (VGG-19)/ 0.933 (VGG-16), respectively.\nThe inclusion of radiomic analysis in deep learning model design improved the performance and robustness of COVID-19/non-COVID-19 pneumonia/healthy individual classification, which holds great potential for clinical applications in the COVID-19 pandemic.", "journal": "Medical physics", "date": "2022-03-10", "authors": ["ZongshengHu", "ZhenyuYang", "Kyle JLafata", "Fang-FangYin", "ChunhaoWang"], "doi": "10.1002/mp.15582\n10.1007/s00259-020-05075-4\n10.1007/s00261-021-03254-x\n10.48550/arXiv.2003.11597\n10.1109/cvpr.2009.5206848\n10.1088/2057-1976/ab779c"}
{"title": "Truncating fined-tuned vision-based models to lightweight deployable diagnostic tools for SARS-CoV-2 infected chest X-rays and CT-scans.", "abstract": "In such a brief period, the recent coronavirus (COVID-19) already infected large populations worldwide. Diagnosing an infected individual requires a Real-Time Polymerase Chain Reaction (RT-PCR) test, which can become expensive and limited in most developing countries, making them rely on alternatives like Chest X-Rays (CXR) or Computerized Tomography (CT) scans. However, results from these imaging approaches radiated confusion for medical experts due to their similarities with other diseases like pneumonia. Other solutions based on Deep Convolutional Neural Network (DCNN) recently improved and automated the diagnosis of COVID-19 from CXRs and CT scans. However, upon examination, most proposed studies focused primarily on accuracy rather than deployment and reproduction, which may cause them to become difficult to reproduce and implement in locations with inadequate computing resources. Therefore, instead of focusing only on accuracy, this work investigated the effects of parameter reduction through a proposed truncation method and analyzed its effects. Various DCNNs had their architectures truncated, which retained only their initial core block, reducing their parameter sizes to <1\u00a0M. Once trained and validated, findings have shown that a DCNN with robust layer aggregations like the InceptionResNetV2 had less vulnerability to the adverse effects of the proposed truncation. The results also showed that from its full-length size of 55\u00a0M with 98.67% accuracy, the proposed truncation reduced its parameters to only 441\u00a0K and still attained an accuracy of 97.41%, outperforming other studies based on its size to performance ratio.", "journal": "Multimedia tools and applications", "date": "2022-03-10", "authors": ["Francis JesmarMontalbo"], "doi": "10.1007/s11042-022-12484-0\n10.1016/j.chaos.2020.110120\n10.2196/19673\n10.1016/j.radi.2020.09.010\n10.1148/radiol.2020201491\n10.1109/ACCESS.2014.2325029\n10.1007/s13246-020-00888-x\n10.1134/s1054661816010065\n10.1016/j.cmpb.2018.01.025\n10.1016/j.bsheal.2020.05.002\n10.1016/j.ibmed.2021.100027\n10.5121/ijdkp.2015.5201\n10.1016/j.compbiomed.2021.104348\n10.3390/app10103359\n10.1155/2018/2061516\n10.1148/radiol.2020200905\n10.1016/j.bspc.2021.102583\n10.1016/j.patrec.2020.10.001\n10.1016/j.ijnss.2020.03.012\n10.5555/2627435.2670313\n10.1016/j.immuni.2020.05.004\n10.1089/omi.2019.0142\n10.1021/acsnano.0c02624\n10.1016/j.eng.2020.04.010\n10.1016/j.inffus.2019.06.024"}
{"title": "The year in cardiovascular medicine 2021: imaging.", "abstract": "This article reviews the most relevant literature published in 2021 on the role of cardiovascular imaging in cardiovascular medicine. Coronavirus disease 2019 (COVID-19) continued to impact the healthcare landscape, resulting in reduced access to hospital-based cardiovascular care including reduced routine diagnostic cardiovascular testing. However, imaging has also facilitated the understanding of the presence and extent of myocardial damage caused by the coronavirus infection. What has dominated the imaging literature beyond the pandemic are novel data on valvular heart disease, the increasing use of artificial intelligence (AI) applied to imaging, and the use of advanced imaging modalities in both ischaemic heart disease and cardiac amyloidosis.", "journal": "European heart journal", "date": "2022-03-09", "authors": ["ChiaraBucciarelli-Ducci", "NinaAjmone-Marsan", "MarceloDi Carli", "EdwardNicol"], "doi": "10.1093/eurheartj/ehac033"}
{"title": "An Improved COVID-19 Detection using GAN-Based Data Augmentation and Novel QuNet-Based Classification.", "abstract": "COVID-19 is a fatal disease caused by the SARS-CoV-2 virus that has caused around 5.3 Million deaths globally as of December 2021. The detection of this disease is a time taking process that have worsen the situation around the globe, and the disease has been identified as a world pandemic by the WHO. Deep learning-based approaches are being widely used to diagnose the COVID-19 cases, but the limitation of immensity in the publicly available dataset causes the problem of model over-fitting. Modern artificial intelligence-based techniques can be used to increase the dataset to avoid from the over-fitting problem. This research work presents the use of various deep learning models along with the state-of-the-art augmentation methods, namely, classical and generative adversarial network- (GAN-) based data augmentation. Furthermore, four existing deep convolutional networks, namely, DenseNet-121, InceptionV3, Xception, and ResNet101 have been used for the detection of the virus in X-ray images after training on augmented dataset. Additionally, we have also proposed a novel convolutional neural network (QuNet) to improve the COVID-19 detection. The comparative analysis of achieved results reflects that both QuNet and Xception achieved high accuracy with classical augmented dataset, whereas QuNet has also outperformed and delivered 90% detection accuracy with GAN-based augmented dataset.", "journal": "BioMed research international", "date": "2022-03-09", "authors": ["UsmanAsghar", "MuhammadArif", "KhurramEjaz", "DragosVicoveanu", "DianaIzdrui", "OanaGeman"], "doi": "10.1155/2022/8925930\n10.1109/ACCESS.2020.2994762\n10.1007/s00521-020-05437-x\n10.1016/j.compbiomed.2021.104930\n10.1007/s10462-021-10066-4\n10.32604/cmc.2021.014265\n10.1007/s00500-021-06075-8\n10.1016/j.compbiomed.2020.104130\n10.1016/j.bspc.2020.102365\n10.1007/s10489-020-01826-w\n10.1016/j.compbiomed.2020.104181\n10.1007/978-3-030-86340-1_47\n10.1007/s40009-020-01009-8"}
{"title": "Radiological Analysis of COVID-19 Using Computational Intelligence: A Broad Gauge Study.", "abstract": "Pulmonary medical image analysis using image processing and deep learning approaches has made remarkable achievements in the diagnosis, prognosis, and severity check of lung diseases. The epidemic of COVID-19 brought out by the novel coronavirus has triggered a critical need for artificial intelligence assistance in diagnosing and controlling the disease to reduce its effects on people and global economies. This study aimed at identifying the various COVID-19 medical imaging analysis models proposed by different researchers and featured their merits and demerits. It gives a detailed discussion on the existing COVID-19 detection methodologies (diagnosis, prognosis, and severity/risk detection) and the challenges encountered for the same. It also highlights the various preprocessing and post-processing methods involved to enhance the detection mechanism. This work also tries to bring out the different unexplored research areas that are available for medical image analysis and how the vast research done for COVID-19 can advance the field. Despite deep learning methods presenting high levels of efficiency, some limitations have been briefly described in the study. Hence, this review can help understand the utilization and pros and cons of deep learning in analyzing medical images.", "journal": "Journal of healthcare engineering", "date": "2022-03-08", "authors": ["SVineth Ligi", "Soumya SnigdhaKundu", "RKumar", "RNarayanamoorthi", "Khin WeeLai", "SamiappanDhanalakshmi"], "doi": "10.1155/2022/5998042\n10.1001/jama.2020.3786\n10.1148/rg.2020200159\n10.1016/j.mayocp.2020.04.004\n10.1148/ryct.2020200034\n10.1016/s0140-6736(20)30183-5\n10.12669/pjms.36.COVID19-S4.2778\n10.1007/s12065-020-00540-3\n10.1148/radiol.2018180547\n10.1016/j.procs.2018.05.198\n10.1109/tmi.2016.2528162\n10.1109/tpami.2021.3059968\n10.1007/978-3-030-01261-8_1\n10.1007/s11042-020-09894-3\n10.2174/1573405614666180402124438\n10.1109/icectech.2011.5941891\n10.1186/2193-8636-1-6\n10.1109/icosec49089.2020.9215356\n10.1016/j.compeleceng.2021.107225\n10.1109/cvpr.2018.00474\n10.1113/jphysiol.1959.sp006308\n10.1007/bf00344251\n10.1109/cvpr.2015.7298594\n10.1109/acpr.2015.7486599\n10.1109/cvpr.2016.90\n10.1007/978-3-319-46493-0_38\n10.1109/cvpr.2017.195\n10.1109/cvpr.2017.634\n10.1109/cvpr.2018.00745\n10.1109/cvpr.2018.00907\n10.1109/iccv.2017.74\n10.3390/s21020455\n10.1007/s00521-020-05636-6\n10.1186/s40537-020-00392-9\n10.1038/s41467-020-17971-2\n10.1109/tmi.2020.2995508\n10.1016/j.ejrad.2020.109041\n10.1016/j.compbiomed.2020.103795\n10.1109/jbhi.2020.3019505\n10.1007/s10044-021-00984-y\n10.1109/cvpr.2017.369\n10.1016/j.bbe.2020.08.005\n10.1016/j.cell.2018.02.010\n10.1007/s13246-020-00952-6\n10.1038/s41598-020-76550-z\n10.1109/access.2020.3010287\n10.1109/tmi.2020.2994908\n10.3390/e22050517\n10.1109/tcbb.2021.3065361\n10.1016/j.compbiomed.2020.103805\n10.1038/s41598-020-74539-2\n10.1016/j.bspc.2020.102257\n10.3390/healthcare9050522\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103869\n10.1016/j.chaos.2020.110182\n10.1007/s10489-020-01831-z\n10.1007/s10096-020-03901-z\n10.1109/cbms.2015.49\n10.3390/ijerph18063056\n10.1038/s41598-020-76282-0\n10.1093/jamia/ocv080\n10.1038/s41598-020-74164-z\n10.1016/j.chaos.2020.109944\n10.1016/j.bspc.2021.102588\n10.7150/ijms.46684\n10.1148/ryct.2020200082\n10.1016/j.asoc.2020.106897\n10.1148/ryct.2020200075\n10.1183/13993003.00775-2020\n10.1038/s41467-020-18786-x\n10.1001/jamainternmed.2020.2033\n10.7150/thno.46428\n10.7759/cureus.9448\n10.1148/ryai.2019180041\n10.1609/aaai.v33i01.3301590\n10.1148/radiol.2019191293\n10.1016/j.media.2020.101797\n10.1371/journal.pone.0236621\n10.1109/jbhi.2020.3037127\n10.1016/j.patcog.2020.107700\n10.1155/2020/9756518\n10.1186/s12864-019-6413-7\n10.26355/eurrev_202011_23640\n10.1109/access.2021.3058537\n10.1016/j.cmpb.2018.06.006\n10.1038/s41598-019-39071-y\n10.1155/2021/9208138\n10.1001/jamainternmed.2013.3023\n10.1093/intqhc/mzaa144\n10.1002/jmv.27281\n10.1109/wacv48630.2021.00362\n10.1109/cvpr.2016.319\n10.1186/s12880-015-0068-x\n10.1016/j.media.2021.101978\n10.1109/icacci.2014.6968381\n10.1016/s0734-189x(87)80186-x\n10.1109/42.816070\n10.1007/978-3-642-34303-2_11\n10.1109/icaca.2016.7887983\n10.1016/j.ijleo.2021.166652\n10.1109/access.2020.2994762\n10.1142/s021800142051009x\n10.1016/j.patcog.2020.107747\n10.1016/j.inffus.2021.04.008\n10.1007/s00500-020-05424-3"}
{"title": "A CNN based coronavirus disease prediction system for chest X-rays.", "abstract": "Coronavirus disease (COVID-19) proliferated globally in early 2020, causing existential dread in the whole world. Radiography is crucial in the clinical staging and diagnosis of COVID-19 and offers high potential to improve healthcare plans for tackling the pandemic. However high variations in infection characteristics and low contrast between normal and infected regions pose great challenges in preparing radiological reports. To address these challenges, this study presents CODISC-CNN (CNN based Coronavirus DIsease Prediction System for Chest X-rays) that can automatically extract the features from chest X-ray images for the disease prediction. However, to get the infected region of X-ray, edges of the images are detected by applying image preprocessing. Furthermore, to attenuate the shortage of labeled datasets data augmentation has been adapted. Extensive experiments have been performed to classify X-ray images into two classes (Normal and COVID), three classes (Normal, COVID, and Virus Bacteria), and four classes (Normal, COVID, and Virus Bacteria, and Virus Pneumonia) with the accuracy of 97%, 89%, and 84% respectively. The proposed CNN-based model outperforms many cutting-edge classification models and boosts state-of-the-art performance.", "journal": "Journal of ambient intelligence and humanized computing", "date": "2022-03-08", "authors": ["UmairHafeez", "MuhammadUmer", "AhmadHameed", "HassanMustafa", "AhmedSohaib", "MicheleNappi", "Hamza AhmadMadni"], "doi": "10.1007/s12652-022-03775-3\n10.1007/s10489-020-01829-7\n10.1007/s1324\n10.1016/j.compbiomed.2020.103795\n10.1016/j.compbiomed.2020.103795\n10.1093/aje/kws259\n10.1109/TII.2021.3057524\n10.1016/j.chaos.2020.109864\n10.3390/ijerph17082690\n10.1073/pnas.2004168117\n10.1111/vox.12939\n10.1016/S0140-6736(20)30183-5\n10.1145/3065386\n10.1371/journal.ppat.0030151\n10.1016/j.chaos.2020.109853\n10.1001/jama.2020.1585\n10.1038/s41598-019-56847-4\n10.1016/S2213-2600(20)30076-X\n10.1016/j.compbiomed.2020.103671\n10.1016/j.compbiolchem.2009.07.005"}
{"title": "COVID-19 diagnosis on CT images with Bayes optimization-based deep neural networks and machine learning algorithms.", "abstract": "Early diagnosis of COVID-19, the new coronavirus disease, is considered important for the treatment and control of this disease. The diagnosis of COVID-19 is based on two basic approaches of laboratory and chest radiography, and there has been a significant increase in studies performed in recent months by using chest computed tomography (CT) scans and artificial intelligence techniques. Classification of patient CT scans results in a serious loss of radiology professionals' valuable time. Considering the rapid increase in COVID-19 infections, in order to automate the analysis of CT scans and minimize this loss of time, in this paper a new method is proposed using BO (BO)-based MobilNetv2, ResNet-50 models, SVM and kNN machine learning algorithms. In this method, an accuracy of 99.37% was achieved with an average precision of 99.38%, 99.36% recall and 99.37% F-score on datasets containing COVID and non-COVID classes. When we examine the performance results of the proposed method, it is predicted that it can be used as a decision support mechanism with high classification success for the diagnosis of COVID-19 with CT scans.", "journal": "Neural computing & applications", "date": "2022-03-08", "authors": ["MuratCanayaz", "Sanem\u015eehribano\u011flu", "Recep\u00d6zda\u011f", "MuratDemir"], "doi": "10.1007/s00521-022-07052-4\n10.1155/2020/9756518\n10.1148/radiol.2020200642\n10.1007/s10096-020-03901-z\n10.1148/radiol.2020200823\n10.1016/j.crad.2020.06.005\n10.1049/trit.2019.0017\n10.1148/radiol.2020200432\n10.1016/j.imu.2020.100427\n10.1101/2020.04.16.20064709\n10.1109/TMI.2020.2995965\n10.1080/07391102.2020.1788642\n10.3390/math9222921\n10.3389/fpubh.2020.00441\n10.1016/j.mehy.2020.109761\n10.1007/s42979-021-00980-3\n10.1016/j.asoc.2020.106580\n10.1371/journal.pcbi.1009472\n10.3390/app10186448\n10.1016/j.media.2020.101836\n10.1016/j.chaos.2020.110190\n10.1101/2020.04.13.20063941\n10.1109/TIP.2021.3058783\n10.1016/j.jnlest.2020.100007\n10.1007/s00138-020-01087-0\n10.1016/j.catena.2019.104249\n10.1109/JPROC.2015.2494218\n10.4304/jcp.8.10.2632-2639\n10.1088/1742-6596/1442/1/012027\n10.1155/2014/795624\n10.1023/A:1012487302797\n10.1007/s10489-017-0992-2\n10.1007/BF00994018\n10.21037/atm.2016.03.37\n10.4249/scholarpedia.1883\n10.1101/2020.04.24.20078584\n10.1016/j.bspc.2020.102257"}
{"title": "Artificial intelligence computed tomography helps evaluate the severity of COVID-19 patients: A retrospective study.", "abstract": "Computed tomography (CT) is a noninvasive imaging approach to assist the early diagnosis of pneumonia. However, coronavirus disease 2019 (COVID-19) shares similar imaging features with other types of pneumonia, which makes differential diagnosis problematic. Artificial intelligence (AI) has been proven successful in the medical imaging field, which has helped disease identification. However, whether AI can be used to identify the severity of COVID-19 is still underdetermined.\nData were extracted from 140 patients with confirmed COVID-19. The severity of COVID-19 patients (severe vs. non-severe) was defined at admission, according to American Thoracic Society (ATS) guidelines for community-acquired pneumonia (CAP). The AI-CT rating system constructed by Hangzhou YITU Healthcare Technology Co., Ltd. was used as the analysis tool to analyze chest CT images.\nA total of 117 diagnosed cases were enrolled, with 40 severe cases and 77 non-severe cases. Severe patients had more dyspnea symptoms on admission (12 vs. 3), higher acute physiology and chronic health evaluation (APACHE) II (9 vs. 4) and sequential organ failure assessment (SOFA) (3 vs. 1) scores, as well as higher CT semiquantitative rating scores (4 vs. 1) and AI-CT rating scores than non-severe patients (\nAI technology could be used to evaluate disease severity in COVID-19 patients. Although it could not be considered an independent factor, there was no doubt that GGOs displayed more predictive value for further mechanical ventilation.", "journal": "World journal of emergency medicine", "date": "2022-03-04", "authors": ["YiHan", "Su-ChengMu", "Hai-DongZhang", "WeiWei", "Xing-YueWu", "Chao-YuanJin", "Guo-RongGu", "Bao-JunXie", "Chao-YangTong"], "doi": "10.5847/wjem.j.1920-8642.2022.026"}
{"title": "MedRDF: A Robust and Retrain-Less Diagnostic Framework for Medical Pretrained Models Against Adversarial Attack.", "abstract": "Deep neural networks are discovered to be non-robust when attacked by imperceptible adversarial examples, which is dangerous for it applied into medical diagnostic system that requires high reliability. However, the defense methods that have good effect in natural images may not be suitable for medical diagnostic tasks. The pre-processing methods (e.g., random resizing, compression) may lead to the loss of the small lesions feature in the medical image. Retraining the network on the augmented data set is also not practical for medical models that have already been deployed online. Accordingly, it is necessary to design an easy-to-deploy and effective defense framework for medical diagnostic tasks. In this paper, we propose a Robust and Retrain-Less Diagnostic Framework for Medical pretrained models against adversarial attack (i.e., MedRDF). It acts on the inference time of the pretrained medical model. Specifically, for each test image, MedRDF firstly creates a large number of noisy copies of it, and obtains the output labels of these copies from the pretrained medical diagnostic model. Then, based on the labels of these copies, MedRDF outputs the final robust diagnostic result by majority voting. In addition to the diagnostic result, MedRDF produces the Robust Metric (RM) as the confidence of the result. Therefore, it is convenient and reliable to utilize MedRDF to convert pretrained non-robust diagnostic models into robust ones. The experimental results on COVID-19 and DermaMNIST datasets verify the effectiveness of our MedRDF in improving the robustness of medical diagnostic models.", "journal": "IEEE transactions on medical imaging", "date": "2022-03-03", "authors": ["MengtingXu", "TaoZhang", "DaoqiangZhang"], "doi": "10.1109/TMI.2022.3156268"}
{"title": "Distribution Atlas of COVID-19 Pneumonia on Computed Tomography: A Deep Learning Based Description.", "abstract": "To construct a distribution atlas of coronavirus disease 2019 (COVID-19) pneumonia on computed tomography (CT) and further explore the difference in distribution by location and disease severity through a retrospective study of 484 cases in Jiangsu, China.\nAll patients diagnosed with COVID-19 from January 10 to February 18 in Jiangsu Province, China, were enrolled in our study. The patients were further divided into asymptomatic/mild, moderate, and severe/critically ill groups. A deep learning algorithm was applied to the anatomic pulmonary segmentation and pneumonia lesion extraction. The frequency of opacity on CT was calculated, and a color-coded distribution atlas was built. A further comparison was made between the upper and lower lungs, between bilateral lungs, and between various severity groups. Additional lesion-based radiomics analysis was performed to ascertain the features associated with the disease severity.\nA total of 484 laboratory-confirmed patients with 945 repeated CT scans were included. Pulmonary opacity was mainly distributed in the subpleural and peripheral areas. The distances from the opacity to the nearest parietal/visceral pleura were shortest in the asymptomatic/mild group. More diffused lesions were found in the severe/critically ill group. The frequency of opacity increased with increased severity and peaked at about 3-4 or 7-8 o'clock direction in the upper lungs, as opposed to the 5 or 6 o'clock direction in the lower lungs. Lesions with greater energy, more circle-like, and greater surface area were more likely found in severe/critically ill cases than the others.\nThis study constructed a detailed distribution atlas of COVID-19 pneumonia and compared specific patterns in different parts of the lungs at various severities. The radiomics features most associated with the severity were also found. These results may be valuable in determining the COVID-19 sub-phenotype.\nThe online version contains supplementary material available at 10.1007/s43657-021-00011-4.", "journal": "Phenomics (Cham, Switzerland)", "date": "2022-03-03", "authors": ["ShanHuang", "YuanchengWang", "ZhenZhou", "QianYu", "YizhouYu", "YiYang", "ShenghongJu"], "doi": "10.1007/s43657-021-00011-4\n10.1148/radiol.2020200642\n10.1016/j.compbiomed.2020.104037\n10.1148/radiol.2020201237\n10.1148/radiol.2020200230\n10.1038/s41591-018-0177-5\n10.1038/s41467-020-18786-x\n10.1056/NEJMoa2002032\n10.1148/ryct.2020200075\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200241\n10.1007/s10278-018-0079-6\n10.1016/j.ejca.2011.11.036\n10.1148/radiol.2020200236\n10.3389/fmed.2020.00190\n10.1001/jama.2020.4683\n10.1007/s00330-020-06731-x\n10.1002/mp.13264\n10.2214/AJR.20.23034\n10.1007/s00330-018-5509-9\n10.1148/radiol.2020200269\n10.1016/S1473-3099(20)30086-4\n10.1016/S1470-2045(18)30413-3\n10.1016/j.jinf.2020.02.017\n10.1007/s00134-020-05976-w"}
{"title": "Evaluating the Association Between Comorbidities and COVID-19 Severity Scoring on Chest CT Examinations Between the Two Waves of COVID-19: An Imaging Study Using Artificial Intelligence.", "abstract": "Background Coronavirus disease 2019 (COVID-19) has accounted for over 352 million cases and five million deaths globally. Although it affects populations across all nations, developing or transitional, of all genders and ages, the extent of the specific involvement is not very well known. This study aimed to analyze and determine how different were the first and second waves of the COVID-19 pandemic by assessing computed tomography severity scores (CT-SS). Methodology This was a retrospective, cross-sectional, observational study performed at a tertiary care Institution. We included 301 patients who underwent CT of the chest between June and October 2020 and 1,001 patients who underwent CT of the chest between February and April 2021. All included patients were symptomatic and were confirmed to be COVID-19 positive. We compared the CT-SS between the two datasets. In addition, we analyzed the distribution of CT-SS concerning age, comorbidities, and gender, as well as their differences between the two waves of COVID-19.\u00a0Analysis was performed using the SPSS version 22 (IBM Corp., Armonk, NY, USA). The artificial intelligence platform U-net architecture with Xception encoder was used in the analysis. Results The study data revealed that while the mean CT-SS did not differ statistically between the two waves of COVID-19, the age group most affected in the second wave was almost a decade younger. While overall the disease had a predilection toward affecting males, our findings showed that females were more afflicted in the second wave of COVID-19 compared to the first wave. In particular, the disease had an increased severity in cases with comorbidities such as hypertension, diabetes mellitus, bronchial asthma, and tuberculosis. Conclusions This assessment demonstrated no significant difference in radiological severity score between the two waves of COVID-19. The secondary objective revealed that the two waves showed demographical differences. Hence, we iterate that no demographical subset of the population should be considered low risk as the disease manifestation was heterogeneous.", "journal": "Cureus", "date": "2022-03-03", "authors": ["PranavAjmera", "AmitKharat", "SatvikDhirawani", "Sanjay MKhaladkar", "VirajKulkarni", "VinayDuddalwar", "PurnachandraLamghare", "SnehalRathi"], "doi": "10.7759/cureus.21656"}
{"title": "A novel adaptive momentum method for medical image classification using convolutional neural network.", "abstract": "AI for medical diagnosis has made a tremendous impact by applying convolutional neural networks (CNNs) to medical image classification and momentum plays an essential role in stochastic gradient optimization algorithms for accelerating or improving training convolutional neural networks. In traditional optimizers in CNNs, the momentum is usually weighted by a constant. However, tuning hyperparameters for momentum can be computationally complex. In this paper, we propose a novel adaptive momentum for fast and stable convergence.\nApplying adaptive momentum rate proposes increasing or decreasing based on every epoch's error changes, and it eliminates the need for momentum hyperparameter optimization. We tested the proposed method with 3 different datasets: REMBRANDT Brain Cancer, NIH Chest X-ray, COVID-19 CT scan. We compared the performance of a novel adaptive momentum optimizer with Stochastic gradient descent (SGD) and other adaptive optimizers such as Adam and RMSprop.\nProposed method improves SGD performance by reducing classification error from 6.12 to 5.44%, and it achieved the lowest error and highest accuracy compared with other optimizers. To strengthen the outcomes of this study, we investigated the performance comparison for the state-of-the-art CNN architectures with adaptive momentum. The results shows that the proposed method achieved the highest with 95% compared to state-of-the-art CNN architectures while using the same dataset. The proposed method improves convergence performance by reducing classification error and achieves high accuracy compared with other optimizers.", "journal": "BMC medical imaging", "date": "2022-03-03", "authors": ["Utku CanAyta\u00e7", "AliG\u00fcne\u015f", "NaimAjlouni"], "doi": "10.1186/s12880-022-00755-z\n10.1186/s40537-021-00444-8\n10.1016/j.ins.2020.05.013\n10.1109/TIM.2018.2887069\n10.3390/diagnostics10090662\n10.1007/s12149-020-01510-6\n10.1016/j.asoc.2014.03.019\n10.1371/journal.pone.0144479\n10.1007/s10044-017-0597-8\n10.1093/neuonc/nou159\n10.1016/j.jocs.2018.12.003\n10.1016/j.compmedimag.2019.05.001\n10.1109/78.134446\n10.1109/tcyb.2013.2279211\n10.1016/j.eswa.2014.08.018\n10.1142/S0129065702001114\n10.1049/el:19920236\n10.1007/s10278-013-9622-7\n10.1016/j.ipm.2008.09.004\n10.1016/j.knosys.2016.10.001\n10.1038/323533a0\n10.18517/ijaseit.7.5.2972\n10.1016/j.cogsys.2019.06.003\n10.1016/j.asoc.2016.03.014\n10.1016/j.patrec.2019.11.020\n10.1145/3065386"}
{"title": "The use of advanced spectral imaging to reveal nanoparticle identity in biological samples.", "abstract": "Nanoparticles (NPs) have been used in drug delivery therapies, medical diagnostic strategies, and as current Covid-19 vaccine carriers. Many microscope-based imaging systems have been introduced to facilitate detection and visualization of NPs. Unfortunately, none can differentiate the core and the shell of NPs. Spectral imaging has been used to distinguish a drug molecule and its metabolite. We have recently integrated this technology to a resolution of 9 nm by using artificial intelligence-driven analyses. Such a resolution allowed us to collect many robust datapoints for each pixel of an image. Our analyses could recognize 45 spectral points within a pixel to detect unlabeled Ag-NPs and Au-NPs in single live cells and tissues (liver, heart, spleen and kidneys). The improved resolution and software provided a more specific fingerprinting for each single molecule, allowing simultaneous analyses of 990 complex interactions from the 45 points for each molecule within a pixel of an image. This in turn allowed us to detect surface-functionalization of Ag-NPs to distinguish the core from the shell of Ag-NPs for the first time. Our studies were validated using various laborious and time-consuming conventional techniques. We propose that spectral imaging has tremendous potential to study NP localization and identification in biological samples at a high temporal and spatial resolution, based primarily on spectral identity information.", "journal": "Nanoscale", "date": "2022-03-02", "authors": ["Qamar AAlshammari", "RajasekharreddyPala", "Ayan KBarui", "Saud OAlshammari", "Andromeda MNauli", "NirKatzir", "Ashraf MMohieldin", "Surya MNauli"], "doi": "10.1039/d1nr07551a"}
{"title": "Deep Residual Neural Network for COVID-19 Detection from Chest X-ray Images.", "abstract": "The COVID-19 diffused quickly throughout the world and converted as a pandemic. It has caused a destructive effect on both regular lives, common health and global business. It is crucial to identify positive patients as shortly as desirable to limit this epidemic's further diffusion and to manage immediately affected cases. The demand for quick assistant distinguishing devices has developed. Recent findings achieved utilizing radiology imaging systems propose that such images include salient data about the COVID-19. The utilization of progressive artificial intelligence (AI) methods linked by radiological imaging can help the reliable diagnosis of COVID-19. As radiography images can recognize pneumonia infections, this research brings an accurate and automatic technique based on a deep residual network to analyze chest X-ray images to monitor COVID-19 and diagnose verified patients. The physician states that it is significantly challenging to separate COVID-19 from common viral and bacterial pneumonia, while COVID-19 is additionally a variety of viruses. The proposed network is expanded to perform detailed diagnostics for two multi-class classification (COVID-19, Normal, Viral Pneumonia) and (COVID-19, Normal, Viral Pneumonia, Bacterial Pneumonia) and binary classification. By comparing the proposed network with the popular methods on public databases, the results show that the proposed algorithm can provide an accuracy of 92.1% in classifying multi-classes of COVID-19, normal, viral pneumonia, and bacterial pneumonia cases. It can be applied to support radiologists in verifying their first viewpoint.", "journal": "SN computer science", "date": "2022-03-01", "authors": ["AmirhosseinPanahi", "RezaAskari Moghadam", "MohammadrezaAkrami", "KuroshMadani"], "doi": "10.1007/s42979-022-01067-3\n10.1016/j.scitotenv.2020.138817\n10.1016/S0140-6736(20)30211-7\n10.1016/j.jaut.2020.102433\n10.1016/j.jbef.2020.100326\n10.1148/radiol.11092149\n10.1016/j.crad.2018.12.015\n10.3390/app10093233\n10.3390/s20040957\n10.1148/radiol.2019181960\n10.1016/j.cell.2018.02.010\n10.1007/s00345-019-03059-0\n10.1016/j.mehy.2020.109761\n10.1016/j.cmpb.2020.105581\n10.1007/s13246-020-00865-4\n10.1016/j.patrec.2020.09.010\n10.1007/s10489-020-01829-7\n10.1038/s41598-019-56847-4\n10.1007/s10044-020-00887-4\n10.1016/j.compbiomed.2020.103869\n10.1007/s42979-021-00881-5\n10.1016/j.bspc.2021.103272\n10.1016/j.bbe.2021.09.004\n10.1109/ACCESS.2020.3010287\n10.1007/s10916-021-01747-2\n10.1109/TGRS.2017.2755542\n10.1109/TCSVT.2018.2869680\n10.1016/j.compbiomed.2020.103792"}
{"title": "Machine learning-based automatic detection of novel coronavirus (COVID-19) disease.", "abstract": "The pandemic was announced by the world health organization coronavirus (COVID-19) universal health dilemma. Any scientific appliance which contributes expeditious detection of coronavirus with a huge recognition rate may be excessively fruitful to doctors. In this environment, innovative automation like deep learning, machine learning, image processing and medical image like chest radiography (CXR), computed tomography (CT) has been refined promising solution contrary to COVID-19. Currently, a reverse transcription-polymerase chain reaction (RT-PCR) test has been used to detect the coronavirus. Due to the moratorium period is high on results tested and huge false negative estimates, substitute solutions are desired. Thus, an automated machine learning-based algorithm is proposed for the detection of COVID-19 and the grading of nine different datasets. This research impacts the grant of image processing and machine learning to expeditious and definite coronavirus detection using CXR and CT medical imaging. This results in early detection, diagnosis, and cure for the accomplishment of COVID-19 as early as possible. Firstly, images are preprocessed by normalization to enhance the quality of the image and removing of noise. Secondly, segmentation of images is done by fuzzy c-means clustering. Then various features namely, statistical, textural, histogram of gradients, and discrete wavelet transform are extracted (92) and selected from the feature vector by principle component analysis. Lastly, k-NN, SRC, ANN, and SVM are used to make decisions for normal, pneumonia, COVID-19 positive patients. The performance of the system has been validated by the k (5) fold cross-validation technique. The proposed algorithm achieves 91.70% (k-Nearest Neighbor), 94.40% (Sparse Representation Classifier), 96.16% (Artificial Neural Network), and 99.14% (Support Vector Machine) for COVID detection. The proposed results show feature combination and selection improves the performance in 14.34\u00a0s with machine learning and image processing techniques. Among k-NN, SRC, ANN, and SVM classifiers, SVM shows more efficient results that are promising and comparable with the literature. The proposed approach results in an improved recognition rate as compared to the literature review. Therefore, the algorithm proposed shows immense potential to benefit the radiologist for their findings. Also, fruitful in prior virus diagnosis and discriminate pneumonia between COVID-19 and other pandemics.", "journal": "Multimedia tools and applications", "date": "2022-03-01", "authors": ["AnujaBhargava", "AtulBansal", "VishalGoyal"], "doi": "10.1007/s11042-022-12508-9\n10.1016/j.bbe.2020.08.005\n10.1016/j.chaos.2020.110071\n10.1016/j.asoc.2020.106912\n10.1016/j.compag.2017.05.019\n10.3389/fnins.2019.01346\n10.1109/MC.2013.42\n10.1109/ACCESS.2020.3005510\n10.32604/cmc.2020.010691\n10.1016/j.compbiomed.2020.104181\n10.1056/NEJMoa2001316\n10.1109/JBHI.2020.3018181\n10.1093/bib/bbx044\n10.1016/j.ijpharm.2013.10.024\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compag.2012.11.009\n10.1007/s10096-019-03782-x\n10.1109/JBHI.2020.3019505\n10.1007/s10661-012-2874-8\n10.1016/j.eng.2020.04.010\n10.1016/j.asoc.2020.106885"}
{"title": "Detecting COVID-19 from chest computed tomography scans using AI-driven android application.", "abstract": "The COVID-19 (coronavirus disease 2019) pandemic affected more than 186 million people with over 4 million deaths worldwide by June 2021. The magnitude of which has strained global healthcare systems. Chest Computed Tomography (CT) scans have a potential role in the diagnosis and prognostication of COVID-19. Designing a diagnostic system, which is cost-efficient and convenient to operate on resource-constrained devices like mobile phones would enhance the clinical usage of chest CT scans and provide swift, mobile, and accessible diagnostic capabilities. This work proposes developing a novel Android application that detects COVID-19 infection from chest CT scans using a highly efficient and accurate deep learning algorithm. It further creates an attention heatmap, augmented on the segmented lung parenchyma region in the chest CT scans which shows the regions of infection in the lungs through an algorithm developed as a part of this work, and verified through radiologists. We propose a novel selection approach combined with multi-threading for a faster generation of heatmaps on a Mobile Device, which reduces the processing time by about 93%. The neural network trained to detect COVID-19 in this work is tested with a F1 score and accuracy, both of 99.58% and sensitivity of 99.69%, which is better than most of the results in the domain of COVID diagnosis from CT scans. This work will be beneficial in high-volume practices and help doctors triage patients for the early diagnosis of COVID-19 quickly and efficiently.", "journal": "Computers in biology and medicine", "date": "2022-02-28", "authors": ["AryanVerma", "Sagar BAmin", "MuhammadNaeem", "MonjoySaha"], "doi": "10.1016/j.compbiomed.2022.105298\n10.1016/j.ijantimicag.2020.105924\n10.7326/M20-1382\n10.1101/2021.07.06.21260109\n10.1148/radiol.2020200230\n10.1109/IPRIA53572.2021.9483563\n10.1109/INMIC50486.2020.9318212\n10.1016/j.compbiomed.2020.103792\n10.1109/TIM.2020.3033072\n10.1007/s00530-020-00728-8\n10.1016/S0531-5131(03)00388-1\n10.1118/1.4793409\n10.1007/s10278-009-9229-1\n10.1109/CVPR.2009.5206848\n10.1007/s00521-020-05410-8\n10.3390/v12070769"}
{"title": "COVID-rate: an automated framework for segmentation of COVID-19 lesions from chest CT images.", "abstract": "Novel Coronavirus disease (COVID-19) is a highly contagious respiratory infection that has had devastating effects on the world. Recently, new COVID-19 variants are emerging making the situation more challenging and threatening. Evaluation and quantification of COVID-19 lung abnormalities based on chest Computed Tomography (CT) images can help determining the disease stage, efficiently allocating limited healthcare resources, and making informed treatment decisions. During pandemic era, however, visual assessment and quantification of COVID-19 lung lesions by expert radiologists become expensive and prone to error, which raises an urgent quest to develop practical autonomous solutions. In this context, first, the paper introduces an open-access COVID-19 CT segmentation dataset containing 433 CT images from 82 patients that have been annotated by an expert radiologist. Second, a Deep Neural Network (DNN)-based framework is proposed, referred to as the [Formula: see text], that autonomously segments lung abnormalities associated with COVID-19 from chest CT images. Performance of the proposed [Formula: see text] framework is evaluated through several experiments based on the introduced and external datasets. Third, an unsupervised enhancement approach is introduced that can reduce the gap between the training set and test set and improve the model generalization. The enhanced results show a dice score of 0.8069 and specificity and sensitivity of 0.9969 and 0.8354, respectively. Furthermore, the results indicate that the [Formula: see text] model can efficiently segment COVID-19 lesions in both 2D CT images and whole lung volumes. Results on the external dataset illustrate generalization capabilities of the [Formula: see text] model to CT images obtained from a different scanner.", "journal": "Scientific reports", "date": "2022-02-27", "authors": ["NastaranEnshaei", "AnastasiaOikonomou", "Moezedin JavadRafiee", "ParnianAfshar", "ShahinHeidarian", "ArashMohammadi", "Konstantinos NPlataniotis", "FarnooshNaderkhani"], "doi": "10.1038/s41598-022-06854-9\n10.1016/j.ygeno.2021.05.006\n10.1148/radiol.2020200230\n10.2214/AJR.20.22975\n10.1148/radiol.2462070712\n10.1371/journal.pone.0230548\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020200274\n10.1148/ryai.2020200048\n10.1007/s00330-020-07033-y\n10.1038/s41597-021-00900-3\n10.21037/qims-20-564\n10.1007/s42399-020-00341-w\n10.1109/TPAMI.2017.2699184\n10.1038/s41598-019-56847-4\n10.1186/s41747-020-00173-2\n10.2196/24572\n10.1148/ryct.2020200389\n10.1038/s41746-021-00431-6\n10.1109/TNNLS.2021.3054746\n10.1109/TMI.2020.2996645\n10.1038/s41591-020-0931-3\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020201491\n10.1016/j.patrec.2020.09.010\n10.1109/ACCESS.2020.2994762\n10.1016/j.cmpb.2020.105581\n10.1007/s10096-020-03901-z\n10.1109/ACCESS.2020.3005510\n10.1109/TMI.2020.3000314\n10.1016/j.compbiomed.2020.104037\n10.1109/TIP.2021.3058783\n10.1109/ACCESS.2020.3027738\n10.1016/j.media.2021.102205\n10.1016/j.knosys.2020.106647\n10.1016/j.patcog.2021.107826"}
{"title": "Optimized chest X-ray image semantic segmentation networks for COVID-19 early detection.", "abstract": "Although detection of COVID-19 from chest X-ray radiography (CXR) images is faster than PCR sputum testing, the accuracy of detecting COVID-19 from CXR images is lacking in the existing deep learning models.\nThis study aims to classify COVID-19 and normal patients from CXR images using semantic segmentation networks for detecting and labeling COVID-19 infected lung lobes in CXR images.\nFor semantically segmenting infected lung lobes in CXR images for COVID-19 early detection, three structurally different deep learning (DL) networks such as SegNet, U-Net and hybrid CNN with SegNet plus U-Net, are proposed and investigated. Further, the optimized CXR image semantic segmentation networks such as GWO SegNet, GWO U-Net, and GWO hybrid CNN are developed with the grey wolf optimization (GWO) algorithm. The proposed DL networks are trained, tested, and validated without and with optimization on the openly available dataset that contains 2,572 COVID-19 CXR images including 2,174 training images and 398 testing images. The DL networks and their GWO optimized networks are also compared with other state-of-the-art models used to detect COVID-19 CXR images.\nAll optimized CXR image semantic segmentation networks for COVID-19 image detection developed in this study achieved detection accuracy higher than 92%. The result shows the superiority of optimized SegNet in segmenting COVID-19 infected lung lobes and classifying with an accuracy of 98.08% compared to optimized U-Net and hybrid CNN.\nThe optimized DL networks has potential to be utilised to more objectively and accurately identify COVID-19 disease using semantic segmentation of COVID-19 CXR images of the lungs.", "journal": "Journal of X-ray science and technology", "date": "2022-02-26", "authors": ["AnandbabuGopatoti", "PVijayalakshmi"], "doi": "10.3233/XST-211113"}
{"title": "Proposing a novel deep network for detecting COVID-19 based on chest images.", "abstract": "The rapid outbreak of coronavirus threatens humans' life all around the world. Due to the insufficient diagnostic infrastructures, developing an accurate, efficient, inexpensive, and quick diagnostic tool is of great importance. To date, researchers have proposed several detection models based on chest imaging analysis, primarily based on deep neural networks; however, none of which could achieve a reliable and highly sensitive performance yet. Therefore, the nature of this study is primary epidemiological research that aims to overcome the limitations mentioned above by proposing a large-scale publicly available dataset of chest computed tomography scan (CT-scan) images consisting of more than 13k samples. Secondly, we propose a more sensitive deep neural networks model for CT-scan images of the lungs, providing a pixel-wise attention layer on top of the high-level features extracted from the network. Moreover, the proposed model is extended through a transfer learning approach for being applicable in the case of chest X-Ray (CXR) images. The proposed model and its extension have been trained and evaluated through several experiments. The inclusion criteria were patients with suspected PE and positive real-time reverse-transcription polymerase chain reaction (RT-PCR) for SARS-CoV-2. The exclusion criteria were negative or inconclusive RT-PCR and other chest CT indications. Our model achieves an AUC score of 0.886, significantly better than its closest competitor, whose AUC is 0.843. Moreover, the obtained results on another commonly-used benchmark show an AUC of 0.899, outperforming related models. Additionally, the sensitivity of our model is 0.858, while that of its closest competitor is 0.81, explaining the efficiency of pixel-wise attention strategy in detecting coronavirus. Our promising results and the efficiency of the models imply that the proposed models can be considered reliable tools for assisting doctors in detecting coronavirus.", "journal": "Scientific reports", "date": "2022-02-26", "authors": ["MaryamDialameh", "AliHamzeh", "HosseinRahmani", "Amir RezaRadmard", "SafouraDialameh"], "doi": "10.1038/s41598-022-06802-7\n10.1371/journal.pone.0230548\n10.1016/j.clindermatol.2020.12.009\n10.1021/acs.molpharmaceut.5b00982\n10.26599/BDMA.2018.9020001\n10.1109/JBHI.2016.2636665\n10.1016/j.drudis.2018.01.039\n10.1038/s41467-020-17971-2\n10.1109/ACCESS.2020.3007939\n10.1016/j.sysarc.2020.101830\n10.1038/srep10241\n10.1016/S2213-2600(13)70164-4\n10.1016/j.neunet.2020.07.010\n10.1016/j.ins.2009.12.010"}
{"title": "COVID-19 Detection in CT/X-ray Imagery Using Vision Transformers.", "abstract": "The steady spread of the 2019 Coronavirus disease has brought about human and economic losses, imposing a new lifestyle across the world. On this point, medical imaging tests such as computed tomography (CT) and X-ray have demonstrated a sound screening potential. Deep learning methodologies have evidenced superior image analysis capabilities with respect to prior handcrafted counterparts. In this paper, we propose a novel deep learning framework for Coronavirus detection using CT and X-ray images. In particular, a Vision Transformer architecture is adopted as a backbone in the proposed network, in which a Siamese encoder is utilized. The latter is composed of two branches: one for processing the original image and another for processing an augmented view of the original image. The input images are divided into patches and fed through the encoder. The proposed framework is evaluated on public CT and X-ray datasets. The proposed system confirms its superiority over state-of-the-art methods on CT and X-ray data in terms of accuracy, precision, recall, specificity, and F1 score. Furthermore, the proposed system also exhibits good robustness when a small portion of training data is allocated.", "journal": "Journal of personalized medicine", "date": "2022-02-26", "authors": ["Mohamad MahmoudAl Rahhal", "YakoubBazi", "Rami MJomaa", "AhmadAlShibli", "NaifAlajlan", "Mohamed LamineMekhalfi", "FaridMelgani"], "doi": "10.3390/jpm12020310\n10.1016/j.arth.2020.04.055\n10.1080/14737159.2020.1757437\n10.1001/jama.2020.2783\n10.1080/22221751.2020.1745095\n10.1109/JAS.2020.1003450\n10.1016/j.ins.2016.01.082\n10.1109/JBHI.2016.2635663\n10.1109/JBHI.2018.2793534\n10.1109/JBHI.2018.2866873\n10.1016/j.ijid.2020.05.021\n10.1007/s11547-020-01232-9\n10.1007/s42399-020-00553-0\n10.1111/acem.14004\n10.1016/j.acra.2020.04.016\n10.1183/13993003.04188-2020\n10.1136/bmjopen-2020-042946\n10.4329/wjr.v12.i9.195\n10.1007/s11517-006-0044-2\n10.1109/TMI.2002.806290\n10.1016/j.cmpb.2015.10.010\n10.1049/ic:19981039\n10.2147/BCTT.S175311\n10.1016/j.compbiomed.2011.06.010\n10.1002/scj.4690250207\n10.1016/j.nima.2006.08.134\n10.1007/s10278-019-00227-x\n10.1007/s00330-019-06170-3\n10.1109/TMI.2018.2833385\n10.1038/s41591-019-0447-x\n10.1002/mp.13300\n10.1148/radiol.2017162326\n10.1038/nature21056\n10.1148/radiol.2020200527\n10.1148/radiol.2021204522\n10.1016/j.pulmoe.2020.04.011\n10.1038/s42256-021-00307-0\n10.1101/2020.03.30.20047787v1\n10.1016/j.asoc.2020.106691\n10.1016/j.imu.2020.100412\n10.1109/JBHI.2021.3058293\n10.26599/BDMA.2020.9020012\n10.1109/ACCESS.2021.3085418\n10.1109/TII.2021.3057683\n10.1007/s10489-020-01829-7\n10.1148/ryct.2020200337\n10.1016/j.bea.2021.100003\n10.1016/j.compbiomed.2020.104037\n10.1016/j.eng.2020.04.010\n10.1109/JBHI.2020.3019505\n10.2196/19569\n10.1109/TIP.2021.3058783\n10.1109/JBHI.2020.3042523\n10.1002/ppul.25313\n10.21037/atm.2020.02.71\n10.1007/s12553-021-00520-2\n10.3390/sym12040651\n10.32604/cmc.2021.014956\n10.1038/s41598-020-76550-z\n10.1101/2020.04.24.20078584\n10.1148/radiol.2020200463\n10.1016/j.imu.2020.100427\n10.1109/TCBB.2020.3009859"}
{"title": "COVID-19 Identification from Low-Quality Computed Tomography Using a Modified Enhanced Super-Resolution Generative Adversarial Network Plus and Siamese Capsule Network.", "abstract": "Computed Tomography has become a vital screening method for the detection of coronavirus 2019 (COVID-19). With the high mortality rate and overload for domain experts, radiologists, and clinicians, there is a need for the application of a computerized diagnostic technique. To this effect, we have taken into consideration improving the performance of COVID-19 identification by tackling the issue of low quality and resolution of computed tomography images by introducing our method. We have reported about a technique named the modified enhanced super resolution generative adversarial network for a better high resolution of computed tomography images. Furthermore, in contrast to the fashion of increasing network depth and complexity to beef up imaging performance, we incorporated a Siamese capsule network that extracts distinct features for COVID-19 identification.The qualitative and quantitative results establish that the proposed model is effective, accurate, and robust for COVID-19 screening. We demonstrate the proposed model for COVID-19 identification on a publicly available dataset COVID-CT, which contains 349 COVID-19 and 463 non-COVID-19 computed tomography images. The proposed method achieves an accuracy of 97.92%, sensitivity of 98.85%, specificity of 97.21%, AUC of 98.03%, precision of 98.44%, and F1 score of 97.52%. Our approach obtained state-of-the-art performance, according to experimental results, which is helpful for COVID-19 screening. This new conceptual framework is proposed to play an influential task in the issue facing COVID-19 and related ailments, with the availability of few datasets.", "journal": "Healthcare (Basel, Switzerland)", "date": "2022-02-26", "authors": ["Grace UgochiNneji", "JianhuaDeng", "Happy NkantaMonday", "Md AltabHossin", "SandraObiora", "SaifunNahar", "JingyeCai"], "doi": "10.3390/healthcare10020403\n10.1148/radiol.2020200343\n10.1002/cpe.5130\n10.3389/fnins.2019.00422\n10.1109/TMI.2020.3040950\n10.1109/ACCESS.2020.3010287\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.10.001\n10.1007/s10096-020-03901-z\n10.1109/TMI.2020.2995508\n10.1101/2020.03.12.20027185\n10.1148/radiol.2020200905\n10.3390/diagnostics12020325\n10.1088/1361-6560/abe838\n10.1038/s41467-020-18685-1\n10.1109/TCBB.2021.3065361\n10.1016/j.eng.2020.04.010\n10.1007/s00330-021-07715-1\n10.1007/s10489-020-02149-6\n10.1016/j.eswa.2021.116366\n10.3390/s21217286\n10.3390/app11199023\n10.1109/TMI.2019.2922960\n10.1109/ACCESS.2020.2994762\n10.2196/19673\n10.1109/TGRS.2018.2871782"}
{"title": "An Efficient Deep Learning Model to Detect COVID-19 Using Chest X-ray Images.", "abstract": "The tragic pandemic of COVID-19, due to the Severe Acute Respiratory Syndrome coronavirus-2 or SARS-CoV-2, has shaken the entire world, and has significantly disrupted healthcare systems in many countries. Because of the existing challenges and controversies to testing for COVID-19, improved and cost-effective methods are needed to detect the disease. For this purpose, machine learning (ML) has emerged as a strong forecasting method for detecting COVID-19 from chest X-ray images. In this paper, we used a Deep Learning Method (DLM) to detect COVID-19 using chest X-ray (CXR) images. Radiographic images are readily available and can be used effectively for COVID-19 detection compared to other expensive and time-consuming pathological tests. We used a dataset of 10,040 samples, of which 2143 had COVID-19, 3674 had pneumonia (but not COVID-19), and 4223 were normal (not COVID-19 or pneumonia). Our model had a detection accuracy of 96.43% and a sensitivity of 93.68%. The area under the ROC curve was 99% for COVID-19, 97% for pneumonia (but not COVID-19 positive), and 98% for normal cases. In conclusion, ML approaches may be used for rapid analysis of CXR images and thus enable radiologists to filter potential candidates in a time-effective manner to detect COVID-19.", "journal": "International journal of environmental research and public health", "date": "2022-02-26", "authors": ["SomenathChakraborty", "BeddhuMurali", "Amal KMitra"], "doi": "10.3390/ijerph19042013\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMe2002387\n10.1038/s41586-020-2008-3\n10.1007/s10238-020-00648-x\n10.1001/jama.2020.3786\n10.15585/mmwr.mm7019a3\n10.1148/radiol.2020200642\n10.1007/s42058-020-00031-5\n10.1148/radiol.2020200823\n10.1101/2020.04.13.20063941v1.full.pdf\n10.3389/fmed.2020.608525/full\n10.1038/s41467-020-17971-2\n10.1007/s13246-020-00888-x\n10.1016/j.measurement.2019.05.076\n10.1148/radiol.2019194005\n10.1038/nature14539\n10.2214/ajr.174.1.1740071\n10.3390/s20041068\n10.1016/j.media.2005.02.002\n10.1109/CVPRW.2017.156\n10.1109/CVPR.2016.90\n10.1145/3065386\n10.1109/CVPR.2017.243\n10.1109/TPAMI.2015.2502579\n10.1016/j.irbm.2020.07.001\n10.1038/s41598-020-76550-z\n10.1007/s10044-021-00984-y\n10.1016/j.chaos.2020.110071\n10.1016/j.compbiomed.2020.103792\n10.1016/j.media.2020.101794\n10.1016/j.cmpb.2020.105581\n10.3390/app10134640\n10.1038/s41598-021-99015-3\n10.1016/j.imu.2020.100412\n10.1016/j.cmpb.2020.105532\n10.1016/j.chaos.2020.110190\n10.1109/ISCAS.2010.5537907\n10.1016/j.cell.2020.04.045\n10.3233/XST-200715\n10.1117/1.JMI.8.S1.017503\n10.1016/j.mlwa.2021.100138\n10.1109/ACCESS.2019.2899578\n10.1109/ACCESS.2021.3102399\n10.1109/ACCESS.2020.3007801\n10.1109/ACCESS.2019.2939755\n10.1109/ACCESS.2020.2991800"}
{"title": "COVID-19 Pneumonia and Lung Cancer: A Challenge for the RadiologistReview of the Main Radiological Features, Differential Diagnosis and Overlapping Pathologies.", "abstract": "The COVID-19 pneumonia pandemic represents the most severe health emergency of the 21st century and has been monopolizing health systems' economic and human resources world-wide. Cancer patients have been suffering from the health systems' COVID-19 priority management with evidence of late diagnosis leading to patients' poor prognosis and late medical treatment. The radiologist plays a pivotal role as CT represents a non-invasive radiological technique which may help to identify possible overlap and differential diagnosis between COVID-19 pneumonia and lung cancer, which represents the most frequent cancer histology in COVID-19 patients. Our aims are: to present the main CT features of COVID-19 pneumonia; to provide the main differential diagnosis with lung cancer, chemotherapy-, immunotherapy-, and radiotherapy-induced lung disease; and to suggest practical tips and key radiological elements to identify possible overlap between COVID-19 pneumonia and lung cancer. Despite similarities or overlapping findings, the combination of clinics and some specific radiological findings, which are also identified by comparison with previous and follow-up CT scans, may guide differential diagnosis. It is crucial to search for typical COVID-19 pneumonia phase progression and typical radiological features on HRTC. The evidence of atypical findings such as lymphadenopathies and mediastinal and vessel invasion, as well as the absence of response to therapy, should arouse the suspicion of lung cancer and require contrast administration. Ground-glass areas and/or consolidations bound to radiotherapy fields or pneumonitis arising during and after oncological therapy should always arouse the suspicion of radiation-induced lung disease and chemo/immunotherapy-induced lung disease. The radiological elements we suggest for COVID-19 and lung cancer differential diagnosis may be used to develop AI protocols to guarantee an early and proper diagnosis and treatment to improve patients' quality of life and life expectancy.", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-02-25", "authors": ["AlessiaGuarnera", "ElenaSantini", "PierfrancescoPodda"], "doi": "10.3390/tomography8010041\n10.1016/S0140-6736(20)30154-9\n10.7759/cureus.18689\n10.3390/jpm11100993\n10.1155/2021/6658058\n10.1016/j.compbiomed.2021.104585\n10.1002/cncr.34011\n10.1016/S1470-2045(20)30265-5\n10.1016/S1470-2045(09)70069-5\n10.1016/S0140-6736(20)31187-9\n10.1093/jnci/djaa168\n10.20892/j.issn.2095-3941.2020.0289\n10.1016/j.radcr.2020.05.080\n10.1177/0300891620951863\n10.1038/s41586-020-2012-7\n10.1111/jth.14821\n10.1111/jth.14817\n10.3389/fphys.2021.748972\n10.1148/radiol.2020201365\n10.1148/radiol.2020201160\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020200370\n10.3390/tomography7030035\n10.1148/radiol.2020200463\n10.1148/radiol.2462070712\n10.1148/radiol.2020200527\n10.4187/respcare.01731\n10.1186/s13244-021-00967-x\n10.1097/JTO.0000000000000630\n10.1053/j.sult.2018.11.009\n10.1148/radiographics.21.2.g01mr17403\n10.1007/s00247-020-04821-y\n10.1186/1477-7819-12-66\n10.3747/co.v18i1.647\n10.1016/j.amjmed.2015.04.007\n10.1148/radiol.12120240\n10.1097/JTO.0b013e318206a221\n10.1148/radiol.2017161659\n10.1016/j.jtho.2021.11.003\n10.1148/rg.346140178\n10.1148/rg.346140052\n10.1148/radiographics.20.5.g00se081245\n10.2214/ajr.159.6.1442375\n10.1148/rg.244035160\n10.1016/j.crad.2013.01.013\n10.1148/rg.210146"}
{"title": "Rethinking Clinical Trial Radiology Workflows and Student Training: Integrated Virtual Student Shadowing Experience, Education, and Evaluation.", "abstract": "There is consistent demand for clinical exposure from students interested in radiology; however, the COVID-19 pandemic resulted in fewer available options and limited student access to radiology departments. Additionally, there is increased demand for radiologists to manage more complex quantification in reports on patients enrolled in clinical trials. We present an online educational curriculum that addresses both of these gaps by virtually immersing students (radiology preprocessors, or RPs) into radiologists' workflows where they identify and measure target lesions in advance of radiologists, streamlining report quantification. RPs switched to remote work at the beginning of the COVID-19 pandemic in our National Institutes of Health (NIH). We accommodated them by transitioning our curriculum on cross-sectional anatomy and advanced PACS tools to a publicly available online curriculum. We describe collaborations between multiple academic research centers and industry through contributions of academic content to this curriculum. Further, we describe how we objectively assess educational effectiveness with cross-sectional anatomical quizzes and decreasing RP miss rates as they gain experience. Our RP curriculum generated significant interest evidenced by a dozen academic and research institutes providing online presentations including radiology modality basics and quantification in clinical trials. We report a decrease in RP miss rate percentage, including one virtual RP over a period of 1\u00a0year. Results reflect training effectiveness through decreased discrepancies with radiologist reports and improved tumor identification over time. We present our RP curriculum and multicenter experience as a pilot experience in a clinical trial research setting. Students are able to obtain useful clinical radiology experience in a virtual learning environment by immersing themselves into a clinical radiologist's workflow. At the same time, they help radiologists improve patient care with more valuable quantitative reports, previously shown to improve radiologist efficiency. Students identify and measure lesions in clinical trials before radiologists, and then review their reports for self-evaluation based on included measurements from the radiologists. We consider our virtual approach as a supplement to student education while providing a model for how artificial intelligence will improve patient care with more consistent quantification while improving radiologist efficiency.", "journal": "Journal of digital imaging", "date": "2022-02-24", "authors": ["Lillian GSpear", "Jane ADimperio", "Sherry SWang", "Huy MDo", "Les RFolio"], "doi": "10.1007/s10278-022-00605-y\n10.1016/j.acra.2019.09.014\n10.1016/j.jacr.2018.03.051\n10.1016/j.acra.2012.12.013\n10.1148/radiol.2017162664\n10.1016/j.jacr.2013.12.016\n10.1148/radiol.13122665\n10.1007/s10278-016-9938-1.PMID:28074302;PMCID:PMC5422230\n10.1016/j.jacr.2009.10.012\n10.1145/1232743.1232769\n10.2214/AJR.12.10136.PMID:23971455;PMCID:PMC6771287\n10.1007/s10278-019-00214-2\n10.1016/j.jtho.2016.01.021"}
{"title": "A deep adversarial model for segmentation-assisted COVID-19 diagnosis using CT images.", "abstract": "The outbreak of coronavirus disease 2019 (COVID-19) is spreading rapidly around the world, resulting in a global pandemic. Imaging techniques such as computed tomography (CT) play an essential role in the diagnosis and treatment of the disease since lung infection or pneumonia is a common complication. However, training a deep network to learn how to diagnose COVID-19 rapidly and accurately in CT images and segment the infected regions like a radiologist is challenging. Since the infectious area is difficult to distinguish manually annotation, the segmentation results are time-consuming. To tackle these problems, we propose an efficient method based on a deep adversarial network to segment the infection regions automatically. Then, the predicted segment results can assist the diagnostic network in identifying the COVID-19 samples from the CT images. On the other hand, a radiologist-like segmentation network provides detailed information of the infectious regions by separating areas of ground-glass, consolidation, and pleural effusion, respectively. Our method can accurately predict the COVID-19 infectious probability and provide lesion regions in CT images with limited training data. Additionally, we have established a public dataset for multitask learning. Extensive experiments on diagnosis and segmentation show superior performance over state-of-the-art methods.", "journal": "EURASIP journal on advances in signal processing", "date": "2022-02-24", "authors": ["Hai-YanYao", "Wang-GenWan", "XiangLi"], "doi": "10.1186/s13634-022-00842-x\n10.1016/S0140-6736(20)30183-5\n10.1002/mp.14609\n10.1109/TMI.2020.2992546\n10.1038/s41591-020-0931-3\n10.1109/JBHI.2020.3023246\n10.1109/TMI.2020.2996645\n10.1109/TMI.2020.2995965\n10.1148/ryct.2020200082\n10.1148/ryct.2020200044\n10.1145/3422622\n10.1038/s41551-020-00633-5\n10.1007/s10489-020-02149-6"}
{"title": "WEENet: An Intelligent System for Diagnosing COVID-19 and Lung Cancer in IoMT Environments.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has caused a major outbreak around the world with severe impact on health, human lives, and economy globally. One of the crucial steps in fighting COVID-19 is the ability to detect infected patients at early stages and put them under special care. Detecting COVID-19 from radiography images using computational medical imaging method is one of the fastest ways to diagnose the patients. However, early detection with significant results is a major challenge, given the limited available medical imaging data and conflicting performance metrics. Therefore, this work aims to develop a novel deep learning-based computationally efficient medical imaging framework for effective modeling and early diagnosis of COVID-19 from chest x-ray and computed tomography images. The proposed work presents \"WEENet\" by exploiting efficient convolutional neural network to extract high-level features, followed by classification mechanisms for COVID-19 diagnosis in medical image data. The performance of our method is evaluated on three benchmark medical chest x-ray and computed tomography image datasets using eight evaluation metrics including a novel strategy of cross-corpse evaluation as well as robustness evaluation, and the results are surpassing state-of-the-art methods. The outcome of this work can assist the epidemiologists and healthcare authorities in analyzing the infected medical chest x-ray and computed tomography images, management of the COVID-19 pandemic, bridging the early diagnosis, and treatment gap for Internet of Medical Things environments.", "journal": "Frontiers in oncology", "date": "2022-02-22", "authors": ["KhanMuhammad", "HayatUllah", "Zulfiqar AhmadKhan", "Abdul Khader JilaniSaudagar", "AbdullahAlTameem", "MohammedAlKhathami", "Muhammad BadruddinKhan", "Mozaherul HoqueAbul Hasanat", "KhalidMahmood Malik", "MohammadHijji", "MuhammadSajjad"], "doi": "10.3389/fonc.2021.811355\n10.1148/radiol.2020200274\n10.1148/radiol.2020200343\n10.1016/S0140-6736(20)30183-5\n10.3389/fmed.2020.612962\n10.3389/frai.2021.598932\n10.1093/jtm/taaa008\n10.3389/fmed.2020.00427\n10.1016/S1473-3099(20)30086-4\n10.3389/fcvm.2021.638011\n10.1109/ACCESS.2020.3005510\n10.1109/CBMS52027.2021.00103\n10.1007/s10044-021-00984-y\n10.1016/j.eswa.2020.114054\n10.1109/TII.2021.3057683\n10.1007/s10489-020-01902-1\n10.3389/fmed.2021.704256\n10.1016/j.media.2020.101794\n10.1109/TII.2021.3057524\n10.1109/TMI.2020.2993291\n10.1109/TIP.2021.3058783\n10.1109/TNNLS.2021.3054306\n10.1109/JIOT.2020.3032544\n10.1109/JIOT.2020.2981557\n10.1109/MCOM.2018.1701148\n10.1109/TNSE.2018.2843326\n10.1109/JIOT.2020.3038009\n10.1109/JSAC.2020.3020598\n10.3389/frsc.2021.638743\n10.1109/JIOT.2021.3056185\n10.1016/j.asoc.2021.107330\n10.1016/j.compbiomed.2021.104319\n10.1002/aic.690370209\n10.1109/TCE.2020.3043683\n10.1109/TII.2021.3089462\n10.1109/TII.2021.3070544\n10.1016/j.compbiomed.2021.104348\n10.3389/fmed.2021.707602"}
{"title": "COVID-19 mortality prediction in the intensive care unit with deep learning based on longitudinal chest X-rays and clinical data.", "abstract": "We aimed to develop deep learning models using longitudinal chest X-rays (CXRs) and clinical data to predict in-hospital mortality of COVID-19 patients in the intensive care unit (ICU).\nSix hundred fifty-four patients (212 deceased, 442 alive, 5645 total CXRs) were identified across two institutions. Imaging and clinical data from one institution were used to train five longitudinal transformer-based networks applying five-fold cross-validation. The models were tested on data from the other institution, and pairwise comparisons were used to determine the best-performing models.\nA higher proportion of deceased patients had elevated white blood cell count, decreased absolute lymphocyte count, elevated creatine concentration, and incidence of cardiovascular and chronic kidney disease. A model based on pre-ICU CXRs achieved an AUC of 0.632 and an accuracy of 0.593, and a model based on ICU CXRs achieved an AUC of 0.697 and an accuracy of 0.657. A model based on all longitudinal CXRs (both pre-ICU and ICU) achieved an AUC of 0.702 and an accuracy of 0.694. A model based on clinical data alone achieved an AUC of 0.653 and an accuracy of 0.657. The addition of longitudinal imaging to clinical data in a combined model significantly improved performance, reaching an AUC of 0.727 (p = 0.039) and an accuracy of 0.732.\nThe addition of longitudinal CXRs to clinical data significantly improves mortality prediction with deep learning for COVID-19 patients in the ICU.\n\u2022 Deep learning was used to predict mortality in COVID-19 ICU patients. \u2022 Serial radiographs and clinical data were used. \u2022 The models could inform clinical decision-making and resource allocation.", "journal": "European radiology", "date": "2022-02-21", "authors": ["JianhongCheng", "JohnSollee", "CelinaHsieh", "HailinYue", "NicholasVandal", "JustinShanahan", "Ji WhaeChoi", "Thi My LinhTran", "KaseyHalsey", "FranklinIheanacho", "JamesWarren", "AbdullahAhmed", "CarstenEickhoff", "MichaelFeldman", "EduardoMortani Barbosa", "IhabKamel", "Cheng TingLin", "ThomasYi", "TerranceHealey", "PaulZhang", "JingWu", "MichaelAtalay", "Harrison XBai", "ZhichengJiao", "JianxinWang"], "doi": "10.1007/s00330-022-08588-8\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2108891\n10.1007/s11547-020-01200-3\n10.1016/j.ejro.2020.100231\n10.1148/radiol.2020201160\n10.1007/s00330-020-06827-4\n10.1148/radiol.2020201491\n10.1016/S2589-7500(21)00039-X\n10.1148/radiol.2020200823\n10.1148/radiol.2020201491\n10.17849/insm-47-01-31-39.1\n10.1007/s11548-020-02299-5\n10.1007/s00330-020-07504-2\n10.1002/emp2.12205\n10.1093/ije/dyaa171\n10.2196/25442\n10.1038/s41467-019-13993-7\n10.2196/24018\n10.3390/ijerph17228386\n10.2196/20259\n10.1080/07853890.2020.1868564\n10.2196/23458\n10.1017/S0950268820001727\n10.1016/j.smhl.2020.100178\n10.1038/s41379-020-00700-x\n10.1007/s00330-020-07269-8"}
{"title": "Optical imaging spectroscopy for rapid, primary screening of SARS-CoV-2: a proof of concept.", "abstract": "Effective testing is essential to control the coronavirus disease 2019 (COVID-19) transmission. Here we report a-proof-of-concept study on hyperspectral image analysis in the visible and near-infrared range for primary screening at the point-of-care of SARS-CoV-2. We apply spectral feature descriptors, partial least square-discriminant analysis, and artificial intelligence to extract information from optical diffuse reflectance measurements from 5 \u00b5L fluid samples at pixel, droplet, and patient levels. We discern preparations of engineered lentiviral particles pseudotyped with the spike protein of the SARS-CoV-2 from those with the G protein of the vesicular stomatitis virus in saline solution and artificial saliva. We report a quantitative analysis of 72 samples of nasopharyngeal exudate in a range of SARS-CoV-2 viral loads, and a descriptive study of another 32 fresh human saliva samples. Sensitivity for classification of exudates was 100% with peak specificity of 87.5% for discernment from PCR-negative but symptomatic cases. Proposed technology is reagent-free, fast, and scalable, and could substantially reduce the number of molecular tests currently required for COVID-19 mass screening strategies even in resource-limited settings.", "journal": "Scientific reports", "date": "2022-02-20", "authors": ["EmilioGomez-Gonzalez", "AlejandroBarriga-Rivera", "BeatrizFernandez-Mu\u00f1oz", "Jose ManuelNavas-Garcia", "IsabelFernandez-Lizaranzu", "Francisco JavierMunoz-Gonzalez", "RubenParrilla-Giraldez", "DesireeRequena-Lancharro", "PedroGil-Gamboa", "CristinaRosell-Valle", "CarmenGomez-Gonzalez", "Maria JoseMayorga-Buiza", "MariaMartin-Lopez", "OlgaMu\u00f1oz", "Juan CarlosGomez-Martin", "Maria IsabelRelimpio-Lopez", "JesusAceituno-Castro", "Manuel APerales-Esteve", "AntonioPuppo-Moreno", "Francisco JoseGarcia-Cozar", "LuciaOlvera-Collantes", "RaquelGomez-Diaz", "Silviade Los Santos-Trigo", "MonserratHuguet-Carrasco", "ManuelRey", "EmiliaGomez", "RosarioSanchez-Pernaute", "JavierPadillo-Ruiz", "JavierMarquez-Rivas"], "doi": "10.1038/s41598-022-06393-3"}
{"title": "Non-invasive coronary imaging in patients with COVID-19: A narrative review.", "abstract": "SARS-CoV-2 infection, responsible for COVID-19 outbreak, can cause cardiac complications, worsening outcome and prognosis. In particular, it can exacerbate any underlying cardiovascular condition, leading to atherosclerosis and increased plaque vulnerability, which may cause acute coronary syndrome. We review current knowledge on the mechanisms by which SARS-CoV-2 can trigger endothelial/myocardial damage and cause plaque formation, instability and deterioration. The aim of this review is to evaluate current non-invasive diagnostic techniques for coronary arteries evaluation in COVID-19 patients, such as coronary CT angiography and atherosclerotic plaque imaging, and their clinical implications. We also discuss the role of artificial intelligence, deep learning and radiomics in the context of coronary imaging in COVID-19 patients.", "journal": "European journal of radiology", "date": "2022-02-19", "authors": ["CarlottaOnnis", "GiuseppeMuscogiuri", "PierPaolo Bassareo", "RiccardoCau", "LorenzoMannelli", "ChristianCadeddu", "Jasjit SSuri", "GiuliaCerrone", "ClaraGerosa", "SandroSironi", "GavinoFaa", "AlessandroCarriero", "GianlucaPontone", "LucaSaba"], "doi": "10.1016/j.ejrad.2022.110188\n10.1001/jamacardio.2020.0950\n10.1093/cvr/cvz062\n10.1016/j.lfs.2020.117723\n10.1016/j.atherosclerosis.2010.05.034\n10.1161/ATVBAHA.120.312470\n10.1111/micc.v28.710.1111/micc.12718\n10.1016/j.jcct.2021.02.004\n10.1161/CIRCULATIONAHA.120.046941\n10.2478/jce-2020-0008\n10.1016/j.jacc.2019.12.012\n10.1016/j.jcct.2019.06.008\n10.1148/ryct.2019180003\n10.1148/radiol.2018172523\n10.1016/j.jcmg.2020.04.012\n10.1016/j.compbiomed.2020.103960\n10.1016/j.jcmg.2019.06.009\n10.1016/j.ejrad.2019.02.038\n10.1183/13993003.00775-2020\n10.1109/RBME.2020.2990959\n10.1016/j.cmpb.2020.105651\n10.1016/j.atherosclerosis.2019.12.001\n10.1126/scitranslmed.aal2658\n10.1016/j.metabol.2020.154436\n10.1097/RTI.0000000000000268\n10.1136/heartjnl-2021-BCS.238\n10.1161/circ.142.suppl_3.16467\n10.1016/j.jcct.2019.07.010\n10.1093/eurheartj/ehaa381"}
{"title": "A review of deep learning-based detection methods for COVID-19.", "abstract": "COVID-19 is a fast-spreading pandemic, and early detection is crucial for stopping the spread of infection. Lung images are used in the detection of coronavirus infection. Chest X-ray (CXR) and computed tomography (CT) images are available for the detection of COVID-19. Deep learning methods have been proven efficient and better performing in many computer vision and medical imaging applications. In the rise of the COVID pandemic, researchers are using deep learning methods to detect coronavirus infection in lung images. In this paper, the currently available deep learning methods that are used to detect coronavirus infection in lung images are surveyed. The available methodologies, public datasets, datasets that are used by each method and evaluation metrics are summarized in this paper to help future researchers. The evaluation metrics that are used by the methods are comprehensively compared.", "journal": "Computers in biology and medicine", "date": "2022-02-19", "authors": ["NandhiniSubramanian", "OmarElharrouss", "SomayaAl-Maadeed", "MuhammedChowdhury"], "doi": "10.1016/j.compbiomed.2022.105233\n10.1109/ICIoT48696.2020.9089566\n10.1109/ACCESS.2021.3113953\n10.1007/s11263-015-0816-y\n10.1109/cvpr.2017.369\n10.1016/j.cell.2018.02.010"}
{"title": "Assessment of Smartphone-Based Spiral Tracing in Multiple Sclerosis Reveals Intra-Individual Reproducibility as a Major Determinant of the Clinical Utility of the Digital Test.", "abstract": "Technological advances, lack of medical professionals, high cost of face-to-face encounters, and disasters such as the COVID-19 pandemic fuel the telemedicine revolution. Numerous smartphone apps have been developed to measure neurological functions. However, their psychometric properties are seldom determined. It is unclear which designs underlie the eventual clinical utility of the smartphone tests. We have developed the smartphone Neurological Function Tests Suite (NeuFun-TS) and are systematically evaluating their psychometric properties against the gold standard of complete neurological examination digitalized into the NeurEx", "journal": "Frontiers in medical technology", "date": "2022-02-19", "authors": ["Komi SMessan", "LinhPham", "ThomasHarris", "YujinKim", "VanessaMorgan", "PeterKosa", "BibianaBielekova"], "doi": "10.3389/fmedt.2021.714682\n10.1212/WNL.0000000000000509\n10.1212/WNL.33.11.1444\n10.1212/WNL.34.10.1368\n10.3389/fneur.2016.00131\n10.1002/acn3.640\n10.1177/2055217316634754\n10.3389/fneur.2018.00740\n10.1212/NXI.0000000000000162\n10.1088/1361-6579/ab8771\n10.1016/S0165-0270(01)00373-9\n10.1016/j.humov.2006.05.005\n10.1111/ene.14091\n10.2196/14863\n10.1038/s41746-021-00401-y\n10.1016/j.cmpb.2017.02.012\n10.1186/s12883-018-1027-2\n10.1016/j.jneumeth.2006.09.019\n10.3389/fneur.2019.00358\n10.1177/2055217316688930\n10.1016/j.msard.2015.08.009\n10.1016/j.imu.2017.05.005\n10.3390/s150923727\n10.1109/ICPR.1994.576361\n10.1109/34.232073\n10.1016/j.ijggc.2017.02.005\n10.18637/jss.v051.i04\n10.1137/0717021\n10.1137/0904045\n10.5120/ijca2018917899\n10.1155/2018/9801308\n10.1111/j.2517-6161.1995.tb02031.x\n10.2307/3001968\n10.1016/j.jcm.2016.02.012\n10.1519/00124278-200502000-00038\n10.18637/jss.v028.i05\n10.18637/jss.v033.i01\n10.1088/0957-0233/24/2/027001"}
{"title": "Optimal Deep-Learning-Enabled Intelligent Decision Support System for SARS-CoV-2 Classification.", "abstract": "Intelligent decision support systems (IDSS) for complex healthcare applications aim to examine a large quantity of complex healthcare data to assist doctors, researchers, pathologists, and other healthcare professionals. A decision support system (DSS) is an intelligent system that provides improved assistance in various stages of health-related disease diagnosis. At the same time, the SARS-CoV-2 infection that causes COVID-19 disease has spread globally from the beginning of 2020. Several research works reported that the imaging pattern based on computed tomography (CT) can be utilized to detect SARS-CoV-2. Earlier identification and detection of the diseases is essential to offer adequate treatment and avoid the severity of the disease. With this motivation, this study develops an efficient deep-learning-based fusion model with swarm intelligence (EDLFM-SI) for SARS-CoV-2 identification. The proposed EDLFM-SI technique aims to detect and classify the SARS-CoV-2 infection or not. Also, the EDLFM-SI technique comprises various processes, namely, data augmentation, preprocessing, feature extraction, and classification. Moreover, a fusion of capsule network (CapsNet) and MobileNet based feature extractors are employed. Besides, a water strider algorithm (WSA) is applied to fine-tune the hyperparameters involved in the DL models. Finally, a cascaded neural network (CNN) classifier is applied for detecting the existence of SARS-CoV-2. In order to showcase the improved performance of the EDLFM-SI technique, a wide range of simulations take place on the COVID-19 CT data set and the SARS-CoV-2 CT scan data set. The simulation outcomes highlighted the supremacy of the EDLFM-SI technique over the recent approaches.", "journal": "Journal of healthcare engineering", "date": "2022-02-19", "authors": ["Ashit KumarDutta", "Nasser AliAljarallah", "TAbirami", "MSundarrajan", "SeifedineKadry", "YunyoungNam", "Chang-WonJeong"], "doi": "10.1155/2022/4130674\n10.3390/s21020455\n10.1155/2021/8864522\n10.1155/2021/8869372\n10.1155/2021/8829829\n10.1016/s0140-6736(20)30607-3\n10.1016/s0140-6736(20)30211-7\n10.1148/radiol.2020200642\n10.3390/app11157004\n10.1007/s12652-021-03282-x\n10.1007/s00500-020-05275-y\n10.1038/s41591-020-0931-3\n10.22266/ijies2020.1031.07\n10.1016/b978-0-12-824536-1.00039-3\n10.1007/978-981-16-2594-7_30\n10.1016/j.procs.2019.08.147\n10.1109/jstars.2020.2968930\n10.3311/ppci.16872\n10.1088/1742-6596/1025/1/012097\n10.1101/2020.04.13.20063941"}
{"title": "The application research of AI image recognition and processing technology in the early diagnosis of the COVID-19.", "abstract": "This study intends to establish a combined prediction model that integrates the clinical symptoms,the lung lesion volume, and the radiomics features of patients with COVID-19, resulting in a new model to predict the severity of COVID-19.\nThe clinical data of 386 patients with COVID-19 at several hospitals, as well as images of certain patients during their hospitalization, were collected retrospectively to create a database of patients with COVID-19 pneumonia. The contour of lungs and lesion locations may be retrieved from CT scans using a CT-image-based quantitative discrimination and trend analysis method for COVID-19 and the Mask R-CNN deep neural network model to create 3D data of lung lesions. The quantitative COVID-19 factors were then determined, on which the diagnosis of the development of the patients' symptoms could be established. Then, using an artificial neural network, a prediction model of the severity of COVID-19 was constructed by combining characteristic imaging features on CT slices with clinical factors. ANN neural network was used for training, and tenfold cross-validation was used to verify the prediction model. The diagnostic performance of this model is verified by the receiver operating characteristic (ROC) curve.\nCT radiomics features extraction and analysis based on a deep neural network can detect COVID-19 patients with an 86% sensitivity and an 85% specificity. According to the ROC curve, the constructed severity prediction model indicates that the AUC of patients with severe COVID-19 is 0.761, with sensitivity and specificity of 79.1% and 73.1%, respectively.\nThe combined prediction model for severe COVID-19 pneumonia, which is based on deep learning and integrates clinical aspects, pulmonary lesion volume, and radiomics features of patients, has a remarkable differential ability for predicting the course of disease in COVID-19 patients. This may assist in the early prevention of severe COVID-19 symptoms.", "journal": "BMC medical imaging", "date": "2022-02-19", "authors": ["WenyuChen", "MingYao", "ZhenyuZhu", "YanbaoSun", "XiupingHan"], "doi": "10.1186/s12880-022-00753-1\n10.1631/jzus.B2000083\n10.1002/jmv.25689\n10.1080/14787210.2020.1797487\n10.2174/1568009620666200414151419\n10.1080/14737159.2020.1757437\n10.1371/journal.pone.0242958\n10.1016/j.diii.2020.03.014\n10.1148/radiol.2020201160\n10.1097/RLI.0000000000000672\n10.1007/s11604-020-00967-9\n10.1148/radiol.2020200343\n10.1016/S2213-2600(18)30286-8\n10.1016/S2213-2600(20)30003-5\n10.2196/20756\n10.1001/jamainternmed.2020.3539\n10.2116/analsci.19R006\n10.1007/s00330-020-06801-0\n10.1007/s00330-019-06163-2\n10.1016/j.media.2017.07.005\n10.1097/RLI.0000000000000341\n10.1016/j.cell.2018.02.010\n10.7326/M20-6817\n10.1002/jmv.26250\n10.1148/radiol.2020200905\n10.1016/j.radi.2020.09.010\n10.1016/j.ijid.2020.10.036"}
{"title": "Deep learning approach based on superpixel segmentation assisted labeling for automatic pressure ulcer diagnosis.", "abstract": "A pressure ulcer is an injury of the skin and underlying tissues adjacent to a bony eminence. Patients who suffer from this disease may have difficulty accessing medical care. Recently, the COVID-19 pandemic has exacerbated this situation. Automatic diagnosis based on machine learning (ML) brings promising solutions. Traditional ML requires complicated preprocessing steps for feature extraction. Its clinical applications are thus limited to particular datasets. Deep learning (DL), which extracts features from convolution layers, can embrace larger datasets that might be deliberately excluded in traditional algorithms. However, DL requires large sets of domain specific labeled data for training. Labeling various tissues of pressure ulcers is a challenge even for experienced plastic surgeons. We propose a superpixel-assisted, region-based method of labeling images for tissue classification. The boundary-based method is applied to create a dataset for wound and re-epithelialization (re-ep) segmentation. Five popular DL models (U-Net, DeeplabV3, PsPNet, FPN, and Mask R-CNN) with encoder (ResNet-101) were trained on the two datasets. A total of 2836 images of pressure ulcers were labeled for tissue classification, while 2893 images were labeled for wound and re-ep segmentation. All five models had satisfactory results. DeeplabV3 had the best performance on both tasks with a precision of 0.9915, recall of 0.9915 and accuracy of 0.9957 on the tissue classification; and a precision of 0.9888, recall of 0.9887 and accuracy of 0.9925 on the wound and re-ep segmentation task. Combining segmentation results with clinical data, our algorithm can detect the signs of wound healing, monitor the progress of healing, estimate the wound size, and suggest the need for surgical debridement.", "journal": "PloS one", "date": "2022-02-18", "authors": ["Che WeiChang", "MesakhChristian", "Dun HaoChang", "FeipeiLai", "Tom JLiu", "Yo ShenChen", "Wei JenChen"], "doi": "10.1371/journal.pone.0264139\n10.1111/j.1532-5415.2004.52106.x\n10.1016/j.jaad.2018.12.069\n10.1111/j.1365-2753.2006.00684.x\n10.1097/00129334-200311000-00012\n10.1038/s41698-020-0122-1\n10.1038/s41598-019-56847-4\n10.1016/j.artmed.2021.102020\n10.1371/journal.pone.0204155\n10.1371/journal.pone.0218808\n10.1016/j.artmed.2019.101742\n10.1016/j.cmpb.2018.02.018\n10.1109/JBHI.2017.2743526\n10.1111/jocn.13726\n10.1007/s11517-018-1835-y\n10.1016/j.addr.2018.06.019\n10.1093/neuonc/noab071\n10.1016/j.media.2016.03.002\n10.1109/TIP.2017.2778569\n10.1109/TMI.2009.2033595"}
{"title": "Augmenting existing deterioration indices with chest radiographs to predict clinical deterioration.", "abstract": "When hospitals are at capacity, accurate deterioration indices could help identify low-risk patients as potential candidates for home care programs and alleviate hospital strain. To date, many existing deterioration indices are based entirely on structured data from the electronic health record (EHR) and ignore potentially useful information from other sources.\nTo improve the accuracy of existing deterioration indices by incorporating unstructured imaging data from chest radiographs.\nMachine learning models were trained to predict deterioration of patients hospitalized with acute dyspnea using existing deterioration index scores and chest radiographs. Models were trained on hospitalized patients without coronavirus disease 2019 (COVID-19) and then subsequently tested on patients with COVID-19 between January 2020 and December 2020 at a single tertiary care center who had at least one radiograph taken within 48 hours of hospital admission.\nPatient deterioration was defined as the need for invasive or non-invasive mechanical ventilation, heated high flow nasal cannula, IV vasopressor administration or in-hospital mortality at any time following admission. The EPIC deterioration index was augmented with unstructured data from chest radiographs to predict risk of deterioration. We compared discriminative performance of the models with and without incorporating chest radiographs using area under the receiver operating curve (AUROC), focusing on comparing the fraction and total patients identified as low risk at different negative predictive values (NPV).\nData from 6278 hospitalizations were analyzed, including 5562 hospitalizations without COVID-19 (training cohort) and 716 with COVID-19 (216 in validation, 500 in held-out test cohort). At a NPV of 0.95, the best-performing image-augmented deterioration index identified 49 more (9.8%) individuals as low-risk compared to the deterioration index based on clinical data alone in the first 48 hours of admission. At a NPV of 0.9, the EPIC image-augmented deterioration index identified 26 more individuals (5.2%) as low-risk compared to the deterioration index based on clinical data alone in the first 48 hours of admission.\nAugmenting existing deterioration indices with chest radiographs results in better identification of low-risk patients. The model augmentation strategy could be used in the future to incorporate other forms of unstructured data into existing disease models.", "journal": "PloS one", "date": "2022-02-16", "authors": ["EmilyMu", "SarahJabbour", "Adrian VDalca", "JohnGuttag", "JennaWiens", "Michael WSjoding"], "doi": "10.1371/journal.pone.0263922\n10.1016/j.jbi.2013.06.011\n10.1164/rccm.201406-1022OC\n10.1101/2020.04.24.20079012\n10.1136/bmj-2021-068576\n10.1017/ice.2018.16\n10.7326/0003-4819-144-3-200602070-00009\n10.1007/s10278-008-9115-2\n10.1259/bjr.73.874.11271897\n10.1186/s12890-019-1042-0\n10.7326/M20-1260\n10.1038/s41597-018-0005-2\n10.1101/2021.02.11.20196766\n10.1007/s13246-020-00888-x\n10.1148/radiol.2018181422\n10.1155/2020/9756518\n10.1016/j.patcog.2021.107826\n10.1056/NEJMoa1214726"}
{"title": "COVID Detection From Chest X-Ray Images Using Multi-Scale Attention.", "abstract": "Deep learning based methods have shown great promise in achieving accurate automatic detection of Coronavirus Disease (covid) - 19 from Chest X-Ray (cxr) images.However, incorporating explainability in these solutions remains relatively less explored. We present a hierarchical classification approach for separating normal, non-covid pneumonia (ncp) and covid cases using cxr images. We demonstrate that the proposed method achieves clinically consistent explainations. We achieve this using a novel multi-scale attention architecture called Multi-scale Attention Residual Learning (marl) and a new loss function based on conicity for training the proposed architecture. The proposed classification strategy has two stages. The first stage uses a model derived from DenseNet to separate pneumonia cases from normal cases while the second stage uses the marl architecture to discriminate between covid and ncp cases. With a five-fold cross validation the proposed method achieves 93%, 96.28%, and 84.51% accuracy respectively over three large, public datasets for normal vs. ncp vs. covid classification. This is competitive to the state-of-the-art methods. We also provide explanations in the form of GradCAM attributions, which are well aligned with expert annotations. The attributions are also seen to clearly indicate that marl deems the peripheral regions of the lungs to be more important in the case of covid cases while central regions are seen as more important in ncp cases. This observation matches the criteria described by radiologists in clinical literature, thereby attesting to the utility of the derived explanations.", "journal": "IEEE journal of biomedical and health informatics", "date": "2022-02-15", "authors": ["AbhinavDhere", "JayanthiSivaswamy"], "doi": "10.1109/JBHI.2022.3151171"}
{"title": "ADA-COVID: Adversarial Deep Domain Adaptation-Based Diagnosis of COVID-19 from Lung CT Scans Using Triplet Embeddings.", "abstract": "Rapid diagnosis of COVID-19 with high reliability is essential in the early stages. To this end, recent research often uses medical imaging combined with machine vision methods to diagnose COVID-19. However, the scarcity of medical images and the inherent differences in existing datasets that arise from different medical imaging tools, methods, and specialists may affect the generalization of machine learning-based methods. Also, most of these methods are trained and tested on the same dataset, reducing the generalizability and causing low reliability of the obtained model in real-world applications. This paper introduces an adversarial deep domain adaptation-based approach for diagnosing COVID-19 from lung CT scan images, termed ADA-COVID. Domain adaptation-based training process receives multiple datasets with different input domains to generate domain-invariant representations for medical images. Also, due to the excessive structural similarity of medical images compared to other image data in machine vision tasks, we use the triplet loss function to generate similar representations for samples of the same class (infected cases). The performance of ADA-COVID is evaluated and compared with other state-of-the-art COVID-19 diagnosis algorithms. The obtained results indicate that ADA-COVID achieves classification improvements of at least 3%, 20%, 20%, and 11% in accuracy, precision, recall, and F", "journal": "Computational intelligence and neuroscience", "date": "2022-02-15", "authors": ["MehradAria", "EsmaeilNourani", "AminGolzari Oskouei"], "doi": "10.1155/2022/2564022\n10.1148/radiol.2020200642\n10.3390/s21020455\n10.1109/TNNLS.2021.3054306\n10.1016/j.ejrad.2020.108961\n10.1145/3472813.3472820\n10.1148/ryct.2020200034\n10.1007/s42600-021-00151-6\n10.1038/s41598-020-76550-z\n10.1016/j.patrec.2020.10.001\n10.1016/j.compbiomed.2020.104037\n10.1007/s00330-021-07715-1\n10.2196/27468\n10.1038/s41597-021-00900-3\n10.1016/j.bspc.2021.102588\n10.1016/j.imu.2020.100427\n10.3390/ijerph17186933\n10.1109/jbhi.2020.3037127\n10.1016/j.patrec.2020.09.010\n10.1038/s42256-020-00257-z\n10.1148/radiol.2020200905\n10.1016/j.eng.2020.04.010\n10.1038/s41598-020-76282-0\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110122\n10.1109/jbhi.2020.3023246\n10.1038/s41746-021-00399-3\n10.1007/s10096-020-03901-z\n10.1007/s13246-020-00865-4\n10.1109/cvpr.2017.195\n10.1016/j.media.2020.101794\n10.1109/cvpr.2016.90\n10.1109/cvpr.2017.243\n10.1016/j.cmpb.2020.105608\n10.1080/07391102.2020.1788642\n10.1109/tcbb.2021.3065361\n10.1016/j.chaos.2020.110190\n10.1007/s10489-020-02055-x\n10.1007/s13755-021-00152-w\n10.1016/j.cmpb.2020.105532\n10.1109/CVPR.2016.308\n10.3390/e22050517\n10.1109/cvpr.2009.5206848\n10.3390/s19194139\n10.1109/cvpr.2015.7298682\n10.1016/j.asoc.2020.106897\n10.1038/s41598-021-83424-5\n10.1038/s41467-020-18685-1\n10.1371/journal.pone.0250952\n10.14299/ijser.2020.03.02\n10.1016/j.asoc.2019.02.038\n10.1016/j.asoc.2021.108005\n10.1016/j.chaos.2021.111494\n10.1016/j.compbiomed.2020.103795\n10.1117/1.JMI.3.4.044506\n10.1118/1.3528204\n10.1016/j.compmedimag.2011.07.003\n10.1148/ryct.2020200026"}
{"title": "Spectral decoupling for training transferable neural networks in medical imaging.", "abstract": "Many neural networks for medical imaging generalize poorly to data unseen during training. Such behavior can be caused by overfitting easy-to-learn features while disregarding other potentially informative features. A recent implicit bias mitigation technique called spectral decoupling provably encourages neural networks to learn more features by regularizing the networks' unnormalized prediction scores with an L2 penalty. We show that spectral decoupling increases the networks' robustness for data distribution shifts and prevents overfitting on easy-to-learn features in medical images. To validate our findings, we train networks with and without spectral decoupling to detect prostate cancer on tissue slides and COVID-19 in chest radiographs. Networks trained with spectral decoupling achieve up to 9.5 percent point higher performance on external datasets. Spectral decoupling alleviates generalization issues associated with neural networks and can be used to complement or replace computationally expensive explicit bias mitigation methods, such as stain normalization in histological images.", "journal": "iScience", "date": "2022-02-12", "authors": ["JoonaPohjonen", "CarolinSt\u00fcrenberg", "AnttiRannikko", "TuomasMirtti", "EsaPitk\u00e4nen"], "doi": "10.1016/j.isci.2022.103767\n10.3390/info11020125\n10.1109/WACV.2018.00097\n10.1109/ACCESS.2020.3010287\n10.1109/ISBI.2009.5193250\n10.1038/s41598-020-76550-z\n10.5281/zenodo.4414861"}
{"title": "Artificial intelligence for early diagnosis of lung cancer through incidental nodule detection in low- and middle-income countries-acceleration during the COVID-19 pandemic but here to stay.", "abstract": "Although the coronavirus disease of 2019 (COVID-19) pandemic had profound pernicious effects, it revealed deficiencies in health systems, particularly among low- and middle-income countries (LMICs). With increasing uncertainty in healthcare, existing unmet needs such as poor outcomes of lung cancer (LC) patients in LMICs, mainly due to late stages at diagnosis, have been challenging-necessitating a shift in focus for judicious health resource utilization. Leveraging artificial intelligence (AI) for screening large volumes of pulmonary images performed for noncancerous reasons, such as health checks, immigration, tuberculosis screening, or other lung conditions, including but not limited to COVID-19, can facilitate easy and early identification of incidental pulmonary nodules (IPNs), which otherwise could have been missed. AI can review every chest X-ray or computed tomography scan through a trained pair of eyes, thus strengthening the infrastructure and enhancing capabilities of manpower for interpreting images in LMICs for streamlining accurate and early identification of IPNs. AI can be a catalyst for driving LC screening with enhanced efficiency, particularly in primary care settings, for timely referral and adequate management of coincidental IPN. AI can facilitate shift in the stage of LC diagnosis for improving survival, thus fostering optimal health-resource utilization and sustainable healthcare systems resilient to crisis. This article highlights the challenges for organized LC screening in LMICs and describes unique opportunities for leveraging AI. We present pilot initiatives from Asia, Latin America, and Russia illustrating AI-supported IPN identification from routine imaging to facilitate early diagnosis of LC at a potentially curable stage.", "journal": "American journal of cancer research", "date": "2022-02-11", "authors": ["SusanaGoncalves", "Pei-ChiehFong", "MariyaBlokhina"], "doi": null}
{"title": "Use of Artificial Intelligence to Triage Patients with Flu-Like Symptoms Using Imaging in Non-COVID-19 Hospitals during COVID-19 Pandemic: An Ongoing 8-Month Experience.", "abstract": "", "journal": "The Indian journal of radiology & imaging", "date": "2022-02-10", "authors": ["AtulKapoor", "AprajitaKapoor", "GoldaaMahajan"], "doi": "10.1055/s-0041-1741103"}
{"title": "Investigating phenotypes of pulmonary COVID-19 recovery: A longitudinal observational prospective multicenter trial.", "abstract": "The optimal procedures to prevent, identify, monitor, and treat long-term pulmonary sequelae of COVID-19 are elusive. Here, we characterized the kinetics of respiratory and symptom recovery following COVID-19.\nWe conducted a longitudinal, multicenter observational study in ambulatory and hospitalized COVID-19 patients recruited in early 2020 (n = 145). Pulmonary computed tomography (CT) and lung function (LF) readouts, symptom prevalence, and clinical and laboratory parameters were collected during acute COVID-19 and at 60, 100, and 180 days follow-up visits. Recovery kinetics and risk factors were investigated by logistic regression. Classification of clinical features and participants was accomplished by unsupervised and semi-supervised multiparameter clustering and machine learning.\nAt the 6-month follow-up, 49% of participants reported persistent symptoms. The frequency of structural lung CT abnormalities ranged from 18% in the mild outpatient cases to 76% in the intensive care unit (ICU) convalescents. Prevalence of impaired LF ranged from 14% in the mild outpatient cases to 50% in the ICU survivors. Incomplete radiological lung recovery was associated with increased anti-S1/S2 antibody titer, IL-6, and CRP levels at the early follow-up. We demonstrated that the risk of perturbed pulmonary recovery could be robustly estimated at early follow-up by clustering and machine learning classifiers employing solely non-CT and non-LF parameters.\nThe severity of acute COVID-19 and protracted systemic inflammation is strongly linked to persistent structural and functional lung abnormality. Automated screening of multiparameter health record data may assist in the prediction of incomplete pulmonary recovery and optimize COVID-19 follow-up management.\nThe State of Tyrol (GZ 71934), Boehringer Ingelheim/Investigator initiated study (IIS 1199-0424).\nClinicalTrials.gov: NCT04416100.", "journal": "eLife", "date": "2022-02-09", "authors": ["ThomasSonnweber", "PiotrTymoszuk", "SabinaSahanic", "AnnaBoehm", "AlexPizzini", "AnnaLuger", "ChristophSchwabl", "ManfredNairz", "PhilippGrubwieser", "KatharinaKurz", "SabineKoppelst\u00e4tter", "MagdalenaAichner", "BernhardPuchner", "AlexanderEgger", "GregorHoermann", "EwaldW\u00f6ll", "G\u00fcnterWeiss", "GerligWidmann", "IvanTancevski", "JudithL\u00f6ffler-Ragg"], "doi": "10.7554/eLife.72500\n10.1007/978-3-030-32047-8\n10.2196/25988\n10.1111/j.2517-6161.1995.tb02031.x\n10.1137/1.9781611972788.22\n10.1023/A:1010933404324\n10.1148/radiol.2021210834\n10.1016/j.chemolab.2007.01.004\n10.1016/j.eclinm.2021.101019\n10.1016/j.cels.2021.05.005\n10.1016/S1473-3099(20)30120-1\n10.1186/s12916-021-02115-0\n10.1016/S2213-2600(21)00383-0\n10.1159/000518141\n10.1016/j.cca.2021.08.024\n10.1037/h0028106\n10.18637/jss.v033.i01\n10.1016/S2213-2600(20)30168-5\n10.1513/AnnalsATS.202103-340OC\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)32656-8\n10.1016/S0140-6736(21)01755-4\n10.1378/chest.128.4.2247\n10.1164/rccm.201903-0563CI\n10.1007/978-3-642-97610-0\n10.18637/jss.v028.i05\n10.18637/jss.v082.i13\n10.1001/archinternmed.2009.384\n10.1162/089976604773717621\n10.3923/jse.2014.14.22\n10.1378/chest.10-2438\n10.1136/thx.2004.023762\n10.1111/j.1440-1843.2010.01720.x\n10.1016/S1473-3099(20)30584-3\n10.1093/bioinformatics/bty109\n10.1016/S2213-2600(20)30349-0\n10.1017/CBO9780511812651\n10.18637/jss.v079.c02\n10.1093/cid/ciab978\n10.1136/bmj.n136\n10.1186/s12931-020-01546-2\n10.1183/13993003.03481-2020\n10.1126/sciadv.abd4177\n10.1038/s41591-021-01292-y\n10.1002/art.39405\n10.1016/S2213-2600(21)00031-X\n10.1109/72.846731\n10.1093/biomet/asq061\n10.18637/jss.v087.i07\n10.1007/978-3-319-24277-4\n10.21105/joss.01686\n10.1378/chest.12-0685\n10.3389/fmed.2021.717194"}
{"title": "Measurement of SARS-CoV-2 Antibody Titers Improves the Prediction Accuracy of COVID-19 Maximum Severity by Machine Learning in Non-Vaccinated Patients.", "abstract": "Numerous studies have suggested that the titers of antibodies against SARS-CoV-2 are associated with the COVID-19 severity, however, the types of antibodies associated with the disease maximum severity and the timing at which the associations are best observed, especially within one week after symptom onset, remain controversial. We attempted to elucidate the antibody responses against SARS-CoV-2 that are associated with the maximum severity of COVID-19 in the early phase of the disease, and to investigate whether antibody testing might contribute to prediction of the disease maximum severity in COVID-19 patients. We classified the patients into four groups according to the disease maximum severity (severity group 1 (did not require oxygen supplementation), severity group 2a (required oxygen supplementation at low flow rates), severity group 2b (required oxygen supplementation at relatively high flow rates), and severity group 3 (required mechanical ventilatory support)), and serially measured the titers of IgM, IgG, and IgA against the nucleocapsid protein, spike protein, and receptor-binding domain of SARS-CoV-2 until day 12 after symptom onset. The titers of all the measured antibody responses were higher in severity group 2b and 3, especially severity group 2b, as early as at one week after symptom onset. Addition of data obtained from antibody testing improved the ability of analysis models constructed using a machine learning technique to distinguish severity group 2b and 3 from severity group 1 and 2a. These models constructed with non-vaccinated COVID-19 patients could not be applied to the cases of breakthrough infections. These results suggest that antibody testing might help physicians identify non-vaccinated COVID-19 patients who are likely to require admission to an intensive care unit.", "journal": "Frontiers in immunology", "date": "2022-02-08", "authors": ["MakotoKurano", "HirokoOhmiya", "YoshiroKishi", "JunOkada", "YukiNakano", "RinYokoyama", "ChungenQian", "FuzhenXia", "FanHe", "LiangZheng", "YiYu", "DaisukeJubishi", "KohOkamoto", "KyojiMoriya", "TatsuhikoKodama", "YutakaYatomi"], "doi": "10.3389/fimmu.2022.811952\n10.1101/2020.07.27.20161810\n10.1038/s41598-021-88130-w\n10.1080/07853890.2020.1840621\n10.1111/ijcp.14571\n10.1016/j.jcv.2020.104611\n10.1007/s12250-020-00270-x\n10.1093/ajcp/aqaa123\n10.1080/22221751.2020.1791738\n10.1093/infdis/jiaa463\n10.1126/sciimmunol.abe0240\n10.1172/JCI138759\n10.1371/journal.pone.0248918\n10.1016/j.jaci.2020.10.040\n10.3390/ijerph18031318\n10.7326/M20-3337\n10.1016/j.jiac.2021.01.006\n10.1038/s41423-020-00588-2\n10.1089/vim.2020.0321\n10.1126/sciadv.abf2467\n10.4269/ajtmh.21-0014\n10.1016/j.biopha.2020.110629\n10.1093/infdis/jiaa618\n10.18632/aging.103417\n10.1371/journal.pone.0241104\n10.1038/s42003-020-01526-8\n10.3389/fimmu.2020.628971\n10.1093/cid/ciaa344\n10.1016/S2666-5247(21)00025-2\n10.1016/j.cell.2020.12.015\n10.1016/j.ijid.2021.04.080\n10.1038/s41392-021-00611-6\n10.1016/j.xcrm.2021.100329\n10.1038/s41598-021-83108-0\n10.1038/s41598-021-81862-9\n10.1038/s41564-020-00813-8\n10.1080/22221751.2020.1823890\n10.1126/scitranslmed.abd5487\n10.1038/s41598-021-82428-5\n10.1515/cclm-2020-0548\n10.1002/(SICI)1521-4036(200001)42:1\n10.2307/2347973\n10.1038/s41421-020-00231-4\n10.1080/07853890.2020.1868564\n10.1002/ctm2.323\n10.1038/s41591-021-01355-0"}
{"title": "Diagnosis of hypercritical chronic pulmonary disorders using dense convolutional network through chest radiography.", "abstract": "Lung-related ailments are prevalent all over the world which majorly includes asthma, chronic obstructive pulmonary disease (COPD), tuberculosis, pneumonia, fibrosis, etc. and now COVID-19 is added to this list. Infection of COVID-19 poses respirational complications with other indications like cough, high fever, and pneumonia. WHO had identified cancer in the lungs as a fatal cancer type amongst others and thus, the timely detection of such cancer is pivotal for an individual's health. Since the elementary convolutional neural networks have not performed fairly well in identifying atypical image types hence, we recommend a novel and completely automated framework with a deep learning approach for the recognition and classification of chronic pulmonary disorders (CPD) and COVID-pneumonia using Thoracic or Chest X-Ray (CXR) images. A novel three-step, completely automated, approach is presented that first extracts the region of interest from CXR images for preprocessing, and they are then used to detects infected lungs X-rays from the Normal ones. Thereafter, the infected lung images are further classified into COVID-pneumonia, pneumonia, and other chronic pulmonary disorders (OCPD), which might be utilized in the current scenario to help the radiologist in substantiating their diagnosis and in starting well in time treatment of these deadly lung diseases. And finally, highlight the regions in the CXR which are indicative of severe chronic pulmonary disorders like COVID-19 and pneumonia. A detailed investigation of various pivotal parameters based on several experimental outcomes are made here. This paper presents an approach that detects the Normal lung X-rays from infected ones and the infected lung images are further classified into COVID-pneumonia, pneumonia, and other chronic pulmonary disorders with an utmost accuracy of 96.8%. Several other collective performance measurements validate the superiority of the presented model. The proposed framework shows effective results in classifying lung images into Normal, COVID-pneumonia, pneumonia, and other chronic pulmonary disorders (OCPD). This framework can be effectively utilized in this current pandemic scenario to help the radiologist in substantiating their diagnosis and in starting well in time treatment of these deadly lung diseases.", "journal": "Multimedia tools and applications", "date": "2022-02-08", "authors": ["RajatMehrotra", "RajeevAgrawal", "M AAnsari"], "doi": "10.1007/s11042-021-11748-5\n10.2214/ajr.181.4.1811083\n10.1016/j.cmpb.2019.105162\n10.3233/HIS-190263\n10.3390/app10020559\n10.1016/j.compmedimag.2007.02.002\n10.1016/j.compbiomed.2018.10.011\n10.1016/j.crad.2018.12.015\n10.1016/j.cmpb.2020.105581\n10.1145/3065386\n10.1016/j.crad.2019.08.005\n10.26599/BDMA.2018.9020001\n10.1016/j.zemedi.2018.11.002\n10.1016/S0140-6736(96)07492-2\n10.1164/ajrccm.162.4.2002019\n10.1109/TKDE.2009.191\n10.1016/j.media.2017.06.015\n10.1109/TPAMI.2016.2572683\n10.1016/j.compbiomed.2017.04.006\n10.1007/s13244-018-0639-9"}
{"title": "A complete framework for accurate recognition and prognosis of COVID-19 patients based on deep transfer learning and feature classification approach.", "abstract": "The sudden appearance of COVID-19 has put the world in a serious situation. Due to the rapid spread of the virus and the increase in the number of infected patients and deaths, COVID-19 was declared a pandemic. This pandemic has its destructive effect not only on humans but also on the economy. Despite the development and availability of different vaccines for COVID-19, scientists still warn the citizens of new severe waves of the virus, and as a result, fast diagnosis of COVID-19 is a critical issue. Chest imaging proved to be a powerful tool in the early detection of COVID-19. This study introduces an entire framework for the early detection and early prognosis of COVID-19 severity in the diagnosed patients using laboratory test results. It consists of two phases (1) Early Diagnostic Phase (EDP) and (2) Early Prognostic Phase (EPP). In EDP, COVID-19 patients are diagnosed using CT chest images. In the current study, 5,\u00a0159 COVID-19 and 10,\u00a0376 normal computed tomography (CT) images of Egyptians were used as a dataset to train 7 different convolutional neural networks using transfer learning. Data augmentation normal techniques and generative adversarial networks (GANs), CycleGAN and CCGAN, were used to increase the images in the dataset to avoid overfitting issues. 28 experiments were applied and multiple performance metrics were captured. Classification with no augmentation yielded ", "journal": "Artificial intelligence review", "date": "2022-02-08", "authors": ["Hossam MagdyBalaha", "Eman MEl-Gendy", "Mahmoud MSaafan"], "doi": "10.1007/s10462-021-10127-8\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103795\n10.1109/ACCESS.2021.3060940\n10.7717/peerj-cs.555\n10.1007/s00521-020-05137-6\n10.1007/s00521-020-05397-2\n10.1016/j.bspc.2019.101734\n10.1016/j.cmpb.2020.105608\n10.1109/ACCESS.2018.2837621\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2019.2946622\n10.1007/s10898-007-9162-0\n10.1016/j.compbiomed.2019.103345\n10.1016/j.neucom.2015.08.112\n10.1007/s00603-015-0733-y\n10.1007/s11042-018-5714-1\n10.1016/j.procs.2016.05.512\n10.1111/j.1469-1809.1936.tb02137.x\n10.1080/01621459.1989.10478752\n10.1016/j.eswa.2017.11.028\n10.1109/ACCESS.2020.3016780\n10.1109/ACCESS.2020.3005510\n10.1109/TPAMI.2013.178\n10.1109/ACCESS.2020.3001973\n10.1109/ACCESS.2017.2672677\n10.1007/s40747-020-00199-4\n10.1016/j.eswa.2019.05.041\n10.1016/j.cmpb.2020.105581\n10.1109/ACCESS.2019.2901568\n10.1109/72.554195\n10.1109/ACCESS.2018.2833888\n10.1016/j.neucom.2016.12.038\n10.1002/mrm.26841\n10.1093/bioinformatics/btq302\n10.1016/j.asoc.2020.106580\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110190\n10.1148/radiol.2020202504\n10.1109/ACCESS.2020.3003810\n10.1038/s42256-021-00307-0\n10.1109/ACCESS.2017.2779794\n10.1007/s42399-020-00655-9\n10.1109/ACCESS.2020.3025010\n10.1109/LSP.2017.2657381\n10.1016/j.bspc.2021.102717\n10.1093/bioinformatics/btl170\n10.1186/s40537-019-0197-0\n10.1007/s12098-020-03263-6\n10.1016/j.ijsu.2020.02.034\n10.1016/j.catena.2016.06.004\n10.1109/ACCESS.2018.2796018\n10.1109/ACCESS.2019.2959033\n10.1109/ACCESS.2020.2994762\n10.1109/ACCESS.2019.2892795\n10.1016/S0140-6736(20)30185-9\n10.1016/j.inffus.2020.11.005\n10.1148/radiol.2020201160\n10.1109/ACCESS.2019.2918221\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30845-X\n10.1109/ACCESS.2019.2930958\n10.1007/s13244-018-0639-9\n10.3390/s16071148\n10.1109/ACCESS.2018.2868813\n10.1109/TNNLS.2017.2673241\n10.1016/j.isprsjprs.2017.07.014\n10.1016/j.cell.2020.04.045\n10.1016/j.patrec.2021.06.021"}
{"title": "Quantitative CT comparison between COVID-19 and mycoplasma pneumonia suspected as COVID-19: a longitudinal study.", "abstract": "The purpose of this study was to compare imaging features between COVID-19 and mycoplasma pneumonia (MP).\nThe data of patients with mild COVID-19 and MP who underwent chest computed tomography (CT) examination from February 1, 2020 to April 17, 2020 were retrospectively analyzed. The Pneumonia-CT-LKM-PP model based on a deep learning algorithm was used to automatically quantify the number, volume, and involved lobes of pulmonary lesions, and longitudinal changes in quantitative parameters were assessed in three CT follow-ups.\nA total of 10 patients with mild COVID-19 and 13 patients with MP were included in this study. There was no difference in lymphocyte counts at baseline between the two groups (1.43\u2009\u00b1\u20090.45 vs. 1.44\u2009\u00b1\u20090.50, p\u2009=\u20090.279). C-reactive protein levels were significantly higher in MP group than in COVID-19 group (p\u2009<\u20090.05). The number, volume, and involved lobes of pulmonary lesions reached a peak in 7-14\u00a0days in the COVID-19 group, but there was no peak or declining trend over time in the MP group (p\u2009<\u20090.05).\nBased on the longitudinal changes of quantitative CT, pulmonary lesions peaked at 7-14\u00a0days in patients with COVID-19, and this may be useful to distinguish COVID-19 from MP and evaluate curative effects and prognosis.", "journal": "BMC medical imaging", "date": "2022-02-08", "authors": ["JunzhongLiu", "YuzhenWang", "GuanghuiHe", "XinhuaWang", "MinfengSun"], "doi": "10.1186/s12880-022-00750-4\n10.1007/s00330-020-06934-2\n10.1259/bjr.20200243\n10.1007/s00330-005-0026-z\n10.1186/1471-2342-9-7\n10.2214/ajr.174.1.1740037\n10.1148/radiol.2381040088\n10.1148/radiol.2020201178\n10.1007/s11547-020-01195-x\n10.1007/s11547-020-01197-9\n10.1148/radiol.2020202708\n10.1016/S0140-6736(20)30183-5\n10.1148/ryct.2020200075\n10.1148/radiol.2020200370\n10.1148/radiol.2020200843\n10.1016/j.ejrad.2020.108972\n10.3348/kjr.2020.0181\n10.2214/AJR.20.22975\n10.1016/j.ejrad.2020.109009\n10.2214/AJR.20.22959"}
{"title": "COVID-19 Detection Based on Lung Ct Scan Using Deep Learning Techniques.", "abstract": "SARS-CoV-2 is a novel virus, responsible for causing the COVID-19 pandemic that has emerged as a pandemic in recent years. Humans are becoming infected with the virus. In 2019, the city of Wuhan reported the first-ever incidence of COVID-19. COVID-19 infected people have symptoms that are related to pneumonia, and the virus affects the body's respiratory organs, making breathing difficult. A real-time reverse transcriptase-polymerase chain reaction (RT-PCR) kit is used to diagnose the disease. Due to a shortage of kits, suspected patients cannot be treated promptly, resulting in disease spread. To develop an alternative, radiologists looked at the changes in radiological imaging, like CT scans, that produce comprehensive pictures of the body of excellent quality. The suspected patient's computed tomography (CT) scan is used to distinguish between a healthy individual and a COVID-19 patient using deep learning algorithms. A lot of deep learning methods have been proposed for COVID-19. The proposed work utilizes CNN architectures like VGG16, DeseNet121, MobileNet, NASNet, Xception, and EfficientNet. The dataset contains 3873 total CT scan images with \"COVID\" and \"Non-COVID.\" The dataset is divided into train, test, and validation. Accuracies obtained for VGG16 are 97.68%, DenseNet121 is 97.53%, MobileNet is 96.38%, NASNet is 89.51%, Xception is 92.47%, and EfficientNet is 80.19%, respectively. From the obtained analysis, the results show that the VGG16 architecture gives better accuracy compared to other architectures.", "journal": "Computational and mathematical methods in medicine", "date": "2022-02-05", "authors": ["S VKogilavani", "JPrabhu", "RSandhiya", "M SandeepKumar", "UmaShankarSubramaniam", "AlagarKarthick", "MMuhibbullah", "Sharmila Banu SheikImam"], "doi": "10.1155/2022/7672196\n10.1007/s12652-020-02641-4\n10.1108/IJPCC-06-2020-0054\n10.1007/s13198-021-01072-4\n10.1016/j.measurement.2020.108432\n10.1016/j.patcog.2020.107747\n10.1016/j.eswa.2021.114883\n10.1016/j.chaos.2020.110170\n10.1016/j.bspc.2021.102750\n10.1016/j.irbm.2021.01.004\n10.1016/j.bbe.2021.05.013\n10.1016/j.bspc.2020.102365\n10.1016/j.compbiomed.2021.104306\n10.1016/j.compbiomed.2020.103795\n10.1155/2021/1896762\n10.1016/j.compbiomed.2020.103792\n10.1016/j.bspc.2021.102920\n10.1016/j.bbe.2021.04.006\n10.1166/jmihi.2019.2654\n10.1166/jmihi.2020.3169\n10.1155/2021/5990999\n10.1155/2021/5582418\n10.1155/2021/5584004\n10.1016/j.comcom.2021.06.011\n10.1155/2021/2921737\n10.1016/j.jbi.2021.103751\n10.1016/j.bbe.2020.08.005\n10.1109/ACCESS.2021.3121791\n10.1155/2021/7894849\n10.1007/s12559-021-09836-7\n10.1007/s11356-021-16398-6"}
{"title": "Semantic segmentation of COVID-19 lesions with a multiscale dilated convolutional network.", "abstract": "Automatic segmentation of infected lesions from computed tomography (CT) of COVID-19 patients is crucial for accurate diagnosis and follow-up assessment. The remaining challenges are the obvious scale difference between different types of COVID-19 lesions and the similarity between the lesions and normal tissues. This work aims to segment lesions of different scales and lesion boundaries correctly by utilizing multiscale and multilevel features. A novel multiscale dilated convolutional network (MSDC-Net) is proposed against the scale difference of lesions and the low contrast between lesions and normal tissues in CT images. In our MSDC-Net, we propose a multiscale feature capture block (MSFCB) to effectively capture multiscale features for better segmentation of lesions at different scales. Furthermore, a multilevel feature aggregate (MLFA) module is proposed to reduce the information loss in the downsampling process. Experiments on the publicly available COVID-19 CT Segmentation dataset demonstrate that the proposed MSDC-Net is superior to other existing methods in segmenting lesion boundaries and large, medium, and small lesions, and achieves the best results in Dice similarity coefficient, sensitivity and mean intersection-over-union (mIoU) scores of 82.4%, 81.1% and 78.2%, respectively. Compared with other methods, the proposed model has an average improvement of 10.6% and 11.8% on Dice and mIoU. Compared with the existing methods, our network achieves more accurate segmentation of lesions at various scales and lesion boundaries, which will facilitate further clinical analysis. In the future, we consider integrating the automatic detection and segmentation of COVID-19, and conduct research on the automatic diagnosis system of COVID-19.", "journal": "Scientific reports", "date": "2022-02-05", "authors": ["JianxiongZhang", "XuefengDing", "DashaHu", "YumingJiang"], "doi": "10.1038/s41598-022-05527-x\n10.1148/radiol.2020200236\n10.1148/radiol.2020200642\n10.1109/TMI.2016.2535865\n10.1007/s10278-021-00434-5\n10.1109/JBHI.2020.2986926\n10.1007/s10462-020-09825-6\n10.1101/2020.02.25.20021568\n10.1016/j.asoc.2020.106897\n10.1016/j.chaos.2020.110170\n10.1155/2021/6653879\n10.1109/TMI.2020.2996645\n10.1109/TMI.2020.3000314\n10.1101/2020.03.12.20027185\n10.1080/00207454.2021.1883602\n10.1155/2021/9995073\n10.1016/j.future.2021.04.019\n10.1007/s12652-020-02843-w\n10.1049/iet-ipr.2020.0088\n10.1109/TPAMI.2016.2644615\n10.1109/JBHI.2018.2818620"}
{"title": "Explainable Machine Learning for COVID-19 Pneumonia Classification With Texture-Based Features Extraction in Chest Radiography.", "abstract": "Both reverse transcription-PCR (RT-PCR) and chest X-rays are used for the diagnosis of the coronavirus disease-2019 (COVID-19). However, COVID-19 pneumonia does not have a defined set of radiological findings. Our work aims to investigate radiomic features and classification models to differentiate chest X-ray images of COVID-19-based pneumonia and other types of lung patterns. The goal is to provide grounds for understanding the distinctive COVID-19 radiographic texture features using supervised ensemble machine learning methods based on trees through the interpretable Shapley Additive Explanations (SHAP) approach. We use 2,611 COVID-19 chest X-ray images and 2,611 non-COVID-19 chest X-rays. After segmenting the lung in three zones and laterally, a histogram normalization is applied, and radiomic features are extracted. SHAP recursive feature elimination with cross-validation is used to select features. Hyperparameter optimization of XGBoost and Random Forest ensemble tree models is applied using random search. The best classification model was XGBoost, with an accuracy of 0.82 and a sensitivity of 0.82. The explainable model showed the importance of the middle left and superior right lung zones in classifying COVID-19 pneumonia from other lung patterns.", "journal": "Frontiers in digital health", "date": "2022-02-04", "authors": ["Lu\u00eds Vin\u00edciusde Moura", "ChristianMattjie", "Caroline MachadoDartora", "Rodrigo CBarros", "Ana MariaMarques da Silva"], "doi": "10.3389/fdgth.2021.662343\n10.3389/fmed.2020.00532\n10.1016/j.ijsu.2020.02.034\n10.2196/19673\n10.3390/v12080854\n10.1001/jama.2020.12839\n10.4103/0970-2113.99248\n10.1002/14651858.CD013639.pub3\n10.3390/ijerph18062842\n10.1038/s41551-020-00633-5\n10.1148/radiol.2020201160\n10.1038/s41746-021-00453-0\n10.1148/ryct.2020200280\n10.1183/09031936.01.00213501\n10.1148/rg.2018170048\n10.1016/j.ejrad.2004.03.010\n10.1148/radiol.2015151169\n10.21037/atm.2019.11.26\n10.29384/rbfm.2019.v13.n3.p38-42\n10.7717/peerj.10086\n10.7717/peerj-cs.306\n10.21203/rs.3.rs-36353/v2\n10.1109/TMI.2021.3066161\n10.1016/j.asoc.2021.107692\n10.3390/app10165683\n10.1109/ACCESS.2021.3064927\n10.1016/j.patcog.2021.108274\n10.1038/s42256-021-00338-7\n10.1016/j.ejmp.2020.10.029\n10.3390/jpm11090842\n10.2214/ajr.174.1.1740071\n10.1158/0008-5472.CAN-17-0339\n10.1023/A:1010933404324\n10.1186/s12864-019-6413-7\n10.1038/s42256-019-0138-9\n10.1080/07391102.2020.1767212\n10.1186/s41747-018-0068-z\n10.1109/RBME.2020.2987975\n10.1016/j.imu.2020.100505\n10.21203/rs.3.rs-37657/v1\n10.1007/s12539-020-00403-6\n10.1016/j.cmpb.2020.105608\n10.7717/peerj.10309\n10.3390/jcm10143100\n10.1007/s11547-021-01402-3\n10.3389/fpubh.2021.663965\n10.1186/s12967-020-02692-3\n10.1016/j.compbiomed.2021.104304\n10.1097/RTI.0000000000000532\n10.1109/TMI.2020.2993291\n10.3348/kjr.2020.0536\n10.3934/mbe.2021004\n10.1080/17455030.2020.1810364\n10.1037/1082-989X.2.3.292\n10.1109/ACCESS.2020.3034032"}
{"title": "Effective deep learning approaches for predicting COVID-19 outcomes from chest computed tomography volumes.", "abstract": "The rapid evolution of the novel coronavirus disease (COVID-19) pandemic has resulted in an urgent need for effective clinical tools to reduce transmission and manage severe illness. Numerous teams are quickly developing artificial intelligence approaches to these problems, including using deep learning to predict COVID-19 diagnosis and prognosis from chest computed tomography (CT) imaging data. In this work, we assess the value of aggregated chest CT data for COVID-19 prognosis compared to clinical metadata alone. We develop a novel patient-level algorithm to aggregate the chest CT volume into a 2D representation that can be easily integrated with clinical metadata to distinguish COVID-19 pneumonia from chest CT volumes from healthy participants and participants with other viral pneumonia. Furthermore, we present a multitask model for joint segmentation of different classes of pulmonary lesions present in COVID-19 infected lungs that can outperform individual segmentation models for each task. We directly compare this multitask segmentation approach to combining feature-agnostic volumetric CT classification feature maps with clinical metadata for predicting mortality. We show that the combination of features derived from the chest CT volumes improve the AUC performance to 0.80 from the 0.52 obtained by using patients' clinical data alone. These approaches enable the automated extraction of clinically relevant features from chest CT volumes for risk stratification of COVID-19 patients.", "journal": "Scientific reports", "date": "2022-02-04", "authors": ["AnthonyOrtiz", "AnusuaTrivedi", "JocelynDesbiens", "MarianBlazes", "CalebRobinson", "SunilGupta", "RahulDodhia", "Pavan KBhatraju", "W ConradLiles", "AaronLee", "Juan M LavistaFerres"], "doi": "10.1038/s41598-022-05532-0\n10.1016/S1473-3099(20)30134-1\n10.1016/S1473-3099(20)30086-4\n10.3389/fbioe.2020.00898\n10.1016/j.cell.2020.04.045\n10.1148/ryct.2020200034\n10.1371/journal.pone.0230548\n10.1148/ryai.2020200048\n10.1016/j.acha.2012.07.005\n10.1016/j.mri.2012.06.010"}
{"title": "COVID-19 detection from chest x-ray using MobileNet and residual separable convolution block.", "abstract": "A newly emerged coronavirus disease affects the social and economical life of the world. This virus mainly infects the respiratory system and spreads with airborne communication. Several countries witness the serious consequences of the COVID-19 pandemic. Early detection of COVID-19 infection is the critical step to survive a patient from death. The chest radiography examination is the fast and cost-effective way for COVID-19 detection. Several researchers have been motivated to automate COVID-19 detection and diagnosis process using chest x-ray images. However, existing models employ deep networks and are suffering from high training time. This work presents transfer learning and residual separable convolution block for COVID-19 detection. The proposed model utilizes pre-trained MobileNet for binary image classification. The proposed residual separable convolution block has improved the performance of basic MobileNet. Two publicly available datasets COVID5K, and COVIDRD have considered for the evaluation of the proposed model. Our proposed model exhibits superior performance than existing state-of-art and pre-trained models with 99% accuracy on both datasets. We have achieved similar performance on noisy datasets. Moreover, the proposed model outperforms existing pre-trained models with less training time and competitive performance than basic MobileNet. Further, our model is suitable for mobile applications as it uses fewer parameters and lesser training time.", "journal": "Soft computing", "date": "2022-02-03", "authors": ["V Santhosh KumarTangudu", "JagadeeshKakarla", "Isunuri BalaVenkateswarlu"], "doi": "10.1007/s00500-021-06579-3\n10.1007/s10489-020-01829-7\n10.1016/j.patrec.2020.09.010\n10.1109/JBHI.2020.2982103\n10.1109/TETCI.2018.2866254\n10.1016/j.ijmedinf.2020.104284\n10.1109/ACCESS.2020.3016780\n10.1109/TNSRE.2018.2834554\n10.1016/j.jinf.2020.03.007\n10.1016/j.media.2020.101794\n10.1109/JBHI.2020.2991043\n10.1186/s12890-020-01286-5\n10.1109/ACCESS.2020.3025010"}
{"title": "Disease-Course Adapting Machine Learning Prognostication Models in Elderly Patients Critically Ill With COVID-19: Multicenter Cohort Study With External Validation.", "abstract": "The COVID-19 pandemic caused by SARS-CoV-2 is challenging health care systems globally. The disease disproportionately affects the elderly population, both in terms of disease severity and mortality risk.\nThe aim of this study was to evaluate machine learning-based prognostication models for critically ill elderly COVID-19 patients, which dynamically incorporated multifaceted clinical information on evolution of the disease.\nThis multicenter cohort study (COVIP study) obtained patient data from 151 intensive care units (ICUs) from 26 countries. Different models based on the Sequential Organ Failure Assessment (SOFA) score, logistic regression (LR), random forest (RF), and extreme gradient boosting (XGB) were derived as baseline models that included admission variables only. We subsequently included clinical events and time-to-event as additional variables to derive the final models using the same algorithms and compared their performance with that of the baseline group. Furthermore, we derived baseline and final models on a European patient cohort, which were externally validated on a non-European cohort that included Asian, African, and US patients.\nIn total, 1432 elderly (\u226570 years old) COVID-19-positive patients admitted to an ICU were included for analysis. Of these, 809 (56.49%) patients survived up to 30 days after admission. The average length of stay was 21.6 (SD 18.2) days. Final models that incorporated clinical events and time-to-event information provided superior performance (area under the receiver operating characteristic curve of 0.81; 95% CI 0.804-0.811), with respect to both the baseline models that used admission variables only and conventional ICU prediction models (SOFA score, P<.001). The average precision increased from 0.65 (95% CI 0.650-0.655) to 0.77 (95% CI 0.759-0.770).\nIntegrating important clinical events and time-to-event information led to a superior accuracy of 30-day mortality prediction compared with models based on the admission information and conventional ICU prediction models. This study shows that machine-learning models provide additional information and may support complex decision-making in critically ill elderly COVID-19 patients.\nClinicalTrials.gov NCT04321265; https://clinicaltrials.gov/ct2/show/NCT04321265.", "journal": "JMIR medical informatics", "date": "2022-02-01", "authors": ["ChristianJung", "BehroozMamandipoor", "JesperFj\u00f8lner", "Raphael RomanoBruno", "BernhardWernly", "AntonioArtigas", "BernardoBollen Pinto", "Joerg CSchefold", "GeorgWolff", "MalteKelm", "MichaelBeil", "SigalSviri", "Peter Vvan Heerden", "WojciechSzczeklik", "MiroslawCzuczwar", "MuhammedElhadi", "MichaelJoannidis", "SandraOeyen", "TilemachosZafeiridis", "BrianMarsh", "Finn HAndersen", "RuiMoreno", "MaurizioCecconi", "SusannahLeaver", "Dylan WDe Lange", "BertrandGuidet", "HansFlaatten", "VenetOsmani"], "doi": "10.2196/32949\n10.1007/s00134-021-06409-y\n10.3390/jcm9072106\n10.1016/j.eclinm.2020.100502\n10.1136/bmjopen-2020-044921\n10.12688/f1000research.26186.1\n10.1007/s00134-017-4940-8\n10.1371/journal.pone.0236618\n10.1371/journal.pone.0236618\n10.3238/arztebl.2020.0668\n10.1016/j.ijmedinf.2020.104312\n10.1007/s00134-018-5475-3\n10.1186/s13613-021-00911-8\n10.1016/S2589-7500(20)30226-0\n10.1007/s40520-020-01631-y\n10.1016/j.jacc.2020.11.030\n10.2196/21801\n10.2196/24246\n10.2196/25884\n10.2196/24207\n10.2196/26211\n10.1038/s41746-021-00456-x\n10.2196/23811\n10.2196/23128\n10.2196/24225\n10.3390/jcm9051514\n10.1016/j.chest.2020.12.009\n10.1186/s13054-021-03551-3\n10.1186/s13054-021-03551-3\n10.1136/bmj.m3919\n10.1007/s00134-019-05853-1\n10.1145/2939672.2939785\n10.1109/icdar.1995.598994\n10.1136/bmj.m1328\n10.1371/journal.pone.0118432\n10.1371/journal.pone.0118432\n10.1007/s00134-018-5339-x\n10.1001/jamaoncol.2015.3336\n10.1186/s40635-019-0286-6\n10.1186/s13054-021-03739-7\n10.1186/s13054-021-03739-7\n10.3233/CH-219202\n10.1093/ehjcvp/pvaa083\n10.1183/13993003.00979-2021\n10.1007/s00134-021-06365-7"}
{"title": "COVID-19 CT image recognition algorithm based on transformer and CNN.", "abstract": "Novel corona virus pneumonia (COVID-19) broke out in 2019, which had a great impact on the development of world economy and people's lives. As a new mainstream image processing method, deep learning network has been constructed to extract medical features from chest CT images, and has been used as a new detection method in clinical practice. However, due to the medical characteristics of COVID-19 CT images, the lesions are widely distributed and have many local features. Therefore, it is difficult to diagnose directly by using the existing deep learning model. According to the medical features of CT images in COVID-19, a parallel bi-branch model (Trans-CNN Net) based on Transformer module and Convolutional Neural Network module is proposed by making full use of the local feature extraction capability of Convolutional Neural Network and the global feature extraction advantage of Transformer. According to the principle of cross-fusion, a bi-directional feature fusion structure is designed, in which features extracted from two branches are fused bi-directionally, and the parallel structures of branches are fused by a feature fusion module, forming a model that can extract features of different scales. To verify the effect of network classification, the classification accuracy on COVIDx-CT dataset is 96.7%, which is obviously higher than that of typical CNN network (ResNet-152) (95.2%) and Transformer network (Deit-B) (75.8%). These results demonstrate accuracy is improved. This model also provides a new method for the diagnosis of COVID-19, and through the combination of deep learning and medical imaging, it promotes the development of real-time diagnosis of lung diseases caused by COVID-19 infection, which is helpful for reliable and rapid diagnosis, thus saving precious lives.", "journal": "Displays", "date": "2022-02-01", "authors": ["XiaoleFan", "XiufangFeng", "YunyunDong", "HuichaoHou"], "doi": "10.1016/j.displa.2022.102150\n10.1016/j.tmaid.2020.101623"}
{"title": "Dynamic 3D radiomics analysis using artificial intelligence to assess the stage of COVID-19 on CT images.", "abstract": "To develop a dynamic 3D radiomics analysis method using artificial intelligence technique for automatically assessing four disease stages (i.e., early, progressive, peak, and absorption stages) of COVID-19 patients on CT images.\nThe dynamic 3D radiomics analysis method was composed of three AI algorithms (the lung segmentation, lesion segmentation, and stage-assessing AI algorithms) that were trained and tested on 313,767 CT images from 520 COVID-19 patients. This proposed method used 3D lung lesion that was segmented by the lung and lesion segmentation algorithms to extract radiomics features, and then combined with clinical metadata to assess the possible stage of COVID-19 patients using stage-assessing algorithm. Area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity were used to evaluate diagnostic performance.\nOf 520 patients, 66 patients (mean age, 57\u00a0years\u2009\u00b1\u200915 [standard deviation]; 35 women), including 203 CT scans, were tested. The dynamic 3D radiomics analysis method used 30 features, including 27 radiomics features and 3 clinical features to assess the possible disease stage of COVID-19 with an accuracy of 90%. For the prediction of each stage, the AUC of stage 1 was 0.965 (95% CI: 0.934, 0.997), AUC of stage 2 was 0.958 (95% CI: 0.931, 0.984), AUC of stage 3 was 0.998 (95% CI: 0.994, 1.000), and AUC of stage 4 was 0.975 (95% CI: 0.956, 0.994).\nWith high diagnostic performance, the dynamic 3D radiomics analysis using artificial intelligence could represent a potential tool for helping hospitals make appropriate resource allocations and follow-up of treatment response.\n\u2022 The AI segmentation algorithms were able to accurately segment the lung and lesion of COVID-19 patients of different stages. \u2022 The dynamic 3D radiomics analysis method successfully extracted the radiomics features from the 3D lung lesion. \u2022 The stage-assessing AI algorithm combining with clinical metadata was able to assess the four stages with an accuracy of 90%, a macro-average AUC of 0.975.", "journal": "European radiology", "date": "2022-01-31", "authors": ["ShengpingCai", "YangChen", "ShixuanZhao", "DehuaiHe", "YongjieLi", "NianXiong", "ZhidanLi", "ShaopingHu"], "doi": "10.1007/s00330-021-08533-1\n10.1148/radiol.2020200432\n10.3346/jkms.2020.35.e61\n10.1007/s11432-020-2849-3\n10.1148/radiol.2020200370\n10.1038/ncomms5006\n10.1111/eva.12524\n10.1109/TPAMI.2010.215\n10.1002/jmv.25884\n10.2214/AJR.20.22961\n10.18632/aging.103372\n10.1186/s12931-019-1261-1\n10.1016/j.kint.2020.03.005\n10.1148/radiol.2020190553\n10.1172/JCI142004\n10.1016/j.metabol.2020.154262\n10.1161/HYPERTENSIONAHA.120.15324\n10.1148/ryct.2020200322\n10.7326/M18-1377"}
{"title": "An international cross-sectional investigation on social media, fitspiration content exposure, and related risks during the COVID-19 self-isolation period.", "abstract": "With the global COVID-19 pandemic, governments from many countries in the world implemented various restrictions to prevent the SARS-Cov-2 virus's spread, including social distancing measures, quarantine, in-home lockdown, and the closure of services and public spaces. This led to an in-creased use of social media platforms to make people feel more connected, but also to maintain physical activity while self-isolating. Concerns about physical appearance and the desire to keep or reach a muscular and toned ideal body, might have further reinforced the engagement in fitness-related social media activities, like sharing progresses in training achievements or following more fitness contents on popular profiles. To better understand the underlying relation among these factors, the present study investigates 729 responses to the Exercise Addiction Inventory (EAI), the Appearance Anxiety Inventory (AAI), the Self-Compassion Scale (SCS) and their association to social media usage and compares the results cross-culturally in five countries (Spain, Lithuania, United Kingdom, Japan, and Hungary). Findings highlight significant differences between males and females, espe-cially in regard to the time spent online (U\u00a0=\u00a0477.5, p\u00a0=\u00a00.036). Greater levels of appearance anxiety were associated with the exposure to fitness-related contents on social media. These results strongly confirm the previously highlighted association between fitspiration media and body image anxiety predominantly in females. Clinical implications and future considerations in terms of prevention and treatment in a situation of global emergency are also discussed.", "journal": "Journal of psychiatric research", "date": "2022-01-31", "authors": ["IlariaCataldo", "JuliusBurkauskas", "Artemisa RDores", "Irene PCarvalho", "PierluigiSimonato", "IlariaDe Luca", "Maria \u00c1ngelesG\u00f3mez-Mart\u00ednez", "Alejandra RebecaMelero Ventola", "ZsoltDemetrovics", "AttilaSzabo", "Krisztina Edina\u00c1bel", "MamiShibata", "KeiKobayashi", "HironobuFujiwara", "Eva MariaArroyo-Anll\u00f3", "GiovanniMartinotti", "FernandoBarbosa", "IngaGriskova-Bulanova", "AistePranckeviciene", "HenriettaBowden-Jones", "GianlucaEsposito", "OrnellaCorazza"], "doi": "10.1016/j.jpsychires.2022.01.032"}
{"title": "Advanced Imaging Supports the Mechanistic Role of Autoimmunity and Plaque Rupture in COVID-19 Heart Involvement.", "abstract": "The cardiovascular system is frequently affected by coronavirus disease-19 (COVID-19), particularly in hospitalized cases, and these manifestations are associated with a worse prognosis. Most commonly, heart involvement is represented by myocarditis, myocardial infarction, and pulmonary embolism, while arrhythmias, heart valve damage, and pericarditis are less frequent. While the clinical suspicion is necessary for a prompt disease recognition, imaging allows the early detection of cardiovascular complications in patients with COVID-19. The combination of cardiothoracic approaches has been proposed for advanced imaging techniques, i.e., CT scan and MRI, for a simultaneous evaluation of cardiovascular structures, pulmonary arteries, and lung parenchyma. Several mechanisms have been proposed to explain the cardiovascular injury, and among these, it is established that the host immune system is responsible for the aberrant response characterizing severe COVID-19 and inducing organ-specific injury. We illustrate novel evidence to support the hypothesis that molecular mimicry may be the immunological mechanism for myocarditis in COVID-19. The present article provides a comprehensive review of the available evidence of the immune mechanisms of the COVID-19 cardiovascular injury and the imaging tools to be used in the diagnostic workup. As some of these techniques cannot be implemented for general screening of all cases, we critically discuss the need to maximize the sustainability and the specificity of the proposed tests while illustrating the findings of some paradigmatic cases.", "journal": "Clinical reviews in allergy & immunology", "date": "2022-01-29", "authors": ["Maria ElenaLaino", "AngelaAmmirabile", "FrancescaMotta", "MariaDe Santis", "VictorSavevski", "MarcoFrancone", "ArturoChiti", "LorenzoMannelli", "CarloSelmi", "LorenzoMonti"], "doi": "10.1007/s12016-022-08925-1\n10.1016/j.clinimag.2020.11.022\n10.1001/jamacardio.2020.0950\n10.1001/jamacardio.2020.1017\n10.1038/s41569-020-0413-9\n10.1016/j.ajem.2020.04.048\n10.1136/heartjnl-2020-317322\n10.1016/S2213-2600(20)30076-X\n10.1038/s41569-020-0360-5\n10.1093/eurheartj/ehaa231\n10.1161/CIRCULATIONAHA.120.046941\n10.1016/j.autrev.2021.102791\n10.1016/j.lfs.2020.117900\n10.1161/CIRCRESAHA.120.317055\n10.1016/j.cyto.2005.01.009\n10.1161/CIRCRESAHA.116.302317\n10.3390/cells9112508\n10.1016/j.lfs.2020.118482\n10.1016/s1071-9164(96)80047-9\n10.1007/978-3-319-57613-8_8\n10.1016/j.jaut.2020.102592\n10.1016/S0140-6736(20)31103-X\n10.1016/j.cell.2020.09.016\n10.1016/j.cell.2020.09.034\n10.1161/CIRCULATIONAHA.120.048360\n10.1016/S2213-2600(21)00085-0\n10.1016/j.cell.2020.02.052\n10.1016/j.hrthm.2020.05.001\n10.1186/1741-7015-2-19\n10.1093/eurheartj/ehaa664\n10.1016/j.ijid.2020.10.012\n10.1002/ehf2.12958\n10.1007/s00414-020-02500-z\n10.1016/j.jacc.2020.11.031\n10.1001/jamacardio.2020.3557\n10.1016/j.jcmg.2020.05.004\n10.1038/nri3345\n10.1155/2020/8829674\n10.1001/jamacardio.2020.7308\n10.1177/0961203317731532\n10.1002/art.41425\n10.1002/ccd.29114\n10.1016/j.jacc.2018.08.1043\n10.1002/ehf2.13186\n10.1016/j.trsl.2020.04.007\n10.1002/eji.202048930\n10.1016/j.autrev.2020.102556\n10.1016/j.autrev.2020.102591\n10.1126/science.aav3487\n10.1016/j.yjmcc.2017.04.003\n10.1016/j.clim.2009.04.008\n10.1148/rg.2020200195\n10.1016/j.jacc.2020.06.068\n10.1016/j.jacc.2018.08.1038\n10.1016/j.clinimag.2020.11.03\n10.1016/j.tcm.2020.10.001\n10.1093/ehjci/jeaa335\n10.1186/s12968-021-00764-x\n10.1002/ejhf.1828\n10.1186/s12968-020-00656-6\n10.1148/rg.2020200159\n10.1016/j.jcmg.2020.04.012\n10.1093/cvr/cvaa193\n10.1093/ehjci/jeaa202\n10.1038/s41598-017-09536-z\n10.1016/j.jacc.2020.06.080\n10.1007/s11547-020-01279-8\n10.1056/NEJMcibr2113694\n10.1128/JVI.65.1.16-22.1991\n10.1016/j.jccase.2021.04.014\n10.1016/j.jcmg.2020.06.013\n10.1056/NEJMra1808137\n10.1093/eurheartj/ehaa1103\n10.1016/j.jacc.2019.12.012\n10.1161/CIRCULATIONAHA.120.047525\n10.1016/j.jcct.2020.08.013\n10.1016/j.jcct.2020.11.004\n10.1016/j.jcmg.2020.06.001\n10.1136/heartjnl-2013-304680\n10.1007/s15010-020-01424-5\n10.5144/0256-4947.2016.78\n10.1001/jamacardio.2020.1096\n10.4081/idr.2020.8609\n10.1016/j.cjco.2020.05.005\n10.1186/s12968-020-00628-w\n10.1016/j.jacc.2009.02.007\n10.1016/j.jacc.2018.09.072\n10.1007/s10554-020-02097-9\n10.1001/jama.2015.14849\n10.1016/j.jcmg.2020.08.012\n10.1161/CIRCULATIONAHA.120.049252\n10.1007/s10554-017-1063-9\n10.1136/jim-2020-001592\n10.1148/rg.2020200149\n10.1148/radiol.2020203557\n10.1016/j.thromres.2020.05.049\n10.1371/journal.pone.0238413\n10.1055/s-0040-1712097\n10.1016/j.cpcardiol.2020.100692\n10.1055/s-0037-1614014\n10.1016/j.thromres.2020.06.010\n10.1016/j.rmed.2020.106135\n10.1016/j.crad.2020.07.019\n10.1016/j.amjmed.2015.01.023\n10.1016/j.idcr.2020.e00805\n10.3390/medicina56120670\n10.1111/jth.14859\n10.1016/j.jvsv.2020.04.009\n10.1007/s11239-020-02190-9\n10.1007/s10140-020-01859-1\n10.3389/fmed.2020.00557\n10.1097/CCM.0000000000004548\n10.1148/radiol.2020201629\n10.1007/s00330-020-06865-y\n10.1016/j.thromres.2020.07.025\n10.1016/j.crad.2020.07.002\n10.1371/journal.pone.0242475\n10.1067/j.cpradiol.2020.09.014\n10.1007/s00259-020-04837-4\n10.1007/s00259-020-04851-6"}
{"title": "Detecting COVID-19 Pneumonia over Fuzzy Image Enhancement on Computed Tomography Images.", "abstract": "COVID-19 is the worst pandemic that has hit the globe in recent history, causing an increase in deaths. As a result of this pandemic, a number of research interests emerged in several fields such as medicine, health informatics, medical imaging, artificial intelligence and social sciences. Lung infection or pneumonia is the regular complication of COVID-19, and Reverse Transcription Polymerase Chain Reaction (RT-PCR) and computed tomography (CT) have played important roles to diagnose the disease. This research proposes an image enhancement method employing fuzzy expected value to improve the quality of the image for the detection of COVID-19 pneumonia. The principal objective of this research is to detect COVID-19 in patients using CT scan images collected from different sources, which include patients suffering from pneumonia and healthy people. The method is based on fuzzy histogram equalization and is organized with the improvement of the image contrast using fuzzy normalized histogram of the image. The effectiveness of the algorithm has been justified over several experiments on different features of CT images of lung for COVID-19 patients, like Ground-Glass Opacity (GGO), crazy paving, and consolidation. Experimental investigations indicate that among the 254 patients, 81.89% had features on both lungs; 9.5% on the left lung; and 10.24% on the right lung. The predominantly affected lobe was the right lower lobe (79.53%).", "journal": "Computational and mathematical methods in medicine", "date": "2022-01-29", "authors": ["AliAlzahrani", "Md Al-AminBhuiyan", "FahimaAkhter"], "doi": "10.1155/2022/1043299\n10.1186/s40169-020-00271-z\n10.1186/s41256-020-00135-6\n10.1016/j.ijantimicag.2020.106054\n10.1155/2021/5554408\n10.1007/s00330-020-07347-x\n10.1016/j.diii.2020.03.014\n10.1148/radiol.2020200463\n10.1186/s12967-020-02324-w\n10.1016/j.crad.2020.03.004\n10.1016/j.patrec.2019.11.013\n10.1007/s10489-020-01714-3\n10.1016/j.jbi.2018.08.006\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2992546\n10.1109/ACCESS.2020.2994762\n10.1016/j.patcog.2020.107700\n10.1007/978-3-030-00889-5_1\n10.1016/j.patcog.2021.107828\n10.1016/j.patcog.2021.107826\n10.1007/s11042-020-09894-3\n10.1088/1757-899X/993/1/012046\n10.3390/make2040027\n10.1016/j.compbiomed.2021.104319\n10.1109/JIOT.2021.3050775\n10.1016/B978-0-12-824473-9.00001-X\n10.1007/s00500-020-05275-y\n10.1016/j.asoc.2014.09.004\n10.1016/j.bbe.2016.01.001\n10.1016/j.cmpb.2017.06.021\n10.2147/OAEM.S29942\n10.1186/s43055-020-00355-3\n10.4103/1817-1737.53349\n10.1049/el:19800267"}
{"title": "Feasibility study of multi-site split learning for privacy-preserving medical systems under data imbalance constraints in COVID-19, X-ray, and cholesterol dataset.", "abstract": "It seems as though progressively more people are in the race to upload content, data, and information online; and hospitals haven't neglected this trend either. Hospitals are now at the forefront for multi-site medical data sharing to provide ground-breaking advancements in the way health records are shared and patients are diagnosed. Sharing of medical data is essential in modern medical research. Yet, as with all data sharing technology, the challenge is to balance improved treatment with protecting patient's personal information. This paper provides a novel split learning algorithm coined the term, \"multi-site split learning\", which enables a secure transfer of medical data between multiple hospitals without fear of exposing personal data contained in patient records. It also explores the effects of varying the number of end-systems and the ratio of data-imbalance on the deep learning performance. A guideline for the most optimal configuration of split learning that ensures privacy of patient data whilst achieving performance is empirically given. We argue the benefits of our multi-site split learning algorithm, especially regarding the privacy preserving factor, using CT scans of COVID-19 patients, X-ray bone scans, and cholesterol level medical data.", "journal": "Scientific reports", "date": "2022-01-29", "authors": ["Yoo JeongHa", "GusangLee", "MinjaeYoo", "SoyiJung", "SeehwanYoo", "JoongheonKim"], "doi": "10.1038/s41598-022-05615-y\n10.1016/j.knosys.2020.106647\n10.1080/07391102.2021.1875049\n10.1016/j.eswa.2020.114054\n10.3233/XST-200784\n10.1016/j.compbiomed.2021.104319\n10.1002/jemt.23713\n10.1109/JIOT.2021.3055804\n10.1109/ACCESS.2021.3108455\n10.2471/BLT.17.204891\n10.1016/j.asoc.2020.106885\n10.1038/s41597-021-00900-3\n10.1016/j.bspc.2021.102588\n10.1016/j.cca.2012.09.010"}
{"title": "A fuzzy-enhanced deep learning approach for early detection of Covid-19 pneumonia from portable chest X-ray images.", "abstract": "The Covid-19 pandemic is the defining global health crisis of our time. Chest X-Rays (CXR) have been an important imaging modality for assisting in the diagnosis and management of hospitalised Covid-19 patients. However, their interpretation is time intensive for radiologists. Accurate computer aided systems can facilitate early diagnosis of Covid-19 and effective triaging. In this paper, we propose a fuzzy logic based deep learning (DL) approach to differentiate between CXR images of patients with Covid-19 pneumonia and with interstitial pneumonias not related to Covid-19. The developed model here, referred to as ", "journal": "Neurocomputing", "date": "2022-01-27", "authors": ["CosimoIeracitano", "NadiaMammone", "MarioVersaci", "GiuseppeVarone", "Abder-RahmanAli", "AntonioArmentano", "GraziaCalabrese", "AnnaFerrarelli", "LorenaTurano", "CarmelaTebala", "ZainHussain", "ZakariyaSheikh", "AzizSheikh", "GiuseppeSceni", "AmirHussain", "Francesco CarloMorabito"], "doi": "10.1016/j.neucom.2022.01.055"}
{"title": "COVID-19 diagnosis using state-of-the-art CNN architecture features and Bayesian Optimization.", "abstract": "The coronavirus outbreak 2019, called COVID-19, which originated in Wuhan, negatively affected the lives of millions of people and many people died from this infection. To prevent the spread of the disease, which is still in effect, various restriction decisions have been taken all over the world. In addition, the number of COVID-19 tests has been increased to quarantine infected people. However, due to the problems encountered in the supply of RT-PCR tests and the ease of obtaining Computed Tomography and X-ray images, imaging-based methods have become very popular in the diagnosis of COVID-19. Therefore, studies using these images to classify COVID-19 have increased. This paper presents a classification method for computed tomography chest images in the COVID-19 Radiography Database using features extracted by popular Convolutional Neural Networks (CNN) models (AlexNet, ResNet18, ResNet50, Inceptionv3, Densenet201, Inceptionresnetv2, MobileNetv2, GoogleNet). The determination of hyperparameters of Machine Learning (ML) algorithms by Bayesian optimization, and ANN-based image segmentation are the two main contributions in this study. First of all, lung segmentation is performed automatically from the raw image with Artificial Neural Networks (ANNs). To ensure data diversity, data augmentation is applied to the COVID-19 classes, which are fewer than the other two classes. Then these images are applied as input to five different CNN models. The features extracted from each CNN model are given as input to four different ML algorithms, namely Support Vector Machine (SVM), k-Nearest Neighbors (k-NN), Naive Bayes (NB), and Decision Tree (DT) for classification. To achieve the most successful classification accuracy, the hyperparameters of each ML algorithm are determined using Bayesian optimization. With the classification made using these hyperparameters, the highest success is obtained as 96.29% with the DenseNet201 model and SVM algorithm. The Sensitivity, Precision, Specificity, MCC, and F1-Score metric values for this structure are 0.9642, 0.9642, 0.9812, 0.9641 and 0.9453, respectively. These results showed that ML methods with the most optimum hyperparameters can produce successful results.", "journal": "Computers in biology and medicine", "date": "2022-01-26", "authors": ["Muhammet FatihAslan", "KadirSabanci", "AkifDurdu", "Muhammed FahriUnlersen"], "doi": "10.1016/j.compbiomed.2022.105244\n10.1016/j.clinimag.2020.04.001\n10.1109/RBME.2020.2987975\n10.1101/2020.05.01.20088211\n10.1016/j.cell.2020.04.045"}
{"title": "AI Lung Segmentation and Perfusion Analysis of Dual-Energy CT Can Help to Distinguish COVID-19 Infiltrates from Visually Similar Immunotherapy-Related Pneumonitis Findings and Can Optimize Radiological Workflows.", "abstract": "(1) To explore the potential impact of an AI dual-energy CT (DECT) prototype on decision making and workflows by investigating its capabilities to differentiate COVID-19 from immunotherapy-related pneumonitis. (2) Methods: From 3 April 2020 to 12 February 2021, DECT from biometrically matching patients with COVID-19, pneumonitis, and inconspicuous findings were selected from our clinical routine. Three blinded readers independently scored each pulmonary lobe analogous to CO-RADS. Inter-rater agreement was determined with an intraclass correlation coefficient (ICC). Averaged perfusion metrics per lobe (iodine uptake in mg, volume without vessels in ml, iodine concentration in mg/mL) were extracted using manual segmentation and an AI DECT prototype. A generalized linear mixed model was used to investigate metric validity and potential distinctions at equal CO-RADS scores. Multinomial regression measured the contribution \"Reader\", \"CO-RADS score\", and \"perfusion metrics\" to diagnosis. The time to diagnosis was measured for manual vs. AI segmentation. (3) Results: We included 105 patients (62 \u00b1 13 years, mean BMI 27 \u00b1 2). There were no significant differences between manually and AI-extracted perfusion metrics (", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2022-01-26", "authors": ["Andreas SBrendlin", "MarkusMader", "SebastianFaby", "BernhardSchmidt", "Ahmed EOthman", "SebastianGassenmaier", "KonstantinNikolaou", "SaifAfat"], "doi": "10.3390/tomography8010003\n10.1016/j.ijsu.2020.03.036\n10.2214/AJR.20.22954\n10.1097/RLI.0000000000000674\n10.1148/radiol.2020200463\n10.1148/radiol.2020201473\n10.1186/s13244-020-00933-z\n10.1148/rg.2019190036\n10.1136/jitc-2020-000952\n10.1056/NEJMp2015897\n10.1016/j.asoc.2020.106897\n10.3348/kjr.2019.0821\n10.1016/j.jacr.2019.05.036\n10.2214/AJR.12.9116\n10.3390/diagnostics10110870\n10.1002/oby.22859\n10.1007/s00330-019-06607-9\n10.3758/BRM.41.4.1149\n10.1186/s12874-018-0550-6\n10.1111/j.2517-6161.1980.tb01109.x\n10.1148/radiol.2020200823\n10.1148/radiol.2020201629\n10.21037/qims-20-708\n10.1183/13993003.02608-2020\n10.1161/CIRCRESAHA.121.318902\n10.1016/S1473-3099(20)30367-4\n10.1097/RTI.0000000000000498"}
{"title": "Efficient and visualizable convolutional neural networks for COVID-19 classification using Chest CT.", "abstract": "With coronavirus disease 2019 (COVID-19) cases rising rapidly, deep learning has emerged as a promising diagnosis technique. However, identifying the most accurate models to characterize COVID-19 patients is challenging because comparing results obtained with different types of data and acquisition processes is non-trivial. In this paper we designed, evaluated, and compared the performance of 20 convolutional neutral networks in classifying patients as COVID-19 positive, healthy, or suffering from other pulmonary lung infections based on chest computed tomography (CT) scans, serving as the first to consider the EfficientNet family for COVID-19 diagnosis and employ intermediate activation maps for visualizing model performance. All models are trained and evaluated in Python using 4173 chest CT images from the dataset entitled \"A COVID multiclass dataset of CT scans,\" with 2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or suffering from other pulmonary infections, respectively. EfficientNet-B5 was identified as the best model with an F1 score of 0.9769\u00a0\u00b1\u00a00.0046, accuracy of 0.9759\u00a0\u00b1\u00a00.0048, sensitivity of 0.9788\u00a0\u00b1\u00a00.0055, specificity of 0.9730\u00a0\u00b1\u00a00.0057, and precision of 0.9751\u00a0\u00b1\u00a00.0051. On an alternate 2-class dataset, EfficientNetB5 obtained an accuracy of 0.9845\u00a0\u00b1\u00a00.0109, F1 score of 0.9599\u00a0\u00b1\u00a00.0251, sensitivity of 0.9682\u00a0\u00b1\u00a00.0099, specificity of 0.9883\u00a0\u00b1\u00a00.0150, and precision of 0.9526\u00a0\u00b1\u00a00.0523. Intermediate activation maps and Gradient-weighted Class Activation Mappings offered human-interpretable evidence of the model's perception of ground-class opacities and consolidations, hinting towards a promising use-case of artificial intelligence-assisted radiology tools. With a prediction speed of under 0.1\u00a0s on GPUs and 0.5\u00a0s on CPUs, our proposed model offers a rapid, scalable, and accurate diagnostic for COVID-19.", "journal": "Expert systems with applications", "date": "2022-01-26", "authors": ["AkshGarg", "SanaSalehi", "Marianna LaRocca", "RachaelGarner", "DominiqueDuncan"], "doi": "10.1016/j.eswa.2022.116540\n10.1093/COMJNL/BXAB051\n10.1016/j.compbiomed.2020.103795\n10.1007/s10489-020-01714-3\n10.1016/j.compbiomed.2021.104454\n10.1109/CVPR.2017.195\n10.1007/s00521-021-05910-1\n10.1109/CVPR.2016.90\n10.1007/S12652-021-03282-X\n10.1109/ACCESS.2021.3058537\n10.1038/s41467-020-18685-1\n10.1109/ICCCIS51004.2021.9397189\n10.1109/SAMI50585.2021.9378646\n10.1148/radiol.2020201343\n10.1016/j.chaos.2020.110059\n10.1155/2021/5528441\n10.1016/j.asoc.2020.106691\n10.1109/INISTA49547.2020.9194651\n10.1109/ACCESS.2021.3083516\n10.1016/j.compbiomed.2020.103792\n10.1007/s11263-019-01228-7\n10.1007/S11042-021-11158-7\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.308\n10.1016/j.matpr.2020.06.245\n10.1016/j.asoc.2020.106897\n10.1016/j.ejrad.2020.109041\n10.1109/TIP.8310.1109/TIP.2021.3058783\n10.1148/radiol.2020201491\n10.1007/s00330-020-06801-0"}
{"title": "SAM: Self-augmentation mechanism for COVID-19 detection using chest X-ray images.", "abstract": "COVID-19 is a rapidly spreading viral disease and has affected over 100 countries worldwide. The numbers of casualties and cases of infection have escalated particularly in countries with weakened healthcare systems. Recently, reverse transcription-polymerase chain reaction (RT-PCR) is the test of choice for diagnosing COVID-19. However, current evidence suggests that COVID-19 infected patients are mostly stimulated from a lung infection after coming in contact with this virus. Therefore, chest X-ray (i.e., radiography) and chest CT can be a surrogate in some countries where PCR is not readily available. This has forced the scientific community to detect COVID-19 infection from X-ray images and recently proposed machine learning methods offer great promise for fast and accurate detection. Deep learning with convolutional neural networks (CNNs) has been successfully applied to radiological imaging for improving the accuracy of diagnosis. However, the performance remains limited due to the lack of representative X-ray images available in public benchmark datasets. To alleviate this issue, we propose a self-augmentation mechanism for data augmentation in the feature space rather than in the data space using reconstruction independent component analysis (RICA). Specifically, a unified architecture is proposed which contains a deep convolutional neural network (CNN), a feature augmentation mechanism, and a bidirectional LSTM (BiLSTM). The CNN provides the high-level features extracted at the pooling layer where the augmentation mechanism chooses the most relevant features and generates low-dimensional augmented features. Finally, BiLSTM is used to classify the processed sequential information. We conducted experiments on three publicly available databases to show that the proposed approach achieves the state-of-the-art results with accuracy of 97%, 84% and 98%. Explainability analysis has been carried out using feature visualization through PCA projection and t-SNE plots.", "journal": "Knowledge-based systems", "date": "2022-01-25", "authors": ["UsmanMuhammad", "Md ZiaulHoque", "MouradOussalah", "AnjaKeskinarkaus", "TapioSepp\u00e4nen", "PinakiSarder"], "doi": "10.1016/j.knosys.2022.108207"}
{"title": "Contour-enhanced attention CNN for CT-based COVID-19 segmentation.", "abstract": "Accurate detection of COVID-19 is one of the challenging research topics in today's healthcare sector to control the coronavirus pandemic. Automatic data-powered insights for COVID-19 localization from medical imaging modality like chest CT scan tremendously augment clinical care assistance. In this research, a Contour-aware Attention Decoder CNN has been proposed to precisely segment COVID-19 infected tissues in a very effective way. It introduces a novel attention scheme to extract boundary, shape cues from CT contours and leverage these features in refining the infected areas. For every decoded pixel, the attention module harvests contextual information in its spatial neighborhood from the contour feature maps. As a result of incorporating such rich structural details into decoding via dense attention, the CNN is able to capture even intricate morphological details. The decoder is also augmented with a Cross Context Attention Fusion Upsampling to robustly reconstruct deep semantic features back to high-resolution segmentation map. It employs a novel pixel-precise attention model that draws relevant encoder features to aid in effective upsampling. The proposed CNN was evaluated on 3D scans from MosMedData and Jun Ma benchmarked datasets. It achieved state-of-the-art performance with a high dice similarity coefficient of 85.43% and a recall of 88.10%.", "journal": "Pattern recognition", "date": "2022-01-25", "authors": ["RKarthik", "RMenaka", "HariharanM", "DaehanWon"], "doi": "10.1016/j.patcog.2022.108538\n10.1109/JBHI.2020.2986926\n10.1109/TPAMI.2020.3007032"}
{"title": "COVID-19 detection in CT and CXR images using deep learning models.", "abstract": "Infectious diseases pose a threat to human life and could affect the whole world in a very short time. Corona-2019 virus disease (COVID-19) is an example of such harmful diseases. COVID-19 is a pandemic of an emerging infectious disease, called coronavirus disease 2019 or COVID-19, caused by the coronavirus SARS-CoV-2, which first appeared in December 2019 in Wuhan, China, before spreading around the world on a very large scale. The continued rise in the number of positive COVID-19 cases has disrupted the health care system in many countries, creating a lot of stress for governing bodies around the world, hence the need for a rapid way to identify cases of this disease. Medical imaging is a widely accepted technique for early detection and diagnosis of the disease which includes different techniques such as Chest X-ray (CXR), Computed Tomography (CT) scan, etc. In this paper, we propose a methodology to investigate the potential of deep transfer learning in building a classifier to detect COVID-19 positive patients using CT scan and CXR images. Data augmentation technique is used to increase the size of the training dataset in order to solve overfitting and enhance generalization ability of the model. Our contribution consists of a comprehensive evaluation of a series of pre-trained deep neural networks: ResNet50, InceptionV3, VGGNet-19, and Xception, using data augmentation technique. The findings proved that deep learning is effective at detecting COVID-19 cases. From the results of the experiments it was found that by considering each modality separately, the VGGNet-19 model outperforms the other three models proposed by using the CT image dataset where it achieved \u00a088.5%\u00a0precision,\u00a086% recall, 86.5%\u00a0F1-score, and 87% accuracy\u00a0while the refined Xception version gave the highest precision, recall, F1-score, and\u00a0accuracy values which equal 98% using CXR images dataset. On the other hand, and by applying the average of the two modalities X-ray and CT, VGG-19 presents the best score which is 90.5% for the accuracy and the\u00a0F1-score, 90.3% for the recall while the precision\u00a0is 91.5%. These results enables to automatize the process of analyzing chest CT scans and X-ray images with high accuracy and can be used in cases where RT-PCR testing and materials are limited.", "journal": "Biogerontology", "date": "2022-01-23", "authors": ["InesChouat", "AmiraEchtioui", "RafikKhemakhem", "WassimZouch", "MohamedGhorbel", "Ahmed BenHamida"], "doi": "10.1007/s10522-021-09946-7\n10.1148/radiol.2020200642\n10.1007/s13246-020-00865-4\n10.21203/rs.3.rs-32247/v1\n10.1016/S0140-6736(20)30211-7\n10.1007/s11042-021-10783-6\n10.1007/s11042-021-11192-5\n10.1007/S00521-020-05437-X\n10.1101/2020.11.08.20227819\n10.1007/s00330-021-07715-1\n10.1016/j.eng.2020.04.010\n10.1001/jama.2020.2648"}
{"title": "Using a Deep Learning Model to Explore the Impact of Clinical Data on COVID-19 Diagnosis Using Chest X-ray.", "abstract": "The coronavirus pandemic (COVID-19) is disrupting the entire world; its rapid global spread threatens to affect millions of people. Accurate and timely diagnosis of COVID-19 is essential to control the spread and alleviate risk. Due to the promising results achieved by integrating machine learning (ML), particularly deep learning (DL), in automating the multiple disease diagnosis process. In the current study, a model based on deep learning was proposed for the automated diagnosis of COVID-19 using chest X-ray images (CXR) and clinical data of the patient. The aim of this study is to investigate the effects of integrating clinical patient data with the CXR for automated COVID-19 diagnosis. The proposed model used data collected from King Fahad University Hospital, Dammam, KSA, which consists of 270 patient records. The experiments were carried out first with clinical data, second with the CXR, and finally with clinical data and CXR. The fusion technique was used to combine the clinical features and features extracted from images. The study found that integrating clinical data with the CXR improves diagnostic accuracy. Using the clinical data and the CXR, the model achieved an accuracy of 0.970, a recall of 0.986, a precision of 0.978, and an F-score of 0.982. Further validation was performed by comparing the performance of the proposed system with the diagnosis of an expert. Additionally, the results have shown that the proposed system can be used as a tool that can help the doctors in COVID-19 diagnosis.", "journal": "Sensors (Basel, Switzerland)", "date": "2022-01-23", "authors": ["Irfan UllahKhan", "NidaAslam", "TalhaAnwar", "Hind SAlsaif", "Sara Mhd BacharChrouf", "Norah AAlzahrani", "Fatimah AhmedAlamoudi", "Mariam Moataz AlyKamaleldin", "Khaled BassamAwary"], "doi": "10.3390/s22020669\n10.1016/j.cca.2020.03.009\n10.7326/M20-1495\n10.1016/j.chaos.2020.110338\n10.1093/bib/bbw068\n10.1016/j.ibmed.2020.100013\n10.1155/2021/5587188\n10.1109/ACCESS.2021.3097559\n10.3390/ijerph18126429\n10.3390/jcm9092990\n10.1017/dmp.2020.346\n10.1007/s00330-020-07269-8\n10.1148/radiol.2020203511\n10.3390/info11090419\n10.1177/2472630320958376\n10.1148/radiol.2020202944\n10.9781/ijimai.2021.04.001\n10.1016/j.media.2020.101794\n10.3390/app10165683\n10.21227/w3aw-rv39\n10.1016/j.media.2020.101797\n10.1016/j.mlwa.2021.100138\n10.1007/s13755-021-00166-4\n10.1371/journal.pone.0257884\n10.1007/s00330-021-08050-1\n10.1038/s41598-020-74539-2"}
{"title": "The association of obesity with the progression and outcome of COVID-19: The insight from an artificial-intelligence-based imaging quantitative analysis on computed tomography.", "abstract": "To explore the association of obesity with the progression and outcome of coronavirus disease 2019 (COVID-19) at the acute period and 5-month follow-up from the perspectives of computed tomography (CT) imaging with artificial intelligence (AI)-based quantitative evaluation, which may help to predict the risk of obese COVID-19 patients progressing to severe and critical disease.\nThis retrospective cohort enrolled 213 hospitalized COVID-19 patients. Patients were classified into three groups according to their body mass index (BMI): normal weight (from 18.5 to <24\u00a0kg/m\nCompared with normal-weight patients, patients with higher BMI were associated with more lung involvements in lung CT examination (lung lesions volume [cm\nObesity was associated with severe pneumonia lesions on CT and adverse clinical outcomes. The AI-based model with combinational use of clinical and CT parameters had incremental prognostic value over the clinical parameters alone.", "journal": "Diabetes/metabolism research and reviews", "date": "2022-01-22", "authors": ["XiaotingLu", "ZhenhaiCui", "XiangMa", "FengPan", "LingliLi", "JiazhengWang", "PengSun", "HuiqingLi", "LianYang", "BoLiang"], "doi": "10.1002/dmrr.3519"}
{"title": "Automated detection of COVID-19 through convolutional neural network using chest x-ray images.", "abstract": "The COVID-19 epidemic has a catastrophic impact on global well-being and public health. More than 27 million confirmed cases have been reported worldwide until now. Due to the growing number of confirmed cases, and challenges to the variations of the COVID-19, timely and accurate classification of healthy and infected patients is essential to control and treat COVID-19. We aim to develop a deep learning-based system for the persuasive classification and reliable detection of COVID-19 using chest radiography. Firstly, we evaluate the performance of various state-of-the-art convolutional neural networks (CNNs) proposed over recent years for medical image classification. Secondly, we develop and train CNN from scratch. In both cases, we use a public X-Ray dataset for training and validation purposes. For transfer learning, we obtain 100% accuracy for binary classification (i.e., Normal/COVID-19) and 87.50% accuracy for tertiary classification (Normal/COVID-19/Pneumonia). With the CNN trained from scratch, we achieve 93.75% accuracy for tertiary classification. In the case of transfer learning, the classification accuracy drops with the increased number of classes. The results are demonstrated by comprehensive receiver operating characteristics (ROC) and confusion metric analysis with 10-fold cross-validation.", "journal": "PloS one", "date": "2022-01-22", "authors": ["RubinaSarki", "KhandakarAhmed", "HuaWang", "YanchunZhang", "KateWang"], "doi": "10.1371/journal.pone.0262052\n10.1001/jama.2020.3786\n10.1148/radiol.2020200642\n10.1007/s13755-021-00152-w\n10.1007/s13755-021-00158-4\n10.1056/NEJMoa2002032\n10.1183/09031936.01.00213501\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1007/s13755-020-00129-1\n10.1038/nature14539\n10.1007/s13755-020-00125-5\n10.1007/s13755-018-0046-0\n10.1007/s13755-019-0084-2\n10.1101/763136\n10.1109/ACCESS.2020.3015258\n10.1016/j.eng.2020.04.010\n10.1007/s10489-020-02051-1\n10.1148/ryct.2020200034\n10.1016/j.media.2020.101794\n10.1016/j.patrec.2020.09.010\n10.1016/j.mehy.2020.109761\n10.1016/j.chaos.2020.110495\n10.1016/j.eswa.2020.114054\n10.1016/j.cell.2018.02.010\n10.1016/j.media.2017.07.005\n10.1016/j.procs.2016.07.014\n10.1049/el:20083469\n10.1016/j.ejrnm.2015.01.004\n10.1016/j.ipm.2009.03.002\n10.1007/s13246-020-00865-4\n10.1136/bjo.80.11.940"}
{"title": "Maladaptive changes in delay discounting in males during the COVID-19 pandemic: the predictive role of functional connectome.", "abstract": "The Coronavirus disease of 2019 (COVID-19) and measures to curb it created population-level changes in male-dominant impulsive and risky behaviors such as violent crimes and gambling. One possible explanation for this is that the pandemic has been stressful, and males, more so than females, tend to respond to stress by altering their focus on immediate versus delayed rewards, as reflected in their delay discounting rates. Delay discounting rates from healthy undergraduate students were collected twice during the pandemic. Discounting rates of males (n=190) but not of females (n=493) increased during the pandemic. Using machine learning, we show that prepandemic functional connectome predict increased discounting rates in males (n=88). Moreover, considering that delay discounting is associated with multiple psychiatric disorders, we found the same neural pattern that predicted increased discounting rates in this study, in secondary datasets of patients with major depression and schizophrenia. The findings point to sex-based differences in maladaptive delay discounting under real-world stress events, and to connectome-based neuromarkers of such effects. They can explain why there was a population-level increase in several impulsive and risky behaviors during the pandemic and point to intriguing questions about the shared underlying mechanisms of stress responses, psychiatric disorders and delay discounting.", "journal": "Cerebral cortex (New York, N.Y. : 1991)", "date": "2022-01-22", "authors": ["ZhibingXiao", "ZhiyiChen", "WantingChen", "WeiGao", "LiHe", "QiangWang", "XuLei", "JiangQiu", "TingyongFeng", "HongChen", "OfirTurel", "AntoineBechara", "QinghuaHe"], "doi": "10.1093/cercor/bhab505\n10.1002/smi.3060\n10.1017/S0033291720003116\n10.18112/openneuro.ds001461.v1.0.3\n10.1093/cercor/bhab333"}
{"title": "COVID-19 detection using chest X-ray images based on a developed deep neural network.", "abstract": "Currently, a new coronavirus called COVID-19 is the biggest challenge of the human at 21st century. Now, the spread of this virus is such that mortality has risen strongly in all cities of countries. Therefore, it is necessary to think of a solution to handle the disease by fast and timely diagnosis. This paper proposes a method that uses chest X-ray imagery to divide 2-4 classes into 7 different Scenarios, including Bacterial, Viral, Healthy, and COVID-19 classes. The aim of this study is to propose a method that uses chest X-ray imagery to divide 2-4 classes into 7 different Scenarios, including Bacterial, Viral, Healthy, and COVID-19 classes.\n6 different databases from chest X-ray imagery that have been widely used in recent studies have been gathered for this aim. A Convolutional Neural Network-Long Short Time Memory model is designed and developed to extract features from raw data hierarchically. In order to make more realistic assumptions and use the Proposed Method in the practical field, white Gaussian noise is added to the raw chest X-ray imagery. Additionally, the proposed network is tested and investigated not only on 6 expressed databases but also on two additional databases.\nOn the test set, the proposed network achieved an accuracy of more than 90% for all Scenarios excluding Scenario V, i.e. Healthy against the COVID-19 against the Viral, and also achieved 99% accuracy for separating the COVID-19 from the Healthy group. The results showed that the proposed network is robust to noise up to 1 dB. It is worth noting that the proposed network for two additional databases, which were only used as test databases, also achieved more than 90% accuracy. In addition, in comparison to the state-of-the-art pneumonia detection approaches, the final results obtained from the proposed network is so promising.\nThe proposed network is effective in detecting COVID-19 and other lung infectious diseases using chest X-ray imagery and can thus assist radiologists in making rapid and accurate detections.", "journal": "SLAS technology", "date": "2022-01-22", "authors": ["ZohrehMousavi", "NahalShahini", "SobhanSheykhivand", "SinaMojtahedi", "AfroozArshadi"], "doi": "10.1016/j.slast.2021.10.011"}
{"title": "Deep Learning-Based Four-Region Lung Segmentation in Chest Radiography for COVID-19 Diagnosis.", "abstract": "Imaging plays an important role in assessing the severity of COVID-19 pneumonia. Recent COVID-19 research indicates that the disease progress propagates from the bottom of the lungs to the top. However, chest radiography (CXR) cannot directly provide a quantitative metric of radiographic opacities, and existing AI-assisted CXR analysis methods do not quantify the regional severity. In this paper, to assist the regional analysis, we developed a fully automated framework using deep learning-based four-region segmentation and detection models to assist the quantification of COVID-19 pneumonia. Specifically, a segmentation model is first applied to separate left and right lungs, and then a detection network of the carina and left hilum is used to separate upper and lower lungs. To improve the segmentation performance, an ensemble strategy with five models is exploited. We evaluated the clinical relevance of the proposed method compared with the radiographic assessment of the quality of lung edema (RALE) annotated by physicians. Mean intensities of segmented four regions indicate a positive correlation to the regional extent and density scores of pulmonary opacities based on the RALE. Therefore, the proposed method can accurately assist the quantification of regional pulmonary opacities of COVID-19 pneumonia patients.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-01-22", "authors": ["Young-GonKim", "KyungsangKim", "DufanWu", "HuiRen", "Won YoungTak", "Soo YoungPark", "Yu RimLee", "Min KyuKang", "Jung GilPark", "Byung SeokKim", "Woo JinChung", "Mannudeep KKalra", "QuanzhengLi"], "doi": "10.3390/diagnostics12010101\n10.5694/mja2.50674\n10.1016/S0140-6736(20)30183-5\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020200642\n10.2214/AJR.20.22975\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1007/s40846-020-00529-4\n10.2214/AJR.19.21512\n10.3389/fphys.2021.672823\n10.1016/j.cmpb.2019.06.005\n10.1136/thoraxjnl-2017-211280\n10.1148/radiol.2020200843\n10.1148/radiol.2020201754\n10.1007/s11547-020-01200-3\n10.1007/978-3-319-24574-4_28\n10.3390/info11020125\n10.1148/rg.2016150115\n10.2214/ajr.174.1.1740071"}
{"title": "COVID-Net CXR-S: Deep Convolutional Neural Network for Severity Assessment of COVID-19 Cases from Chest X-ray Images.", "abstract": "The world is still struggling in controlling and containing the spread of the COVID-19 pandemic caused by the SARS-CoV-2 virus. The medical conditions associated with SARS-CoV-2 infections have resulted in a surge in the number of patients at clinics and hospitals, leading to a significantly increased strain on healthcare resources. As such, an important part of managing and handling patients with SARS-CoV-2 infections within the clinical workflow is severity assessment, which is often conducted with the use of chest X-ray (CXR) images. In this work, we introduce COVID-Net CXR-S, a convolutional neural network for predicting the airspace severity of a SARS-CoV-2 positive patient based on a CXR image of the patient's chest. More specifically, we leveraged transfer learning to transfer representational knowledge gained from over 16,000 CXR images from a multinational cohort of over 15,000 SARS-CoV-2 positive and negative patient cases into a custom network architecture for severity assessment. Experimental results using the RSNA RICORD dataset showed that the proposed COVID-Net CXR-S has potential to be a powerful tool for computer-aided severity assessment of CXR images of COVID-19 positive patients. Furthermore, radiologist validation on select cases by two board-certified radiologists with over 10 and 19 years of experience, respectively, showed consistency between radiologist interpretation and critical factors leveraged by COVID-Net CXR-S for severity assessment. While not a production-ready solution, the ultimate goal for the open source release of COVID-Net CXR-S is to act as a catalyst for clinical scientists, machine learning researchers, as well as citizen scientists to develop innovative new clinical decision support solutions for helping clinicians around the world manage the continuing pandemic.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-01-22", "authors": ["HosseinAboutalebi", "MayaPavlova", "Mohammad JavadShafiee", "AliSabri", "AmerAlaref", "AlexanderWong"], "doi": "10.3390/diagnostics12010025\n10.3389/fpubh.2020.00241\n10.1186/s13054-020-03021-2\n10.1136/thoraxjnl-2020-215518\n10.1038/s41598-020-76550-z\n10.1007/s10489-020-01902-1\n10.3389/fmed.2020.608525\n10.1101/2020.04.13.20063941\n10.1016/j.cell.2020.04.045\n10.7759/cureus.9448\n10.1148/radiol.2021203957"}
{"title": "Sequential Organ Failure Assessment Outperforms Quantitative Chest CT Imaging Parameters for Mortality Prediction in COVID-19 ARDS.", "abstract": "(1) Background: Respiratory insufficiency with acute respiratory distress syndrome (ARDS) and multi-organ dysfunction leads to high mortality in COVID-19 patients. In times of limited intensive care unit (ICU) resources, chest CTs became an important tool for the assessment of lung involvement and for patient triage despite uncertainties about the predictive diagnostic value. This study evaluated chest CT-based imaging parameters for their potential to predict in-hospital mortality compared to clinical scores. (2) Methods: 89 COVID-19 ICU ARDS patients requiring mechanical ventilation or continuous positive airway pressure mask ventilation were included in this single center retrospective study. AI-based lung injury assessment and measurements indicating pulmonary hypertension (PA-to-AA ratio) on admission CT, oxygenation indices, lung compliance and sequential organ failure assessment (SOFA) scores on ICU admission were assessed for their diagnostic performance to predict in-hospital mortality. (3) Results: CT severity scores and PA-to-AA ratios were not significantly associated with in-hospital mortality, whereas the SOFA score showed a significant association (", "journal": "Diagnostics (Basel, Switzerland)", "date": "2022-01-22", "authors": ["DanielPuhr-Westerheide", "JakobReich", "Bastian OSabel", "Wolfgang GKunz", "Matthias PFabritius", "PaulReidler", "JohannesR\u00fcbenthaler", "MichaelIngrisch", "DietmarWassilowsky", "MichaelIrlbeck", "JensRicke", "EvaGresser"], "doi": "10.3390/diagnostics12010010\n10.1016/j.ijsu.2020.04.018\n10.1001/jama.2021.5469\n10.1016/S1473-3099(21)00167-5\n10.1111/all.14657\n10.1016/S0140-6736(20)30566-3\n10.1016/j.metabol.2020.154378\n10.1001/jamainternmed.2020.0994\n10.1016/S2213-2600(20)30316-7\n10.1371/journal.pone.0235653\n10.1186/s13054-020-02957-9\n10.1093/cid/ciaa576\n10.1001/jamainternmed.2020.3539\n10.1016/j.chest.2020.10.014\n10.1111/anae.15201\n10.1016/j.jiph.2020.12.026\n10.1080/17476348.2020.1804365\n10.1016/j.chest.2020.04.003\n10.1038/s41467-020-18786-x\n10.1007/s00330-020-07013-2\n10.1259/bjro.20200016\n10.2214/AJR.20.22976\n10.7150/thno.45985\n10.1097/RLI.0000000000000672\n10.3390/diagnostics10121108\n10.3390/diagnostics11061029\n10.1016/j.cpcardiol.2020.100618\n10.3389/fphys.2021.593223\n10.1259/bjr.71.850.10211060\n10.1186/1471-2342-11-7\n10.1148/radiol.2020201874\n10.1001/jama.2020.1585\n10.1001/jama.2021.1545\n10.1016/j.chest.2020.08.577\n10.1007/s00134-020-06294-x\n10.1186/s13054-019-2316-x"}
{"title": "Prognostic findings for ICU admission in patients with COVID-19 pneumonia: baseline and follow-up chest CT and the added value of artificial intelligence.", "abstract": "Infection with SARS-CoV-2 has dominated discussion and caused global healthcare and economic crisis over the past 18\u00a0months. Coronavirus disease 19 (COVID-19) causes mild-to-moderate symptoms in most individuals. However, rapid deterioration to severe disease with or without acute respiratory distress syndrome (ARDS) can occur within 1-2\u00a0weeks from the onset of symptoms in a proportion of patients. Early identification by risk stratifying such patients who are at risk of severe complications of COVID-19 is of great clinical importance. Computed tomography (CT) is widely available and offers the potential for fast triage, robust, rapid, and minimally invasive diagnosis: Ground glass opacities (GGO), crazy-paving pattern (GGO with superimposed septal thickening), and consolidation are the most common chest CT findings in COVID pneumonia. There is growing interest in the prognostic value of baseline chest CT since an early risk stratification of patients with COVID-19 would allow for better resource allocation and could help improve outcomes. Recent studies have demonstrated the utility of baseline chest CT to predict intensive care unit (ICU) admission in patients with COVID-19. Furthermore, developments and progress integrating artificial intelligence (AI) with computer-aided design (CAD) software for diagnostic imaging allow for objective, unbiased, and rapid assessment of CT images.", "journal": "Emergency radiology", "date": "2022-01-21", "authors": ["Maria ElenaLaino", "AngelaAmmirabile", "LudovicaLofino", "Dara JosephLundon", "ArturoChiti", "MarcoFrancone", "VictorSavevski"], "doi": "10.1007/s10140-021-02008-y\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2002032\n10.1590/0100-3984.2019.0049\n10.1016/j.crad.2020.03.003\n10.1016/j.jacc.2020.04.031\n10.1016/j.chest.2020.06.025\n10.1002/jmv.25910\n10.1148/radiol.2020200527\n10.1007/s00330-020-06801-0\n10.1148/ryct.2020200034\n10.1016/j.ejro.2020.100237\n10.1016/j.ejrad.2020.109209\n10.1007/s00330-020-06969-5\n10.7150/thno.46569\n10.1038/s41598-020-79183-4\n10.1007/s00330-020-07033-y\n10.5114/pjr.2020.98009\n10.1016/j.ejrad.2020.109256\n10.1259/bjro.20200016\n10.1148/ryct.2020200130\n10.4081/jphr.2021.2270\n10.5152/dir.2020.20421\n10.1038/s41598-020-78965-0\n10.1097/RCT.0000000000001073\n10.3906/sag-2009-49\n10.3389/fpubh.2020.567672\n10.3346/jkms.2020.35.e316\n10.1155/2021/9941570\n10.5152/dir.2020.20346\n10.1093/cid/ciaa415\n10.1016/j.metabol.2020.154244\n10.1016/j.orcp.2020.07.005\n10.1016/j.orcp.2020.12.002\n10.4235/agmr.19.0001\n10.1038/s41366-021-00907-1\n10.1016/j.metabol.2020.154436\n10.1186/s12933-021-01327-1\n10.1016/j.ejrad.2020.109274\n10.1371/journal.pone.0253433\n10.1016/j.bone.2020.115790\n10.1016/j.jocd.2021.07.007\n10.17305/bjbms.2020.5466\n10.1016/j.cmi.2020.07.030\n10.1007/s11739-021-02795-9\n10.1016/j.chest.2020.11.026\n10.3390/diagnostics10110929\n10.3390/diagnostics11091616\n10.1016/j.rmed.2020.106271\n10.1007/s10140-021-01903-8\n10.1016/j.cegh.2020.11.006\n10.1007/s00330-020-07623-w\n10.21037/atm.2020.03.132\n10.1038/s41467-020-17971-2\n10.1007/s00330-020-07044-9\n10.1148/radiol.2020202439\n10.1007/s00330-020-07032-z\n10.1148/radiol.2020200905\n10.1002/mco2.14\n10.1148/radiol.2020201491\n10.3389/fbioe.2020.00898\n10.1186/s12880-020-00521-z\n10.1148/ryct.2020200075\n10.1007/s00330-020-07013-2\n10.1109/RBME.2020.2990959\n10.3390/diagnostics11081317\n10.1007/s11548-020-02299-5\n10.3390/ijerph18062842\n10.3348/kjr.2020.0994\n10.7150/thno.45985\n10.2196/24973\n10.2196/21604\n10.5152/dir.2020.20281\n10.1016/j.acra.2020.09.004\n10.3389/fpubh.2021.648360\n10.1016/j.accpm.2020.10.014\n10.1148/radiol.2020201433\n10.5152/dir.2020.20407\n10.7150/thno.46428\n10.1101/2020.11.04.20225797\n10.1016/j.media.2020.101844\n10.1109/JBHI.2020.3036722\n10.1038/s41598-021-95114-3\n10.1371/journal.pone.0248957\n10.1155/2020/5328267\n10.1148/radiology.210.1.r99ja2629\n10.1378/chest.107.4.1062\n10.1148/radiol.2021203153\n10.3389/fmed.2021.636298\n10.1007/s10140-020-01869-z\n10.1186/s12931-021-01625-y"}
{"title": "Objective evaluation of deep uncertainty predictions for COVID-19 detection.", "abstract": "Deep neural networks (DNNs) have been widely applied for detecting COVID-19 in medical images. Existing studies mainly apply transfer learning and other data representation strategies to generate accurate point estimates. The generalization power of these networks is always questionable due to being developed using small datasets and failing to report their predictive confidence. Quantifying uncertainties associated with DNN predictions is a prerequisite for their trusted deployment in medical settings. Here we apply and evaluate three uncertainty quantification techniques for COVID-19 detection using chest X-Ray (CXR) images. The novel concept of uncertainty confusion matrix is proposed and new performance metrics for the objective evaluation of uncertainty estimates are introduced. Through comprehensive experiments, it is shown that networks pertained on CXR images outperform networks pretrained on natural image datasets such as ImageNet. Qualitatively and quantitatively evaluations also reveal that the predictive uncertainty estimates are statistically higher for erroneous predictions than correct predictions. Accordingly, uncertainty quantification methods are capable of flagging risky predictions with high uncertainty estimates. We also observe that ensemble methods more reliably capture uncertainties during the inference. DNN-based solutions for COVID-19 detection have been mainly proposed without any principled mechanism for risk mitigation. Previous studies have mainly focused on on generating single-valued predictions using pretrained DNNs. In this paper, we comprehensively apply and comparatively evaluate three uncertainty quantification techniques for COVID-19 detection using chest X-Ray images. The novel concept of uncertainty confusion matrix is proposed and new performance metrics for the objective evaluation of uncertainty estimates are introduced for the first time. Using these new uncertainty performance metrics, we quantitatively demonstrate when we could trust DNN predictions for COVID-19 detection from chest X-rays. It is important to note the proposed novel uncertainty evaluation metrics are generic and could be applied for evaluation of probabilistic forecasts in all classification problems.", "journal": "Scientific reports", "date": "2022-01-19", "authors": ["HamzehAsgharnezhad", "AfsharShamsi", "RoohallahAlizadehsani", "AbbasKhosravi", "SaeidNahavandi", "Zahra AlizadehSani", "DiptiSrinivasan", "Sheikh Mohammed SharifulIslam"], "doi": "10.1038/s41598-022-05052-x\n10.1038/nature21056\n10.1038/s41591-018-0316-z\n10.26599/BDMA.2020.9020012\n10.26599/TST.2021.9010026\n10.26599/BDMA.2020.9020013\n10.1016/j.cnsns.2020.105372\n10.26599/TST.2019.9010007\n10.1148/ryai.2019180041\n10.1148/radiol.2019191293\n10.1093/jamia/ocv080"}
{"title": "Hybrid PSO-SVM algorithm for Covid-19 screening and quantification.", "abstract": "Corona Virus Disease (COVID) 19 has shaken the earth at its root and the devastation has increased the diagnostic burden of radiologists by large. At this crucial juncture, Artificial Intelligence (AI) will go a long way in decreasing the workload of physicians working in the outbreak zone, aiding them to accurately diagnose the new disease. In this work, a hybrid Particle Swarm Optimization-Support Vector Machine based AI algorithm is deployed to analyze the Computed Tomography images automatically providing a high probability in determining the presence of pneumonia due to COVID19. This paper presents a model for training the system to segregate and classify the presence of pneumonia which will in turn save around 50% of the time frame for physicians. This will be especially useful in places of outbreaks where a team of people are working together with the aid of artificial intelligence and/or medical background. The AI incorporated system was distributed in all areas of across the globe. It has been observed that challenges such as data security, testing time effectiveness of model, data discrepancy etc. were positively handled using the deployed system. Moreover, since the AI integrated system identifies the infected patients immediately physicians can confirm the infection and segregate the patients at the right period. A total of 200 training cases have been observed of which 150 were identified to be infected. The proposed work shows specificity of 0.85, a sensitivity of 0.956 and an accuracy of 95.78%.", "journal": "International journal of information technology : an official journal of Bharati Vidyapeeth's Institute of Computer Applications and Management", "date": "2022-01-18", "authors": ["M SahayaSheela", "C AArun"], "doi": "10.1007/s41870-021-00856-y\n10.1016/S2589-7500(20)30222-3\n10.1038/s41467-020-20657-4\n10.1007/s11042-020-10010-8\n10.3390/make2040027\n10.1109/MNET.011.2000458\n10.1016/j.asoc.2020.106897\n10.1016/j.compbiomed.2021.104306\n10.1148/ryct.2020200075\n10.1007/s10140-020-01886-y\n10.1186/s13640-017-0213-2\n10.1111/jgh.15053\n10.32604/cmc.2020.010691\n10.1145/3381014\n10.1038/s41598-019-56847-4\n10.1136/bmj.m1328\n10.1007/s00330-021-07715-1\n10.1155/2021/1296755\n10.1007/s13246-020-00865-4\n10.1007/s41870-021-00671-5\n10.1007/s10489-020-02002-w"}
{"title": "CXR-RefineDet: Single-Shot Refinement Neural Network for Chest X-Ray Radiograph Based on Multiple Lesions Detection.", "abstract": "The workload of radiologists has dramatically increased in the context of the COVID-19 pandemic, causing misdiagnosis and missed diagnosis of diseases. The use of artificial intelligence technology can assist doctors in locating and identifying lesions in medical images. In order to improve the accuracy of disease diagnosis in medical imaging, we propose a lung disease detection neural network that is superior to the current mainstream object detection model in this paper. By combining the advantages of RepVGG block and Resblock in information fusion and information extraction, we design a backbone RRNet with few parameters and strong feature extraction capabilities. After that, we propose a structure called Information Reuse, which can solve the problem of low utilization of the original network output features by connecting the normalized features back to the network. Combining the network of RRNet and the improved RefineDet, we propose the overall network which was called CXR-RefineDet. Through a large number of experiments on the largest public lung chest radiograph detection dataset VinDr-CXR, it is found that the detection accuracy and inference speed of CXR-RefineDet have reached 0.1686\u2009mAP and 6.8\u2009fps, respectively, which is better than the two-stage object detection algorithm using a strong backbone like ResNet-50 and ResNet-101. In addition, the fast reasoning speed of CXR-RefineDet also provides the possibility for the actual implementation of the computer-aided diagnosis system.", "journal": "Journal of healthcare engineering", "date": "2022-01-18", "authors": ["CongLin", "YongbinZheng", "XiuchunXiao", "JialunLin"], "doi": "10.1155/2022/4182191\n10.1007/s00521-020-04933-4\n10.1016/j.comcom.2021.05.020\n10.1109/tmi.2020.2980117\n10.1007/s00521-020-05518-x\n10.1109/cvpr.2014.81\n10.1016/j.future.2020.07.045\n10.1109/tmi.2016.2536809\n10.1109/access.2021.3053408\n10.1007/s10916-020-1541-9\n10.1109/TNNLS.2018.2876865\n10.1109/CVPR46437.2021.01546\n10.1109/iccv.2015.169\n10.1016/j.artmed.2019.101744\n10.3389/fpubh.2021.671070\n10.1016/j.compmedimag.2021.101889\n10.1109/cvpr.2017.106\n10.1109/iccv.2017.324\n10.1109/cvpr.2016.90\n10.1109/cvpr.2018.00442\n10.1109/cvpr.2017.243\n10.1109/CVPR.2017.634\n10.1109/CVPR.2019.00060\n10.1109/cvpr46437.2021.01352\n10.1109/cvpr.2009.5206848\n10.1109/cvpr.2019.00237\n10.1109/TFUZZ.2021.3058020\n10.1007/s11263-014-0733-5\n10.1109/cvpr42600.2020.00978"}
{"title": "Robust, Primitive, and Unsupervised Quality Estimation for Segmentation Ensembles.", "abstract": "A multitude of image-based machine learning segmentation and classification algorithms has recently been proposed, offering diagnostic decision support for the identification and characterization of glioma, Covid-19 and many other diseases. Even though these algorithms often outperform human experts in segmentation tasks, their limited reliability, and in particular the inability to detect failure cases, has hindered translation into clinical practice. To address this major shortcoming, we propose an unsupervised quality estimation method for segmentation ensembles. Our primitive solution examines discord in binary segmentation maps to automatically flag segmentation results that are particularly error-prone and therefore require special assessment by human readers. We validate our method both on segmentation of brain glioma in multi-modal magnetic resonance - and of lung lesions in computer tomography images. Additionally, our method provides an adaptive prioritization mechanism to maximize efficacy in use of human expert time by enabling radiologists to focus on the most difficult, yet important cases while maintaining full diagnostic autonomy. Our method offers an intuitive and reliable uncertainty estimation from segmentation ensembles and thereby closes an important gap toward successful translation of automatic segmentation into clinical routine.", "journal": "Frontiers in neuroscience", "date": "2022-01-18", "authors": ["FlorianKofler", "IvanEzhov", "LucasFidon", "Carolin MPirkl", "Johannes CPaetzold", "EgonBurian", "SarthakPati", "MalekEl Husseini", "FernandoNavarro", "SuprosannaShit", "JanKirschke", "SpyridonBakas", "ClausZimmer", "BenediktWiestler", "Bjoern HMenze"], "doi": "10.3389/fnins.2021.752780\n10.1038/sdata.2017.117\n10.1007/978-3-030-87735-4_12\n10.1007/s10278-013-9622-7\n10.1038/s41592-018-0261-2\n10.1007/978-3-030-11726-9_25\n10.1038/sdata.2018.158\n10.1007/978-3-030-11726-9_21\n10.1038/s41592-020-01008-z\n10.5281/zenodo.3632567\n10.1016/j.cmpb.2020.105796\n10.1016/S1470-2045(19)30098-1\n10.3389/fnins.2020.00125\n10.1016/j.adro.2020.03.003\n10.1109/TMI.2010.2057442\n10.1007/978-3-030-11726-9_40\n10.1007/978-3-030-46640-4_36\n10.1109/TMI.2020.3006437\n10.1109/TMI.2014.2377694\n10.5281/zenodo.4323059\n10.1016/j.cmpb.2021.106236\n10.1016/j.neuroimage.2019.03.042\n10.1109/JPROC.2021.3052449\n10.1162/089976601750264965\n10.1016/j.media.2021.102166\n10.1016/j.ajo.2020.04.044\n10.1016/S1470-2045(14)70011-7\n10.1007/978-3-030-58452-8_37\n10.1016/j.neuroimage.2006.01.015\n10.1007/978-3-030-32248-9_29"}
{"title": "Survey on Diagnosing CORONA VIRUS from Radiography Chest X-ray Images Using Convolutional Neural Networks.", "abstract": "Corona Virus continues to harms its effects on the people lives across the globe. The screening of infected persons has to be identified is a vital step because it is a fast and low-cost way. Certain above mentioned things can be recognized by chest X-ray images that plays a significant role and also used for examining in detection of CORONA VIRUS(COVID-19). Here radiological chest X-rays are easily available with low cost only. In this survey paper, Convolutional Neural Network(CNN) based solution that will benefit in detection of the Covid-19 positive patients using radiography chest X-Ray images. To test the efficiency of the solution, using data sets of publicly available X-Ray images of Corona virus positive cases and negative cases. Images of positive Corona Virus patients and pictures of healthy person images are divided into testing images and trainable images. The solution which are providing the good results with classification accuracy within the test set-up. Then GUI based application supports for medical examination areas. This GUI application can be used on any computer and performed by any medical examiner or technician to determine Corona Virus positive patients using radiography X-ray images. The result will be precisely obtaining the Covid-19 Patient analysis through the chest X-ray images and also results may be retrieve within a few seconds.", "journal": "Wireless personal communications", "date": "2022-01-18", "authors": ["J TThirukrishna", "Sanda Reddy SaiKrishna", "PolicherlaShashank", "SSrikanth", "VRaghu"], "doi": "10.1007/s11277-022-09463-x\n10.1109/ACCESS.2020.3025010\n10.1109/ACCESS.2020.3033762\n10.1007/s11277-021-08466-4\n10.1109/JAS.2020.1003393\n10.1007/s10489-020-01829-7\n10.1016/j.future.2017.11.042\n10.1504/IJCC.2020.109379"}
{"title": "Segmentation and classification on chest radiography: a systematic survey.", "abstract": "Chest radiography (X-ray) is the most common diagnostic method for pulmonary disorders. A trained radiologist is required for interpreting the radiographs. But sometimes, even experienced radiologists can misinterpret the findings. This leads to the need for computer-aided detection diagnosis. For decades, researchers were automatically detecting pulmonary disorders using the traditional computer vision (CV) methods. Now the availability of large annotated datasets and computing hardware has made it possible for deep learning to dominate the area. It is now the modus operandi for feature extraction, segmentation, detection, and classification tasks in medical imaging analysis. This paper focuses on the research conducted using chest X-rays for the lung segmentation and detection/classification of pulmonary disorders on publicly available datasets. The studies performed using the Generative Adversarial Network (GAN) models for segmentation and classification on chest X-rays are also included in this study. GAN has gained the interest of the CV community as it can help with medical data scarcity. In this study, we have also included the research conducted before the popularity of deep learning models to have a clear picture of the field. Many surveys have been published, but none of them is dedicated to chest X-rays. This study will help the readers to know about the existing techniques, approaches, and their significance.", "journal": "The Visual computer", "date": "2022-01-18", "authors": ["TarunAgrawal", "PrakashChoudhary"], "doi": "10.1007/s00371-021-02352-7\n10.1007/s00371-019-01630-9\n10.1109/42.996338\n10.1007/s10140-008-0763-9\n10.1016/S1076-6332(98)80223-7\n10.1148/radiology.182.1.1727272\n10.1007/s13246-020-00966-0\n10.1109/TPAMI.2016.2644615\n10.1038/s41598-019-42294-8\n10.1109/ACCESS.2018.2877890\n10.1016/S0895-6111(98)00051-2\n10.1109/TMI.2013.2290491\n10.1007/BF01385685\n10.1016/j.artmed.2020.101881\n10.1109/TBME.2012.2226583\n10.1118/1.3561504\n10.1109/TSMCB.2004.831165\n10.1016/j.acra.2005.08.035\n10.1006/cviu.1995.1004\n10.1109/TITB.2003.821313\n10.1093/jamia/ocv080\n10.1016/j.patrec.2020.12.010\n10.1016/j.eswa.2021.115519\n10.1118/1.597539\n10.1118/1.3013555\n10.1016/j.patcog.2017.10.013\n10.1016/j.patrec.2019.11.040\n10.1016/S0140-6736(99)06093-6\n10.1007/s10489-020-02010-w\n10.1007/s00371-020-01799-4\n10.1109/TMI.2013.2284099\n10.1007/s10489-020-01902-1\n10.1016/j.measurement.2019.05.076\n10.1007/s00371-019-01628-3\n10.1007/BF00133570\n10.1016/j.cell.2018.02.010\n10.3390/s21020369\n10.1109/34.387512\n10.1148/radiol.2017162326\n10.1038/nature14539\n10.1109/5.726791\n10.5588/ijtld.11.0425\n10.1109/ACCESS.2018.2817023\n10.1016/S1076-6332(03)80688-8\n10.1016/j.artmed.2019.101744\n10.1016/j.media.2017.07.005\n10.1016/j.compmedimag.2019.05.005\n10.1109/42.476112\n10.1007/s11684-019-0726-4\n10.1016/j.asoc.2020.106691\n10.1016/S1361-8415(96)80007-7\n10.1007/s11277-018-5702-9\n10.1109/ACCESS.2020.3041867\n10.1109/ACCESS.2020.3017915\n10.1109/TMI.2018.2806086\n10.1016/j.acra.2019.10.006\n10.1001/jama.2011.1591\n10.1016/j.chaos.2020.109944\n10.1038/s41598-019-42557-4\n10.1016/j.ajem.2009.07.011\n10.1118/1.596209\n10.3390/app10093233\n10.1109/ACCESS.2020.3031384\n10.1007/s00371-019-01649-y\n10.1109/TIT.1956.1056810\n10.1002/mp.14507\n10.1037/h0042519\n10.1016/j.media.2005.09.003\n10.1148/radiol.2261011924\n10.1109/TMI.2014.2305691\n10.1109/TMI.2007.908130\n10.3390/app11062751\n10.1016/j.cmpb.2019.06.005\n10.1016/j.media.2020.101693\n10.1109/TMI.2002.803121\n10.1109/42.993132\n10.1109/42.974918\n10.1016/j.media.2005.02.002\n10.3390/s21051742\n10.1118/1.598405\n10.1109/ACCESS.2020.2994762\n10.1118/1.597549\n10.1118/1.597738\n10.1007/s12021-018-9377-x\n10.1016/j.media.2019.101552\n10.1007/s10489-020-01867-1\n10.1016/j.bspc.2018.01.011"}
{"title": "Detecting Racial/Ethnic Health Disparities Using Deep Learning From Frontal Chest Radiography.", "abstract": "The aim of this study was to assess racial/ethnic and socioeconomic disparities in the difference between atherosclerotic vascular disease prevalence measured by a multitask convolutional neural network (CNN) deep learning model using frontal chest radiographs (CXRs) and the prevalence reflected by administrative hierarchical condition category codes in two cohorts of patients with coronavirus disease\u00a02019 (COVID-19).\nA CNN model, previously published, was trained to predict atherosclerotic disease from ambulatory frontal CXRs. The model was then validated on two cohorts of patients with COVID-19: 814 ambulatory patients from a suburban location (presenting from March 14, 2020, to October 24, 2020, the internal ambulatory cohort) and 485 hospitalized patients from an inner-city location (hospitalized from March 14, 2020, to August 12, 2020, the external hospitalized cohort). The CNN model predictions were validated against electronic health record administrative codes in both cohorts and assessed using the area under the receiver operating characteristic curve (AUC). The CXRs from the ambulatory cohort were also reviewed by two board-certified radiologists and compared with the CNN-predicted values for the same cohort to produce a receiver operating characteristic curve and the AUC. The atherosclerosis diagnosis discrepancy, \u0394\nThe CNN prediction for vascular disease from frontal CXRs in the ambulatory cohort had an AUC of 0.85 (95% confidence interval, 0.82-0.89) and in the hospitalized cohort had an AUC of 0.69 (95% confidence interval, 0.64-0.75) against the electronic health record data. In the ambulatory cohort, the consensus radiologists' reading had an AUC of 0.89 (95% confidence interval, 0.86-0.92) relative to the CNN. Multivariate linear regression of \u0394\nA CNN model was predictive of aortic atherosclerosis in two cohorts (one ambulatory and one hospitalized) with COVID-19. The discrepancy between the CNN model and the administrative code, \u0394", "journal": "Journal of the American College of Radiology : JACR", "date": "2022-01-17", "authors": ["AyisPyrros", "Jorge MarioRodr\u00edguez-Fern\u00e1ndez", "Stephen MBorstelmann", "Judy WawiraGichoya", "Jeanne MHorowitz", "BrianFornelli", "NasirSiddiqui", "YuryVelichko", "OluwasanmiKoyejo Sanmi", "WilliamGalanter"], "doi": "10.1016/j.jacr.2021.09.010\n10.1001/jama.2020.8598\n10.5334/ijic.2500\n10.1016/j.acra.2021.05.002"}
{"title": "An automated COVID-19 triage pipeline using artificial intelligence based on chest radiographs and clinical data.", "abstract": "While COVID-19 diagnosis and prognosis artificial intelligence models exist, very few can be implemented for practical use given their high risk of bias. We aimed to develop a diagnosis model that addresses notable shortcomings of prior studies, integrating it into a fully automated triage pipeline that examines chest radiographs for the presence, severity, and progression of COVID-19 pneumonia. Scans were collected using the DICOM Image Analysis and Archive, a system that communicates with a hospital's image repository. The authors collected over 6,500 non-public chest X-rays comprising diverse COVID-19 severities, along with radiology reports and RT-PCR data. The authors provisioned one internally held-out and two external test sets to assess model generalizability and compare performance to traditional radiologist interpretation. The pipeline was evaluated on a prospective cohort of 80 radiographs, reporting a 95% diagnostic accuracy. The study mitigates bias in AI model development and demonstrates the value of an end-to-end COVID-19 triage platform.", "journal": "NPJ digital medicine", "date": "2022-01-16", "authors": ["Chris KKim", "Ji WhaeChoi", "ZhichengJiao", "DongcuiWang", "JingWu", "Thomas YYi", "Kasey CHalsey", "FeyisopeEweje", "Thi My LinhTran", "ChangLiu", "RobinWang", "JohnSollee", "CelinaHsieh", "KenChang", "Fang-XueYang", "RitambharaSingh", "Jie-LinOu", "Raymond YHuang", "CaiFeng", "Michael DFeldman", "TaoLiu", "Ji ShengGong", "ShaoleiLu", "CarstenEickhoff", "XueFeng", "IhabKamel", "RonnieSebro", "Michael KAtalay", "TerranceHealey", "YongFan", "Wei-HuaLiao", "JianxinWang", "Harrison XBai"], "doi": "10.1038/s41746-021-00546-w\n10.1056/NEJMoa2004500\n10.1056/NEJMoa2002032\n10.1016/S1473-3099(20)30457-6\n10.3348/kjr.2020.0195\n10.1128/JCM.00297-20\n10.1001/jama.2020.8259\n10.1016/j.compbiomed.2020.103792\n10.1148/radiol.2020201754\n10.1016/j.cmpb.2020.105532\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020201160\n10.1007/s10140-020-01808-y\n10.1038/s41598-021-88807-2\n10.1016/S2589-7500(21)00039-X\n10.1038/s42256-021-00307-0\n10.1007/s11263-019-01228-7\n10.1016/j.inffus.2021.04.008\n10.1007/s10278-021-00488-5\n10.1007/s10140-020-01824-y\n10.1007/s11263-015-0816-y\n10.1214/aos/1176344552\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4"}
{"title": "VECTOR: An algorithm for the detection of COVID-19 pneumonia from velcro-like lung sounds.", "abstract": "The coronavirus disease 2019 (COVID-19) has severely stressed the sanitary systems of all countries in the world. One of the main issues that physicians are called to tackle is represented by the monitoring of pauci-symptomatic COVID-19 patients at home and, generally speaking, everyone the access to the hospital might or should be severely reduced. Indeed, the early detection of interstitial pneumonia is particularly relevant for the survival of these patients. Recent studies on rheumatoid arthritis and interstitial lung diseases have shown that pathological pulmonary sounds can be automatically detected by suitably developed algorithms. The scope of this preliminary work consists of proving that the pathological lung sounds evidenced in patients affected by COVID-19 pneumonia can be automatically detected as well by the same class of algorithms. In particular the software VECTOR, suitably devised for interstitial lung diseases, has been employed to process the lung sounds of 28 patient recorded in the emergency room at the university hospital of Modena (Italy) during December 2020. The performance of VECTOR has been compared with diagnostic techniques based on imaging, namely lung ultrasound, chest X-ray and high resolution computed tomography, which have been assumed as ground truth. The results have evidenced a surprising overall diagnostic accuracy of 75% even if the staff of the emergency room has not been suitably trained for lung auscultation and the parameters of the software have not been optimized to detect interstitial pneumonia. These results pave the way to a new approach for monitoring the pulmonary implication in pauci-symptomatic COVID-19 patients.", "journal": "Computers in biology and medicine", "date": "2022-01-15", "authors": ["FabrizioPancaldi", "Giuseppe StefanoPezzuto", "GiuliaCassone", "MariannaMorelli", "AndreinaManfredi", "MatteoD'Arienzo", "CaterinaVacchi", "FulvioSavorani", "GiovanniVinci", "FrancescoBarsotti", "Maria TeresaMascia", "CarloSalvarani", "MarcoSebastiani"], "doi": "10.1016/j.compbiomed.2022.105220\n10.1111/acem.14048\n10.1002/jmv.25822\n10.1016/j.virol.2020.08.011\n10.1016/s0140-6736(20)30211-7\n10.5152/dir.2020.20260\n10.2214/ajr.20.23202\n10.1159/000509610\n10.7150/ijms.54987\n10.3390/s18113813\n10.26555/ijain.v4i3.273\n10.1016/j.chaos.2020.110246\n10.3390/s21010057\n10.1371/journal.pone.0254134\n10.2214/ajr.20.22976\n10.1016/j.compbiomed.2018.03.006\n10.1109/proc.1975.9792\n10.1109/45.1890\n10.1136/bmj.m998"}
{"title": "Bayesian-based optimized deep learning model to detect COVID-19 patients using chest X-ray image data.", "abstract": "Coronavirus Disease 2019 (COVID-19) is extremely infectious and rapidly spreading around the globe. As a result, rapid and precise identification of COVID-19 patients is critical. Deep Learning has shown promising performance in a variety of domains and emerged as a key technology in Artificial Intelligence. Recent advances in visual recognition are based on image classification and artefacts detection within these images. The purpose of this study is to classify chest X-ray images of COVID-19 artefacts in changed real-world situations. A novel Bayesian optimization-based convolutional neural network (CNN) model is proposed for the recognition of chest X-ray images. The proposed model has two main components. The first one utilizes CNN to extract and learn deep features. The second component is a Bayesian-based optimizer that is used to tune the CNN hyperparameters according to an objective function. The used large-scale and balanced dataset comprises 10,848 images (i.e., 3616 COVID-19, 3616 normal cases, and 3616 Pneumonia). In the first ablation investigation, we compared Bayesian optimization to three distinct ablation scenarios. We used convergence charts and accuracy to compare the three scenarios. We noticed that the Bayesian search-derived optimal architecture achieved 96% accuracy. To assist qualitative researchers, address their research questions in a methodologically sound manner, a comparison of research method and theme analysis methods was provided. The suggested model is shown to be more trustworthy and accurate in real world.", "journal": "Computers in biology and medicine", "date": "2022-01-14", "authors": ["MohamedLoey", "ShakerEl-Sappagh", "SeyedaliMirjalili"], "doi": "10.1016/j.compbiomed.2022.105213\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1007/s10238-020-00648-x\n10.1109/ACCESS.2020.2992341\n10.1016/j.measurement.2020.108288\n10.1109/ISIA51297.2020.9416545\n10.1109/ACCESS.2020.3030090\n10.1016/j.scs.2020.102600\n10.1038/s41562-020-01009-0\n10.1109/ICELTICs50595.2020.9315493\n10.1109/EIConCIT50028.2021.9431852\n10.1038/d41586-021-00785-7\n10.1038/d41586-021-00094-z\n10.1016/j.rbmo.2020.06.001\n10.1007/s11548-020-02305-w\n10.1109/ACCESS.2021.3050852\n10.1007/s12559-020-09787-5\n10.1007/s00521-020-05437-x\n10.1109/ACCESS.2020.3025164\n10.1007/s12652-021-03075-2\n10.1155/2020/8876798\n10.1109/ACPR.2015.7486599\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.90\n10.1371/journal.pone.0242535\n10.1007/s13755-020-00119-3\n10.3390/sym12040651\n10.1007/s10489-020-01829-7\n10.3390/electronics9091439\n10.1016/j.media.2020.101794\n10.1109/ICEIEC49280.2020.9152329\n10.3390/info11090419\n10.1177/2472630320958376\n10.1155/2020/8828855\n10.1186/s40537-019-0192-5\n10.1007/s13748-016-0094-0\n10.1016/j.compbiomed.2021.104319\n10.1109/ACCESS.2020.3010287\n10.1016/j.cell.2018.02.010\n10.11989/JEST.1674-862X.80904120\n10.1007/s12530-020-09345-2\n10.1007/s41664-018-0068-2"}
{"title": "COVID-19 disease diagnosis from paper-based ECG trace image data using a novel convolutional neural network model.", "abstract": "Clinical reports show that COVID-19 disease has impacts on the cardiovascular system in addition to the respiratory system. Available COVID-19 diagnostic methods have been shown to have limitations. In addition to current diagnostic methods such as low-sensitivity standard RT-PCR tests and expensive medical imaging devices, the development of alternative methods for the diagnosis of COVID-19 disease would be beneficial for control of the COVID-19 pandemic. Further, it is important to quickly and accurately detect abnormalities caused by COVID-19 on the cardiovascular system via ECG. In this study, the diagnosis of COVID-19 disease is proposed using a novel deep Convolutional Neural Network model by using only ECG trace images created from ECG signals of COVID-19 infected patients based on the abnormalities caused by the COVID-19 virus on the cardiovascular system. An overall classification accuracy of 98.57%, 93.20%, 96.74% and AUC value of 0.9966, 0.9771, 0.9905 is achieved for COVID-19 vs. Normal, COVID-19 vs. Abnormal Heartbeats, COVID-19 vs. Myocardial Infarction binary classification tasks, respectively. In addition, an overall classification accuracy of 86.55% and 83.05% is achieved for COVID-19 vs. Abnormal Heartbeats vs. Myocardial Infarction and Normal vs. COVID-19 vs. Abnormal Heartbeats vs. Myocardial Infarction multi-classification tasks. This study is believed to have great potential to speed up the diagnosis and treatment of COVID-19 patients, saving clinicians time and facilitating the control of the pandemic.", "journal": "Physical and engineering sciences in medicine", "date": "2022-01-13", "authors": ["EmrahIrmak"], "doi": "10.1007/s13246-022-01102-w\n10.3390/ijerph17082690\n10.1148/radiol.2020200905\n10.1148/radiol.2020201160\n10.1152/physiolgenomics.00084.2020\n10.1049/ipr2.12153\n10.7759/cureus.9540\n10.1016/j.acra.2021.01.022\n10.1111/anec.12815\n10.22037/aaem.v9i1.957\n10.1016/j.ihj.2020.11.007\n10.1016/j.ejim.2020.06.015\n10.1016/j.compbiomed.2020.103805\n10.1007/s13246-020-00865-4\n10.3390/SYM12040651\n10.1007/s10096-020-03901-z\n10.1101/2020.02.25.20021568\n10.1148/ryai.2020200048\n10.1016/j.media.2020.101860\n10.1007/s00330-020-07042-x\n10.1148/ryct.2020200075\n10.1371/journal.pone.0236621\n10.1186/s12911-021-01521-x\n10.1093/eurheartj/ehaa408\n10.1016/j.cjca.2020.03.028\n10.1016/j.echo.2020.05.028\n10.1016/j.dib.2021.106762\n10.3390/electronics8030292\n10.5152/electrica.2020.21004\n10.1007/s40998-021-00426-9\n10.1515/itms-2017-0003\n10.1016/j.icte.2020.04.010\n10.1016/j.compbiomed.2020.103792\n10.1016/j.eswa.2020.114054\n10.1016/j.irbm.2020.05.003\n10.1016/j.compbiomed.2020.104037"}
{"title": "Inverted bell-curve-based ensemble of deep learning models for detection of COVID-19 from chest X-rays.", "abstract": "Novel Coronavirus 2019 disease or COVID-19 is a viral disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The use of chest X-rays (CXRs) has become an important practice to assist in the diagnosis of COVID-19 as they can be used to detect the abnormalities developed in the infected patients' lungs. With the fast spread of the disease, many researchers across the world are striving to use several deep learning-based systems to identify the COVID-19 from such CXR images. To this end, we propose an inverted bell-curve-based ensemble of deep learning models for the detection of COVID-19 from CXR images. We first use a selection of models pretrained on ImageNet dataset and use the concept of transfer learning to retrain them with CXR datasets. Then the trained models are combined with the proposed inverted bell curve weighted ensemble method, where the output of each classifier is assigned a weight, and the final prediction is done by performing a weighted average of those outputs. We evaluate the proposed method on two publicly available datasets: the COVID-19 Radiography Database and the IEEE COVID Chest X-ray Dataset. The accuracy, F1 score and the AUC ROC achieved by the proposed method are 99.66%, 99.75% and 99.99%, respectively, in the first dataset, and, 99.84%, 99.81% and 99.99%, respectively, in the other dataset. Experimental results ensure that the use of transfer learning-based models and their combination using the proposed ensemble method result in improved predictions of COVID-19 in CXRs.", "journal": "Neural computing & applications", "date": "2022-01-12", "authors": ["AshisPaul", "ArpanBasu", "MuftiMahmud", "M ShamimKaiser", "RamSarkar"], "doi": "10.1007/s00521-021-06737-6\n10.1016/S0140-6736(20)30154-9\n10.1007/s12559-020-09751-3\n10.1016/j.cell.2018.02.010\n10.1016/j.chest.2020.04.003\n10.1109/ACCESS.2021.3050193\n10.1109/TNNLS.2018.2790388\n10.1109/34.273716\n10.1109/ACCESS.2020.3010287\n10.1016/j.inffus.2019.02.003\n10.1016/j.cose.2020.101748\n10.1016/j.compmedimag.2019.101660\n10.1016/j.artmed.2019.101749\n10.1109/TMI.2020.2995508\n10.1109/ACCESS.2020.3003810\n10.1007/s12539-020-00393-5\n10.1007/s10489-021-02292-8\n10.1007/s10489-020-01904-z\n10.1016/j.asoc.2020.106742\n10.1109/ACCESS.2020.2994762\n10.3390/jimaging4020039\n10.1109/TSMCB.2008.2009071\n10.1038/s41598-019-56847-4\n10.1109/JAS.2020.1003387\n10.1002/jcph.1644\n10.1093/jamia/ocaa280\n10.1007/s10489-020-01888-w\n10.3390/jimaging6060037\n10.1007/s00521-020-05636-6"}
{"title": "Focus on technology: surgical selfies, ransomware attacks, AI-assisted mental health care.", "abstract": null, "journal": "CMAJ : Canadian Medical Association journal = journal de l'Association medicale canadienne", "date": "2022-01-12", "authors": ["GregBasky", "LaurenVogel"], "doi": "10.1503/cmaj.1095981"}
{"title": "Classifier Fusion for Detection of COVID-19 from CT Scans.", "abstract": "The coronavirus disease (COVID-19) is an infectious disease caused by the SARS-CoV-2 virus. COVID-19 is found to be the most infectious disease in last few decades. This disease has infected millions of people worldwide. The inadequate availability and the limited sensitivity of the testing kits have motivated the clinicians and the scientist to use Computer Tomography (CT) scans to screen COVID-19. Recent advances in technology and the availability of deep learning approaches have proved to be very promising in detecting COVID-19 with increased accuracy. However, deep learning approaches require a huge labeled training dataset, and the current availability of benchmark COVID-19 data is still small. For the limited training data scenario, the CNN usually overfits after several iterations. Hence, in this work, we have investigated different pre-trained network architectures with transfer learning for COVID-19 detection that can work even on a small medical imaging dataset. Various variants of the pre-trained ResNet model, namely ResNet18, ResNet50, and ResNet101, are investigated in the current paper for the detection of COVID-19. The experimental results reveal that transfer learned ResNet50 model outperformed other models by achieving a recall of 98.80% and an F1-score of 98.41%. To further improvise the results, the activations from different layers of best performing model are also explored for the detection using the support vector machine, logistic regression and K-nearest neighbor classifiers. Moreover, a classifier fusion strategy is also proposed that fuses the predictions from the different classifiers via majority voting. Experimental results reveal that via using learned image features and classification fusion strategy, the recall, and F1-score have improvised to 99.20% and 99.40%.", "journal": "Circuits, systems, and signal processing", "date": "2022-01-11", "authors": ["TaranjitKaur", "Tapan KumarGandhi"], "doi": "10.1007/s00034-021-01939-8"}
{"title": "BEMD-3DCNN-based method for COVID-19 detection.", "abstract": "The coronavirus outbreak continues to spread around the world and no one knows when it will stop. Therefore, from the first day of the identification of the virus in Wuhan, China, scientists have launched numerous research projects to understand the nature of the virus, how to detect it, and search for the most effective medicine to help and protect patients. Importantly, a rapid diagnostic and detection system is a priority and should be developed to stop COVID-19 from spreading. Medical imaging techniques have been used for this purpose. Current research is focused on exploiting different backbones like VGG, ResNet, DenseNet, or combining them to detect COVID-19. By using these backbones many aspects cannot be analyzed like the spatial and contextual information in the images, although this information can be useful for more robust detection performance. In this paper, we used 3D representation of the data as input for the proposed 3DCNN-based deep learning model. The process includes using the Bi-dimensional Empirical Mode Decomposition (BEMD) technique to decompose the original image into IMFs, and then building a video of these IMF images. The formed video is used as input for the 3DCNN model to classify and detect the COVID-19 virus. The 3DCNN model consists of a 3D VGG-16 backbone followed by a Context-aware attention (CAA) module, and then fully connected layers for classification. Each CAA module takes the feature maps of different blocks of the backbone, which allows learning from different feature maps. In our experiments, we used 6484 X-ray images, of which 1802 were COVID-19 positive cases, 1910 normal cases, and 2772 pneumonia cases. The experiment results showed that our proposed technique achieved the desired results on the selected dataset. Additionally, the use of the 3DCNN model with contextual information processing exploited CAA networks to achieve better performance.", "journal": "Computers in biology and medicine", "date": "2022-01-09", "authors": ["AliRiahi", "OmarElharrouss", "SomayaAl-Maadeed"], "doi": "10.1016/j.compbiomed.2021.105188\n10.1038/s41598-021-96601-3\n10.20944/preprints202003.0300.v1\n10.1109/RBME.2020.2987975\n10.1109/ACCESS.2020.3010287\n10.1155/2015/769478\n10.1109/ICASSP.2011.5946778\n10.1007/s42600-021-00151-6\n10.1109/TII.2021.3057683"}
{"title": "Brain Networks Associated With COVID-19 Risk: Data From 3662 Participants.", "abstract": "Our behavioral traits, and subsequent actions, could affect the risk of exposure to the coronavirus disease of 2019 (COVID-19). The current study aimed to determine whether unique brain networks are associated with the COVID-19 infection risk.\nThis research was conducted using the UK Biobank Resource. Functional magnetic resonance imaging scans in a cohort of general population (n\u2009=\u20093662) were used to compute the whole-brain functional connectomes. A network-informed machine learning approach was used to identify connectome and nodal fingerprints that are associated with positive COVID-19 status during the pandemic up to February fourth, 2021.\nThe predictive models successfully identified 6 fingerprints that were associated with COVID-19 positive, compared to negative status (all \nIndividuals are at increased risk of COVID-19 infections if their brain connectome is consistent with reduced connectivity in the top-down attention and executive networks, along with increased internal connectivity in the introspective and instinctive networks. These identified risk networks could be investigated as target for treatment of illnesses with impulse control deficits.", "journal": "Chronic stress (Thousand Oaks, Calif.)", "date": "2022-01-08", "authors": ["Chadi GAbdallah"], "doi": "10.1177/24705470211066770"}
{"title": "Artificial intelligence for stepwise diagnosis and monitoring of COVID-19.", "abstract": "Main challenges for COVID-19 include the lack of a rapid diagnostic test, a suitable tool to monitor and predict a patient's clinical course and an efficient way for data sharing among multicenters. We thus developed a novel artificial intelligence system based on deep learning (DL) and federated learning (FL) for the diagnosis, monitoring, and prediction of a patient's clinical course.\nCT imaging derived from 6 different multicenter cohorts were used for stepwise diagnostic algorithm to diagnose COVID-19, with or without clinical data. Patients with more than 3 consecutive CT images were trained for the monitoring algorithm. FL has been applied for decentralized refinement of independently built DL models.\nA total of 1,552,988 CT slices from 4804 patients were used. The model can diagnose COVID-19 based on CT alone with the AUC being 0.98 (95% CI 0.97-0.99), and outperforms the radiologist's assessment. We have also successfully tested the incorporation of the DL diagnostic model with the FL framework. Its auto-segmentation analyses co-related well with those by radiologists and achieved a high Dice's coefficient of 0.77. It can produce a predictive curve of a patient's clinical course if serial CT assessments are available.\nThe system has high consistency in diagnosing COVID-19 based on CT, with or without clinical data. Alternatively, it can be implemented on a FL platform, which would potentially encourage the data sharing in the future. It also can produce an objective predictive curve of a patient's clinical course for visualization.\n\u2022 CoviDet could diagnose COVID-19 based on chest CT with high consistency; this outperformed the radiologist's assessment. Its auto-segmentation analyses co-related well with those by radiologists and could potentially monitor and predict a patient's clinical course if serial CT assessments are available. It can be integrated into the federated learning framework. \u2022 CoviDet can be used as an adjunct to aid clinicians with the CT diagnosis of COVID-19 and can potentially be used for disease monitoring; federated learning can potentially open opportunities for global collaboration.", "journal": "European radiology", "date": "2022-01-07", "authors": ["HengruiLiang", "YuchenGuo", "XiangruChen", "Keng-LeongAng", "YuweiHe", "NaJiang", "QiangDu", "QingsiZeng", "LigongLu", "ZebinGao", "LinduoLi", "QuanzhengLi", "FangxingNie", "GuiguangDing", "GaoHuang", "AilanChen", "YiminLi", "WeijieGuan", "LingSang", "YuandaXu", "HuaiChen", "ZishengChen", "ShiyueLi", "NuofuZhang", "YingChen", "DanxiaHuang", "RunLi", "JianfuLi", "BoCheng", "YiZhao", "CaichenLi", "ShanXiong", "RunchenWang", "JunLiu", "WeiWang", "JunHuang", "FeiCui", "TaoXu", "Fleming Y MLure", "MeixiaoZhan", "YuanyiHuang", "QiangYang", "QionghaiDai", "WenhuaLiang", "JianxingHe", "NanshanZhong"], "doi": "10.1007/s00330-021-08334-6\n10.1016/j.crad.2017.11.015\n10.21037/atm.2020.03.132\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2020.103795\n10.1016/j.ejrad.2020.109041\n10.2196/19569\n10.1038/s41467-020-17971-2\n10.1109/TMI.2020.2994908\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2996256\n10.1109/TIP.2018.2857219"}
{"title": "Fully automatic pipeline of convolutional neural networks and capsule networks to distinguish COVID-19 from community-acquired pneumonia via CT images.", "abstract": "Chest computed tomography (CT) is crucial in the diagnosis of coronavirus disease 2019 (COVID-19). However, the persistent pandemic and similar CT manifestations between COVID-19 and community-acquired pneumonia (CAP) raise methodological requirements.\nA fully automatic pipeline of deep learning is proposed for distinguishing COVID-19 from CAP using CT images. Inspired by the diagnostic process of radiologists, the pipeline comprises four connected modules for lung segmentation, selection of slices with lesions, slice-level prediction, and patient-level prediction. The roles of the first and second modules and the effectiveness of the capsule network for slice-level prediction were investigated. A dataset of 326 CT scans was collected to train and test the pipeline. Another public dataset of 110 patients was used to evaluate the generalization capability.\nLinkNet exhibited the largest intersection over union (0.967) and Dice coefficient (0.983) for lung segmentation. For the selection of slices with lesions, the capsule network with the ResNet50 block achieved an accuracy of 92.5% and an area under the curve (AUC) of 0.933. The capsule network using the DenseNet121 block demonstrated better performance for slice-level prediction, with an accuracy of 97.1% and AUC of 0.992. For both datasets, the prediction accuracy of our pipeline was 100% at the patient level.\nThe proposed fully automatic deep learning pipeline of deep learning can distinguish COVID-19 from CAP via CT images rapidly and accurately, thereby accelerating diagnosis and augmenting the performance of radiologists. This pipeline is convenient for use by radiologists and provides explainable predictions.", "journal": "Computers in biology and medicine", "date": "2022-01-04", "authors": ["QianqianQi", "ShouliangQi", "YananWu", "ChenLi", "BinTian", "ShuyueXia", "JigangRen", "LimingYang", "HanlinWang", "HuiYu"], "doi": "10.1016/j.compbiomed.2021.105182\n10.1148/radiol.2020200905\n10.1016/S0140-6736(20)30628-0\n10.1016/S2213-2600(20)30079-5\n10.1148/radiol.2020200642\n10.1093/cid/ciaa461\n10.1016/j.jinf.2020.03.051\n10.1016/j.ajem.2020.04.016\n10.1109/ACCESS.2020.3005510\n10.1016/j.patcog.2021.107848\n10.1016/j.jinf.2020.04.004\n10.1016/j.patrec.2021.09.012\n10.1007/s00330-020-07628-5\n10.1148/radiol.2020200823\n10.1109/TMI.2020.2993291\n10.1101/2020.12.19.20248530\n10.1109/ICCCS49678.2020.9277077\n10.1109/ACCESS.2020.2994762\n10.3233/XST-200715\n10.1101/2020.04.24.20078584\n10.3390/s21020455\n10.1109/tmi.2020.2995508\n10.1016/j.cmpb.2021.106406\n10.1109/VCIP.2017.8305148\n10.1016/j.cell.2020.04.045\n10.1007/978-3-319-24574-4_28\n10.1117/1.JMI.6.1.014006\n10.1007/978-3-030-00889-5_1\n10.1109/TMI.2019.2959609\n10.1109/CVPR.2016.90\n10.1038/s41746-021-00399-3\n10.1007/s00259-020-04929-1\n10.1016/j.patrec.2021.10.027\n10.1056/NEJMoa2001316\n10.1002/jmv.27335\n10.1016/j.brainresbull.2021.08.012\n10.15446/ing.investig.v42n1.90289\n10.1101/2020.11.07.20227504\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001017\n10.1056/NEJMra072149\n10.1177/1063293X211021435\n10.1111/exsy.12776\n10.5152/dir.2015.15221\n10.1148/radiol.2020200230\n10.1016/j.patcog.2020.107613\n10.1016/j.patcog.2020.107747\n10.1016/j.patcog.2021.108071\n10.3390/s21175878\n10.3390/diagnostics11050893\n10.1016/j.bspc.2020.102296\n10.1186/s12880-021-00640-1\n10.1016/j.neucom.2020.07.144\n10.3390/rs11050494\n10.1109/TGRS.2018.2871782\n10.3390/s21165575\n10.1016/j.mehy.2020.109761\n10.1109/ACCESS.2019.2920980\n10.1109/ACCESS.2019.2933670\n10.7150/thno.46428\n10.1016/j.patcog.2021.108168"}
{"title": "COV-ADSX: An Automated Detection System using X-ray Images, Deep Learning, and XGBoost for COVID-19.", "abstract": "Following the COVID-19 pandemic, scientists have been looking for different ways to diagnose COVID-19, and these efforts have led to a variety of solutions. One of the common methods of detecting infected people is chest radiography. In this paper, an Automated Detection System using X-ray images (COV-ADSX) is proposed, which employs a deep neural network and XGBoost to detect COVID-19. COV-ADSX was implemented using the Django web framework, which allows the user to upload an X-ray image and view the results of the COVID-19 detection and image's heatmap, which helps the expert to evaluate the chest area more accurately.", "journal": "Software impacts", "date": "2022-01-04", "authors": ["SharifHasani", "HamidNasiri"], "doi": "10.1016/j.simpa.2021.100210\n10.1038/s41586-020-2008-3\n10.1016/j.ringps.2021.100034\n10.1016/j.ijmst.2021.10.006\n10.1016/j.apt.2021.09.020"}
{"title": "A Review on Deep Learning Techniques for the Diagnosis of Novel Coronavirus (COVID-19).", "abstract": "Novel coronavirus (COVID-19) outbreak, has raised a calamitous situation all over the world and has become one of the most acute and severe ailments in the past hundred years. The prevalence rate of COVID-19 is rapidly rising every day throughout the globe. Although no vaccines for this pandemic have been discovered yet, deep learning techniques proved themselves to be a powerful tool in the arsenal used by clinicians for the automatic diagnosis of COVID-19. This paper aims to overview the recently developed systems based on deep learning techniques using different medical imaging modalities like Computer Tomography (CT) and X-ray. This review specifically discusses the systems developed for COVID-19 diagnosis using deep learning techniques and provides insights on well-known data sets used to train these networks. It also highlights the data partitioning techniques and various performance measures developed by researchers in this field. A taxonomy is drawn to categorize the recent works for proper insight. Finally, we conclude by addressing the challenges associated with the use of deep learning methods for COVID-19 detection and probable future trends in this research area. The aim of this paper is to facilitate experts (medical or otherwise) and technicians in understanding the ways deep learning techniques are used in this regard and how they can be potentially further utilized to combat the outbreak of COVID-19.", "journal": "IEEE access : practical innovations, open solutions", "date": "2022-01-04", "authors": ["Md MilonIslam", "FakhriKarray", "RedaAlhajj", "JiaZeng"], "doi": "10.1109/ACCESS.2021.3058537\n10.1080/07391102.2020.1767212\n10.1109/ICCSRE.2019.8807741\n10.1007/s10489-020-01900-3"}
{"title": "The year in cardiovascular medicine 2021: heart failure and cardiomyopathies.", "abstract": "In the year 2021, the universal definition and classification of heart failure (HF) was published that defines HF as a clinical syndrome with symptoms and/or signs caused by a cardiac abnormality and corroborated by elevated natriuretic peptide levels or objective evidence of cardiogenic congestion. This definition and the classification of HF with reduced ejection fraction (HFrEF), mildly reduced, and HF with preserved ejection fraction (HFpEF) is consistent with the 2021 ESC Guidelines on HF. Among several other new recommendations, these guidelines give a Class I indication for the use of the sodium-glucose co-transporter 2 (SGLT2) inhibitors dapagliflozin and empagliflozin in HFrEF patients. As the first evidence-based treatment for HFpEF, in the EMPEROR-Preserved trial, empagliflozin reduced the composite endpoint of cardiovascular death and HF hospitalizations. Several reports in 2021 have provided novel and detailed analyses of device and medical therapy in HF, especially regarding sacubitril/valsartan, SGLT2 inhibitors, mineralocorticoid receptor antagonists, ferric carboxymaltose, soluble guanylate cyclase activators, and cardiac myosin activators. In patients hospitalized with COVID-19, acute HF and myocardial injury is quite frequent, whereas myocarditis and long-term damage to the heart are rather uncommon.", "journal": "European heart journal", "date": "2022-01-03", "authors": ["JohannBauersachs", "Rudolf Ade Boer", "JoAnnLindenfeld", "BiykemBozkurt"], "doi": "10.1093/eurheartj/ehab887\n10.1002/ejhf.2264\n10.1093/eurheartj/ehab674"}
{"title": "Development of computer-aided model to differentiate COVID-19 from pulmonary edema in lung CT scan: EDECOVID-net.", "abstract": "The efforts made to prevent the spread of COVID-19 face specific challenges in diagnosing COVID-19 patients and differentiating them from patients with pulmonary edema. Although systemically administered pulmonary vasodilators and acetazolamide are of great benefit for treating pulmonary edema, they should not be used to treat COVID-19 as they carry the risk of several adverse consequences, including worsening the matching of ventilation and perfusion, impaired carbon dioxide transport, systemic hypotension, and increased work of breathing. This study proposes a machine learning-based method (EDECOVID-net) that automatically differentiates the COVID-19 symptoms from pulmonary edema in lung CT scans using radiomic features. To the best of our knowledge, EDECOVID-net is the first method to differentiate COVID-19 from pulmonary edema and a helpful tool for diagnosing COVID-19 at early stages. The EDECOVID-net has been proposed as a new machine learning-based method with some advantages, such as having simple structure and few mathematical calculations. In total, 13\u2009717 imaging patches, including 5759 COVID-19 and 7958 edema images, were extracted using a CT incision by a specialist radiologist. The EDECOVID-net can distinguish the patients with COVID-19 from those with pulmonary edema with an accuracy of 0.98. In addition, the accuracy of the EDECOVID-net algorithm is compared with other machine learning methods, such as VGG-16 (Acc\u00a0=\u00a00.94), VGG-19 (Acc\u00a0=\u00a00.96), Xception (Acc\u00a0=\u00a00.95), ResNet101 (Acc\u00a0=\u00a00.97), and DenseNet20l (Acc\u00a0=\u00a00.97).", "journal": "Computers in biology and medicine", "date": "2022-01-02", "authors": ["ElenaVelichko", "FaridoddinShariaty", "MahdiOrooji", "VitaliiPavlov", "TatianaPervunina", "SergeyZavjalov", "RaziehKhazaei", "Amir RezaRadmard"], "doi": "10.1016/j.compbiomed.2021.105172"}
{"title": "Tiled Sparse Coding in Eigenspaces for Image Classification.", "abstract": "The automation in the diagnosis of medical images is currently a challenging task. The use of Computer Aided Diagnosis (CAD) systems can be a powerful tool for clinicians, especially in situations when hospitals are overflowed. These tools are usually based on artificial intelligence (AI), a field that has been recently revolutionized by deep learning approaches. These alternatives usually obtain a large performance based on complex solutions, leading to a high computational cost and the need of having large databases. In this work, we propose a classification framework based on sparse coding. Images are first partitioned into different tiles, and a dictionary is built after applying PCA to these tiles. The original signals are then transformed as a linear combination of the elements of the dictionary. Then, they are reconstructed by iteratively deactivating the elements associated with each component. Classification is finally performed employing as features the subsequent reconstruction errors. Performance is evaluated in a real context where distinguishing between four different pathologies: control versus bacterial pneumonia versus viral pneumonia versus COVID-19. Our system differentiates between pneumonia patients and controls with an accuracy of 97.74%, whereas in the 4-class context the accuracy is 86.73%. The excellent results and the pioneering use of sparse coding in this scenario evidence that our proposal can assist clinicians when their workload is high.", "journal": "International journal of neural systems", "date": "2021-12-31", "authors": ["Juan EArco", "Andr\u00e9sOrtiz", "JavierRam\u00edrez", "Yu-DongZhang", "Juan MG\u00f3rriz"], "doi": "10.1142/S0129065722500071"}
{"title": "Radiology During the COVID-19 Pandemic: Mapping Radiology Literature in 2020.", "abstract": "Our aim was to assess articles published in the field of radiology, nuclear medicine, and medical imaging in 2020, analyzing the linkage of radiology-related topics with coronavirus disease 2019 (COVID-19) through literature mapping, along with a bibliometric analysis for publications.\nWe performed a search on Web of Science Core Collection database for articles in the field of radiology, nuclear medicine, and medical imaging published in 2020. We analyzed the included articles using VOS viewer software, where we analyzed the co-occurrence of keywords, which represents major topics discussed. Of the resulting topics, literature map created, and linkage analysis done.\nA total of 24,748 articles were published in the field of radiology, nuclear medicine, and medical imaging in 2020. We found a total of 61,267 keywords, only 78 keywords occurred more than 250 times. COVID-19 had 449 occurrences, 29 links, with a total link strength of 271. MRI was the topic most commonly appearing in 2020 radiology publications, while \"computed tomography\" has the highest linkage strength with COVID-19, with a linkage strength of 149, representing 54.98% of the total COVID-19 linkage strength, followed by \"radiotherapy, and \"deep and machine learning\". The top cited paper had a total of 1,687 citations. Nine out of the 10 most cited articles discussed COVID-19 and included \"COVID-19\" or \"coronavirus\" in their title, including the top cited paper.\nWhile MRI was the topic that dominated, CT had the highest linkage strength with COVID-19 and represent the topic of top cited articles in 2020 radiology publications.", "journal": "Current medical imaging", "date": "2021-12-31", "authors": ["NosaibaAl-Ryalat", "LnaMalkawi", "Ala'aAbu Salhiyeh", "FaisalAbualteen", "GhaidaAbdallah", "BayanAl Omari", "Saif AldeenAlRyalat"], "doi": "10.2174/1573405618666211230105631"}
{"title": "Membrane-Based In-Gel Loop-Mediated Isothermal Amplification (mgLAMP) System for SARS-CoV-2 Quantification in Environmental Waters.", "abstract": "Since the COVID-19 pandemic is expected to become endemic, quantification of severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) in ambient waters is critical for environmental surveillance and for early detection of outbreaks. Herein, we report the development of a membrane-based in-gel loop-mediated isothermal amplification (mgLAMP) system that is designed for the rapid point-of-use quantification of SARS-CoV-2 particles in environmental waters. The mgLAMP system integrates the viral concentration, in-assay viral lysis, and on-membrane hydrogel-based RT-LAMP quantification using enhanced fluorescence detection with a target-specific probe. With a sample-to-result time of less than 1 h, mgLAMP successfully detected SARS-CoV-2 below 0.96 copies/mL in Milli-Q water. In surface water, the lowest detected SARS-CoV-2 concentration was 93 copies/mL for mgLAMP, while the reverse transcription quantitative polymerase chain reaction (RT-qPCR) with optimal pretreatment was inhibited at 930 copies/mL. A 3D-printed portable device is designed to integrate heated incubation and fluorescence illumination for the simultaneous analysis of nine mgLAMP assays. Smartphone-based imaging and machine learning-based image processing are used for the interpretation of results. In this report, we demonstrate that mgLAMP is a promising method for large-scale environmental surveillance of SARS-CoV-2 without the need for specialized equipment, highly trained personnel, and labor-intensive procedures.", "journal": "Environmental science & technology", "date": "2021-12-31", "authors": ["YanzheZhu", "XunyiWu", "AlanGu", "LeopoldDobelle", "Cl\u00e9ment ACid", "JingLi", "Michael RHoffmann"], "doi": "10.1021/acs.est.1c04623\n10.1038/d41586-021-00396-2\n10.1126/science.abe6522\n10.3201/eid2608.200681\n10.1007/s11427-020-1783-9\n10.1038/s41586-020-2196-x\n10.1021/acsestwater.0c00246\n10.1016/j.scitotenv.2020.140911\n10.1016/j.scitotenv.2020.140405\n10.1016/j.watres.2021.116810\n10.1016/j.watres.2020.115942\n10.1016/j.scitotenv.2020.138764\n10.1016/j.watres.2020.116296\n10.1016/j.scitotenv.2020.138149\n10.1039/d0ew90015j\n10.1038/d41586-020-00973-x\n10.1016/j.watres.2020.115907\n10.1016/j.jwpe.2021.101947\n10.1039/d0ew00946f\n10.1016/j.trac.2020.116125\n10.1021/acs.est.0c06191\n10.1016/j.scitotenv.2020.139076\n10.1126/scitranslmed.aag0538\n10.1186/1471-2180-14-38\n10.1021/acs.est.8b00241\n10.1093/cid/ciaa498\n10.1016/j.watres.2021.117172\n10.1016/j.jbbm.2006.08.008\n10.1016/j.watres.2019.05.049\n10.1126/sciadv.abe3703\n10.1073/pnas.2014739117\n10.1590/0074-02760200196\n10.1016/j.scitotenv.2020.144105\n10.1038/srep20516\n10.1021/acsmaterialslett.0c00348\n10.1007/s12560-017-9311-7\n10.1039/d0ay01658f\n10.1371/journal.pone.0234682\n10.1093/nar/gki591\n10.1038/nmeth.2089\n10.1021/acsnano.8b05384\n10.1016/j.bios.2021.113199\n10.1021/acsabm.0c01615\n10.1056/nejmoa2001017\n10.1074/jbc.m106096200\n10.1016/j.cell.2020.09.018\n10.1038/s41467-020-18611-5\n10.1093/clinchem/hvaa267\n10.1016/j.jcv.2020.104579\n10.1016/0005-2736(80)90530-1\n10.1016/j.biologicals.2008.06.002\n10.1016/s0166-0934(99)00102-0\n10.1073/pnas.2011221117\n10.1016/0168-1605(92)90017-w\n10.1021/acs.analchem.5b04054\n10.1038/srep40125\n10.1186/s12896-019-0549-z\n10.3791/3998\n10.4236/jtr.2014.24021\n10.3390/molecules20069487\n10.1186/1472-6750-9-7\n10.1529/biophysj.104.050682\n10.1038/nmeth.3955\n10.1016/j.scitotenv.2020.139960\n10.1021/ci00027a006\n10.1016/S1474-6670(17)50715-6\n10.3389/fmicb.2019.00878"}
{"title": "A Rapid Artificial Intelligence-Based Computer-Aided Diagnosis System for COVID-19 Classification from CT Images.", "abstract": "The excessive number of COVID-19 cases reported worldwide so far, supplemented by a high rate of false alarms in its diagnosis using the conventional polymerase chain reaction method, has led to an increased number of high-resolution computed tomography (CT) examinations conducted. The manual inspection of the latter, besides being slow, is susceptible to human errors, especially because of an uncanny resemblance between the CT scans of COVID-19 and those of pneumonia, and therefore demands a proportional increase in the number of expert radiologists. Artificial intelligence-based computer-aided diagnosis of COVID-19 using the CT scans has been recently coined, which has proven its effectiveness in terms of accuracy and computation time. In this work, a similar framework for classification of COVID-19 using CT scans is proposed. The proposed method includes four core steps: (i) preparing a database of three different classes such as COVID-19, pneumonia, and normal; (ii) modifying three pretrained deep learning models such as VGG16, ResNet50, and ResNet101 for the classification of COVID-19-positive scans; (iii) proposing an activation function and improving the firefly algorithm for feature selection; and (iv) fusing optimal selected features using descending order serial approach and classifying using multiclass supervised learning algorithms. We demonstrate that once this method is performed on a publicly available dataset, this system attains an improved accuracy of 97.9% and the computational time is almost 34 (sec).", "journal": "Behavioural neurology", "date": "2021-12-31", "authors": ["Hassaan HaiderSyed", "Muhammad AttiqueKhan", "UsmanTariq", "AmmarArmghan", "FayadhAlenezi", "Junaid AliKhan", "SeungminRho", "SeifedineKadry", "VenkatesanRajinikanth"], "doi": "10.1155/2021/2560388\n10.1590/1806-9282.66.7.880\n10.1016/j.chaos.2020.110190\n10.1016/j.apacoust.2020.107256\n10.1007/s10489-020-01888-w\n10.32604/cmc.2021.017337\n10.1007/s00521-020-05410-8\n10.3390/s21217286\n10.11591/ijece.v11i1.pp365-374\n10.1007/s10044-020-00950-0\n10.1007/s00779-020-01494-0\n10.32604/cmc.2021.016816\n10.1016/j.ins.2021.05.035\n10.1109/TIM.2020.3033072\n10.32604/cmc.2021.018040\n10.32604/cmc.2021.017101\n10.1111/exsy.12497\n10.1155/2021/5524637\n10.4018/978-1-7998-1230-2\n10.32604/cmc.2021.013191\n10.32604/cmc.2022.020140\n10.3390/v12070769\n10.1002/jemt.23447\n10.1007/s00521-021-06490-w\n10.1002/int.22691\n10.1016/j.patrec.2019.11.034\n10.1109/ACCESS.2020.3010448\n10.32604/cmes.2020.011380\n10.1007/s12559-020-09787-5\n10.1016/j.patrec.2020.12.015\n10.1007/s12652-021-02967-7\n10.1007/s11760-020-01820-2\n10.1016/j.compeleceng.2020.106960\n10.1109/JBHI.2020.3019505\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1016/j.imu.2020.100412\n10.1007/s12652-020-02669-6\n10.1016/j.asoc.2020.106906\n10.1166/jmihi.2020.3222\n10.1109/ACCESS.2020.3034217\n10.1016/j.eswa.2019.112957\n10.1007/s11042-021-10567-y\n10.1016/j.patrec.2020.09.010"}
{"title": "Protocol of the Healthy Brain Study: An accessible resource for understanding the human brain and how it dynamically and individually operates in its bio-social context.", "abstract": "The endeavor to understand the human brain has seen more progress in the last few decades than in the previous two millennia. Still, our understanding of how the human brain relates to behavior in the real world and how this link is modulated by biological, social, and environmental factors is limited. To address this, we designed the Healthy Brain Study (HBS), an interdisciplinary, longitudinal, cohort study based on multidimensional, dynamic assessments in both the laboratory and the real world. Here, we describe the rationale and design of the currently ongoing HBS. The HBS is examining a population-based sample of 1,000 healthy participants (age 30-39) who are thoroughly studied across an entire year. Data are collected through cognitive, affective, behavioral, and physiological testing, neuroimaging, bio-sampling, questionnaires, ecological momentary assessment, and real-world assessments using wearable devices. These data will become an accessible resource for the scientific community enabling the next step in understanding the human brain and how it dynamically and individually operates in its bio-social context. An access procedure to the collected data and bio-samples is in place and published on https://www.healthybrainstudy.nl/en/data-and-methods/access. Trail registration: https://www.trialregister.nl/trial/7955.", "journal": "PloS one", "date": "2021-12-30", "authors": ["NoneNone", "EstherAarts", "AgnesAkkerman", "MareikeAltgassen", "RonaldBartels", "BeckyBeckers", "KirstenBevelander", "ErikBijleveld", "Esmeralda BlaneyDavidson", "AnnemarieBoleij", "JanitaBralten", "ToonCillessen", "JurgenClaassen", "RoshanCools", "InekeCornelissen", "MartinDresler", "ThijsEijsvogels", "MyrtheFaber", "Guill\u00e9nFern\u00e1ndez", "BerndFigner", "MatthiasFritsche", "SaschaF\u00fcllbrunn", "SuryaGayet", "Marleen M. H. J.van Gelder", "Marcelvan Gerven", "SabineGeurts", "Corina U.Greven", "MartineGroefsema", "KoenHaak", "PeterHagoort", "YvonneHartman", "Beatricevan der Heijden", "ErnoHermans", "VivianHeuvelmans", "FlorianHintz", "Janetden Hollander", "Anneloes M.Hulsman", "SebastianIdesis", "MartinJaeger", "EstherJanse", "JoostJanzing", "Roy P. C.Kessels", "Johan C.Karremans", "Willemiende Kleijn", "MariekeKlein", "FlorisKlumpers", "NilsKohn", "HubertKorzilius", "BasKrahmer", "Florisde Lange", "Judithvan Leeuwen", "HuaiyuLiu", "MaartjeLuijten", "PeggyManders", "KaterinaManevska", "Jos\u00e9 P.Marques", "JonMatthews", "JamesM. McQueen", "PieterMedendorp", "Ren\u00e9Melis", "AntjeMeyer", "JoukjeOosterman", "LucyOverbeek", "MariusPeelen", "JeanPopma", "GeertPostma", "KarinRoelofs", "Yvonne G. T.van Rossenberg", "GabiSchaap", "PaulScheepers", "LucSelen", "MarianneStarren", "Dorine W.Swinkels", "IndiraTendolkar", "DickThijssen", "HansTimmerman", "RayyanTutunji", "AnilTuladhar", "HarmVeling", "MaaikeVerhagen", "JasperVerkroost", "JacquelineVink", "VivianVriezekolk", "JannaVrijsen", "JanaVyrastekova", "Selinavan der Wal", "RoelWillems", "ArthurWillemsen"], "doi": "10.1371/journal.pone.0260952\n10.1007/s11229-016-1239-1\n10.3389/fnhum.2015.00237\n10.1016/s0166-2236(97)01149-1\n10.1126/science.1209603\n10.1016/j.nicl.2019.101933\n10.1016/j.jad.2017.12.106\n10.1002/da.22130\n10.1016/j.maturitas.2018.01.016\n10.1016/j.psyneuen.2016.07.201\n10.1371/journal.pone.0090731\n10.1001/archinte.164.8.863\n10.1136/heartjnl-2014-305623\n10.7554/eLife.44443\n10.1038/nn.4125\n10.1002/wps.20513\n10.1080/17470919.2014.994786\n10.1093/aje/kwx246\n10.1017/S0140525X0999152X\n10.1007/s10654-006-9022-0\n10.1016/j.dcn.2017.10.002\n10.1111/desc.12763\n10.1007/s10654-007-9199-x\n10.1371/journal.pmed.1001779\n10.1016/j.arr.2020.101184\n10.1038/nn1008\n10.1016/j.tins.2016.12.005\n10.1001/jama.2013.281053\n10.1016/j.addbeh.2004.05.023\n10.1111/j.1360-0443.1993.tb02093.x\n10.1093/ajcn/58.4.489\n10.1017/S0007114510000401\n10.1017/S0007114511000067\n10.1038/sj.ejcn.1602581\n10.1123/jpah.7.6.697\n10.1016/0165-1781(89)90047-4\n10.2466/pms.98.3c.1422-1426\n10.1037/pas0000062\n10.1207/s15327752jpa4901_13\n10.1177/1073191111408231\n10.1152/jappl.1954.7.2.218\n10.1249/MSS.0b013e31820ce174\n10.1136/bjsm.2006.027276\n10.1111/j.1469-8986.1991.tb01999.x\n10.1016/s0167-8760(03)00143-0\n10.1016/s0304-3959(97)00005-5\n10.1186/1471-2377-14-94\n10.2147/JPR.S154698\n10.1161/CIRCHEARTFAILURE.112.000186\n10.1097/HJH.0000000000001274\n10.1126/science.1237439\n10.3389/fnagi.2014.00004\n10.1016/j.psyneuen.2008.03.001\n10.1093/geront/gnu174\n10.1021/acs.est.7b05039\n10.1016/j.neuroimage.2013.04.127\n10.3389/fpsyg.2014.00271\n10.1016/j.neuroimage.2009.10.002\n10.1016/j.neuroimage.2012.05.049\n10.3758/s13415-015-0350-y\n10.1523/JNEUROSCI.2189-17.2017\n10.1093/brain/awv329\n10.1002/hipo.20794\n10.1080/09084282.2012.670150\n10.1016/j.cortex.2014.09.014\n10.1037/a0027600\n10.1037//0021-843x.90.4.286\n10.1111/acer.13165\n10.1037/a0014983\n10.1037/xap0000112\n10.1016/j.ijheh.2015.08.006\n10.1017/s0033291704002892\n10.1038/mp.2017.98\n10.1017/s0033291700035558\n10.1016/s0005-7967(03)00074-3\n10.1002/ab.21458\n10.1007/BF00844845\n10.1016/s0145-2134(02)00541-0\n10.1111/j.1600-0447.1990.tb01360.x\n10.1037//0022-3514.39.3.472\n10.1080/00223891.1990.9674095\n10.1037/a0034751\n10.1177/1088868311434213\n10.1002/1097-4679(199511)51:6&lt;768::aid-jclp2270510607&gt;3.0.co;2-1\n10.1111/j.0022-3506.2004.00263.x\n10.1037/pspp0000031\n10.1037/a0019265\n10.1002/bdm.1751\n10.1177/0272989X10373805\n10.1016/j.euroecorev.2015.01.008\n10.1037/xlm0000768\n10.1037/0033-295x.103.3.403\n10.1093/geronb/50b.1.p33\n10.3758/s13428-010-0024-1\n10.1037/a0033861\n10.1006/brcg.1998.1039\n10.5334/joc.95\n10.1016/j.bandl.2016.08.006\n10.3389/fpsyg.2014.00772\n10.3389/fpsyg.2017.01164\n10.1177/0023830920911079\n10.1016/s0010-0277(99)00059-1\n10.1037/xlm0000388\n10.1073/pnas.1011492107\n10.7326/M14-1651\n10.1016/S0140-6736(16)30370-1\n10.7326/M17-0212\n10.1073/pnas.2001284117\n10.1371/journal.pone.0087756\n10.1186/s40249-020-0622-9\n10.1016/j.neuroimage.2012.02.018\n10.1038/nn.4393\n10.1186/s12883-019-1394-3\n10.1371/journal.pone.0169649\n10.1016/j.jamda.2017.04.017"}
{"title": "Contribution of artificial intelligence applications developed with the deep learning method to the diagnosis of COVID-19 pneumonia on computed tomography.", "abstract": "Computed tomography (CT) is an auxiliary modality in the diagnosis of the novel Coronavirus (COVID-19) disease and can guide physicians in the presence of lung involvement. In this study, we aimed to investigate the contribution of deep learning to diagnosis in patients with typical COVID-19 pneumonia findings on CT.\nThis study retrospectively evaluated 690 lesions obtained from 35 patients diagnosed with COVID-19 pneumonia based on typical findings on non-contrast high-resolution CT (HRCT) in our hospital. The diagnoses of the patients were also confirmed by other necessary tests. HRCT images were assessed in the parenchymal window. In the images obtained, COVID-19 lesions were detected. For the deep Convolutional Neural Network (CNN) algorithm, the Confusion matrix was used based on a Tensorflow Framework in Python.\nA total of 596 labeled lesions obtained from 224 sections of the images were used for the training of the algorithm, 89 labeled lesions from 27 sections were used in validation, and 67 labeled lesions from 25 images in testing. Fifty-six of the 67 lesions used in the testing stage were accurately detected by the algorithm while the remaining 11 were not recognized. There was no false positive. The Recall, Precision and F1 score values in the test group were 83.58, 1, and 91.06, respectively.\nWe successfully detected the COVID-19 pneumonia lesions on CT images using the algorithms created with artificial intelligence. The integration of deep learning into the diagnostic stage in medicine is an important step for the diagnosis of diseases that can cause lung involvement in possible future pandemics.", "journal": "Tuberkuloz ve toraks", "date": "2021-12-28", "authors": ["NevinAyd\u0131n", "\u00d6zer\u00c7elik"], "doi": "10.5578/tt.20219606"}
{"title": "xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography.", "abstract": "", "journal": "IEEE journal of translational engineering in health and medicine", "date": "2021-12-28", "authors": ["Arnab KumarMondal", "ArnabBhattacharjee", "ParagSingla", "A PPrathosh"], "doi": "10.1109/JTEHM.2021.3134096"}
{"title": "COVID-19 Pandemic-Associated Changes in the Acuity of Brain MRI Findings: A Secondary Analysis of Reports Using Natural Language Processing.", "abstract": "We aimed to assess early COVID-19 pandemic-associated changes in brain MRI examination frequency and acuity of imaging findings acuity.\nUsing a natural language processing model, we retrospectively categorized reported findings of 12,346 brain MRI examinations performed during 6-month pre-pandemic and early pandemic time periods across a large metropolitan health system into 3 acuity levels: (1) normal or near normal; (2) incidental or chronic findings not requiring a management change; and (3) new or progressive findings requiring a management change. Brain MRI frequency and imaging finding acuity level were compared over time.\nBetween March and August of 2019 (pre-pandemic) and 2020 (early pandemic), our health system brain MRI examination volumes decreased 17.0% (6745 vs 5601). Comparing calendar-matched 6-month periods, the proportion of higher acuity findings increased significantly (P< 0.001) from pre-pandemic (22.5%, 43.6% and 34.0% in acuity level 1, 2, and 3, respectively) to early pandemic periods (19.1%, 40.9%, and 40.1%). During the second 3 months of the early pandemic period, as MRI volumes recovered to near baseline, the proportion of higher acuity findings remained high (42.6% vs 34.1%) compared with a similar pre-pandemic period. In a multivariable analysis, Black (B coefficient, 0.16) and underinsured population (B coefficient, 0.33) presented with higher acuity findings (P< 0.05).\nAs the volume of brain MRI examinations decreased during the early COVID-19 pandemic, the relative proportion of examinations with higher acuity findings increased significantly. Pandemic-related changes in patient outcomes related to reduced imaging access merits further attention.", "journal": "Current problems in diagnostic radiology", "date": "2021-12-28", "authors": ["Taejin LMin", "LiyanXu", "Jinho DChoi", "RanliangHu", "Jason WAllen", "ChristopherReeves", "DerekHsu", "RichardDuszak", "JeffreySwitchenko", "GelarehSadigh"], "doi": "10.1067/j.cpradiol.2021.11.001"}
{"title": "Review and classification of AI-enabled COVID-19 CT imaging models based on computer vision tasks.", "abstract": "This article presents a systematic overview of artificial intelligence (AI) and computer vision strategies for diagnosing the coronavirus disease of 2019 (COVID-19) using computerized tomography (CT) medical images. We analyzed the previous review works and found that all of them ignored classifying and categorizing COVID-19 literature based on computer vision tasks, such as classification, segmentation, and detection. Most of the COVID-19 CT diagnosis methods comprehensively use segmentation and classification tasks. Moreover, most of the review articles are diverse and cover CT as well as X-ray images. Therefore, we focused on the COVID-19 diagnostic methods based on CT images. Well-known search engines and databases such as Google, Google Scholar, Kaggle, Baidu, IEEE Xplore, Web of Science, PubMed, ScienceDirect, and Scopus were utilized to collect relevant studies. After deep analysis, we collected 114 studies and reported highly enriched information for each selected research. According to our analysis, AI and computer vision have substantial potential for rapid COVID-19 diagnosis as they could significantly assist in automating the diagnosis process. Accurate and efficient models will have real-time clinical implications, though further research is still required. Categorization of literature based on computer vision tasks could be helpful for future research; therefore, this review article will provide a good foundation for conducting such research.", "journal": "Computers in biology and medicine", "date": "2021-12-26", "authors": ["HaseebHassan", "ZhaoyuRen", "HuishiZhao", "ShoujinHuang", "DanLi", "ShaohuaXiang", "YanKang", "SifanChen", "BingdingHuang"], "doi": "10.1016/j.compbiomed.2021.105123\n10.1016/s0140-6736(20)30183-5\n10.1056/nejmoa2002032\n10.1056/NEJMe2002387\n10.1093/clinchem/hvaa071\n10.1002/jmv.25678\n10.3390/diagnostics10060434\n10.1016/j.jinf.2020.03.005\n10.1056/NEJMp2015897\n10.1007/s00330-020-07347-x\n10.1016/j.jcv.2020.104455\n10.1016/j.puhe.2020.04.009\n10.1080/17476348.2021.1917389\n10.1016/j.patrec.2019.11.013\n10.1109/TMI.2020.2996645\n10.1007/s12195-020-00629-w\n10.1016/S2666-5247(20)30068-9\n10.1007/s00330-020-06801-0\n10.1016/j.patcog.2020.107747\n10.1148/radiol.2020201491\n10.1038/s41598-020-76282-0\n10.1101/2020.03.19.20039354\n10.1007/s11390-020-0679-8\n10.1016/j.chaos.2020.110059\n10.1109/ACCESS.2020.3007939\n10.1155/2020/9756518\n10.1111/codi.15252\n10.1109/ACCESS.2021.3058537\n10.1101/2020.04.13.20063941\n10.1080/07391102.2020.1788642\n10.1016/j.ejrad.2020.109041\n10.1007/s00330-021-07715-1\n10.1155/2020/8843664\n10.1007/s10140-020-01886-y\n10.1101/2020.05.11.20097907\n10.1038/s41551-020-00633-5\n10.1016/j.irbm.2020.05.003\n10.1109/TII.2021.3057524\n10.1007/s10489-020-02149-6\n10.1016/j.eng.2020.04.010\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2994908\n10.1016/j.patrec.2020.10.001\n10.1155/2021/5522729\n10.1016/j.chaos.2020.110153\n10.1016/B978-0-08-051581-6.50065-9\n10.4236/jbise.2020.137014\n10.1101/2020.04.24.20078998\n10.3390/s19224827\n10.1016/j.compbiomed.2021.104348\n10.1016/j.bspc.2021.102588\n10.1016/j.patcog.2021.107826\n10.1101/2021.04.19.21255763\n10.1007/s12539-020-00408-1\n10.1007/s12559-020-09785-7\n10.1145/3065386\n10.1007/s10489-020-01965-0\n10.21437/Interspeech.2018-1301\n10.1016/j.compbiomed.2021.104306\n10.1016/j.advengsoft.2016.01.008\n10.1109/ICCV.1998.710790\n10.1016/j.cell.2020.04.045\n10.1109/TMI.2020.3000314\n10.1002/ima.22527\n10.1145/3453892.3461322\n10.1109/TMI.2021.3066161\n10.1109/ACCESS.2021.3067047\n10.1109/TII.2021.3059023\n10.1109/TNNLS.2021.3054746\n10.1016/j.knosys.2020.106647\n10.1109/TPAMI.2019.2938758\n10.1016/j.cmpbup.2021.100007\n10.1109/TMI.2020.2995108\n10.1016/j.compbiomed.2020.104037\n10.1016/j.media.2020.101836\n10.1109/TIP.2021.3058783\n10.1007/s12559-020-09751-3\n10.1109/3DV.2016.79\n10.1148/ryai.2020200048\n10.1109/TMI.2020.3001810\n10.1016/j.patcog.2021.107828\n10.1002/ima.22525\n10.1007/978-3-030-32245-8_67\n10.1002/mp.14676\n10.1023/B:JMIV.0000011325.36760.1e\n10.3109/15412550903499522\n10.1183/13993003.00775-2020\n10.1109/TMI.2020.2995965\n10.1007/s10489-020-01826-w\n10.1101/2020.06.08.20125963\n10.1002/ima.22558\n10.1038/s41467-020-17971-2\n10.1109/ACCESS.2020.3005510\n10.1007/978-3-030-00889-5_1\n10.1007/s10489-020-01831-z\n10.1109/IPTA.2008.4743780\n10.1016/j.asoc.2020.106897\n10.1007/s00330-020-07044-9\n10.1007/s00259-020-04953-1\n10.3390/s21020455\n10.1145/2939672.2939778\n10.3390/diagnostics11050893\n10.1088/1361-6560/abc04e/meta\n10.1016/j.patcog.2021.108006\n10.1007/978-3-030-00934-2_94\n10.1002/mp.13141\n10.1109/TMI.2010.2076300\n10.7150/thno.46465\n10.1007/978-3-030-32226-7_2\n10.1109/TIP.2008.2002304\n10.1109/TCBB.2021.3065361\n10.1038/s41597-021-00900-3\n10.17632/zxmcf7553w.1\n10.3390/bioengineering8020026\n10.1148/radiol.2021203957\n10.1038/s41597-020-00741-6\n10.1007/s00521-020-05437-x\n10.1364/BOE.9.003244\n10.1016/j.neunet.2020.07.010\n10.1109/TPAMI.2017.2699184\n10.1007/s12021-018-9377-x\n10.1016/j.compbiomed.2019.03.014\n10.31224/osf.io/9fdyp"}
{"title": "Automatic coronavirus disease 2019 diagnosis based on chest radiography and deep learning - Success story or dataset bias?", "abstract": "Over the last 2 years, the artificial intelligence (AI) community has presented several automatic screening tools for coronavirus disease 2019 (COVID-19) based on chest radiography (CXR), with reported accuracies often well over 90%. However, it has been noted that many of these studies have likely suffered from dataset bias, leading to overly optimistic results. The purpose of this study was to thoroughly investigate to what extent biases have influenced the performance of a range of previously proposed and promising convolutional neural networks (CNNs), and to determine what performance can be expected with current CNNs on a realistic and unbiased dataset.\nFive CNNs for COVID-19 positive/negative classification were implemented for evaluation, namely VGG19, ResNet50, InceptionV3, DenseNet201, and COVID-Net. To perform both internal and cross-dataset evaluations, four datasets were created. The first dataset Valencian Region Medical Image Bank (BIMCV) followed strict reverse transcriptase-polymerase chain reaction (RT-PCR) test criteria and was created from a single reliable open access databank, while the second dataset (COVIDxB8) was created through a combination of six online CXR repositories. The third and fourth datasets were created by combining the opposing classes from the BIMCV and COVIDxB8 datasets. To decrease inter-dataset variability, a pre-processing workflow of resizing, normalization, and histogram equalization were applied to all datasets. Classification performance was evaluated on unseen test sets using precision and recall. A qualitative sanity check was performed by evaluating saliency maps displaying the top 5%, 10%, and 20% most salient segments in the input CXRs, to evaluate whether the CNNs were using relevant information for decision making. In an additional experiment and to further investigate the origin of potential dataset bias, all pixel values outside the lungs were set to zero through automatic lung segmentation before training and testing.\nWhen trained and evaluated on the single online source dataset (BIMCV), the performance of all CNNs is relatively low (precision: 0.65-0.72, recall: 0.59-0.71), but remains relatively consistent during external evaluation (precision: 0.58-0.82, recall: 0.57-0.72). On the contrary, when trained and internally evaluated on the combinatory datasets, all CNNs performed well across all metrics (precision: 0.94-1.00, recall: 0.77-1.00). However, when subsequently evaluated cross-dataset, results dropped substantially (precision: 0.10-0.61, recall: 0.04-0.80). For all datasets, saliency maps revealed the CNNs rarely focus on areas inside the lungs for their decision-making. However, even when setting all pixel values outside the lungs to zero, classification performance does not change and dataset bias remains.\nResults in this study confirm that when trained on a combinatory dataset, CNNs tend to learn the origin of the CXRs rather than the presence or absence of disease, a behavior known as short-cut learning. The bias is shown to originate from differences in overall pixel values rather than embedded text or symbols, despite consistent image pre-processing. When trained on a reliable, and realistic single-source dataset in which non-lung pixels have been masked, CNNs currently show limited sensitivity\u00a0(<70%) for COVID-19 infection in CXR, questioning their use as a reliable automatic screening tool.", "journal": "Medical physics", "date": "2021-12-25", "authors": ["JenniferDhont", "CecileWolfs", "FrankVerhaegen"], "doi": "10.1002/mp.15419\n10.1101/2020.04.21.20063263\n10.1101/2020.05.24.20111922\n10.1109/access.2020.3010287\n10.1109/BIBM49941.2020.9313304\n10.1109/CVPR.2011.5995347\n10.1109/CVPR.2016.90\n10.1109/ICCC51575.2020.9344870\n10.1109/ICCV.2019.00505\n10.1101/2021.02.11.20196766\n10.1101/2020.04.24.20078949\n10.1109/ICIIP47207.2019.8985892"}
{"title": "Commercial AI solutions in detecting COVID-19 pneumonia in chest CT: not yet ready for clinical implementation?", "abstract": "In response to the COVID-19 pandemic, many researchers have developed artificial intelligence (AI) tools to differentiate COVID-19 pneumonia from other conditions in chest CT. However, in many cases, performance has not been clinically validated. The aim of this study was to evaluate the performance of commercial AI solutions in differentiating COVID-19 pneumonia from other lung conditions.\nFour commercial AI solutions were evaluated on a dual-center clinical dataset consisting of 500 CT studies; COVID-19 pneumonia was microbiologically proven in 50 of these. Sensitivity, specificity, positive and negative predictive values, and AUC were calculated. In a subgroup analysis, the performance of the AI solutions in differentiating COVID-19 pneumonia from other conditions was evaluated in CT studies with ground-glass opacities (GGOs).\nSensitivity and specificity ranges were 62-96% and 31-80%, respectively. Negative and positive predictive values ranged between 82-99% and 19-25%, respectively. AUC was in the range 0.54-0.79. In CT studies with GGO, sensitivity remained unchanged. However, specificity was lower, and ranged between 15 and 53%. AUC for studies with GGO was in the range 0.54-0.69.\nThis study highlights the variable specificity and low positive predictive value of AI solutions in diagnosing COVID-19 pneumonia in chest CT. However, one solution yielded acceptable values for sensitivity. Thus, with further improvement, commercial AI solutions currently under development have the potential to be integrated as alert tools in clinical routine workflow. Randomized trials are needed to assess the true benefits and also potential harms of the use of AI in image analysis.\n\u2022 Commercial AI solutions achieved a sensitivity and specificity ranging from 62 to 96% and from 31 to 80%, respectively, in identifying patients suspicious for COVID-19 in a clinical dataset. \u2022 Sensitivity remained within the same range, while specificity was even lower in subgroup analysis of CT studies with ground-glass opacities, and interrater agreement between the commercial AI solutions was minimal to nonexistent. \u2022 Thus, commercial AI solutions have the potential to be integrated as alert tools for the detection of patients with lung changes suspicious for COVID-19 pneumonia in a clinical routine workflow, if further improvement is made.", "journal": "European radiology", "date": "2021-12-25", "authors": ["FlorianJungmann", "LukasM\u00fcller", "FelixHahn", "MaximilianWeustenfeld", "Ann-KathrinDapper", "AlineM\u00e4hringer-Kunz", "DirkGraafen", "ChristophD\u00fcber", "DariusSchafigh", "DanielPinto Dos Santos", "PeterMildenberger", "RomanKloeckner"], "doi": "10.1007/s00330-021-08409-4\n10.1016/j.tmaid.2020.101619\n10.1148/radiol.20202040130\n10.1148/radiol.2020201343\n10.1097/RLI.0000000000000700\n10.1148/radiol.2020201473\n10.1055/a-1388-7950\n10.1038/s41586-019-1799-6\n10.1148/radiol.2020200905\n10.1038/s41467-019-13993-7\n10.1007/s00330-020-07553-7\n10.1148/radiol.2020201491\n10.1038/s42256-021-00307-0\n10.1148/radiol.2015151516\n10.1148/ryct.2020200034\n10.1183/09031936.00047908\n10.1016/S2589-7500(20)30199-0\n10.1007/s00330-021-07937-3\n10.1007/s00330-020-07347-x\n10.1186/s13244-020-00956-6\n10.1007/s00330-020-07453-w\n10.1007/s11547-020-01197-9\n10.1007/s00330-020-07684-x\n10.1016/S0140-6736(19)32498-5"}
{"title": "A Deep Learning Ensemble Approach for Automated COVID-19 Detection from Chest CT Images.", "abstract": "The aim of this study was to evaluate the performance of an automated COVID-19 detection method based on a transfer learning technique that makes use of chest computed tomography (CT) images.\nIn this study, we used a publicly available multiclass CT scan dataset containing 4171 CT scans of 210 different patients. In particular, we extracted features from the CT images using a set of convolutional neural networks (CNNs) that had been pretrained on the ImageNet dataset as feature extractors, and we then selected a subset of these features using the Information Gain filter. The resulting feature vectors were then used to train a set of k Nearest Neighbors classifiers with 10-fold cross validation to assess the classification performance of the features that had been extracted by each CNN. Finally, a majority voting approach was used to classify each image into two different classes: COVID-19 and NO COVID-19.\nA total of 414 images of the test set (10% of the complete dataset) were correctly classified, and only 4 were misclassified, yielding a final classification accuracy of 99.04%.\nThe high performance that was achieved by the method could make it feasible option that could be used to assist radiologists in COVID-19 diagnosis through the use of CT images.", "journal": "Journal of clinical medicine", "date": "2021-12-25", "authors": ["GaetanoZazzaro", "FrancescoMartone", "GianpaoloRomano", "LuigiPavone"], "doi": "10.3390/jcm10245982\n10.1016/S2468-2667(20)30074-8\n10.1007/s10044-021-00984-y\n10.1148/radiol.2020200330\n10.1128/JCM.00297-20\n10.3348/kjr.2020.0195\n10.1001/jama.2020.8259\n10.1016/j.jinf.2020.03.007\n10.1148/radiol.2020200230\n10.1097/JCMA.0000000000000336\n10.1136/bmjopen-2020-042946\n10.1148/radiol.2020200823\n10.1016/j.artmed.2020.101935\n10.1007/s13246-020-00865-4\n10.1109/JAS.2020.1003393\n10.1007/s10489-020-01902-1\n10.1016/j.bbe.2021.05.013\n10.1117/12.2588672\n10.3390/s21020455\n10.1016/j.eng.2020.04.010\n10.1007/s00330-021-07715-1\n10.1007/s42979-021-00782-7\n10.1016/j.iot.2021.100377\n10.1101/2020.04.24.20078584\n10.3390/app11178227\n10.1109/TKDE.2009.191\n10.1109/JPROC.2020.3004555\n10.1109/cvpr.2017.243\n10.1109/cvpr.2015.7298594\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.195\n10.1109/INISTA.2017.8001122\n10.1007/BF00153759\n10.11591/eei.v9i1.1464"}
{"title": "3D virtual histopathology of cardiac tissue from Covid-19 patients based on phase-contrast X-ray tomography.", "abstract": "For the first time, we have used phase-contrast X-ray tomography to characterize the three-dimensional (3d) structure of cardiac tissue from patients who succumbed to Covid-19. By extending conventional histopathological examination by a third dimension, the delicate pathological changes of the vascular system of severe Covid-19 progressions can be analyzed, fully quantified and compared to other types of viral myocarditis and controls. To this end, cardiac samples with a cross-section of 3.5mm were scanned at a laboratory setup as well as at a parallel beam setup at a synchrotron radiation facility the synchrotron in a parallel beam configuration. The vascular network was segmented by a deep learning architecture suitable for 3d datasets (V-net), trained by sparse manual annotations. Pathological alterations of vessels, concerning the variation of diameters and the amount of small holes, were observed, indicative of elevated occurrence of intussusceptive angiogenesis, also confirmed by high-resolution cone beam X-ray tomography and scanning electron microscopy. Furthermore, we implemented a fully automated analysis of the tissue structure in the form of shape measures based on the structure tensor. The corresponding distributions show that the histopathology of Covid-19 differs from both influenza and typical coxsackie virus myocarditis.", "journal": "eLife", "date": "2021-12-22", "authors": ["MariusReichardt", "PatrickMoller Jensen", "VedranaAndersen Dahl", "AndersBjorholm Dahl", "MaximilianAckermann", "HarshitShah", "FlorianL\u00e4nger", "ChristopherWerlein", "Mark PKuehnel", "DannyJonigk", "TimSalditt"], "doi": "10.7554/eLife.71359\n10.1007/s10456-012-9294-9\n10.1007/978-1-4939-1462-3_5\n10.1183/13993003.03147-2020\n10.1056/NEJMoa2015432\n10.1093/eurheartj/ehaa092\n10.1161/CIRCULATIONAHA.120.050097\n10.1063/1.4818737\n10.1161/CIRCULATIONAHA.120.050754\n10.1007/s00059-020-04909-z\n10.1063/1.125225\n10.1364/josaa.26.000890\n10.1038/s41598-019-43407-z\n10.1016/j.ijcard.2020.03.087\n10.7554/eLife.60408\n10.1097/SLA.0b013e31820563a8\n10.1107/S1600577520011327\n10.1080/09540091.2012.664122\n10.1007/s00414-020-02500-z\n10.1016/j.carpath.2020.107300\n10.1016/j.cell.2020.02.052\n10.1007/978-3-030-20205-7\n10.1016/j.jacc.2020.11.031\n10.1088/1367-2630/aa764b\n10.1007/s10853-009-4016-4\n10.1006/cgip.1994.1042\n10.1107/S1600577520002398\n10.1111/his.14134\n10.1007/s10456-014-9428-3\n10.1093/bioinformatics/btz423\n10.1109/3DV.2016.79\n10.1038/s41569-020-0413-9\n10.1117/1.JMI.7.2.023501\n10.1101/2021.09.16.460594\n10.1002/ejhf.1828\n10.1073/pnas.1801678115\n10.1364/opex.12.002960\n10.1016/j.ultramic.2015.05.002\n10.1364/OE.24.025129\n10.1101/2021.02.03.429481\n10.1093/cvr/cvaa160\n10.1016/s1361-8415(02)00053-1\n10.7326/M20-2003\n10.1007/s15010-020-01424-5\n10.1038/s41569-020-0360-5"}
{"title": "The COVID-19 epidemic analysis and diagnosis using deep learning: A systematic literature review and future directions.", "abstract": "Since December 2019, the COVID-19 outbreak has resulted in countless deaths and has harmed all facets of human existence. COVID-19 has been designated an epidemic by the World Health Organization (WHO), which has placed a tremendous burden on nearly all countries, especially those with weak health systems. However, Deep Learning (DL) has been applied in several applications and many types of detection applications in the medical field, including thyroid diagnosis, lung nodule recognition, fetal localization, and detection of diabetic retinopathy. Furthermore, various clinical imaging sources, like Magnetic Resonance Imaging (MRI), X-ray, and Computed Tomography (CT), make DL a perfect technique to tackle the epidemic of COVID-19. Inspired by this fact, a considerable amount of research has been done. A Systematic Literature Review (SLR) has been used in this study to discover, assess, and integrate findings from relevant studies. DL techniques used in COVID-19 have also been categorized into seven main distinct categories as Long Short Term Memory Networks (LSTM), Self-Organizing Maps (SOMs), Conventional Neural Networks (CNNs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs), Autoencoders, and hybrid approaches. Then, the state-of-the-art studies connected to DL techniques and applications for health problems with COVID-19 have been highlighted. Moreover, many issues and problems associated with DL implementation for COVID-19 have been addressed, which are anticipated to stimulate more investigations to control the prevalence and disaster control in the future. According to the findings, most papers are assessed using characteristics such as accuracy, delay, robustness, and scalability. Meanwhile, other features are underutilized, such as security and convergence time. Python is also the most commonly used language in papers, accounting for 75% of the time. According to the investigation, 37.83% of applications have identified chest CT/chest X-ray images for patients.", "journal": "Computers in biology and medicine", "date": "2021-12-21", "authors": ["ArashHeidari", "NimaJafari Navimipour", "MehmetUnal", "ShivaToumaj"], "doi": "10.1016/j.compbiomed.2021.105141"}
{"title": "Clinical Applicable AI System Based on Deep Learning Algorithm for Differentiation of Pulmonary Infectious Disease.", "abstract": "", "journal": "Frontiers in medicine", "date": "2021-12-21", "authors": ["Yu-HanZhang", "Xiao-FeiHu", "Jie-ChaoMa", "Xian-QiWang", "Hao-RanLuo", "Zi-FengWu", "ShuZhang", "De-JunShi", "Yi-ZhouYu", "Xiao-MingQiu", "Wen-BingZeng", "WeiChen", "JianWang"], "doi": "10.3389/fmed.2021.753055\n10.1016/S0140-6736(18)32203-7\n10.1086/431588\n10.1038/s41467-019-12898-9\n10.3390/jcm8040514\n10.1148/radiol.2020200905\n10.1148/ryct.2020200075\n10.1109/TMI.2020.2994908\n10.1148/radiol.2021204522\n10.1016/j.cell.2018.02.010\n10.3390/app8101715\n10.1183/13993003.00398-2020\n10.1016/j.cell.2020.04.045\n10.7150/thno.46465\n10.1038/s41551-018-0304-0\n10.1145/2939672.2939785\n10.1080/10629360500107527\n10.11613/BM.2013.018\n10.1111/j.1558-5646.1995.tb04456.x\n10.1101/2020.05.16.20103408\n10.3390/jcm9010248\n10.1148/radiol.2020200230\n10.1148/radiol.2020200463\n10.2214/AJR.17.17857\n10.1148/radiol.2020200432\n10.1038/s41598-020-80061-2\n10.1016/j.ejrad.2020.108961\n10.1002/sim.5328"}
{"title": "Automatic detection of COVID-19 in chest radiographs using serially concatenated deep and handcrafted features.", "abstract": "Since the infectious disease occurrence rate in the human community is gradually rising due to varied reasons, appropriate diagnosis and treatments are essential to control its spread. The recently discovered COVID-19 is one of the contagious diseases, which infected numerous people globally. This contagious disease is arrested by several diagnoses and handling actions. Medical image-supported diagnosis of COVID-19 infection is an approved clinical practice. This research aims to develop a new Deep Learning Method (DLM) to detect the COVID-19 infection using the chest X-ray. The proposed work implemented two methods namely, detection of COVID-19 infection using (i) a Firefly Algorithm (FA) optimized deep-features and (ii) the combined deep and machine features optimized with FA. In this work, a 5-fold cross-validation method is engaged to train and test detection methods. The performance of this system is analyzed individually resulting in the confirmation that the deep feature-based technique helps to achieve a detection accuracy of >\u200a92% with SVM-RBF classifier and combining deep and machine features achieves >\u200a96% accuracy with Fine KNN classifier. In the future, this technique may have potential to play a vital role in testing and validating the X-ray images collected from patients suffering from the infection diseases.", "journal": "Journal of X-ray science and technology", "date": "2021-12-21", "authors": ["SRajesh Kannan", "JSivakumar", "PEzhilarasi"], "doi": "10.3233/XST-211050"}
{"title": "Recent advances in methods for the diagnosis of Corona Virus Disease 2019.", "abstract": "Since the beginning of the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) pandemic, it has been clear that effective methods for the diagnosis of Corona Virus Disease 2019 (COVID-19) are the key tools to control its epidemic. The current gold standard for diagnosing COVID-19 is the real-time quantitative reverse transcription-polymerase chain reaction (qRT-PCR), which is a sensitive and specific method to detect SARS-CoV-2. Other RNA-based methods include RNA sequencing (RNA-seq), droplet digital reverse transcription-polymerase chain reaction (ddRT-PCR), reverse transcription loop-mediated isothermal amplification (RT-LAMP), and clustered regularly interspaced short palindromic repeats (CRISPR). The serological testing of antibodies (IgM and IgG), nanoparticle-based lateral-flow assay, and enzyme-linked immunosorbent assay (ELISA) can be used to enhance the detection sensitivity and accuracy. Because antibodies are usually detected a week after the onset of symptoms, these tests are used to assess the overall infection rate in the community. Sine the fact that healthcare varies from country to country across the world, different types of diagnosing COVID-19 imaging technologies including chest computed tomography (CT), chest radiography, and lung ultrasound are used in different degrees. Besides, the pooling test is an important public health tool to reduce cost and increase testing capacity in low-risk area, while artificial intelligence (AI) may aid to increase the diagnostic efficiency of imaging-based methods. Finally, depending on the type of samples and stages of the disease, a combination of information on patient demographics and histories, clinical symptoms, results of molecular and serological diagnostic tests, and imaging information is highly recommended to achieve adequate diagnosis of patients with COVID-19.", "journal": "Journal of clinical laboratory analysis", "date": "2021-12-19", "authors": ["JieGuo", "JiaxinGe", "YananGuo"], "doi": "10.1002/jcla.24178\n10.1097/CM9.0000000000001627\n10.7554/eLife.61312\n10.1101/2020.02.27.20028787"}
{"title": "Automated COVID-19 diagnosis and prognosis with medical imaging and who is publishing: a systematic review.", "abstract": "\u00a0To conduct a systematic survey of published techniques for automated diagnosis and prognosis of COVID-19 diseases using medical imaging, assessing the validity of reported performance and investigating the proposed clinical use-case. To conduct a scoping review into the authors publishing such work.\n\u00a0The Scopus database was queried and studies were screened for article type, and minimum source normalized impact per paper and citations, before manual relevance assessment and a bias assessment derived from a subset of the Checklist for Artificial Intelligence in Medical Imaging (CLAIM). The number of failures of the full CLAIM was adopted as a surrogate for risk-of-bias. Methodological and performance measurements were collected from each technique. Each study was assessed by one author. Comparisons were evaluated for significance with a two-sided independent t-test.\n\u00a0Of 1002 studies identified, 390 remained after screening and 81 after relevance and bias exclusion. The ratio of exclusion for bias was 71%, indicative of a high level of bias in the field. The mean number of CLAIM failures per study was 8.3\u2009\u00b1\u20093.9 [1,17] (mean\u2009\u00b1\u2009standard deviation [min,max]). 58% of methods performed diagnosis versus 31% prognosis. Of the diagnostic methods, 38% differentiated COVID-19 from healthy controls. For diagnostic techniques, area under the receiver operating curve (AUC)\u2009=\u20090.924\u2009\u00b1\u20090.074 [0.810,0.991] and accuracy\u2009=\u200991.7%\u2009\u00b1\u20096.4 [79.0,99.0]. For prognostic techniques, AUC\u2009=\u20090.836\u2009\u00b1\u20090.126 [0.605,0.980] and accuracy\u2009=\u200978.4%\u2009\u00b1\u20099.4 [62.5,98.0]. CLAIM failures did not correlate with performance, providing confidence that the highest results were not driven by biased papers. Deep learning techniques reported higher AUC (p\u2009<\u20090.05) and accuracy (p\u2009<\u20090.05), but no difference in CLAIM failures was identified.\n\u00a0A majority of papers focus on the less clinically impactful diagnosis task, contrasted with prognosis, with a significant portion performing a clinically unnecessary task of differentiating COVID-19 from healthy. Authors should consider the clinical scenario in which their work would be deployed when developing techniques. Nevertheless, studies report superb performance in a potentially impactful application. Future work is warranted in translating techniques into clinical tools.", "journal": "Physical and engineering sciences in medicine", "date": "2021-12-18", "authors": ["Ashley GGillman", "FebrioLunardo", "JosephPrinable", "GreggBelous", "AaronNicolson", "HangMin", "AndrewTerhorst", "Jason ADowling"], "doi": "10.1007/s13246-021-01093-0\n10.1016/S0140-6736(21)02758-6\n10.1148/radiol.2020200343\n10.1148/radiol.2020200527\n10.1148/ryct.2020200152\n10.1148/radiol.2020203173\n10.1002/14651858.CD013639.pub4\n10.1038/s42256-021-00338-7\n10.1016/j.media.2021.102225\n10.1109/RBME.2020.2987975\n10.1016/j.jiph.2020.06.028\n10.21203/rs.3.rs-30432/v1\n10.1136/bmj.m1328\n10.7326/M18-1377\n10.1038/s42256-021-00307-0\n10.1148/ryai.2020200029\n10.1038/nrclinonc.2017.141\n10.21105/joss.01686\n10.1002/int.22449\n10.1109/TMI.2020.2996256\n10.1007/s13246-020-00888-x\n10.1016/j.cell.2018.02.010\n10.1109/TMI.2021.3079709\n10.1186/s12967-020-02692-3\n10.1016/j.compbiomed.2020.104181\n10.1016/j.compbiomed.2021.104252\n10.1088/1361-6560/abbf9e\n10.7150/thno.46428\n10.1007/s00330-020-07042-x\n10.1038/s41598-021-91305-0\n10.1109/JBHI.2020.3034296\n10.1016/j.media.2020.101824\n10.1148/radiol.2020203465\n10.1148/radiol.2020201365\n10.1002/jum.15406\n10.1111/j.1445-5994.2011.02528.x\n10.1016/j.patter.2021.100269\n10.1038/s41746-020-00369-1\n10.1109/TMI.2020.2995965\n10.1109/JBHI.2020.3037127\n10.1371/journal.pone.0242301\n10.1038/s42003-020-01535-7\n10.1016/j.compbiomed.2021.104835\n10.1016/j.compbiomed.2021.104375\n10.1016/j.compbiomed.2021.104575\n10.1038/s41598-021-96755-0\n10.1016/j.media.2021.102096\n10.1007/s00330-020-07156-2\n10.1016/j.compbiomed.2021.104399\n10.1016/j.bbe.2021.04.006\n10.1109/TNNLS.2021.3054746\n10.1016/j.media.2021.102054\n10.1016/j.media.2020.101860\n10.1038/s41598-021-95537-y\n10.1186/s12967-021-02992-2\n10.1038/s41598-021-90991-0\n10.1016/j.ejrad.2021.109602\n10.1155/2021/6649591\n10.1038/s41467-020-18685-1\n10.1109/TMI.2021.3066161\n10.1007/s11547-021-01370-8\n10.1016/j.inffus.2021.02.013\n10.1109/JBHI.2021.3076086\n10.1038/s41598-020-80261-w\n10.1016/j.compbiomed.2021.104526\n10.1016/j.bspc.2021.102588\n10.1259/bjr.20201007\n10.1016/j.media.2020.101836\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.cell.2020.04.045\n10.1109/TUFFC.2021.3068190\n10.1016/j.compbiomed.2021.104296\n10.1148/radiol.2020200905\n10.1371/journal.pone.0242535\n10.1016/j.chaos.2020.110153\n10.1109/JBHI.2020.3023246\n10.1016/j.media.2021.101992\n10.1016/j.media.2021.101975\n10.1016/j.knosys.2020.106270\n10.1148/radiol.2020201491\n10.1136/bmjopen-2020-045120\n10.1109/TMI.2020.2994908\n10.1109/ACCESS.2020.3044858\n10.1148/ryai.2020200048\n10.1007/s00330-021-07715-1\n10.1109/TBDATA.2021.3056564\n10.1097/RLI.0000000000000763\n10.1038/s41598-020-76141-y\n10.1016/j.bspc.2021.102622\n10.1016/j.eswa.2021.114677\n10.3390/app11020672\n10.3390/app10165683\n10.1109/TCBB.2021.3065361\n10.1109/JBHI.2020.3036722\n10.1038/s41598-020-76550-z\n10.1109/JBHI.2020.3030853\n10.1109/JBHI.2020.3018181\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.3001810\n10.1109/TMI.2020.2994459\n10.1007/s10479-021-04154-5\n10.1016/j.media.2021.102205\n10.1016/j.compbiomed.2021.104887"}
{"title": "Transfer learning based novel ensemble classifier for COVID-19 detection from chest CT-scans.", "abstract": "Coronavirus Disease 2019 (COVID-19) is a deadly infection that affects the respiratory organs in humans as well as animals. By 2020, this disease turned out to be a pandemic affecting millions of individuals across the globe. Conducting rapid tests for a large number of suspects preventing the spread of the virus has become a challenge. In the recent past, several deep learning based approaches have been developed for automating the process of detecting COVID-19 infection from Lung Computerized Tomography (CT) scan images. However, most of them rely on a single model prediction for the final decision which may or may not be accurate. In this paper, we propose a novel ensemble approach that aggregates the strength of multiple deep neural network architectures before arriving at the final decision. We use various pre-trained models such as VGG16, VGG19, InceptionV3, ResNet50, ResNet50V2, InceptionResNetV2, Xception, and MobileNet and fine-tune them using Lung CT Scan images. All these trained models are further used to create a strong ensemble classifier that makes the final prediction. Our experiments exhibit that the proposed ensemble approach is superior to existing ensemble approaches and set state-of-the-art results for detecting COVID-19 infection from lung CT scan images.", "journal": "Computers in biology and medicine", "date": "2021-12-17", "authors": ["Nagur ShareefShaik", "Teja KrishnaCherukuri"], "doi": "10.1016/j.compbiomed.2021.105127\n10.1007/s11760-021-02022-0"}
{"title": "MRFGRO: a hybrid meta-heuristic feature selection method for screening COVID-19 using deep features.", "abstract": "COVID-19 is a respiratory disease that causes infection in both lungs and the upper respiratory tract. The World Health Organization (WHO) has declared it a global pandemic because of its rapid spread across the globe. The most common way for COVID-19 diagnosis is real-time reverse transcription-polymerase chain reaction (RT-PCR) which takes a significant amount of time to get the result. Computer based medical image analysis is more beneficial for the diagnosis of such disease as it can give better results in less time. Computed Tomography (CT) scans are used to monitor lung diseases including COVID-19. In this work, a hybrid model for COVID-19 detection has developed which has two key stages. In the first stage, we have fine-tuned the parameters of the pre-trained convolutional neural networks (CNNs) to extract some features from the COVID-19 affected lungs. As pre-trained CNNs, we have used two standard CNNs namely, GoogleNet and ResNet18. Then, we have proposed a hybrid meta-heuristic feature selection (FS) algorithm, named as Manta Ray Foraging based Golden Ratio Optimizer (MRFGRO) to select the most significant feature subset. The proposed model is implemented over three publicly available datasets, namely, COVID-CT dataset, SARS-COV-2 dataset, and MOSMED dataset, and attains state-of-the-art classification accuracies of 99.15%, 99.42% and 95.57% respectively. Obtained results confirm that the proposed approach is quite efficient when compared to the local texture descriptors used for COVID-19 detection from chest CT-scan images.", "journal": "Scientific reports", "date": "2021-12-17", "authors": ["ArijitDey", "SohamChattopadhyay", "Pawan KumarSingh", "AliAhmadian", "MassimilianoFerrara", "NorazakSenu", "RamSarkar"], "doi": "10.1038/s41598-021-02731-z\n10.1148/radiol.2020200527\n10.1007/s10096-020-03901-z\n10.1080/07391102.2020.1758788\n10.1093/femspd/ftaa042.OCLC823140442\n10.1080/07391102.2020.1763199\n10.1016/j.ejrad.2020.109017\n10.1109/4235.585893\n10.1109/ACCESS.2020.2994762\n10.1109/ACCESS.2020.3016780\n10.1016/j.asoc.2020.106912\n10.1038/s41598-019-56847-4\n10.1101/2020.02.23.20026930\n10.1016/j.asoc.2021.107698\n10.1101/2020.05.14.20101873\n10.1016/j.eng.2020.04.010\n10.1101/2020.04.24.20078584\n10.1038/s41598-020-79139-8\n10.3390/diagnostics11020315\n10.1057/palgrave.jors.2600781\n10.1007/s00521-020-05297-5\n10.1109/ACCESS.2020.3028241\n10.1109/ACCESS.2020.3031718\n10.1007/s12065-019-00279-6\n10.1109/ACCESS.2020.3005827\n10.1016/j.engappai.2019.103300\n10.1007/s00500-019-03949-w\n10.1016/j.patcog.2006.12.019\n10.1016/j.neucom.2005.12.126\n10.1016/j.compstruc.2004.01.002\n10.3390/en12101884\n10.1016/j.knosys.2019.105190\n10.1016/j.knosys.2020.106270\n10.1016/j.chaos.2020.110190\n10.1007/s11356-020-10133-3"}
{"title": "Weakly-supervised lesion analysis with a CNN-based framework for COVID-19.", "abstract": "", "journal": "Physics in medicine and biology", "date": "2021-12-15", "authors": ["KaichaoWu", "BethJelfs", "XiangyuanMa", "RuitianKe", "XueruiTan", "QiangFang"], "doi": "10.1088/1361-6560/ac4316"}
{"title": "Multi-COVID-Net: Multi-objective optimized network for COVID-19 diagnosis from chest X-ray images.", "abstract": "Coronavirus Disease 2019 (COVID-19) had already spread worldwide, and healthcare services have become limited in many countries. Efficient screening of hospitalized individuals is vital in the struggle toward COVID-19 through chest radiography, which is one of the important assessment strategies. This allows researchers to understand medical information in terms of chest X-ray (CXR) images and evaluate relevant irregularities, which may result in a fully automated identification of the disease. Due to the rapid growth of cases every day, a relatively small number of COVID-19 testing kits are readily accessible in health care facilities. Thus it is imperative to define a fully automated detection method as an instant alternate treatment possibility to limit the occurrence of COVID-19 among individuals. In this paper, a two-step Deep learning (DL) architecture has been proposed for COVID-19 diagnosis using CXR. The proposed DL architecture consists of two stages, \"feature extraction and classification\". The \"Multi-Objective Grasshopper Optimization Algorithm (MOGOA)\" is presented to optimize the DL network layers; hence, these networks have named as \"Multi-COVID-Net\". This model classifies the Non-COVID-19, COVID-19, and pneumonia patient images automatically. The Multi-COVID-Net has been tested by utilizing the publicly available datasets, and this model provides the best performance results than other state-of-the-art methods.", "journal": "Applied soft computing", "date": "2021-12-15", "authors": ["TriptiGoel", "RMurugan", "SeyedaliMirjalili", "Deba KumarChakrabartty"], "doi": "10.1016/j.asoc.2021.108250"}
{"title": "Fusion of multi-scale bag of deep visual words features of chest X-ray images to detect COVID-19 infection.", "abstract": "Chest X-ray (CXR) images have been one of the important diagnosis tools used in the COVID-19 disease diagnosis. Deep learning (DL)-based methods have been used heavily to analyze these images. Compared to other DL-based methods, the bag of deep visual words-based method (BoDVW) proposed recently is shown to be a prominent representation of CXR images for their better discriminability. However, single-scale BoDVW features are insufficient to capture the detailed semantic information of the infected regions in the lungs as the resolution of such images varies in real application. In this paper, we propose a new multi-scale bag of deep visual words (MBoDVW) features, which exploits three different scales of the 4th pooling layer's output feature map achieved from VGG-16 model. For MBoDVW-based features, we perform the Convolution with Max pooling operation over the 4th pooling layer using three different kernels: [Formula: see text], [Formula: see text], and [Formula: see text]. We evaluate our proposed features with the Support Vector Machine (SVM) classification algorithm on four CXR public datasets (CD1, CD2, CD3, and CD4) with over 5000 CXR images. Experimental results show that our method produces stable and prominent classification accuracy (84.37%, 88.88%, 90.29%, and 83.65% on CD1, CD2, CD3, and CD4, respectively).", "journal": "Scientific reports", "date": "2021-12-15", "authors": ["ChiranjibiSitaula", "Tej BahadurShahi", "SunilAryal", "FaezehMarzbanrad"], "doi": "10.1038/s41598-021-03287-8\n10.1038/s41598-021-85875-2\n10.1007/s42979-020-00401-x\n10.1007/s42979-019-0007-y\n10.1023/A:1011139631724\n10.1007/s13755-020-00131-7\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00382-x\n10.3390/sym12040651\n10.1016/j.jvcir.2016.05.022\n10.1109/LGRS.2018.2864116\n10.1007/s10489-020-02055-x\n10.1016/j.cell.2018.02.010\n10.3390/math8091441\n10.1109/ACCESS.2019.2925002\n10.3390/app10020559\n10.2299/jsp.16.343\n10.1007/s42600-021-00151-6\n10.1016/j.imu.2020.100412\n10.1109/72.788646\n10.1023/A:1010933404324\n10.1007/s11227-020-03481-x\n10.1016/j.cmpb.2020.105581\n10.1016/j.imu.2020.100505\n10.1109/5254.708428\n10.4310/SII.2009.v2.n3.a8\n10.1007/s10044-021-00970-4"}
{"title": "Performance of a computer aided diagnosis system for SARS-CoV-2 pneumonia based on ultrasound images.", "abstract": "In this study we aimed to leverage deep learning to develop a computer aided diagnosis (CAD) system toward helping radiologists in the diagnosis of SARS-CoV-2 virus syndrome on Lung ultrasonography (LUS).\nA CAD system is developed based on a transfer learning of a residual network (ResNet) to extract features on LUS and help radiologists to distinguish SARS-CoV-2 virus syndrome from healthy and non-SARS-CoV-2 pneumonia. A publicly available LUS dataset for SARS-CoV-2 virus syndrome consisting of 3909 images has been employed. Six radiologists with different experiences participated in the experiment. A comprehensive LUS data set was constructed and employed to train and verify the proposed method. Several metrics such as accuracy, recall, precision, and F1-score, are used to evaluate the performance of the proposed CAD approach. The performances of the radiologists with and without the help of CAD are also evaluated quantitively. The p-values of the t-test shows that with the help of the CAD system, both junior and senior radiologists significantly improve their diagnosis performance on both balanced and unbalanced datasets.\nExperimental results indicate the proposed CAD approach and the machine features from it can significantly improve the radiologists' performance in the SARS-CoV-2 virus syndrome diagnosis. With the help of the proposed CAD system, the junior and senior radiologists achieved F1-score values of 91.33% and 95.79% on balanced dataset and 94.20% and 96.43% on unbalanced dataset. The proposed approach is verified on an independent test dataset and reports promising performance.\nThe proposed CAD system reports promising performance in facilitating radiologists' diagnosis SARS-CoV-2 virus syndrome and might assist the development of a fast, accessible screening method for pulmonary diseases.", "journal": "European journal of radiology", "date": "2021-12-14", "authors": ["ShiyaoShang", "ChunwangHuang", "WenxiaoYan", "RuminChen", "JinglinCao", "YukunZhang", "YanhuiGuo", "GuoqingDu"], "doi": "10.1016/j.ejrad.2021.110066\n10.7326/M20-1495\n10.1148/radiol.2020200642\n10.3346/jkms.2020.35.e142\n10.5811/westjem.2020.5.47743\n10.1186/1465-9921-15-50\n10.1002/jum.15284\n10.1002/jum.15683\n10.1186/s13054-020-02876-9\n10.1148/radiol.2020200847\n10.1007/s10396-021-01081-7\n10.1002/14651858.CD013639.pub4\n10.1109/ICCV.2017.74\n10.1023/A:1022627411411\n10.1109/JBHI.2019.2936151\n10.1109/TUFFC.2020.3005512\n10.1007/s10278-020-00356-8\n10.1109/TMI.2020.2994459\n10.1016/S2213-2600(20)30120-X\n10.1016/j.ejphar.2020.173375"}
{"title": "Fully automatic deep convolutional approaches for the analysis of COVID-19 using chest X-ray images.", "abstract": "Covid-19 is a new infectious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Given the seriousness of the situation, the World Health Organization declared a global pandemic as the Covid-19 rapidly around the world. Among its applications, chest X-ray images are frequently used for an early diagnostic/screening of Covid-19 disease, given the frequent pulmonary impact in the patients, critical issue to prevent further complications caused by this highly infectious disease. In this work, we propose 4 fully automatic approaches for the classification of chest X-ray images under the analysis of 3 different categories: Covid-19, pneumonia and healthy cases. Given the similarity between the pathological impact in the lungs between Covid-19 and pneumonia, mainly during the initial stages of both lung diseases, we performed an exhaustive study of differentiation considering different pathological scenarios. To address these classification tasks, we evaluated 6 representative state-of-the-art deep network architectures on 3 different public datasets: (I) Chest X-ray dataset of the Radiological Society of North America (RSNA); (II) Covid-19 Image Data Collection; (III) SIRM dataset of the Italian Society of Medical Radiology. To validate the designed approaches, several representative experiments were performed using 6,070 chest X-ray radiographs. In general, satisfactory results were obtained from the designed approaches, reaching a global accuracy values of 0.9706 ", "journal": "Applied soft computing", "date": "2021-12-14", "authors": ["Joaquimde Moura", "JorgeNovo", "MarcosOrtega"], "doi": "10.1016/j.asoc.2021.108190\n10.1101/2020.03.30.20047787\n10.1007/s40846-020-00529-4"}
{"title": "Randomly initialized convolutional neural network for the recognition of COVID-19 using X-ray images.", "abstract": "By the start of 2020, the novel coronavirus (COVID-19) had been declared a worldwide pandemic, and because of its infectiousness and severity, several strands of research have focused on combatting its ongoing spread. One potential solution to detecting COVID-19 rapidly and effectively is by analyzing chest X-ray images using Deep Learning (DL) models. Convolutional Neural Networks (CNNs) have been presented as particularly efficient techniques for early diagnosis, but most still include limitations. In this study, we propose a novel randomly initialized CNN (RND-CNN) architecture for the recognition of COVID-19. This network consists of a set of differently-sized hidden layers all created from scratch. The performance of this RND-CNN is evaluated using two public datasets: the COVIDx and the enhanced COVID-19 datasets. Each of these datasets consists of medical images (X-rays) in one of three different classes: chests with COVID-19, with pneumonia, or in a normal state. The proposed RND-CNN model yields encouraging results for its accuracy in detecting COVID-19 results, achieving 94% accuracy for the COVIDx dataset and 99% accuracy on the enhanced COVID-19 dataset.", "journal": "International journal of imaging systems and technology", "date": "2021-12-14", "authors": ["SafaBen Atitallah", "MahaDriss", "WadiiBoulila", "HendaBen Gh\u00e9zala"], "doi": "10.1002/ima.22654"}
{"title": "The effect of deep feature concatenation in the classification problem: An approach on COVID-19 disease detection.", "abstract": "In image classification applications, the most important thing is to obtain useful features. Convolutional neural networks automatically learn the extracted features during training. The classification process is carried out with the obtained features. Therefore, obtaining successful features is critical to achieving high classification success. This article focuses on providing effective features to enhance classification performance. For this purpose, the success of the process of concatenating features in classification is taken as basis. At first, the features acquired by feature transfer method are extracted from AlexNet, Xception, NASNETLarge, and EfficientNet-B0 architectures, which are known to be successful in classification problems. Concatenating the features results in the creation of a new feature set. The method is completed by subjecting the features to various classification algorithms. The proposed pipeline is applied to the three datasets: \"COVID-19 Image Dataset,\" \"COVID-19 Pneumonia Normal Chest X-ray (PA) Dataset,\" and \"COVID-19 Radiography Database\" for COVID-19 disease detection. The whole datasets contain three classes (normal, COVID, and pneumonia). The best classification accuracies for the three datasets are 98.8%, 95.9%, and 99.6%, respectively. Performance metrics are given such as: sensitivity, precision, specificity, and F1-score values, as well. Contribution of paper is as follows: COVID-19 disease is similar to other lung infections. This situation makes diagnosis difficult. Furthermore, the virus's rapid spread necessitates the need to detect cases as soon as possible. There has been an increased curiosity in computer-aided deep learning models to provide the requirements. The use of the proposed method will be beneficial as it provides high accuracy.", "journal": "International journal of imaging systems and technology", "date": "2021-12-14", "authors": ["EmineCengil", "Ahmet\u00c7\u0131nar"], "doi": "10.1002/ima.22659\n10.21203/rs.3.rs-65967/v2"}
{"title": "COLI-Net: Deep learning-assisted fully automated COVID-19 lung and infection pneumonia lesion detection and segmentation from chest computed tomography images.", "abstract": "We present a deep learning (DL)-based automated whole lung and COVID-19 pneumonia infectious lesions (COLI-Net) detection and segmentation from chest computed tomography (CT) images. This multicenter/multiscanner study involved 2368 (347'259 2D slices) and 190 (17\u2009341 2D slices) volumetric CT exams along with their corresponding manual segmentation of lungs and lesions, respectively. All images were cropped, resized, and the intensity values clipped and normalized. A residual network with non-square Dice loss function built upon TensorFlow was employed. The accuracy of lung and COVID-19 lesions segmentation was evaluated on an external reverse transcription-polymerase chain reaction positive COVID-19 dataset (7'333 2D slices) collected at five different centers. To evaluate the segmentation performance, we calculated different quantitative metrics, including radiomic features. The mean Dice coefficients were 0.98\u2009\u00b1\u20090.011 (95% CI, 0.98-0.99) and 0.91\u2009\u00b1\u20090.038 (95% CI, 0.90-0.91) for lung and lesions segmentation, respectively. The mean relative Hounsfield unit differences were 0.03\u2009\u00b1\u20090.84% (95% CI, -0.12 to 0.18) and -0.18\u2009\u00b1\u20093.4% (95% CI, -0.8 to 0.44) for the lung and lesions, respectively. The relative volume difference for lung and lesions were 0.38\u2009\u00b1\u20091.2% (95% CI, 0.16-0.59) and 0.81\u2009\u00b1\u20096.6% (95% CI, -0.39 to 2), respectively. Most radiomic features had a mean relative error less than 5% with the highest mean relative error achieved for the lung for the ", "journal": "International journal of imaging systems and technology", "date": "2021-12-14", "authors": ["IsaacShiri", "HosseinArabi", "YazdanSalimi", "AmirhosseinSanaat", "AzadehAkhavanallaf", "GhasemHajianfar", "DariushAskari", "ShakibaMoradi", "ZahraMansouri", "MasoumehPakbin", "SalehSandoughdaran", "HamidAbdollahi", "Amir RezaRadmard", "KiaraRezaei-Kalantari", "MostafaGhelich Oghli", "HabibZaidi"], "doi": "10.1002/ima.22672\n10.1007/s12350-020-02119-y"}
{"title": "Local binary pattern and deep learning feature extraction fusion for COVID-19 detection on computed tomography images.", "abstract": "The deadly coronavirus virus (COVID-19) was confirmed as a pandemic by the World Health Organization (WHO) in December 2019. It is important to identify suspected patients as early as possible in order to control the spread of the virus, improve the efficacy of medical treatment, and, as a result, lower the mortality rate. The adopted method of detecting COVID-19 is the reverse-transcription polymerase chain reaction (RT-PCR), the process is affected by a scarcity of RT-PCR kits as well as its complexities. Medical imaging using machine learning and deep learning has proved to be one of the most efficient methods of detecting respiratory diseases, but to train machine learning features needs to be extracted manually, and in deep learning, efficiency is affected by deep learning architecture and low data. In this study, handcrafted local binary pattern (LBP) and automatic seven deep learning models extracted features were used to train support vector machines (SVM) and K-nearest neighbour (KNN) classifiers, to improve the performance of the classifier, a concatenated LBP and deep learning feature was proposed to train the KNN and SVM, based on the performance criteria, the models VGG-19\u2009+\u2009LBP achieved the highest accuracy of 99.4%. The SVM and KNN classifiers trained on the hybrid feature outperform the state of the art model. This shows that the proposed feature can improve the performance of the classifiers in detecting COVID-19.", "journal": "Expert systems", "date": "2021-12-14", "authors": ["Auwalu SalehMubarak", "SertanSerte", "FadiAl-Turjman", "Zubaida Sa'idAmeen", "MehmetOzsoz"], "doi": "10.1111/exsy.12842\n10.3390/make2040027\n10.1109/iotm.0001.2000123\n10.2307/2685209\n10.2214/ajr.20.23012\n10.3390/rs9010067\n10.3390/sym12040651\n10.1007/s00779-020-01462-8\n10.1155/2021/8828404"}
{"title": "Cryo-shift: reducing domain shift in cryo-electron subtomograms with unsupervised domain adaptation and randomization.", "abstract": "Cryo-Electron Tomography (cryo-ET) is a 3D imaging technology that enables the visualization of subcellular structures in situ at near-atomic resolution. Cellular cryo-ET images help in resolving the structures of macromolecules and determining their spatial relationship in a single cell, which has broad significance in cell and structural biology. Subtomogram classification and recognition constitute a primary step in the systematic recovery of these macromolecular structures. Supervised deep learning methods have been proven to be highly accurate and efficient for subtomogram classification, but suffer from limited applicability due to scarcity of annotated data. While generating simulated data for training supervised models is a potential solution, a sizeable difference in the image intensity distribution in generated data as compared with real experimental data will cause the trained models to perform poorly in predicting classes on real subtomograms.\nIn this work, we present Cryo-Shift, a fully unsupervised domain adaptation and randomization framework for deep learning-based cross-domain subtomogram classification. We use unsupervised multi-adversarial domain adaption to reduce the domain shift between features of simulated and experimental data. We develop a network-driven domain randomization procedure with 'warp' modules to alter the simulated data and help the classifier generalize better on experimental data. We do not use any labeled experimental data to train our model, whereas some of the existing alternative approaches require labeled experimental samples for cross-domain classification. Nevertheless, Cryo-Shift outperforms the existing alternative approaches in cross-domain subtomogram classification in extensive evaluation studies demonstrated herein using both simulated and experimental data.\nhttps://github.com/xulabs/aitom.\nSupplementary data are available at Bioinformatics online.", "journal": "Bioinformatics (Oxford, England)", "date": "2021-12-14", "authors": ["HmrishavBandyopadhyay", "ZihaoDeng", "LeitingDing", "SinuoLiu", "Mostofa RafidUddin", "XiangruiZeng", "SimaBehpour", "MinXu"], "doi": "10.1093/bioinformatics/btab794"}
{"title": "Lung detection and severity prediction of pneumonia patients based on COVID-19 DET-PRE network.", "abstract": "The sudden outbreak of COVID-19 pneumonia has brought a heavy disaster to individuals globally. Facing this new virus, the clinicians have no automatic tools to assess the severity of pneumonia patients.\nIn the current work, a COVID-19 DET-PRE network with two pipelines was proposed. Firstly, the lungs in X-rays were detected and segmented through the improved YOLOv3 Dense network to remove redundant features. Then, the VGG16 classifier was pre-trained on the source domain, and the severity of the disease was predicted on the target domain by means of transfer learning.\nThe experiment results demonstrated that the COVID-19 DET-PRE network can effectively detect the lungs from X-rays and accurately predict the severity of the disease. The mean average precisions (mAPs) of lung detection in patients with mild and severe illness were 0.976 and 0.983 respectively. Moreover, the accuracy of severity prediction of COVID-19 pneumonia can reach 86.1%.\nThe proposed neural network has high accuracy, which is suitable for the clinical diagnosis of COVID-19 pneumonia.", "journal": "Expert review of medical devices", "date": "2021-12-14", "authors": ["JiaqiaoZhang", "YanYan", "HongjunNi", "ZhonghuaNi"], "doi": "10.1080/17434440.2022.2014319"}
{"title": "Assessing Lobe-wise Burden of COVID-19 Infection in Computed Tomography of Lungs using Knowledge Fusion from Multiple Datasets.", "abstract": "Segmentation of COVID-19 infection in the lung tissue and its quantification in individual lobes is pivotal to understanding the disease's effect. It helps to determine the disease progression and gauge the extent of medical support required. Automation of this process is challenging due to the lack of a standardized dataset with voxel-wise annotations of the lung field, lobes, and infections like ground-glass opacity (GGO) and consolidation. However, multiple datasets have been found to contain one or more classes of the required annotations. Typical deep learning-based solutions overcome such challenges by training neural networks under adversarial and multi-task constraints. We propose to train a convolutional neural network to solve the challenge while it learns from multiple data sources, each of which is annotated for only a few classes. We have experimentally verified our approach by training the model on three publicly available datasets and evaluating its ability to segment the lung field, lobes and COVID-19 infected regions. Additionally, eight scans that previously had annotations for infection and lung have been annotated for lobes. Our model quantifies infection per lobe in these scans with an average error of 4.5%.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2021-12-12", "authors": ["MahalakshumiVisvanathan", "VelmuruganBalasubramanian", "RachanaSathish", "SuhasiniBalasubramaniam", "DebdootSheet"], "doi": "10.1109/EMBC46164.2021.9629591"}
{"title": "COVID-19 Volumetric Pulmonary Lesion Estimation on CT Images using a U-NET and Probabilistic Active Contour Segmentation.", "abstract": "A two-step method for obtaining a volumetric estimation of COVID-19 related lesion from CT images is proposed. The first step consists in applying a U-NET convolutional neural network to provide a segmentation of the lung-parenchyma. This architecture is trained and validated using the Thoracic Volume and Pleural Effusion Segmentations in Diseased Lungs for Benchmarking Chest CT Processing Pipelines (PleThora) dataset, which is publicly available. The second step consists in obtaining the volumetric lesion estimation using an automatic algorithm based on a probabilistic active contour (PACO) region delimitation approach. Our pipeline successfully segmented COVID-19 related lesions in CT images, with exception of some mislabeled regions including lung airways and vasculature. Our workflow was applied to images in a cohort of 50 patients.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2021-12-12", "authors": ["LeopoldoCendejas-Zaragoza", "Diomar ERodriguez-Obregon", "Aldo RMejia-Rodriguez", "Edgar RArce-Santana", "AlejandroSantos-Diaz"], "doi": "10.1109/EMBC46164.2021.9629532"}
{"title": "A Denoising Self-supervised Approach for COVID-19 Pneumonia Lesion Segmentation with Limited Annotated CT Images.", "abstract": "The coronavirus disease 2019 (COVID-19) has become a global pandemic. The segmentation of COVID-19 pneumonia lesions from CT images is important in quantitative evaluation and assessment of the infection. Though many deep learning segmentation methods have been proposed, the performance is limited when pixel-level annotations are hard to obtain. In order to alleviate the performance limitation brought by the lack of pixel-level annotation in COVID-19 pneumonia lesion segmentation task, we construct a denoising self-supervised framework, which is composed of a pretext denoising task and a downstream segmentation task. Through the pretext denoising task, the semantic features from massive unlabelled data are learned in an unsupervised manner, so as to provide additional supervisory signal for the downstream segmentation task. Experimental results showed that our method can effectively leverage unlabelled images to improve the segmentation performance, and outperformed reconstruction-based self-supervised learning when only a small set of training images are annotated.Clinical relevance-The proposed method can effectively leverage unlabelled images to improve the performance for COVID-19 pneumonia lesion segmentation when only a small set of CT images are annotated.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2021-12-12", "authors": ["YiboGao", "HuanWang", "XinglongLiu", "NingHuang", "GuotaiWang", "ShaotingZhang"], "doi": "10.1109/EMBC46164.2021.9630215"}
{"title": "Automated Detection of COVID-19 Cases using Recent Deep Convolutional Neural Networks and CT images", "abstract": "COVID-19 is an acute severe respiratory disease caused by a novel coronavirus SARS-CoV-2. After its first appearance in Wuhan (China), it spread rapidly across the world and became a pandemic. It had a devastating effect on everyday life, public health, and the world economy. The use of advanced artificial intelligence (AI) techniques combined with radiological imaging can be helpful in speeding-up the detection of this disease. In this study, we propose the development of recent deep learning models for automatic COVID-19 detection using computed tomography (CT) images. The proposed models are fine-tuned and optimized to provide accurate results for multiclass classification of COVID-19 vs. Community Acquired Pneumonia (CAP) vs. Normal cases. Tests were conducted both at the image and patient-level and show that the proposed algorithms achieve very high scores. In addition, an explainability algorithm was developed to help visualize the symptoms of the disease detected by the best performing deep model.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2021-12-12", "authors": ["MohamedChetoui", "Moulay AAkhloufi"], "doi": "10.1109/EMBC46164.2021.9629689"}
{"title": "CNN Filter Learning from Drawn Markers for the Detection of Suggestive Signs of COVID-19 in CT Images.", "abstract": "Early detection of COVID-19 is vital to control its spread. Deep learning methods have been presented to detect suggestive signs of COVID-19 from chest CT images. However, due to the novelty of the disease, annotated volumetric data are scarce. Here we propose a method that does not require either large annotated datasets or backpropagation to estimate the filters of a convolutional neural network (CNN). For a few CT images, the user draws markers at representative normal and abnormal regions. The method generates a feature extractor composed of a sequence of convolutional layers, whose kernels are specialized in enhancing regions similar to the marked ones, and the decision layer of our CNN is a support vector machine. As we have no control over the CT image acquisition, we also propose an intensity standardization approach. Our method can achieve mean accuracy and kappa values of 0.97 and 0.93, respectively, on a dataset with 117 CT images extracted from different sites, surpassing its counterpart in all scenarios.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2021-12-12", "authors": ["Azael MSousa", "FabianoReis", "RachelZerbini", "Joao L DComba", "Alexandre XFalcao"], "doi": "10.1109/EMBC46164.2021.9629806"}
{"title": "Multi-feature Multi-Scale CNN-Derived COVID-19 Classification from Lung Ultrasound Data.", "abstract": "The global pandemic of the novel coronavirus disease 2019 (COVID-19) has put tremendous pressure on the medical system. Imaging plays a complementary role in the management of patients with COVID-19. Computed tomography (CT) and chest X-ray (CXR) are the two dominant screening tools. However, difficulty in eliminating the risk of disease transmission, radiation exposure and not being cost-effective are some of the challenges for CT and CXR imaging. This fact induces the implementation of lung ultrasound (LUS) for evaluating COVID-19 due to its practical advantages of noninvasiveness, repeatability, and sensitive bedside property. In this paper, we utilize a deep learning model to perform the classification of COVID-19 from LUS data, which could produce objective diagnostic information for clinicians. Specifically, all LUS images are processed to obtain their corresponding local phase filtered images and radial symmetry transformed images before fed into the multi-scale residual convolutional neural network (CNN). Secondly, image combination as the input of the network is used to explore rich and reliable features. Feature fusion strategy at different levels is adopted to investigate the relationship between the depth of feature aggregation and the classification accuracy. Our proposed method is evaluated on the point-of-care US (POCUS) dataset together with the Italian COVID-19 Lung US database (ICLUS-DB) and shows promising performance for COVID-19 prediction.", "journal": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference", "date": "2021-12-12", "authors": ["HuiChe", "JaredRadbel", "JagSunderram", "John LNosher", "Vishal MPatel", "IlkerHacihaliloglu"], "doi": "10.1109/EMBC46164.2021.9631069"}
{"title": "Precision Medicine: Using Artificial Intelligence to Improve Diagnostics and Healthcare.", "abstract": "The continued generation of large amounts of data within healthcare-from imaging to electronic medical health records to genomics and multi-omics -necessitates tools and methods to parse and interpret these data to improve healthcare outcomes. Artificial intelligence, and in particular deep learning, has enabled researchers to gain new insights from large scale and multimodal data. At the 2022 Pacific Symposium on Biocomputing (PSB) session entitled \"Precision Medicine: Using Artificial Intelligence to Improve Diagnostics and Healthcare\", we showcase the latest research, influenced and inspired by the idea of using technology to build a more fair, tailored, and cost-effective healthcare system after the COVID-19 pandemic.", "journal": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing", "date": "2021-12-11", "authors": ["RoxanaDaneshjou", "Steven EBrenner", "Jonathan HChen", "Dana CCrawford", "Samuel GFinlayson", "\u0141ukaszKidzi\u0144ski", "Martha LBulyk"], "doi": null}
{"title": "The diagnostic performance of deep-learning-based CT severity score to identify COVID-19 pneumonia.", "abstract": "To determine the diagnostic accuracy of a deep-learning (DL)-based algorithm using chest computed tomography (CT) scans for the rapid diagnosis of coronavirus disease 2019 (COVID-19), as compared to the reference standard reverse-transcription polymerase chain reaction (RT-PCR) test.\nIn this retrospective analysis, data of COVID-19 suspected patients who underwent RT-PCR and chest CT examination for the diagnosis of COVID-19 were assessed. By quantifying the affected area of the lung parenchyma, severity score was evaluated for each lobe of the lung with the DL-based algorithm. The diagnosis was based on the total lung severity score ranging from 0 to 25. The data were randomly split into a 40% training set and a 60% test set. Optimal cut-off value was determined using Youden-index method on the training cohort.\nA total of 1259 patients were enrolled in this study. The prevalence of RT-PCR positivity in the overall investigated period was 51.5%. As compared to RT-PCR, sensitivity, specificity, positive predictive value, negative predictive value and accuracy on the test cohort were 39.0%, 80.2%, 68.0%, 55.0% and 58.9%, respectively. Regarding the whole data set, when adding those with positive RT-PCR test at any time during hospital stay or \"COVID-19 without virus detection\", as final diagnosis to the true positive cases, specificity increased from 80.3% to 88.1% and the positive predictive value increased from 68.4% to 81.7%.\nDL-based CT severity score was found to have a good specificity and positive predictive value, as compared to RT-PCR. This standardized scoring system can aid rapid diagnosis and clinical decision making.\nDL-based CT severity score can detect COVID-19-related lung alterations even at early stages, when RT-PCR is not yet positive.", "journal": "The British journal of radiology", "date": "2021-12-11", "authors": ["Anna S\u00e1raKardos", "JuditSimon", "ChiaraNardocci", "Istv\u00e1n ViktorSzab\u00f3", "NorbertNagy", "Renad HeyamAbdelrahman", "EmeseZsarn\u00f3czay", "BenceFej\u00e9r", "Bal\u00e1zsFut\u00e1csi", "VeronikaM\u00fcller", "B\u00e9laMerkely", "P\u00e1lMaurovich-Horvat"], "doi": "10.1259/bjr.20210759\n10.1007/s11357-020-00226-9\n10.1001/jama.2020.12839\n10.1016/S0140-6736(20)30183-5\n10.1016/j.ijsu.2020.02.034\n10.1001/jama.2020.1097\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1001/jama.2020.1585\n10.1097/RTI.0000000000000524\n10.1148/radiol.2020200230\n10.1177/0846537120913033\n10.1148/radiol.2020201365\n10.1148/radiol.2020201473\n10.1038/s41591-020-0931-3\n10.1148/radiol.2020200905\n10.1109/RBME.2020.2987975\n10.26355/eurrev_202008_22510\n10.1016/j.cell.2020.04.045\n10.1136/bmj.h5527\n10.2214/AJR.20.22976\n10.1556/1647.2020.00002\n10.1148/radiol.2020200330\n10.1148/radiol.2020200343\n10.1016/S2214-109X(20)30068-1\n10.1016/j.asoc.2020.106897\n10.1183/13993003.00775-2020\n10.1148/radiol.2020200463"}
{"title": "Role of Artificial Intelligence in COVID-19 Detection.", "abstract": "The global pandemic of coronavirus disease (COVID-19) has caused millions of deaths and affected the livelihood of many more people. Early and rapid detection of COVID-19 is a challenging task for the medical community, but it is also crucial in stopping the spread of the SARS-CoV-2 virus. Prior substantiation of artificial intelligence (AI) in various fields of science has encouraged researchers to further address this problem. Various medical imaging modalities including X-ray, computed tomography (CT) and ultrasound (US) using AI techniques have greatly helped to curb the COVID-19 outbreak by assisting with early diagnosis. We carried out a systematic review on state-of-the-art AI techniques applied with X-ray, CT, and US images to detect COVID-19. In this paper, we discuss approaches used by various authors and the significance of these research efforts, the potential challenges, and future trends related to the implementation of an AI system for disease detection during the COVID-19 pandemic.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-12-11", "authors": ["AnjanGudigar", "URaghavendra", "SnehaNayak", "Chui PingOoi", "Wai YeeChan", "Mokshagna RohitGangavarapu", "ChinmayDharmik", "JyothiSamanth", "Nahrizul AdibKadri", "KhairunnisaHasikin", "Prabal DattaBarua", "SubrataChakraborty", "Edward JCiaccio", "U RajendraAcharya"], "doi": "10.3390/s21238045\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1038/s41423-020-0402-2\n10.1016/S1473-3099(20)30230-9\n10.1128/JVI.00127-20\n10.1016/j.cell.2020.02.052\n10.1128/JVI.79.24.15511-15524.2005\n10.1373/clinchem.2005.054460\n10.1016/S2213-2600(20)30076-X\n10.1016/j.clim.2020.108427\n10.1016/S0140-6736(03)13410-1\n10.1056/NEJMc2010419\n10.1056/NEJMoa2002032\n10.1001/jama.2020.3204\n10.1053/j.gastro.2020.03.065\n10.1016/j.cca.2020.03.009\n10.1186/s42492-021-00078-w\n10.1007/s10489-020-01862-6\n10.3390/app11083414\n10.1155/2021/5528144\n10.1155/2021/6677314\n10.1155/2020/9756518\n10.1007/s42979-021-00605-9\n10.1007/s42600-021-00135-6\n10.1016/j.imu.2021.100564\n10.1016/j.scs.2020.102589\n10.1016/j.chaos.2020.110338\n10.1016/j.ijsu.2010.02.007\n10.1148/radiol.2020200330\n10.1007/s10140-021-01905-6\n10.1101/2020.04.24.20078584\n10.5281/zenodo.3757476\n10.3390/app11020672\n10.1016/0010-4809(71)90034-6\n10.1016/S0734-189X(85)90153-7\n10.1016/0165-1684(94)90060-4\n10.1007/s12559-020-09779-5\n10.1016/j.eswa.2020.113909\n10.1613/jair.953\n10.1109/ACCESS.2020.2994762\n10.1109/34.192463\n10.1109/TSMC.1973.4309314\n10.1111/1365-2478.12234\n10.1016/j.chemolab.2020.104054\n10.1109/cvpr.2005.177\n10.1016/j.ijleo.2013.05.132\n10.1155/2021/5544742\n10.1371/journal.pone.0235187\n10.1109/TPAMI.2002.1017623\n10.1371/journal.pone.0250688\n10.1109/CVPR.2016.90\n10.1145/3065386\n10.1167/17.10.296\n10.1007/s11263-015-0816-y\n10.1109/CVPR.2017.195\n10.3390/sym12091526\n10.1007/s42979-021-00690-w\n10.1016/j.advengsoft.2017.07.002\n10.1109/ACCESS.2021.3061058\n10.1177/003754970107600201\n10.1016/j.bspc.2020.102173\n10.1016/j.advengsoft.2013.12.007\n10.1007/s12559-021-09848-3\n10.1038/s41598-020-71294-2\n10.1109/TPAMI.2005.159\n10.1016/j.csda.2004.07.026\n10.1007/s00521-015-1920-1\n10.1109/ACCESS.2020.3028012\n10.3390/e22050517\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.asoc.2021.107238\n10.1109/TEVC.2008.919004\n10.1007/s10096-020-03901-z\n10.1109/34.709601\n10.1080/00401706.1996.10484565\n10.1007/BF00058655\n10.1109/101.8118\n10.1109/TSMCB.2011.2168604\n10.1006/jcss.1997.1504\n10.1016/j.jksuci.2020.12.010\n10.1016/j.procs.2020.09.258\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100360\n10.1016/j.cmpb.2020.105581\n10.1016/j.ibmed.2020.100014\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.110071\n10.1016/j.cmpb.2020.105608\n10.1016/j.bbe.2020.08.008\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.media.2020.101794\n10.1016/j.patrec.2020.09.010\n10.1016/j.chaos.2020.109944\n10.1155/2020/8828855\n10.1155/2020/8889023\n10.3390/ai1030027\n10.3390/app10134640\n10.3390/app10165683\n10.3390/electronics9091388\n10.3390/ijerph17186933\n10.3390/info11090419\n10.3390/jpm10040213\n10.3390/proceedings2020054031\n10.3390/sym12040651\n10.3390/sym12091530\n10.1007/s13246-020-00865-4\n10.1007/s13246-020-00888-x\n10.1088/1757-899X/982/1/012004\n10.1371/journal.pone.0243963\n10.1038/s41598-020-76550-z\n10.1371/journal.pone.0242535\n10.1177/2472630320958376\n10.1109/TMI.2020.2993291\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2020.3025010\n10.1109/ACCESS.2021.3077592\n10.1155/2021/3277988\n10.1155/2021/3604900\n10.1155/2021/5513679\n10.1155/2021/6621607\n10.1155/2021/6658058\n10.1155/2021/7804540\n10.1155/2021/8828404\n10.1155/2021/8829829\n10.1155/2021/8890226\n10.1155/2021/9929274\n10.1155/2021/9989237\n10.1016/j.radi.2020.10.018\n10.1016/j.asoc.2021.107184\n10.1016/j.imu.2020.100505\n10.1016/j.imu.2020.100506\n10.1016/j.neucom.2021.03.034\n10.1016/j.compbiomed.2020.104181\n10.3390/a14060183\n10.3390/app11062884\n10.3390/computation9010003\n10.3390/diagnostics11050775\n10.3390/diagnostics11050895\n10.3390/math9091002\n10.3390/s21041480\n10.3390/s21051742\n10.3390/app11041424\n10.3390/ijerph18158052\n10.1007/s10489-020-01829-7\n10.1007/s00530-021-00794-6\n10.1007/s42600-021-00151-6\n10.1007/s10044-021-00984-y\n10.1134/S1054661821020140\n10.1186/s41747-020-00203-z\n10.1007/s42979-021-00496-w\n10.1007/s42600-020-00120-5\n10.1007/s10489-020-01888-w\n10.1007/s12652-021-02917-3\n10.1007/s00354-021-00121-7\n10.1007/s10044-021-00970-4\n10.1007/s12530-021-09385-2\n10.1007/s40031-021-00589-3\n10.1007/s12559-020-09774-w\n10.1007/s10489-020-01902-1\n10.1371/journal.pone.0247839\n10.1371/journal.pone.0252573\n10.1109/ACCESS.2021.3061621\n10.1109/TCBB.2021.3066331\n10.1109/JBHI.2021.3067333\n10.1109/ACCESS.2021.3083516\n10.1109/TNNLS.2021.3082015\n10.1109/ACCESS.2021.3086229\n10.1109/TNNLS.2021.3086570\n10.1016/j.compbiomed.2020.103795\n10.1016/j.imu.2020.100427\n10.1155/2021/6649591\n10.3390/diagnostics10110901\n10.3390/e23020204\n10.1007/s11548-020-02286-w\n10.1007/s00521-020-05437-x\n10.1007/s10489-020-02149-6\n10.1109/TMI.2020.2996645\n10.1109/TMI.2020.2995508\n10.1109/JSEN.2020.3025855\n10.1109/JBHI.2020.3030853\n10.1016/j.compbiomed.2021.104356\n10.1016/j.iot.2021.100377\n10.1016/j.compbiomed.2021.104304\n10.1016/j.ejrad.2021.109602\n10.1016/j.neucom.2020.07.144\n10.1016/j.irbm.2021.01.004\n10.1016/j.patcog.2021.107828\n10.1016/j.media.2020.101836\n10.1016/j.compbiomed.2021.104306\n10.1155/2021/5522729\n10.1155/2021/5527271\n10.1155/2021/5527923\n10.1155/2021/5528441\n10.1155/2021/5554408\n10.1155/2021/6633755\n10.1155/2021/8840835\n10.1155/2021/9999368\n10.1155/2021/6680455\n10.3390/ai2020016\n10.3390/diagnostics11020158\n10.3390/diagnostics11050893\n10.3390/ijerph18062842\n10.3390/s21020455\n10.3390/s21062215\n10.1007/s10489-020-01826-w\n10.1007/s00521-021-05910-1\n10.1007/s10489-020-02002-w\n10.1186/s43055-021-00524-y\n10.1007/s10489-021-02292-8\n10.1007/s10140-020-01886-y\n10.1007/s13755-021-00140-0\n10.1007/s00330-020-07087-y\n10.1007/s11042-020-09894-3\n10.3233/JIFS-201985\n10.1371/journal.pone.0244416\n10.1371/journal.pone.0249450\n10.1371/journal.pone.0250952\n10.1109/TBDATA.2021.3056564\n10.1109/TNNLS.2021.3054746\n10.1016/j.inffus.2021.02.013\n10.1016/j.compbiomed.2021.104296\n10.1016/j.chaos.2020.110190\n10.1016/j.compbiomed.2021.104348\n10.1007/s11042-021-10783-6\n10.1016/j.bspc.2021.102490\n10.1038/s41598-021-87523-1\n10.1007/s10489-020-01831-z\n10.1007/s40846-021-00630-2\n10.1016/j.bbe.2021.05.013\n10.1016/j.patcog.2021.107848\n10.1016/j.bspc.2021.102602\n10.1007/s10489-020-01943-6\n10.3390/app11094233\n10.1016/j.aej.2021.03.052\n10.1007/s10489-020-02122-3\n10.1109/ACCESS.2020.3016780\n10.3390/ijerph18126499\n10.1016/j.infrared.2019.103041\n10.1016/j.cmpb.2019.105205\n10.3389/fcvm.2021.638011\n10.1016/j.ibmed.2020.100013\n10.1016/j.media.2021.102046\n10.1007/s00259-020-04953-1\n10.1016/j.media.2021.102054\n10.1016/j.patcog.2020.107747\n10.3390/cancers13081960\n10.1016/j.bspc.2021.102622\n10.1002/ima.22552\n10.1016/j.knosys.2021.107242\n10.1002/jmv.26699\n10.1016/j.imu.2020.100428\n10.1016/j.patrec.2021.09.012\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.4065/mcp.2010.0260\n10.1007/s00134-020-05996-6\n10.1016/j.compbiomed.2021.104944\n10.3390/diagnostics11111962"}
{"title": "MHA-CoroCapsule: Multi-Head Attention Routing-Based Capsule Network for COVID-19 Chest X-Ray Image Classification.", "abstract": "The outbreak of COVID-19 threatens the lives and property safety of countless people and brings a tremendous pressure to health care systems worldwide. The principal challenge in the fight against this disease is the lack of efficient detection methods. AI-assisted diagnosis based on deep learning can detect COVID-19 cases for chest X-ray images automatically, and also improve the accuracy and efficiency of doctors' diagnosis. However, large scale annotation of chest X-ray images is difficult because of limited resources and heavy burden on the medical system. To meet the challenge, we propose a capsule network model with multi-head attention routing algorithm, called MHA-CoroCapsule, to provide fast and accurate diagnostics for COVID-19 diseases from chest X-ray images. The MHA-CoroCapsule consists of convolutional layers, two capsule layers, and a non-iterative, parameterized multi-head attention routing algorithm is used to quantify the relationship between the two capsule layers. The experiments are performed on a combined dataset constituted by two publicly available datasets including normal, non-COVID pneumonia and COVID-19 images. The model achieves the accuracy of 97.28%, recall of 97.36%, and precision of 97.38% even with a limited number of samples. The experimental results demonstrate that, contrary to the transfer learning and deep feature extraction approaches, the proposed MHA-CoroCapsule has an encouraging performance with fewer trainable parameters and does not require pretraining and plenty of training samples.", "journal": "IEEE transactions on medical imaging", "date": "2021-12-10", "authors": ["FudongLi", "XingyuLu", "JianjunYuan"], "doi": "10.1109/TMI.2021.3134270"}
{"title": "Editorial Comment: Real-World Imaging Artificial Intelligence Considerations for COVID-19 and Beyond.", "abstract": null, "journal": "AJR. American journal of roentgenology", "date": "2021-12-09", "authors": ["Tessa SCook"], "doi": "10.2214/AJR.21.27189"}
{"title": "Analyzing the impact of machine learning and artificial intelligence and its effect on management of lung cancer detection in covid-19 pandemic.", "abstract": "Cancer victims, particularly those with lung cancer, are more susceptible and at higher danger of COVID-19 and associated consequences as a result of their compromised immune systems, which makes them particularly sensitive. Because of a variety of circumstances, cancer patients' diagnosis, treatment, and aftercare are very complicated and time-consuming during an epidemic. In such circumstances, advances in artificial intelligence (AI) and machine learning algorithms (ML) offer the capacity to boost cancer sufferer diagnosis, therapy, and care via the use of cutting technologies. For example, using clinical and imaging data combined with machine learning methods, the researchers may be able to distinguish among lung alterations induced by corona virus and those produced by immunotherapy and radiation. During this epidemic, artificial intelligence (AI) may be utilized to guarantee that the appropriate individuals are recruited in cancer clinical trials more quickly and effectively than in the past, which was done in a conventional and complicated manner. In order to better care for cancer patients and find novel and more effective therapies, It is critical that we move beyond traditional research methods and use artificial intelligence (AI) and machine learning to update our research (ML). Artificial intelligence (AI) and machine learning (ML) are being utilised to help with several aspects of the COVID-19 epidemic, such as epidemiology, molecular research and medication development, medical diagnosis and treatment, and socioeconomics. The use of artificial intelligence (AI) and machine learning (ML) in the diagnosis and treatment of COVID-19 patients is also being investigated. The combination of artificial intelligence and machine learning in COVID-19 may help to identify positive patients more quickly. In order to understand the dynamics of an epidemic that is relevant to artificial intelligence, when used in different patient groups, AI-based algorithms can quickly detect CT scans with COVID-19 linked pneumonia, as well as discriminate non-COVID connected pneumonia with high specificity and accuracy. It is possible to utilize the existing difficulties and future views presented in this study to guide an optimal implementation of AI and machine learning technologies in an epidemic.", "journal": "Materials today. Proceedings", "date": "2021-12-09", "authors": ["Raja Sarath KumarBoddu", "ParthaKarmakar", "AnkanBhaumik", "Vinay KumarNassa", "NoneVandana", "SumantaBhattacharya"], "doi": "10.1016/j.matpr.2021.11.549\n10.1002/hpm.2987\n10.32604/cmc.2020.010691\n10.1038/s41591-020-0931-3\n10.1186/s12911-020-01266-z\n10.2139/ssrn.3551355\n10.2139/ssrn.3541119\n10.2139/ssrn.3638427\n10.1038/s41746-020-00372-6\n10.1371/journal.pone.0239474\n10.1038/s41591-020-0931-3\n10.7150/ijbs.58855\n10.1016/j.scs.2020.102018"}
{"title": "Post-mortem tissue proteomics reveals the pathogenesis of multi-organ injuries of COVID-19.", "abstract": null, "journal": "National science review", "date": "2021-12-09", "authors": ["YangQiu", "DiWu", "WanshanNing", "JiqianXu", "TingShu", "MuhanHuang", "RongChen", "JianchengZhang", "YangHan", "QingyuYang", "RuitingLi", "XiaoboYang", "YaxinWang", "XiaojingZou", "ShangwenPan", "ChaolinHuang", "YuXue", "YouShang", "XiZhou"], "doi": "10.1093/nsr/nwab143\n10.1016/j.immuni.2020.10.008\n10.1080/03007995.2020.1825365\n10.1016/j.cell.2021.01.004\n10.1038/s41392-020-00355-9\n10.1038/s41586-020-2332-7\n10.1038/s41467-020-15562-9\n10.1016/j.chom.2020.12.016\n10.1126/science.abc6027\n10.1038/s41586-020-2286-9"}
{"title": "Application of CycleGAN and transfer learning techniques for automated detection of COVID-19 using X-ray images.", "abstract": "Coronavirus (which is also known as COVID-19) is severely impacting the wellness and lives of many across the globe. There are several methods currently to detect and monitor the progress of the disease such as radiological image from patients' chests, measuring the symptoms and applying polymerase chain reaction (RT-PCR) test. X-ray imaging is one of the popular techniques used to visualise the impact of the virus on the lungs. Although manual detection of this disease using radiology images is more popular, it can be time-consuming, and is prone to human errors. Hence, automated detection of lung pathologies due to COVID-19 utilising deep learning (Bowles et\u00a0al.) techniques can assist with yielding accurate results for huge databases. Large volumes of data are needed to achieve generalizable DL models; however, there are very few public databases available for detecting COVID-19 disease pathologies automatically. Standard data augmentation method can be used to enhance the models' generalizability. In this research, the Extensive COVID-19 X-ray and CT Chest Images Dataset has been used and generative adversarial network (GAN) coupled with trained, semi-supervised CycleGAN (SSA- CycleGAN) has been applied to augment the training dataset. Then a newly designed and finetuned Inception V3 transfer learning model has been developed to train the algorithm for detecting COVID-19 pandemic. The obtained results from the proposed Inception-CycleGAN model indicated Accuracy\u00a0=\u00a094.2%, Area under Curve\u00a0=\u00a092.2%, Mean Squared Error\u00a0=\u00a00.27, Mean Absolute Error\u00a0=\u00a00.16. The developed Inception-CycleGAN framework is ready to be tested with further COVID-19 X-Ray images of the chest.", "journal": "Pattern recognition letters", "date": "2021-12-09", "authors": ["GhazalBargshady", "XujuanZhou", "Prabal DattaBarua", "RajGururajan", "YuefengLi", "U RajendraAcharya"], "doi": "10.1016/j.patrec.2021.11.020\n10.1148/radiol.2020200905\n10.1016/j.compag.2019.01.041\n10.1007/978-1-4842-2766-4_7"}
{"title": "Disentangling post-vaccination symptoms from early COVID-19.", "abstract": "Identifying and testing individuals likely to have SARS-CoV-2 is critical for infection control, including post-vaccination. Vaccination is a major public health strategy to reduce SARS-CoV-2 infection globally. Some individuals experience systemic symptoms post-vaccination, which overlap with COVID-19 symptoms. This study compared early post-vaccination symptoms in individuals who subsequently tested positive or negative for SARS-CoV-2, using data from the COVID Symptom Study (CSS) app.\nWe conducted a prospective observational study in 1,072,313 UK CSS participants who were asymptomatic when vaccinated with Pfizer-BioNTech mRNA vaccine (BNT162b2) or Oxford-AstraZeneca adenovirus-vectored vaccine (ChAdOx1 nCoV-19) between 8 December 2020 and 17 May 2021, who subsequently reported symptoms within seven days (N=362,770) (other than local symptoms at injection site) and were tested for SARS-CoV-2 (N=14,842), aiming to differentiate vaccination side-effects \nDifferentiating post-vaccination side-effects alone from early COVID-19 was challenging, with a sensitivity in identification of individuals testing positive of 0.6 at best. Most of these individuals did not have fever, persistent cough, or anosmia/dysosmia, requisite symptoms for accessing UK testing; and many only had systemic symptoms commonly seen post-vaccination in individuals negative for SARS-CoV-2 (headache, myalgia, and fatigue).\nPost-vaccination symptoms \nUK Government Department of Health and Social Care,\u00a0Wellcome\u00a0Trust, UK Engineering and Physical Sciences Research Council, UK National Institute for Health Research, UK Medical Research Council and British Heart Foundation, Chronic Disease Research Foundation, Zoe\u00a0Limited.", "journal": "EClinicalMedicine", "date": "2021-12-08", "authors": ["Liane SCanas", "Marc F\u00d6sterdahl", "JieDeng", "ChristinaHu", "SomeshSelvachandran", "LorenzoPolidori", "AnnaMay", "ErikaMolteni", "BenjaminMurray", "LiyuanChen", "EricKerfoot", "KerstinKlaser", "MichelaAntonelli", "AlexanderHammers", "TimSpector", "SebastienOurselin", "ClaireSteves", "Carole HSudre", "MarcModat", "Emma LDuncan"], "doi": "10.1016/j.eclinm.2021.101212\n10.1016/S1473-3099(21)00224-3\n10.1056/nejmoa2104882\n10.1056/nejmoa2104840\n10.1136/bmj.m3325\n10.1007/978-3-319-98074-4\n10.1198/tech.2007.s518\n10.1101/2021.03.16.21253719\n10.1001/jama.2021.7152\n10.1093/cid/ciab446"}
{"title": "Deep learning classification of COVID-19 in chest radiographs: performance and influence of supplemental training.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-12-07", "authors": ["Rafael BFricks", "FrancescoRia", "HamidChalian", "PegahKhoshpouri", "EhsanAbadi", "LorenzoBianchi", "William PSegars", "EhsanSamei"], "doi": "10.1117/1.JMI.8.6.064501\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020200432\n10.1148/radiol.2020200823\n10.1148/radiol.2020200642\n10.1148/radiol.2020201365\n10.1148/radiol.2020200905\n10.1118/1.4859315\n10.1148/radiol.2020200905\n10.1148/radiol.2020201874\n10.1148/radiol.2020200230\n10.1038/s41598-019-42294-8\n10.1109/CVPR.2017.243\n10.1109/CVPR.2009.5206848\n10.1109/ICCV.2015.123\n10.2307/2531595\n10.1109/LSP.2014.2337313\n10.7717/peerj.10387\n10.1109/ICCV.2017.74\n10.1016/S0197-2456(00)00097-0\n10.1371/journal.pmed.1002686\n10.1016/j.inffus.2021.04.008\n10.3390/ijerph17186933"}
{"title": "Towards nationally curated data archives for clinical radiology image analysis at scale: Learnings from national data collection in response to a pandemic.", "abstract": "The prevalence of the coronavirus SARS-CoV-2 disease has resulted in the unprecedented collection of health data to support research. Historically, coordinating the collation of such datasets on a national scale has been challenging to execute for several reasons, including issues with data privacy, the lack of data reporting standards, interoperable technologies, and distribution methods. The coronavirus SARS-CoV-2 disease pandemic has highlighted the importance of collaboration between government bodies, healthcare institutions, academic researchers and commercial companies in overcoming these issues during times of urgency. The National COVID-19 Chest Imaging Database, led by NHSX, British Society of Thoracic Imaging, Royal Surrey NHS Foundation Trust and Faculty, is an example of such a national initiative. Here, we summarise the experiences and challenges of setting up the National COVID-19 Chest Imaging Database, and the implications for future ambitions of national data curation in medical imaging to advance the safe adoption of artificial intelligence in healthcare.", "journal": "Digital health", "date": "2021-12-07", "authors": ["DominicCushnan", "RosalindBerka", "OttaviaBertolli", "PeterWilliams", "DanielSchofield", "IndraJoshi", "AlbertoFavaro", "MarkHalling-Brown", "GergelyImreh", "EmilyJefferson", "Neil JSebire", "GerryReilly", "Jonathan C LRodrigues", "GrahamRobinson", "SusanCopley", "RizwanMalik", "ClaireBloomfield", "FergusGleeson", "MoiraCrotty", "ErikaDenton", "JeanetteDickson", "GaryLeeming", "Hayley EHardwick", "KennethBaillie", "Peter JmOpenshaw", "Malcolm GSemple", "CarolineRubin", "AndyHowlett", "Andrea GRockall", "AyubBhayat", "DanielFascia", "CathieSudlow", "NoneNone", "JosephJacob"], "doi": "10.1177/20552076211048654\n10.23889/ijpds.v3i3.432\n10.1093/jamia/ocaa210\n10.1093/gigascience/giaa095\n10.1136/bmj.l6927"}
{"title": "A Novel and Robust Approach to Detect Tuberculosis Using Transfer Learning.", "abstract": "Deep learning has emerged as a promising technique for a variety of elements of infectious disease monitoring and detection, including ", "journal": "Journal of healthcare engineering", "date": "2021-12-07", "authors": ["OmarFaruk", "EshanAhmed", "SakilAhmed", "AnikaTabassum", "TahiaTazin", "SamiBourouis", "MohammadMonirujjaman Khan"], "doi": "10.1155/2021/1002799\n10.1109/ECS.2015.7124909\n10.1164/art.1949.60.4.466\n10.1186/1471-2334-5-111\n10.1109/icsipa.2017.8120663\n10.1038/s41598-019-42557-4\n10.1007/978-981-15-0339-9_13\n10.1109/ACCESS.2020.3031384\n10.11591/ijai.v8.i4.pp429-435\n10.1016/j.bbe.2014.08.002\n10.1109/coase.2009.5234173\n10.37200/ijpr/v24i5/pr2020283\n10.1016/j.neunet.2019.04.025\n10.3902/jnns.24.3\n10.1109/CVPR.2016.308\n10.1109/CVPR.2016.90\n10.4171/zaa/1156\n10.1007/s11042-019-7233-0\n10.18178/ijmlc.2021.11.2.1023\n10.1038/s41698-017-0029-7\n10.14738/tmlai.24.328\n10.1109/CHASE.2016.18"}
{"title": "Lung Ultrasound in COVID-19 and Post-COVID-19 Patients, an Evidence-Based Approach.", "abstract": "Worldwide, lung ultrasound (LUS) was utilized to assess coronavirus disease 2019 (COVID-19) patients. Often, imaging protocols were however defined arbitrarily and not following an evidence-based approach. Moreover, extensive studies on LUS in post-COVID-19 patients are currently lacking. This study analyses the impact of different LUS imaging protocols on the evaluation of COVID-19 and post-COVID-19 LUS data.\nLUS data from 220 patients were collected, 100 COVID-19 positive and 120 post-COVID-19. A validated and standardized imaging protocol based on 14 scanning areas and a 4-level scoring system was implemented. We utilized this dataset to compare the capability of 5 imaging protocols, respectively based on 4, 8, 10, 12, and 14 scanning areas, to intercept the most important LUS findings. This to evaluate the optimal trade-off between a time-efficient imaging protocol and an accurate LUS examination. We also performed a longitudinal study, aimed at investigating how to eventually simplify the protocol during follow-up. Additionally, we present results on the agreement between AI models and LUS experts with respect to LUS data evaluation.\nA 12-areas protocol emerges as the optimal trade-off, for both COVID-19 and post-COVID-19 patients. For what concerns follow-up studies, it appears not to be possible to reduce the number of scanning areas. Finally, COVID-19 and post-COVID-19 LUS data seem to show differences capable to confuse AI models that were not trained on post-COVID-19 data, supporting the hypothesis of the existence of LUS patterns specific to post-COVID-19 patients.\nA 12-areas acquisition protocol is recommended for both COVID-19 and post-COVID-19 patients, also during follow-up.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2021-12-04", "authors": ["LibertarioDemi", "FedericoMento", "AntonioDi Sabatino", "AnnaFiengo", "UmbertoSabatini", "Veronica NarvenaMacioce", "MarcoRobol", "FrancescoTursi", "CarmeloSofia", "ChiaraDi Cienzo", "AndreaSmargiassi", "RiccardoInchingolo", "TizianoPerrone"], "doi": "10.1002/jum.15902\n10.1002/jum.15284\n10.1002/jum.15285\n10.1148/radiol.2020200847\n10.1016/j.ejro.2020.100231\n10.1016/j.jamda.2020.05.050\n10.4269/ajtmh.20-0280\n10.1186/s13054-020-02876-9\n10.1007/s00134-020-05996-6\n10.1007/s00134-020-06058-7\n10.1016/j.ultrasmedbio.2020.07.018\n10.1002/jum.15428\n10.1371/journal.pone.0230548\n10.1007/s00134-021-06407-0\n10.3348/kjr.2020.0132\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200463\n10.1002/jum.15548\n10.1109/TMI.2020.2994459\n10.1121/10.0004855\n10.1002/jum.15580"}
{"title": "Nutrition, atherosclerosis, arterial imaging, cardiovascular risk stratification, and manifestations in COVID-19 framework: a narrative review.", "abstract": "", "journal": "Frontiers in bioscience (Landmark edition)", "date": "2021-12-04", "authors": ["SmikshaMunjral", "PuneetAhluwalia", "Ankush DJamthikar", "AnudeepPuvvula", "LucaSaba", "GavinoFaa", "Inder MSingh", "Paramjit SChadha", "MonikaTurk", "Amer MJohri", "Narendra NKhanna", "KlaudijaViskovic", "SophieMavrogeni", "John RLaird", "GyanPareek", "MartinMiner", "David WSobel", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "AthanasiosProtogerou", "PrasannaMisra", "VikasAgarwal", "George DKitas", "RaghuKolluri", "JagjitTeji", "MustafaAl-Maini", "Surinder KDhanjil", "MeyypanSockalingam", "AjitSaxena", "AdityaSharma", "VijayRathore", "MostafaFatemi", "AzraAlizad", "VijayViswanathan", "P KKrishnan", "TomazOmerzu", "SubbaramNaidu", "AndrewNicolaides", "Jasjit SSuri"], "doi": "10.52586/5026"}
{"title": "Detection of COVID-19 With CT Images Using Hybrid Complex Shearlet Scattering Networks.", "abstract": "With the ongoing worldwide coronavirus disease 2019 (COVID-19) pandemic, it is desirable to develop effective algorithms to automatically detect COVID-19 with chest computed tomography (CT) images. Recently, a considerable number of methods based on deep learning have indeed been proposed. However, training an accurate deep learning model requires a large-scale chest CT dataset, which is hard to collect due to the high contagiousness of COVID-19. To achieve improved detection performance, this paper proposes a hybrid framework that fuses the complex shearlet scattering transform (CSST) and a suitable convolutional neural network into a single model. The introduced CSST cascades complex shearlet transforms with modulus nonlinearities and low-pass filter convolutions to compute a sparse and locally invariant image representation. The features computed from the input chest CT images are discriminative for COVID-19 detection. Furthermore, a wide residual network with a redesigned residual block (WR2N) is developed to learn more granular multiscale representations by applying it to scattering features. The combination of model-based CSST and data-driven WR2N leads to a more convenient neural network for image representation, where the idea is to learn only the image parts that the CSST cannot handle instead of all parts. Experiments on two public datasets demonstrate the superiority of our method. We can obtain more accurate results than several state-of-the-art COVID-19 classification methods in terms of measures such as accuracy, the F1-score, and the area under the receiver operating characteristic curve.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-12-03", "authors": ["QingyunRen", "BingyinZhou", "LiangTian", "WeiGuo"], "doi": "10.1109/JBHI.2021.3132157"}
{"title": "Validation of expert system enhanced deep learning algorithm for automated screening for COVID-Pneumonia on chest X-rays.", "abstract": "SARS-CoV2 pandemic exposed the limitations of artificial intelligence based medical imaging systems. Earlier in the pandemic, the absence of sufficient training data prevented effective deep learning (DL) solutions for the diagnosis of COVID-19 based on X-Ray data. Here, addressing the lacunae in existing literature and algorithms with the paucity of initial training data; we describe CovBaseAI, an explainable tool using an ensemble of three DL models and an expert decision system (EDS) for COVID-Pneumonia diagnosis, trained entirely on pre-COVID-19 datasets. The performance and explainability of CovBaseAI was primarily validated on two independent datasets. Firstly, 1401 randomly selected CxR from an Indian quarantine center to assess effectiveness in excluding radiological COVID-Pneumonia requiring higher care. Second, curated dataset; 434 RT-PCR positive cases and 471 non-COVID/Normal historical scans, to assess performance in advanced medical settings. CovBaseAI had an accuracy of 87% with a negative predictive value of 98% in the quarantine-center data. However, sensitivity was 0.66-0.90 taking RT-PCR/radiologist opinion as ground truth. This work provides new insights on the usage of EDS with DL methods and the ability of algorithms to confidently predict COVID-Pneumonia while reinforcing the established learning; that benchmarking based on RT-PCR may not serve as reliable ground truth in radiological diagnosis. Such tools can pave the path for multi-modal high throughput detection of COVID-Pneumonia in screening and referral.", "journal": "Scientific reports", "date": "2021-12-03", "authors": ["Prashant SadashivGidde", "Shyam SunderPrasad", "Ajay PratapSingh", "NitinBhatheja", "SatyarthaPrakash", "PrateekSingh", "AakashSaboo", "RohitTakhar", "SalilGupta", "SumeetSaurav", "RaghunandananM V", "AmritpalSingh", "VirenSardana", "HarshMahajan", "ArjunKalyanpur", "Atanendu ShekharMandal", "VidurMahajan", "AnuragAgrawal", "AnjaliAgrawal", "Vasantha KumarVenugopal", "SanjaySingh", "DebasisDash"], "doi": "10.1038/s41598-021-02003-w\n10.1001/jama.2020.3786\n10.1101/2020.04.04.20052241\n10.1148/radiol.2020201160\n10.1016/j.clinimag.2020.04.001\n10.1186/s43055-019-0116-6\n10.1016/j.cmpb.2020.105608\n10.1016/j.chaos.2020.110190\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.04.12.20062661\n10.1109/access.2020.3010287\n10.1038/s42256-021-00307-0\n10.1109/TPAMI.2016.2577031\n10.1109/ACCESS.2020.3044858\n10.1016/j.media.2021.102046\n10.1016/j.compbiomed.2020.103869\n10.1016/j.cmpb.2020.105581\n10.1186/s12938-020-00831-x\n10.1109/TMI.1983.4307610\n10.1016/S0140-6736(20)30183-5\n10.1289/ehp.8377"}
{"title": "COVID surveillance robot: Monitoring social distancing constraints in indoor scenarios.", "abstract": "Observing social/physical distancing norms between humans has become an indispensable precaution to slow down the transmission of COVID-19. We present a novel method to automatically detect pairs of humans in a crowded scenario who are not maintaining social distancing, i.e. about 2 meters of space between them using an autonomous mobile robot and existing CCTV (Closed-Circuit TeleVision) cameras. The robot is equipped with commodity sensors, namely an RGB-D (Red Green Blue-Depth) camera and a 2-D lidar to detect social distancing breaches within their sensing range and navigate towards the location of the breach. Moreover, it discreetly alerts the relevant people to move apart by using a mounted display. In addition, we also equip the robot with a thermal camera that transmits thermal images to security/healthcare personnel who monitors COVID symptoms such as a fever. In indoor scenarios, we integrate the mobile robot setup with a static wall-mounted CCTV camera to further improve the number of social distancing breaches detected, accurately pursuing walking groups of people etc. We highlight the performance benefits of our robot + CCTV approach in different static and dynamic indoor scenarios.", "journal": "PloS one", "date": "2021-12-02", "authors": ["Adarsh JaganSathyamoorthy", "UtsavPatel", "MoumitaPaul", "YashSavle", "DineshManocha"], "doi": "10.1371/journal.pone.0259713\n10.1186/1471-2458-11-522\n10.2105/AJPH.2013.301269\n10.1017/S0950268816000169\n10.1371/journal.pone.0004005\n10.1038/nature06732\n10.1109/100.580977\n10.1109/LRA.2020.2996593"}
{"title": "An overview of the National COVID-19 Chest Imaging Database: data quality and cohort analysis.", "abstract": "The National COVID-19 Chest Imaging Database (NCCID) is a centralized database containing mainly chest X-rays and computed tomography scans from patients across the UK. The objective of the initiative is to support a better understanding of the coronavirus SARS-CoV-2 disease (COVID-19) and the development of machine learning technologies that will improve care for patients hospitalized with a severe COVID-19 infection. This article introduces the training dataset, including a snapshot analysis covering the completeness of clinical data, and availability of image data for the various use-cases (diagnosis, prognosis, longitudinal risk). An additional cohort analysis measures how well the NCCID represents the wider COVID-19-affected UK population in terms of geographic, demographic, and temporal coverage.\nThe NCCID offers high-quality DICOM images acquired across a variety of imaging machinery; multiple time points including historical images are available for a subset of patients. This volume and variety make the database well suited to development of diagnostic/prognostic models for COVID-associated respiratory conditions. Historical images and clinical data may aid long-term risk stratification, particularly as availability of comorbidity data increases through linkage to other resources. The cohort analysis revealed good alignment to general UK COVID-19 statistics for some categories, e.g., sex, whilst identifying areas for improvements to data collection methods, particularly geographic coverage.\nThe NCCID is a growing resource that provides researchers with a large, high-quality database that can be leveraged both to support the response to the COVID-19 pandemic and as a test bed for building clinically viable medical imaging models.", "journal": "GigaScience", "date": "2021-12-02", "authors": ["DominicCushnan", "OscarBennett", "RosalindBerka", "OttaviaBertolli", "AshwinChopra", "SamieDorgham", "AlbertoFavaro", "TaraGanepola", "MarkHalling-Brown", "GergelyImreh", "JosephJacob", "EmilyJefferson", "Fran\u00e7oisLemarchand", "DanielSchofield", "Jeremy CWyatt", "NoneNone"], "doi": "10.1093/gigascience/giab076\n10.1136/bmj.m2426\n10.1183/13993003.01809-2020\n10.1136/bmj.m1808\n10.1136/bmj.m1985\n10.2139/ssrn.3618215\n10.1136/bmj.m1966\n10.1101/2020.12.30.20249034\n10.5524/100927"}
{"title": "Trends in the application of deep learning networks in medical image analysis: Evolution between 2012 and 2020.", "abstract": "To evaluate the general rules and future trajectories of deep learning (DL) networks in medical image analysis through bibliometric and hot spot analysis of original articles published between 2012 and 2020.\nOriginal articles related to DL and medical imaging were retrieved from the PubMed database. For the analysis, data regarding radiological subspecialties; imaging techniques; DL networks; sample size; study purposes, setting, origins and design; statistical analysis; funding sources; authors; and first authors' affiliation was manually extracted from each article. The Bibliographic Item Co-Occurrence Matrix Builder and VOSviewer were used to identify the research topics of the included articles and illustrate the future trajectories of studies.\nThe study included 2685 original articles. The number of publications on DL and medical imaging has increased substantially since 2017, accounting for 97.2% of all included articles. We evaluated the rules of the application of 47 DL networks to eight radiological tasks on 11 human organ sites. Neuroradiology, thorax, and abdomen were frequent research subjects, while thyroid was under-represented. Segmentation and classification tasks were the primary purposes. U-Net, ResNet, and VGG were the most frequently used Convolutional neural network-derived networks. GAN-derived networks were widely developed and applied in 2020, and transfer learning was highlighted in the COVID-19 studies. Brain, prostate, and diabetic retinopathy-related studies were mature research topics in the field. Breast- and lung-related studies were in a stage of rapid development.\nThis study evaluates the general rules and future trajectories of DL network application in medical image analyses and provides guidance for future studies.", "journal": "European journal of radiology", "date": "2021-12-01", "authors": ["LuWang", "HairuiWang", "YingnaHuang", "BaihuiYan", "ZhihuiChang", "ZhaoyuLiu", "MingfangZhao", "LeiCui", "JiangdianSong", "FanLi"], "doi": "10.1016/j.ejrad.2021.110069"}
{"title": "The Predictive Role of Artificial Intelligence-Based Chest CT Quantification in Patients with COVID-19 Pneumonia.", "abstract": "We sought to analyze the prognostic value of laboratory and clinical data, and an artificial intelligence (AI)-based algorithm for Coronavirus disease 2019 (COVID-19) severity scoring, on CT-scans of patients hospitalized with COVID-19. Moreover, we aimed to determine personalized probabilities of clinical deterioration. Data of symptomatic patients with COVID-19 who underwent chest-CT-examination at the time of hospital admission between April and November 2020 were analyzed. COVID-19 severity score was automatically quantified for each pulmonary lobe as the percentage of affected lung parenchyma with the AI-based algorithm. Clinical deterioration was defined as a composite of admission to the intensive care unit, need for invasive mechanical ventilation, use of vasopressors or in-hospital mortality. In total 326 consecutive patients were included in the analysis (mean age 66.7 \u00b1 15.3 years, 52.1% male) of whom 85 (26.1%) experienced clinical deterioration. In the multivariable regression analysis prior myocardial infarction (OR = 2.81, 95% CI = 1.12-7.04, ", "journal": "Tomography (Ann Arbor, Mich.)", "date": "2021-11-30", "authors": ["Istv\u00e1n ViktorSzab\u00f3", "JuditSimon", "ChiaraNardocci", "Anna S\u00e1raKardos", "NorbertNagy", "Renad-HeyamAbdelrahman", "EmeseZsarn\u00f3czay", "BenceFej\u00e9r", "Bal\u00e1zsFut\u00e1csi", "VeronikaM\u00fcller", "B\u00e9laMerkely", "P\u00e1lMaurovich-Horvat"], "doi": "10.3390/tomography7040058\n10.3947/ic.2020.52.2.154\n10.1016/S1473-3099(20)30120-1\n10.1007/s11357-020-00226-9\n10.1016/j.ajem.2020.09.056\n10.1007/s11547-021-01335-x\n10.1148/radiol.2020202439\n10.1093/cid/ciaa641\n10.1093/ofid/ofaa153\n10.1016/j.medmal.2020.03.007\n10.1001/jamainternmed.2020.0994\n10.1136/bmj.m3339\n10.1016/j.mayocp.2020.04.006\n10.34133/2020/2402961\n10.1161/CIRCULATIONAHA.120.051936\n10.1038/s41598-020-77791-8\n10.1016/j.diabres.2020.108467\n10.1016/j.clim.2020.108509\n10.1001/jamainternmed.2020.3596\n10.1093/cid/ciaa1459\n10.1093/eurheartj/ehaa408\n10.1038/s41375-020-0986-7\n10.1158/2159-8290.CD-20-0422\n10.1038/s41598-021-84137-5\n10.1200/JCO.20.01307\n10.1385/IR:24:2:163\n10.1007/s15010-019-01383-6\n10.1056/NEJMoa2001017\n10.1016/j.media.2020.101860\n10.1148/radiol.2020201874\n10.1148/radiol.2020200905"}
{"title": "Computer-aided COVID-19 diagnosis and a comparison of deep learners using augmented CXRs.", "abstract": "Coronavirus Disease 2019 (COVID-19) is contagious, producing respiratory tract infection, caused by a newly discovered coronavirus. Its death toll is too high, and early diagnosis is the main problem nowadays. Infected people show a variety of symptoms such as fatigue, fever, tastelessness, dry cough, etc. Some other symptoms may also be manifested by radiographic visual identification. Therefore, Chest X-Rays (CXR) play a key role in the diagnosis of COVID-19.\nIn this study, we use Chest X-Rays images to develop a computer-aided diagnosis (CAD) of the disease. These images are used to train two deep networks, the Convolution Neural Network (CNN), and the Long Short-Term Memory Network (LSTM) which is an artificial Recurrent Neural Network (RNN). The proposed study involves three phases. First, the CNN model is trained on raw CXR images. Next, it is trained on pre-processed CXR images and finally enhanced CXR images are used for deep network CNN training. Geometric transformations, color transformations, image enhancement, and noise injection techniques are used for augmentation. From augmentation, we get 3,220 augmented CXRs as training datasets. In the final phase, CNN is used to extract the features of CXR imagery that are fed to the LSTM model. The performance of the four trained models is evaluated by the evaluation techniques of different models, including accuracy, specificity, sensitivity, false-positive rate, and receiver operating characteristic (ROC) curve.\nWe compare our results with other benchmark CNN models. Our proposed CNN-LSTM model gives superior accuracy (99.02%) than the other state-of-the-art models. Our method to get improved input, helped the CNN model to produce a very high true positive rate (TPR 1) and no false-negative result whereas false negative was a major problem while using Raw CXR images.\nWe conclude after performing different experiments that some image pre-processing and augmentation, remarkably improves the results of CNN-based models. It will help a better early detection of the disease that will eventually reduce the mortality rate of COVID.", "journal": "Journal of X-ray science and technology", "date": "2021-11-30", "authors": ["AsmaNaseer", "MariaTamoor", "ArifahAzhar"], "doi": "10.3233/XST-211047\n10.1155/2020/3263407"}
{"title": "Data augmentation using Generative Adversarial Networks (GANs) for GAN-based detection of Pneumonia and COVID-19 in chest X-ray images.", "abstract": "Successful training of convolutional neural networks (CNNs) requires a substantial amount of data. With small datasets, networks generalize poorly. Data Augmentation techniques improve the generalizability of neural networks by using existing training data more effectively. Standard data augmentation methods, however, produce limited plausible alternative data. Generative Adversarial Networks (GANs) have been utilized to generate new data and improve the performance of CNNs. Nevertheless, data augmentation techniques for training GANs are underexplored compared to CNNs. In this work, we propose a new GAN architecture for augmentation of chest X-rays for semi-supervised detection of pneumonia and COVID-19 using generative models. We show that the proposed GAN can be used to effectively augment data and improve classification accuracy of disease in chest X-rays for pneumonia and COVID-19. We compare our augmentation GAN model with Deep Convolutional GAN and traditional augmentation methods (rotate, zoom, etc.) on two different X-ray datasets and show our GAN-based augmentation method surpasses other augmentation methods for training a GAN in detecting anomalies in X-ray images.", "journal": "Informatics in medicine unlocked", "date": "2021-11-30", "authors": ["SamanMotamed", "PatrikRogalla", "FarzadKhalvati"], "doi": "10.1016/j.imu.2021.100779"}
{"title": "The Progress of Medical Image Semantic Segmentation Methods for Application in COVID-19 Detection.", "abstract": "Image medical semantic segmentation has been employed in various areas, including medical imaging, computer vision, and intelligent transportation. In this study, the method of semantic segmenting images is split into two sections: the method of the deep neural network and previous traditional method. The traditional method and the published dataset for segmentation are reviewed in the first step. The presented aspects, including all-convolution network, sampling methods, FCN connector with CRF methods, extended convolutional neural network methods, improvements in network structure, pyramid methods, multistage and multifeature methods, supervised methods, semiregulatory methods, and nonregulatory methods, are then thoroughly explored in current methods based on the deep neural network. Finally, a general conclusion on the use of developed advances based on deep neural network concepts in semantic segmentation is presented.", "journal": "Computational intelligence and neuroscience", "date": "2021-11-30", "authors": ["AminValizadeh", "MortezaShariatee"], "doi": "10.1155/2021/7265644\n10.1145/3453892.3453902\n10.1109/CVPR.2017.472\n10.1109/TITS.2007.895311\n10.1016/j.compmedimag.2014.12.006\n10.1080/01431160110040323\n10.1007/3-540-45786-0_46\n10.1007/bfb0029257\n10.1016/j.neucom.2015.07.138\n10.1109/tip.2018.2794207\n10.1145/3418215\n10.1007/s11036-020-01717-x\n10.1016/j.cmpb.2017.10.022\n10.1109/jiot.2020.3007518\n10.1038/s41598-021-98851-7\n10.1016/j.artmed.2021.102134\n10.1109/cvpr.2009.5206848\n10.1016/j.jksuci.2021.08.019\n10.1007/s11263-007-0090-8\n10.1109/wnyipw.2018.8576421\n10.1109/icomet.2018.8346384\n10.1016/j.chaos.2020.110017\n10.1016/j.cmpb.2017.06.005\n10.1007/978-3-642-15567-3_13\n10.1023/B:VISI.0000029664.99615.94\n10.1109/TGRS.1990.572934\n10.1016/j.cviu.2007.09.014\n10.1109/cvpr.1994.323794\n10.1016/S0734-189X(87)80181-0\n10.1023/A:1007963824710\n10.1109/ICCV.2005.104\n10.1109/TPAMI.2008.275\n10.1007/978-3-642-15552-9_14\n10.1109/ICCV.2011.6126542\n10.1234/12345678\n10.1109/CVPR.2011.5995659\n10.1007/s11263-005-4638-1\n10.1109/TFUZZ.2020.3018191\n10.1109/jbhi.2020.2986376\n10.1007/s00521-020-05687-9\n10.1016/j.compbiolchem.2018.11.017\n10.2307/2529577\n10.1007/s11263-011-0444-0\n10.1023/A:1023030907417\n10.1016/j.asoc.2014.07.024\n10.1007/s00366-020-01234-1\n10.1006/cviu.1996.0510\n10.1016/j.knosys.2016.01.002\n10.1016/j.knosys.2020.106728\n10.1016/j.ejrnm.2016.05.005\n10.1016/j.procs.2015.08.057\n10.1016/j.oceaneng.2020.108415\n10.1016/j.asoc.2014.03.019\n10.1016/j.chaos.2020.110170\n10.1007/s00500-019-04507-0\n10.1080/10255842.2021.1921164\n10.1109/wacv48630.2021.00250\n10.1109/ICCIC.2010.5705892\n10.1016/j.jvcir.2018.04.007\n10.1016/j.neucom.2017.12.011\n10.1016/j.bbe.2019.05.007\n10.1016/j.eswa.2014.09.020\n10.1016/j.procs.2017.11.219\n10.1016/j.measurement.2015.04.028\n10.1016/j.omega.2017.05.001\n10.1016/j.cmpb.2014.01.014\n10.1093/bib/bbaa171\n10.1016/j.asoc.2015.11.040\n10.1007/s12652-020-01680-1\n10.1109/TUFFC.2020.2972573\n10.1016/j.bspc.2021.102481\n10.1016/j.neucom.2021.04.012\n10.1590/2179-10742017v16i3854\n10.1561/2200000006\n10.1145/1390156.1390224\n10.1007/s11356-020-11644-9\n10.1162/neco.1997.9.8.1735\n10.3390/app10249110\n10.1109/TPAMI.2016.2572683\n10.1109/ICCV.2015.178\n10.1016/S0006-3495(03)74813-9\n10.1007/s11042-019-08597-8\n10.1109/TPAMI.2017.2699184\n10.1109/CVPR.2015.7298959\n10.1109/ICCV.2015.162\n10.1007/978-3-642-75988-8_28\n10.1109/CVPR.2017.75\n10.1145/3065386\n10.1109/CVPR.2016.90\n10.1016/j.patcog.2019.01.006\n10.1109/CVPR.2017.634\n10.1109/CVPR.2016.308\n10.1109/CVPR.2017.684\n10.1117/12.936662\n10.1109/cvprw.2018.00051\n10.1109/CVPR.2016.348\n10.1109/CVPR.2016.396\n10.3390/rs11030339\n10.1007/978-3-319-46487-9_32\n10.1109/TPAMI.2012.231\n10.1109/CVPR.2017.660\n10.3390/rs13050865\n10.1109/CVPR.2017.106\n10.1016/j.ins.2019.04.022\n10.1007/s00500-013-1089-4\n10.1007/s00366-020-01028-5\n10.1016/j.neucom.2020.10.038\n10.1016/j.knosys.2020.106684\n10.1109/CVPR.2015.7298642\n10.1364/OE.410723\n10.1109/CVPR.2015.7299170\n10.1109/cvpr.2001.990517\n10.1109/TMI.2016.2538465\n10.1016/j.media.2016.05.004\n10.1016/j.neucom.2018.05.011\n10.1016/j.media.2016.10.004\n10.1016/j.compbiomed.2015.02.003\n10.1016/j.asoc.2019.02.036\n10.1016/j.neucom.2020.03.097\n10.1007/978-3-319-75238-9_16\n10.1109/ACCESS.2019.2927433\n10.2196/jmir.2930\n10.1016/j.patrec.2019.03.022\n10.1016/j.imu.2019.01.001\n10.1109/access.2019.2892795\n10.1016/j.bios.2021.113418\n10.1117/12.2581314\n10.1016/j.jscs.2021.101348\n10.1063/5.0048123\n10.1148/ryct.2020200075\n10.1109/TMI.2020.2993291\n10.1038/s41467-020-17280-8\n10.1016/j.asoc.2021.107449\n10.1016/j.scitotenv.2020.138705\n10.1007/s11356-021-13249-2\n10.1155/2021/9995073\n10.1007/s11356-021-15292-5\n10.1016/j.compmedimag.2020.101715\n10.1016/j.aej.2020.10.046\n10.1016/j.bspc.2021.102458\n10.1155/2021/6653879\n10.1080/00207454.2021.1883602\n10.1007/s11263-019-01282-1"}
{"title": "Multi-center sparse learning and decision fusion for automatic COVID-19 diagnosis.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic caused by the novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has led to a sharp increase in hospitalized patients with multi-organ disease pneumonia. Early and automatic diagnosis of COVID-19 is essential to slow down the spread of this epidemic and reduce the mortality of patients infected with SARS-CoV-2. In this paper, we propose a joint multi-center sparse learning (MCSL) and decision fusion scheme exploiting chest CT images for automatic COVID-19 diagnosis. Specifically, considering the inconsistency of data in multiple centers, we first convert CT images into histogram of oriented gradient (HOG) images to reduce the structural differences between multi-center data and enhance the generalization performance. We then exploit a 3-dimensional convolutional neural network (3D-CNN) model to learn the useful information between and within 3D HOG image slices and extract multi-center features. Furthermore, we employ the proposed MCSL method that learns the intrinsic structure between multiple centers and within each center, which selects discriminative features to jointly train multi-center classifiers. Finally, we fuse these decisions made by these classifiers. Extensive experiments are performed on chest CT images from five centers to validate the effectiveness of the proposed method. The results demonstrate that the proposed method can improve COVID-19 diagnosis performance and outperform the state-of-the-art methods.", "journal": "Applied soft computing", "date": "2021-11-30", "authors": ["ZhongweiHuang", "HaijunLei", "GuoliangChen", "HaimeiLi", "ChuandongLi", "WenwenGao", "YueChen", "YaofaWang", "HaiboXu", "GuolinMa", "BaiyingLei"], "doi": "10.1016/j.asoc.2021.108088"}
{"title": "Feasibility of Radiomics to Differentiate Coronavirus Disease 2019 (COVID-19) from H1N1 Influenza Pneumonia on Chest Computed Tomography: A Proof of Concept.", "abstract": "Chest computed tomography (CT) plays an essential role in diagnosing coronavirus disease 2019 (COVID-19). However, CT findings are often nonspecific among different viral pneumonia conditions. The differentiation between COVID-19 and influenza can be challenging when seasonal influenza concurs with the COVID-19 pandemic. This study was conducted to test the ability of radiomics-artificial intelligence (AI) to perform this task.\nIn this retrospective study, chest CT images from 47 patients with COVID-19 (after February 2020) and 19 patients with H1N1 influenza (before September 2019) pneumonia were collected from three hospitals affiliated with Arak University of Medical Sciences, Arak, Iran. All pulmonary lesions were segmented on CT images. Multiple radiomics features were extracted from the lesions and used to develop support-vector machine (SVM), k-nearest neighbor (k-NN), decision tree, neural network, adaptive boosting (AdaBoost), and random forest.\nThe patients with COVID-19 and H1N1 influenza were not significantly different in age and sex (P=0.13 and 0.99, respectively). Nonetheless, the average time between initial symptoms/hospitalization and chest CT was shorter in the patients with COVID-19 (P=0.001 and 0.01, respectively). After the implementation of the inclusion and exclusion criteria, 453 pulmonary lesions were included in this study. On the harmonized features, random forest yielded the highest performance (area under the curve=0.97, sensitivity=89%, precision=90%, F1 score=89%, and classification accuracy=89%).\nIn our preliminary study, radiomics feature extraction, conjoined with AI, especially random forest and neural network, appeared to yield very promising results in the differentiation between COVID-19 and H1N1 influenza on chest CT.", "journal": "Iranian journal of medical sciences", "date": "2021-11-30", "authors": ["MohsenTabatabaei", "BaharakTasorian", "ManuGoyal", "AbdollatifMoini", "HoumanSotoudeh"], "doi": "10.30476/ijms.2021.88036.1858\n10.7861/clinmed.2019-coron\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200642\n10.1007/s00330-020-07347-x\n10.21037/tlcr.2019.12.19\n10.3760/cma.j.issn.1001-9391.2019.09.019\n10.1186/s12880-019-0355-z\n10.21037/jtd.2019.06.22\n10.1016/j.ejro.2020.100271\n10.1002/mco2.14\n10.1117/12.2521488\n10.1186/s41747-018-0068-z\n10.1002/mp.12123\n10.1038/s41598-018-31509-z\n10.1088/1361-6560/ab6177\n10.1371/journal.pone.0216308\n10.1002/jmv.26242"}
{"title": "Artificial intelligence for imaging-based COVID-19 detection: Systematic review comparing added value of AI versus human readers.", "abstract": "A growing number of studies have examined whether Artificial Intelligence (AI) systems can support imaging-based diagnosis of COVID-19-caused pneumonia, including both gains in diagnostic performance and speed. However, what is currently missing is a combined appreciation of studies comparing human readers and AI.\nWe followed PRISMA-DTA guidelines for our systematic review, searching EMBASE, PUBMED and Scopus databases. To gain insights into the potential value of AI methods, we focused on studies comparing the performance of human readers versus AI models or versus AI-supported human readings.\nOur search identified 1270 studies, of which 12 fulfilled specific selection criteria. Concerning diagnostic performance, in testing datasets reported sensitivity was 42-100% (human readers, n\u202f=\u202f9 studies), 60-95% (AI systems, n\u202f=\u202f10) and 81-98% (AI-supported readers, n\u202f=\u202f3), whilst reported specificity was 26-100% (human readers, n\u202f=\u202f8), 61-96% (AI systems, n\u202f=\u202f10) and 78-99% (AI-supported readings, n\u202f=\u202f2). One study highlighted the potential of AI-supported readings for the assessment of lung lesion burden changes, whilst two studies indicated potential time savings for detection with AI.\nOur review indicates that AI systems or AI-supported human readings show less performance variability (interquartile range) in general, and may support the differentiation of COVID-19 pneumonia from other forms of pneumonia when used in high-prevalence and symptomatic populations. However, inconsistencies related to study design, reporting of data, areas of risk of bias, as well as limitations of statistical analyses complicate clear conclusions. We therefore support efforts for developing critical elements of study design when assessing the value of AI for diagnostic imaging.", "journal": "European journal of radiology", "date": "2021-11-29", "authors": ["ChristineKriza", "ValeriaAmenta", "AlexandreZeni\u00e9", "DimitrisPanidis", "HubertChassaigne", "PatriciaUrb\u00e1n", "UweHolzwarth", "Aisha VanessaSauer", "VittorioReina", "Claudius BenedictGriesinger"], "doi": "10.1016/j.ejrad.2021.110028\n10.2196/10010\n10.1016/j.ejrad.2020.109402\n10.1016/j.patcog.2020.107613\n10.1186/s41747-020-00203-z\n10.1016/j.ibmed.2020.100014"}
{"title": "Medical image processing and COVID-19: A literature review and bibliometric analysis.", "abstract": "COVID-19 crisis has placed medical systems over the world under unprecedented and growing pressure. Medical imaging processing can help in the diagnosis, treatment, and early detection of diseases. It has been considered as one of the modern technologies applied to fight against the COVID-19 crisis. Although several artificial intelligence, machine learning, and deep learning techniques have been deployed in medical image processing in the context of COVID-19 disease, there is a lack of research considering systematic literature review and categorization of published studies in this field. A systematic review locates, assesses, and interprets research outcomes to address a predetermined research goal to present evidence-based practical and theoretical insights. The main goal of this study is to present a literature review of the deployed methods of medical image processing in the context of the COVID-19 crisis. With this in mind, the studies available in reliable databases were retrieved, studied, evaluated, and synthesized. Based on the in-depth review of literature, this study structured a conceptual map that outlined three multi-layered folds: data gathering and description, main steps of image processing, and evaluation metrics. The main research themes were elaborated in each fold, allowing the authors to recommend upcoming research paths for scholars. The outcomes of this review highlighted that several methods have been adopted to classify the images related to the diagnosis and detection of COVID-19. The adopted methods have presented promising outcomes in terms of accuracy, cost, and detection speed.", "journal": "Journal of infection and public health", "date": "2021-11-28", "authors": ["Rabab AliAbumalloh", "MehrbakhshNilashi", "MuhammedYousoof Ismail", "AshwaqAlhargan", "AbdullahAlghamdi", "Ahmed OmarAlzahrani", "LinahSaraireh", "ReemOsman", "ShahlaAsadi"], "doi": "10.1016/j.jiph.2021.11.013\n10.1007/s10209-018-0618-4\n10.1016/j.infsof.2008.09.009\n10.1016/j.asoc.2016.04.020\n10.1016/j.ijinfomgt.2016.06.005\n10.1016/j.jksuci.2021.01.007"}
{"title": "Can Deep Learning-Based Volumetric Analysis Predict Oxygen Demand Increase in Patients with COVID-19 Pneumonia?", "abstract": "", "journal": "Medicina (Kaunas, Lithuania)", "date": "2021-11-28", "authors": ["MarieTakahashi", "TomoyukiFujioka", "ToshihiroHorii", "KoichiroKimura", "MizukiKimura", "YurikaHashimoto", "YoshioKitazume", "MitsuhiroKishino", "UkihideTateishi"], "doi": "10.3390/medicina57111148\n10.7326/M20-3012\n10.1056/NEJMsb2005114\n10.1016/j.rmed.2020.105941\n10.1007/s00038-020-01390-7\n10.1148/radiol.2020202504\n10.1148/radiol.2020200843\n10.1148/radiol.2020200527\n10.1148/radiol.2020200370\n10.1007/s00330-020-06978-4\n10.1007/s11604-020-01012-5\n10.1007/s11604-019-00831-5\n10.3390/diagnostics10050330\n10.1148/rg.2017170077\n10.1016/j.mri.2020.10.003\n10.1186/s43055-020-00309-9\n10.1148/radiol.2020202439\n10.1007/s00330-020-07044-9\n10.5152/dir.2019.20294\n10.1016/j.media.2020.101836\n10.1038/bmt.2012.244\n10.1371/journal.pone.0251946\n10.7150/thno.45985\n10.1016/j.acra.2020.09.004"}
{"title": "COVID-19 Detection Using Deep Learning Algorithm on Chest X-ray Images.", "abstract": "COVID-19, regarded as the deadliest virus of the 21st century, has claimed the lives of millions of people around the globe in less than two years. Since the virus initially affects the lungs of patients, X-ray imaging of the chest is helpful for effective diagnosis. Any method for automatic, reliable, and accurate screening of COVID-19 infection would be beneficial for rapid detection and reducing medical or healthcare professional exposure to the virus. In the past, Convolutional Neural Networks (CNNs) proved to be quite successful in the classification of medical images. In this study, an automatic deep learning classification method for detecting COVID-19 from chest X-ray images is suggested using a CNN. A dataset consisting of 3616 COVID-19 chest X-ray images and 10,192 healthy chest X-ray images was used. The original data were then augmented to increase the data sample to 26,000 COVID-19 and 26,000 healthy X-ray images. The dataset was enhanced using histogram equalization, spectrum, grays, cyan and normalized with NCLAHE before being applied to CNN models. Initially using the dataset, the symptoms of COVID-19 were detected by employing eleven existing CNN models; VGG16, VGG19, MobileNetV2, InceptionV3, NFNet, ResNet50, ResNet101, DenseNet, EfficientNetB7, AlexNet, and GoogLeNet. From the models, MobileNetV2 was selected for further modification to obtain a higher accuracy of COVID-19 detection. Performance evaluation of the models was demonstrated using a confusion matrix. It was observed that the modified MobileNetV2 model proposed in the study gave the highest accuracy of 98% in classifying COVID-19 and healthy chest X-rays among all the implemented CNN models. The second-best performance was achieved from the pre-trained MobileNetV2 with an accuracy of 97%, followed by VGG19 and ResNet101 with 95% accuracy for both the models. The study compares the compilation time of the models. The proposed model required the least compilation time with 2 h, 50 min and 21 s. Finally, the Wilcoxon signed-rank test was performed to test the statistical significance. The results suggest that the proposed method can efficiently identify the symptoms of infection from chest X-ray images better than existing methods.", "journal": "Biology", "date": "2021-11-28", "authors": ["ShamimaAkter", "F M Javed MehediShamrat", "SovonChakraborty", "AsifKarim", "SamiAzam"], "doi": "10.3390/biology10111174\n10.1016/j.ijid.2020.01.050\n10.1016/j.jaut.2020.102433\n10.1016/S0140-6736(20)30211-7\n10.1038/s41586-020-2008-3\n10.1056/NEJMoa2002032\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30185-9\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1056/NEJMoa2001191\n10.1056/NEJMra1312885\n10.11591/ijece.v11i3.pp2631-2639\n10.1109/ACCESS.2020.3017082\n10.11591/ijeecs.v23.i1.pp463-470\n10.3390/app11167174\n10.1088/0031-9155/60/7/2715\n10.1088/0031-9155/60/10/4015\n10.1016/j.cmpb.2018.05.006\n10.1016/j.cmpb.2018.04.011\n10.1016/j.cmpb.2019.01.005\n10.1049/iet-ipr.2016.0526\n10.1016/j.jviromet.2020.113974\n10.1148/radiol.2020200330\n10.1136/bmj.m641\n10.1148/radiol.2020200527\n10.1148/radiol.2020200343\n10.1136/bmjopen-2020-047110\n10.1183/09031936.01.00213501\n10.1148/ryct.2020200034\n10.1007/s13246-020-00865-4\n10.1007/s10044-021-00984-y\n10.1038/s41746-020-0273-z\n10.1016/j.media.2020.101794\n10.1016/j.patrec.2020.09.010\n10.3390/make2040027\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1016/j.eswa.2020.113909\n10.1177/2472630320958376\n10.1016/j.radi.2020.10.018\n10.1109/ACCESS.2020.3044858\n10.1109/TNNLS.2021.3070467\n10.1038/s41598-020-76550-z\n10.1007/s10489-020-01829-7\n10.1016/j.chaos.2020.110122\n10.1016/j.irbm.2020.07.001\n10.1109/ACCESS.2020.2974242\n10.1016/j.eswa.2020.114054\n10.1016/j.asoc.2021.107878\n10.1609/aaai.v33i01.3301801\n10.14569/IJACSA.2021.0120880\n10.1111/j.0006-341X.2003.00125.x"}
{"title": "Segmentation of infected region in CT images of COVID-19 patients based on QC-HC U-net.", "abstract": "Since the outbreak of COVID-19 in 2019, the rapid spread of the epidemic has brought huge challenges to medical institutions. If the pathological region in the COVID-19 CT image can be automatically segmented, it will help doctors quickly determine the patient's infection, thereby speeding up the diagnosis process. To be able to automatically segment the infected area, we proposed a new network structure and named QC-HC U-Net. First, we combine residual connection and dense connection to form a new connection method and apply it to the encoder and the decoder. Second, we choose to add Hypercolumns in the decoder section. Compared with the benchmark 3D U-Net, the improved network can effectively avoid vanishing gradient while extracting more features. To improve the situation of insufficient data, resampling and data enhancement methods are selected in this paper to expand the datasets. We used 63 cases of MSD lung tumor data for training and testing, continuously verified to ensure the training effect of this model, and then selected 20 cases of public COVID-19 data for training and testing. Experimental results showed that in the segmentation of COVID-19, the specificity and sensitivity were 85.3% and 83.6%, respectively, and in the segmentation of MSD lung tumors, the specificity and sensitivity were 81.45% and 80.93%, respectively, without any fitting.", "journal": "Scientific reports", "date": "2021-11-26", "authors": ["QinZhang", "XiaoqiangRen", "BenzhengWei"], "doi": "10.1038/s41598-021-01502-0\n10.1001/jama.2020.1585\n10.1002/mp.13865\n10.1109/TBDATA.2021.3056564\n10.1016/j.aej.2021.01.011\n10.1007/s11036-020-01703-3\n10.1002/mp.14676\n10.1007/s10489-020-01826-w\n10.1016/j.asoc.2020.106580\n10.1109/TMI.2020.2996645\n10.1109/TMI.2020.2995965\n10.1109/TMI.2019.2894349\n10.32604/cmc.2021.016698\n10.1007/s00330-019-06441-z\n10.1007/s10462-019-09716-5\n10.1016/j.compmedimag.2018.01.006\n10.32604/cmc.2021.017433"}
{"title": "Factors determining generalization in deep learning models for scoring COVID-CT images.", "abstract": "The COVID-19 pandemic has inspired unprecedented data collection and computer vision modelling efforts worldwide, focused on the diagnosis of COVID-19 from medical images. However, these models have found limited, if any, clinical application due in part to unproven generalization to data sets beyond their source training corpus. This study investigates the generalizability of deep learning models using publicly available COVID-19 Computed Tomography data through cross dataset validation. The predictive ability of these models for COVID-19 severity is assessed using an independent dataset that is stratified for COVID-19 lung involvement. Each inter-dataset study is performed using histogram equalization, and contrast limited adaptive histogram equalization with and without a learning Gabor filter. We show that under certain conditions, deep learning models can generalize well to an external dataset with F1 scores up to 86%. The best performing model shows predictive accuracy of between 75% and 96% for lung involvement scoring against an external expertly stratified dataset. From these results we identify key factors promoting deep learning generalization, being primarily the uniform acquisition of training images, and secondly diversity in CT slice position.", "journal": "Mathematical biosciences and engineering : MBE", "date": "2021-11-25", "authors": ["Michael JamesHorry", "SubrataChakraborty", "BiswajeetPradhan", "MaryamFallahpoor", "HosseinChegeni", "ManoranjanPaul"], "doi": "10.3934/mbe.2021456"}
{"title": "Focus, Fusion, and Rectify: Context-Aware Learning for COVID-19 Lung Infection Segmentation.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic is spreading worldwide. Considering the limited clinicians and resources and the evidence that computed tomography (CT) analysis can achieve comparable sensitivity, specificity, and accuracy with reverse-transcription polymerase chain reaction, the automatic segmentation of lung infection from CT scans supplies a rapid and effective strategy for COVID-19 diagnosis, treatment, and follow-up. It is challenging because the infection appearance has high intraclass variation and interclass indistinction in CT slices. Therefore, a new context-aware neural network is proposed for lung infection segmentation. Specifically, the autofocus and panorama modules are designed for extracting fine details and semantic knowledge and capturing the long-range dependencies of the context from both peer level and cross level. Also, a novel structure consistency rectification is proposed for calibration by depicting the structural relationship between foreground and background. Experimental results on multiclass and single-class COVID-19 CT images demonstrate the effectiveness of our work. In particular, our method obtains the mean intersection over union (mIoU) score of 64.8%, 65.2%, and 73.8% on three benchmark datasets for COVID-19 infection segmentation.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-11-24", "authors": ["RuxinWang", "ChaojieJi", "YuxiaoZhang", "YeLi"], "doi": "10.1109/TNNLS.2021.3126305"}
{"title": "From Hume to Wuhan: An Epistemological Journey on the Problem of Induction in COVID-19 Machine Learning Models and its Impact Upon Medical Research.", "abstract": "Advances in computer science have transformed the way artificial intelligence is employed in academia, with Machine Learning (ML) methods easily available to researchers from diverse areas thanks to intuitive frameworks that yield extraordinary results. Notwithstanding, current trends in the mainstream ML community tend to emphasise ", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-11-24", "authors": ["CarlosVega"], "doi": "10.1109/ACCESS.2021.3095222\n10.1101/2021.02.15.21251775\n10.1016/j.tube.2020.102020\n10.1007/3-540-48229-6_39"}
{"title": "Drr4covid: Learning Automated COVID-19 Infection Segmentation From Digitally Reconstructed Radiographs.", "abstract": "Automated infection measurement and COVID-19 diagnosis based on Chest X-ray (CXR) imaging is important for faster examination, where infection segmentation is an essential step for assessment and quantification. However, due to the heterogeneity of X-ray imaging and the difficulty of annotating infected regions precisely, learning automated infection segmentation on CXRs remains a challenging task. We propose a novel approach, called DRR4Covid, to learn COVID-19 infection segmentation on CXRs from digitally reconstructed radiographs (DRRs). DRR4Covid consists of an infection-aware DRR generator, a segmentation network, and a domain adaptation module. Given a labeled Computed Tomography scan, the infection-aware DRR generator can produce infection-aware DRRs with pixel-level annotations of infected regions for training the segmentation network. The domain adaptation module is designed to enable the segmentation network trained on DRRs to generalize to CXRs. The statistical analyses made on experiment results have indicated that our infection-aware DRRs are significantly better than standard DRRs in learning COVID-19 infection segmentation (p < 0.05) and the domain adaptation module can improve the infection segmentation performance on CXRs significantly (p < 0.05). Without using any annotations of CXRs, our network has achieved a classification score of (Accuracy: 0.949, AUC: 0.987, F1-score: 0.947) and a segmentation score of (Accuracy: 0.956, AUC: 0.980, F1-score: 0.955) on a test set with 558 normal cases and 558 positive cases. Besides, by adjusting the strength of radiological signs of COVID-19 infection in infection-aware DRRs, we estimate the detection limit of X-ray imaging in detecting COVID-19 infection. The estimated detection limit, measured by the percent volume of the lung that is infected by COVID-19, is 19.43% \u00b1 16.29%, and the estimated lower bound of infected voxel contribution rate for significant radiological signs of COVID-19 infection is 20.0%. Our codes are made publicly available at https://github.com/PengyiZhang/DRR4Covid.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-11-24", "authors": ["PengyiZhang", "YunxinZhong", "YulinDeng", "XiaoyingTang", "XiaoqiongLi"], "doi": "10.1109/ACCESS.2020.3038279\n10.1016/S0140-6736(20)30260-9\n10.1016/S2213-2600(20)30076-X\n10.1016/S1473-3099(20)30086-4\n10.1109/TMI.2020.2991954\n10.1109/TMI.2020.2993291\n10.1109/RBME.2020.2987975\n10.1118/1.595715\n10.1016/0360-3016(90)90074-T\n10.1109/TMI.2005.856749\n10.1148/radiol.2020201160\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1088/0031-9155/45/10/305\n10.1007/s13246-014-0257-x\n10.1118/1.3190156\n10.1007/s11548-019-02011-2\n10.1109/TPAMI.2019.2903401\n10.1109/TNNLS.2020.2988928\n10.1016/j.neunet.2019.07.010\n10.1371/journal.pone.0130140"}
{"title": "2019 Novel Coronavirus-Infected Pneumonia on CT: A Feasibility Study of Few-Shot Learning for Computerized Diagnosis of Emergency Diseases.", "abstract": "COVID-19 is an emerging disease with transmissibility and severity. So far, there are no effective therapeutic drugs or vaccines for COVID-19. The most serious complication of COVID-19 is a type of pneumonia called 2019 novel coronavirus-infected pneumonia (NCIP) with about 4.3% mortality rate. Comparing to chest Digital Radiography (DR), it is recently reported that chest Computed Tomography (CT) is more useful to serve as the early screening and diagnosis tool for NCIP. In this study, aimed to help physicians make the diagnostic decision, we develop a machine learning (ML) approach for automated diagnosis of NCIP on chest CT. Different from most ML approaches which often require training on thousands or millions of samples, we design a few-shot learning approach, in which we combine few-shot learning with weakly supervised model training, for computerized NCIP diagnosis. A total of 824 patients are retrospectively collected from two Hospitals with IRB approval. We first use 9 patients with clinically confirmed NCIP and 20 patients without known lung diseases for training a location detector which is a multitask deep convolutional neural network (DCNN) designed to output a probability of NCIP and the segmentation of targeted lesion area. An experienced radiologist manually localizes the potential locations of NCIPs on chest CTs of 9 COVID-19 patients and interactively segments the area of the NCIP lesions as the reference standard. Then, the multitask DCNN is furtherly fine-tuned by a weakly supervised learning scheme with 291 case-level labeled samples without lesion labels. A test set of 293 patients is independently collected for evaluation. With our NCIP-Net, the test AUC is 0.91. Our system has potential to serve as the NCIP screening and diagnosis tools for the fight of COVID-19's endemic and pandemic.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-11-24", "authors": ["YaomingLai", "GuangmingLi", "DongmeiWu", "WanminLian", "ChengLi", "JunzhangTian", "XiaofenMa", "HuiChen", "WenXu", "JunWei", "YaqinZhang", "GuihuaJiang"], "doi": "10.1109/ACCESS.2020.3033069\n10.1056/NEJMoa2001017\n10.1136/bmj.m606\n10.1093/cid/ciaa247"}
{"title": "UMLF-COVID: an unsupervised meta-learning model specifically designed to identify X-ray images of COVID-19 patients.", "abstract": "With the rapid spread of COVID-19 worldwide, quick screening for possible COVID-19 patients has become the focus of international researchers. Recently, many deep learning-based Computed Tomography (CT) image/X-ray image fast screening models for potential COVID-19 patients have been proposed. However, the existing models still have two main problems. First, most of the existing supervised models are based on pre-trained model parameters. The pre-training model needs to be constructed on a dataset with features similar to those in COVID-19 X-ray images, which limits the construction and use of the model. Second, the number of categories based on the X-ray dataset of COVID-19 and other pneumonia patients is usually imbalanced. In addition, the quality is difficult to distinguish, leading to non-ideal results with the existing model in the multi-class classification COVID-19 recognition task. Moreover, no researchers have proposed a COVID-19 X-ray image learning model based on unsupervised meta-learning.\nThis paper first constructed an unsupervised meta-learning model for fast screening of COVID-19 patients (UMLF-COVID). This model does not require a pre-trained model, which solves the limitation problem of model construction, and the proposed unsupervised meta-learning framework solves the problem of sample imbalance and sample quality.\nThe UMLF-COVID model is tested on two real datasets, each of which builds a three-category and four-category model. And the experimental results show that the accuracy of the UMLF-COVID model is 3-10% higher than that of the existing models.\nIn summary, we believe that the UMLF-COVID model is a good complement to COVID-19 X-ray fast screening models.", "journal": "BMC medical imaging", "date": "2021-11-24", "authors": ["RuiMiao", "XinDong", "Sheng-LiXie", "YongLiang", "Sio-LongLo"], "doi": "10.1186/s12880-021-00704-2"}
{"title": "A multitask dual-stream attention network for the identification of KRAS mutation in colorectal cancer.", "abstract": "It is of great significance to accurately identify the KRAS gene mutation status for patients in tumor prognosis and personalized treatment. Although the computer-aided diagnosis system based on deep learning has gotten all-round development, its performance still cannot meet the current clinical application requirements due to the inherent limitations of small-scale medical image data set and inaccurate lesion feature extraction. Therefore, our aim is to propose a deep learning model based on T2 MRI of colorectal cancer (CRC) patients to identify whether KRAS gene is mutated.\nIn this research, a multitask attentive model is proposed to identify KRAS gene mutations in patients, which is mainly composed of a segmentation subnetwork and an identification subnetwork. Specifically, at first, the features extracted by the encoder of segmentation model are used as guidance information to guide the two attention modules in the identification network for precise activation of the lesion area. Then the original image of the lesion and the segmentation result are concatenated for feature extraction. Finally, features extracted from the second step are combined with features activated by the attention modules to identify the gene mutation status. In this process, we introduce the interlayer loss function to encourage the similarity of the two subnetwork parameters and ensure that the key features are fully extracted to alleviate the overfitting problem caused by small data set to some\u00a0extent.\nThe proposed identification model is benchmarked primarily using 15-fold cross validation. Three hundred and eighty-two images from 36 clinical cases were used to test the model. For the identification of KRAS mutation status, the average accuracy is 89.95 \nWe developed a novel deep learning-based model to identify the KRAS status in CRC. We demonstrated the excellent properties of the proposed identification through comparison with ground truth gene mutation status of 36 clinical cases. And all these results show that the novel method has great potential for clinical\u00a0application.", "journal": "Medical physics", "date": "2021-11-23", "authors": ["KaiSong", "ZijuanZhao", "YulanMa", "JiaWenWang", "WeiWu", "YanQiang", "JuanjuanZhao", "SumanChaudhary"], "doi": "10.1002/mp.15361"}
{"title": "A review of explainable and interpretable AI with applications in COVID-19 imaging.", "abstract": "The development of medical imaging artificial intelligence (AI) systems for evaluating COVID-19 patients has demonstrated potential for improving clinical decision making and assessing patient outcomes during the recent COVID-19 pandemic. These have been applied to many medical imaging tasks, including disease diagnosis and patient prognosis, as well as augmented other clinical measurements to better inform treatment decisions. Because these systems are used in life-or-death decisions, clinical implementation relies on user trust in the AI output. This has caused many developers to utilize explainability techniques in an attempt to help a user understand when an AI algorithm is likely to succeed as well as which cases may be problematic for automatic assessment, thus increasing the potential for rapid clinical translation. AI application to COVID-19 has been marred with controversy recently. This review discusses several aspects of explainable and interpretable AI as it pertains to the evaluation of COVID-19 disease and it can restore trust in AI application to this disease. This includes the identification of common tasks that are relevant to explainable medical imaging AI, an overview of several modern approaches for producing explainable output as appropriate for a given imaging scenario, a discussion of how to evaluate explainable AI, and recommendations for best practices in explainable/interpretable AI implementation. This review will allow developers of AI systems for COVID-19 to quickly understand the basics of several explainable AI techniques and assist in the selection of an approach that is both appropriate and effective for a given scenario.", "journal": "Medical physics", "date": "2021-11-20", "authors": ["Jordan DFuhrman", "NaveenaGorre", "QiyuanHu", "HuiLi", "IssamEl Naqa", "Maryellen LGiger"], "doi": "10.1002/mp.15359"}
{"title": "Deep Learning Algorithm for COVID-19 Classification Using Chest X-Ray Images.", "abstract": "Early diagnosis of the harmful severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), along with clinical expertise, allows governments to break the transition chain and flatten the epidemic curve. Although reverse transcription-polymerase chain reaction (RT-PCR) offers quick results, chest X-ray (CXR) imaging is a more reliable method for disease classification and assessment. The rapid spread of the coronavirus disease 2019 (COVID-19) has triggered extensive research towards developing a COVID-19 detection toolkit. Recent studies have confirmed that the deep learning-based approach, such as convolutional neural networks (CNNs), provides an optimized solution for COVID-19 classification; however, they require substantial training data for learning features. Gathering this training data in a short period has been challenging during the pandemic. Therefore, this study proposes a new model of CNN and deep convolutional generative adversarial networks (DCGANs) that classify CXR images into normal, pneumonia, and COVID-19. The proposed model contains eight convolutional layers, four max-pooling layers, and two fully connected layers, which provide better results than the existing pretrained methods (AlexNet and GoogLeNet). DCGAN performs two tasks: (1) generating synthetic/fake images to overcome the challenges of an imbalanced dataset and (2) extracting deep features of all images in the dataset. In addition, it enlarges the dataset and represents the characteristics of diversity to provide a good generalization effect. In the experimental analysis, we used four distinct publicly accessible datasets of chest X-ray images (COVID-19 X-ray, COVID Chest X-ray, COVID-19 Radiography, and CoronaHack-Chest X-Ray) to train and test the proposed CNN and the existing pretrained methods. Thereafter, the proposed CNN method was trained with the four datasets based on the DCGAN synthetic images, resulting in higher accuracy (94.8%, 96.6%, 98.5%, and 98.6%) than the existing pretrained models. The overall results suggest that the proposed DCGAN-CNN approach is a promising solution for efficient COVID-19 diagnosis.", "journal": "Computational and mathematical methods in medicine", "date": "2021-11-20", "authors": ["SharmilaV J", "Jemi FlorinabelD"], "doi": "10.1155/2021/9269173\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.2214/AJR.20.22976\n10.1016/j.eswa.2017.11.028\n10.1016/j.procs.2018.10.513\n10.1016/j.ifacol.2019.12.406\n10.1155/2021/8785636\n10.1007/s40846-020-00529-4\n10.1016/j.media.2017.07.005\n10.1016/j.patrec.2020.09.010\n10.1109/ACCESS.2020.2994762\n10.1007/978-3-030-21074-8_24\n10.1148/radiol.2020200343\n10.1007/s15010-020-01427-2\n10.1111/exsy.12759\n10.32604/cmc.2021.012955\n10.32604/cmc.2021.012874\n10.1109/ACCESS.2020.2995597\n10.4103/0970-2113.120610"}
{"title": "Deep Learning Approaches for Detecting COVID-19 From Chest X-Ray Images: A Survey.", "abstract": "Chest X-ray (CXR) imaging is a standard and crucial examination method used for suspected cases of coronavirus disease (COVID-19). In profoundly affected or limited resource areas, CXR imaging is preferable owing to its availability, low cost, and rapid results. However, given the rapidly spreading nature of COVID-19, such tests could limit the efficiency of pandemic control and prevention. In response to this issue, artificial intelligence methods such as deep learning are promising options for automatic diagnosis because they have achieved state-of-the-art performance in the analysis of visual information and a wide range of medical images. This paper reviews and critically assesses the preprint and published reports between March and May 2020 for the diagnosis of COVID-19 via CXR images using convolutional neural networks and other deep learning architectures. Despite the encouraging results, there is an urgent need for public, comprehensive, and diverse datasets. Further investigations in terms of explainable and justifiable decisions are also required for more robust, transparent, and accurate predictions.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-11-18", "authors": ["Hanan SAlghamdi", "GhadaAmoudi", "SalmaElhag", "KawtherSaeedi", "JomanahNasser"], "doi": "10.1109/ACCESS.2021.3054484\n10.1109/TNNLS.2020.2995800\n10.1007/s42600-020-00091-7\n10.3233/XST-200715\n10.17632/rscbjbr9sj.2\n10.1109/ACCESS.2020.3033762"}
{"title": "Deep Convolutional Approaches for the Analysis of COVID-19 Using Chest X-Ray Images From Portable Devices.", "abstract": "The recent human coronavirus disease (COVID-19) is a respiratory infection caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). Given the effects of COVID-19 in pulmonary tissues, chest radiography imaging plays an important role in the screening, early detection, and monitoring of the suspected individuals. Hence, as the pandemic of COVID-19 progresses, there will be a greater reliance on the use of portable equipment for the acquisition of chest X-ray images due to its accessibility, widespread availability, and benefits regarding to infection control issues, minimizing the risk of cross-contamination. This work presents novel fully automatic approaches specifically tailored for the classification of chest X-ray images acquired by portable equipment into 3 different clinical categories: normal, pathological, and COVID-19. For this purpose, 3 complementary deep learning approaches based on a densely convolutional network architecture are herein presented. The joint response of all the approaches allows to enhance the differentiation between patients infected with COVID-19, patients with other diseases that manifest characteristics similar to COVID-19 and normal cases. The proposed approaches were validated over a dataset specifically retrieved for this research. Despite the poor quality of the chest X-ray images that is inherent to the nature of the portable equipment, the proposed approaches provided global accuracy values of 79.62%, 90.27% and 79.86%, respectively, allowing a reliable analysis of portable radiographs to facilitate the clinical decision-making process.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-11-18", "authors": ["JoaquimDe Moura", "Lucia RamosGarcia", "Placido Francisco LizancosVidal", "MilenaCruz", "Laura AbelairasLopez", "Eva CastroLopez", "JorgeNovo", "MarcosOrtega"], "doi": "10.1109/ACCESS.2020.3033762\n10.1056/NEJM199304293281706\n10.1056/NEJMoa2002032\n10.1101/2020.05.01.20087254\n10.1101/2020.06.21.20136598\n10.1007/s00500-020-05275-y\n10.1101/2020.05.04.20090423\n10.1198/10618600152418584"}
{"title": "A deep learning approach using effective preprocessing techniques to detect COVID-19 from chest CT-scan and X-ray images.", "abstract": "Coronavirus disease-19 (COVID-19) is a severe respiratory viral disease first reported in late 2019 that has spread worldwide. Although some wealthy countries have made significant progress in detecting and containing this disease, most underdeveloped countries are still struggling to identify COVID-19 cases in large populations. With the rising number of COVID-19 cases, there are often insufficient COVID-19 diagnostic kits and related resources in such countries. However, other basic diagnostic resources often do exist, which motivated us to develop Deep Learning models to assist clinicians and radiologists to provide prompt diagnostic support to the patients. In this study, we have developed a deep learning-based COVID-19 case detection model trained with a dataset consisting of chest CT scans and X-ray images. A modified ResNet50V2 architecture was employed as deep learning architecture in the proposed model. The dataset utilized to train the model was collected from various publicly available sources and included four class labels: confirmed COVID-19, normal controls and confirmed viral and bacterial pneumonia cases. The aggregated dataset was preprocessed through a sharpening filter before feeding the dataset into the proposed model. This model attained an accuracy of 96.452% for four-class cases (COVID-19/Normal/Bacterial pneumonia/Viral pneumonia), 97.242% for three-class cases (COVID-19/Normal/Bacterial pneumonia) and 98.954% for two-class cases (COVID-19/Viral pneumonia) using chest X-ray images. The model acquired a comprehensive accuracy of 99.012% for three-class cases (COVID-19/Normal/Community-acquired pneumonia) and 99.99% for two-class cases (Normal/COVID-19) using CT-scan images of the chest. This high accuracy presents a new and potentially important resource to enable radiologists to identify and rapidly diagnose COVID-19 cases with only basic but widely available equipment.", "journal": "Computers in biology and medicine", "date": "2021-11-16", "authors": ["Khabir UddinAhamed", "ManowarulIslam", "AshrafUddin", "ArnishaAkhter", "Bikash KumarPaul", "Mohammad AbuYousuf", "ShahadatUddin", "Julian M WQuinn", "Mohammad AliMoni"], "doi": "10.1016/j.compbiomed.2021.105014\n10.1093/bib/bbab197\n10.1093/bib/bbab115\n10.1016/B978-0-12-813087-2.00003-8\n10.1016/B978-0-12-381420-3.00007-2"}
{"title": "Potential diagnosis of COVID-19 from chest X-ray and CT findings using semi-supervised learning.", "abstract": "COVID-19 is an infectious disease, which has adversely affected public health and the economy across the world.\u00a0On account of the highly infectious nature of the disease, rapid automated diagnosis of COVID-19 is urgently needed. A few recent findings suggest that chest X-rays and CT scans can be used by machine learning for the diagnosis of COVID-19. Herein, we employed semi-supervised learning (SSL) approaches to detect COVID-19 cases accurately by analyzing digital chest X-rays and CT scans. On a relatively small COVID-19 radiography dataset, which contains only 219 COVID-19 positive images, 1341 normal and 1345 viral pneumonia images, our algorithm, COVIDCon, which takes advantage of data augmentation, consistency regularization, and multicontrastive learning, attains 97.07% average class prediction accuracy, with 1000 labeled images, which is 7.65% better than the next best SSL method, virtual adversarial training. COVIDCon performs even better on a larger COVID-19 CT Scan dataset that contains 82,767 images. It achieved an excellent accuracy of 99.13%, at 20,000 labels, which is 6.45% better than the next best pseudo-labeling approach. COVIDCon outperforms other state-of-the-art algorithms at every label that we have investigated. These results demonstrate COVIDCon as the benchmark SSL algorithm for potential diagnosis of COVID-19 from chest X-rays and CT-Scans. Furthermore, COVIDCon performs exceptionally well in identifying COVID-19 positive cases from a completely unseen repository with a confirmed COVID-19 case history. COVIDCon, may provide a fast, accurate, and reliable method for screening COVID-19 patients.", "journal": "Physical and engineering sciences in medicine", "date": "2021-11-16", "authors": ["PrachetaSahoo", "IndranilRoy", "RandeepAhlawat", "SaquibIrtiza", "LatifurKhan"], "doi": "10.1007/s13246-021-01075-2\n10.1109/ACCESS.2021.3058537\n10.1109/TCBB.2021.3065361"}
{"title": "Improving motion-mask segmentation in thoracic CT with multiplanar U-nets.", "abstract": "Motion-mask segmentation from thoracic computed tomography (CT) images is the process of extracting the region that encompasses lungs and viscera, where large displacements occur during breathing. It has been shown to help image registration between different respiratory phases. This registration step is, for example, useful for radiotherapy planning or calculating local lung ventilation. Knowing the location of motion discontinuity, that is, sliding motion near the pleura, allows a better control of the registration preventing unrealistic estimates. Nevertheless, existing methods for motion-mask segmentation are not robust enough to be used in clinical routine. This article shows that it is feasible to overcome this lack of robustness by using a lightweight deep-learning approach usable on a standard computer, and this even without data augmentation or advanced model\u00a0design.\nA convolutional neural-network architecture with three 2D U-nets for the three main orientations (sagittal, coronal, axial) was proposed. Predictions generated by the three U-nets were combined by majority voting to provide a single 3D segmentation of the motion mask. The networks were trained on a database of nonsmall cell lung cancer 4D CT images of 43 patients. Training and evaluation were done with a K-fold cross-validation strategy. Evaluation was based on a visual grading by two experts according to the appropriateness of the segmented motion mask for the registration task, and on a comparison with motion masks obtained by a baseline method using level sets. A second database (76 CT images of patients with early-stage COVID-19), unseen during training, was used to assess the generalizability of the trained neural\u00a0network.\nThe proposed approach outperformed the baseline method in terms of quality and robustness: the success rate increased from \nWith 5-s processing time on a mid-range GPU and success rates around ", "journal": "Medical physics", "date": "2021-11-16", "authors": ["LudmillaPenarrubia", "NicolasPinon", "EmmanuelRoux", "Eduardo EnriqueD\u00e1vila Serrano", "Jean-ChristopheRichard", "MaciejOrkisz", "DavidSarrut"], "doi": "10.1002/mp.15347"}
{"title": "COVID-19 Case Recognition from Chest CT Images by Deep Learning, Entropy-Controlled Firefly Optimization, and Parallel Feature Fusion.", "abstract": "In healthcare, a multitude of data is collected from medical sensors and devices, such as X-ray machines, magnetic resonance imaging, computed tomography (CT), and so on, that can be analyzed by artificial intelligence methods for early diagnosis of diseases. Recently, the outbreak of the COVID-19 disease caused many deaths. Computer vision researchers support medical doctors by employing deep learning techniques on medical images to diagnose COVID-19 patients. Various methods were proposed for COVID-19 case classification. A new automated technique is proposed using parallel fusion and optimization of deep learning models. The proposed technique starts with a contrast enhancement using a combination of top-hat and Wiener filters. Two pre-trained deep learning models (AlexNet and VGG16) are employed and fine-tuned according to target classes (COVID-19 and healthy). Features are extracted and fused using a parallel fusion approach-parallel positive correlation. Optimal features are selected using the entropy-controlled firefly optimization method. The selected features are classified using machine learning classifiers such as multiclass support vector machine (MC-SVM). Experiments were carried out using the Radiopaedia database and achieved an accuracy of 98%. Moreover, a detailed analysis is conducted and shows the improved performance of the proposed scheme.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-11-14", "authors": ["Muhammad AttiqueKhan", "MajedAlhaisoni", "UsmanTariq", "NazarHussain", "AbdulMajid", "RobertasDama\u0161evi\u010dius", "RytisMaskeli\u016bnas"], "doi": "10.3390/s21217286\n10.1016/S0140-6736(20)30185-9\n10.1038/s41564-020-0695-z\n10.2807/1560-7917.es.2020.25.6.2000094\n10.1056/NEJMoa2001316\n10.1007/s11869-020-00944-1\n10.1016/j.aquaculture.2020.735881\n10.7717/peerj-cs.564\n10.1111/exsy.12759\n10.1016/j.inffus.2020.11.005\n10.1007/s10044-020-00950-0\n10.3390/app11199023\n10.3390/sym13010113\n10.1007/s10489-020-01826-w\n10.1016/j.compbiomed.2020.103795\n10.1007/s00500-020-05275-y\n10.1007/s12559-020-09751-3\n10.3389/fmed.2020.608525\n10.3390/sym12040651\n10.1007/s00779-020-01494-0\n10.1371/journal.pone.0243189\n10.1016/j.mehy.2020.109761\n10.32604/cmc.2021.013191\n10.1007/s10489-020-01889-9\n10.4108/eai.13-7-2018.163997\n10.1016/j.eswa.2020.114054\n10.1007/s10489-020-01902-1\n10.1155/2021/8829829\n10.1109/tcbb.2021.3065361\n10.1016/j.compeleceng.2020.106960\n10.1007/s10489-020-02149-6\n10.1016/j.patrec.2020.12.010\n10.1080/07391102.2020.1788642\n10.2196/19569\n10.1109/ACCESS.2020.3005510\n10.1016/j.media.2020.101836\n10.1007/s00330-020-07044-9\n10.1109/ACCESS.2020.3016780\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100412\n10.3390/s21062215\n10.3390/s21041480\n10.3390/s21165482\n10.3390/s21103322\n10.1111/exsy.12497\n10.3390/su12125037\n10.3390/diagnostics10110904\n10.1145/3065386\n10.1007/s11263-015-0816-y\n10.1109/ACCESS.2020.3034217\n10.3390/sym12071146\n10.1109/sai.2014.6918213\n10.1016/S1672-6529(09)60240-7\n10.1166/jmihi.2020.3222\n10.3390/diagnostics11071212\n10.7717/peerj-cs.456\n10.1109/TITB.2006.879600\n10.1007/s10489-020-02055-x\n10.32604/cmc.2021.016816\n10.1016/j.chaos.2020.110153\n10.1155/2020/9756518\n10.1007/s00521-021-06490-w\n10.1002/int.22691\n10.32604/cmc.2022.020140\n10.1109/JBHI.2021.3067789\n10.1016/j.compeleceng.2020.106956"}
{"title": "Impact of Lung Segmentation on the Diagnosis and Explanation of COVID-19 in Chest X-ray Images.", "abstract": "COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread, and uses less radiation. Here, we demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and evaluate which contents of the image influenced the most. Semantic segmentation was performed using a U-Net CNN architecture, and the classification using three CNN architectures (VGG, ResNet, and Inception). Explainable Artificial Intelligence techniques were employed to estimate the impact of segmentation. A three-classes database was composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the impact of creating a CXR image database from different sources, and the COVID-19 generalization from one source to another. The segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification using segmented images achieved an F1-Score of 0.88 for the multi-class setup, and 0.83 for COVID-19 identification. In the cross-dataset scenario, we obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented images. Experiments support the conclusion that even after segmentation, there is a strong bias introduced by underlying factors from different sources.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-11-14", "authors": ["Lucas OTeixeira", "Rodolfo MPereira", "DiegoBertolini", "Luiz SOliveira", "LorisNanni", "George D CCavalcanti", "Yandre M GCosta"], "doi": "10.3390/s21217116\n10.1038/s41577-020-0311-8\n10.7326/M20-0504\n10.1152/physiolgenomics.00029.2020\n10.1016/j.ajem.2012.08.041\n10.1016/j.cmpb.2020.105532\n10.1038/s41598-020-76550-z\n10.1016/j.inffus.2021.04.008\n10.3390/ijerph17186933\n10.1109/RBME.2020.2987975\n10.1016/j.scs.2020.102589\n10.1109/ACCESS.2021.3058537\n10.1038/s42256-021-00307-0\n10.1016/j.media.2021.102225\n10.1016/j.cell.2020.04.045\n10.1016/j.cmpb.2020.105608\n10.1109/TMI.2021.3079709\n10.1038/s41551-021-00704-1\n10.1038/s42256-021-00338-7\n10.1148/radiol.2020200527\n10.1007/s12553-021-00520-2\n10.1097/MAJ.0b013e31818ad805\n10.1016/j.ejrnm.2015.11.004\n10.2214/AJR.09.3625\n10.1186/s40537-019-0197-0\n10.3390/info11020125\n10.1037/met0000061\n10.1038/s41563-019-0345-0"}
{"title": "Decision and feature level fusion of deep features extracted from public COVID-19 data-sets.", "abstract": "The Coronavirus disease (COVID-19), which is an infectious pulmonary disorder, has affected millions of people and has been declared as a global pandemic by the WHO. Due to highly contagious nature of COVID-19 and its high possibility of causing severe conditions in the patients, the development of rapid and accurate diagnostic tools have gained importance. The real-time reverse transcription-polymerize chain reaction (RT-PCR) is used to detect the presence of Coronavirus RNA by using the mucus and saliva mixture samples taken by the nasopharyngeal swab technique. But, RT-PCR suffers from having low-sensitivity especially in the early stage. Therefore, the usage of chest radiography has been increasing in the early diagnosis of COVID-19 due to its fast imaging speed, significantly low cost and low dosage exposure of radiation. In our study, a computer-aided diagnosis system for X-ray images based on convolutional neural networks (CNNs) and ensemble learning idea, which can be used by radiologists as a supporting tool in COVID-19 detection, has been proposed. Deep feature sets extracted by using seven CNN architectures were concatenated for feature level fusion and fed to multiple classifiers in terms of decision level fusion idea with the aim of discriminating COVID-19, pneumonia and no-finding classes. In the decision level fusion idea, a majority voting scheme was applied to the resultant decisions of classifiers. The obtained accuracy values and confusion matrix based evaluation criteria were presented for three progressively created data-sets. The aspects of the proposed method that are superior to existing COVID-19 detection studies have been discussed and the fusion performance of proposed approach was validated visually by using Class Activation Mapping technique. The experimental results show that the proposed approach has attained high COVID-19 detection performance that was proven by its comparable accuracy and superior precision/recall values with the existing studies.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2021-11-13", "authors": ["Hamza OsmanIlhan", "GorkemSerbes", "NizamettinAydin"], "doi": "10.1007/s10489-021-02945-8\n10.1109/TPAMI.2015.2500224\n10.1038/s41598-019-42294-8\n10.1017/dmp.2015.38\n10.1016/S0140-6736(20)30460-8\n10.1016/j.coastaleng.2019.103593\n10.1016/j.compag.2020.105339\n10.1109/ACCESS.2020.2992341\n10.1109/JBHI.2015.2425041\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200230\n10.1016/j.patrec.2019.11.025\n10.1161/CIRCULATIONAHA.120.046941\n10.1016/j.compbiomed.2019.103351\n10.1016/j.compmedimag.2007.02.002\n10.1109/JSTARS.2018.2878037\n10.3390/diagnostics10050329\n10.1109/34.927459\n10.2307/2333955\n10.1016/j.measurement.2019.05.076\n10.1016/j.cell.2018.02.010\n10.1093/biomet/58.3.433\n10.1016/j.neucom.2021.01.085\n10.1016/j.bspc.2021.102932\n10.1016/j.cmpb.2020.105581\n10.1109/34.667881\n10.1007/s00392-020-01626-9\n10.1109/JBHI.2021.3058293\n10.1109/TNN.2004.837780\n10.2214/AJR.20.22954\n10.1038/s41598-019-38966-0\n10.1080/02564602.2015.1015631\n10.1016/j.zemedi.2018.11.002\n10.11613/BM.2012.031\n10.1109/ACCESS.2018.2813079\n10.1016/j.compbiomed.2019.103545\n10.1016/j.compbiomed.2021.104399\n10.3390/app8101715\n10.1007/s11263-015-0816-y\n10.1016/j.asoc.2018.10.022\n10.1016/j.ajem.2012.08.041\n10.1016/j.patrec.2019.11.019\n10.1109/TMI.2016.2528162\n10.1016/j.neucom.2019.12.083\n10.1109/TMI.2016.2535302\n10.1016/j.bspc.2017.06.018\n10.1109/ACCESS.2020.2994762\n10.1186/s40537-019-0276-2\n10.1109/TMI.2020.3040950\n10.1016/j.biosystemseng.2019.01.003"}
{"title": "Corona-Nidaan: lightweight deep convolutional neural network for chest X-Ray based COVID-19 infection detection.", "abstract": "The coronavirus COVID-19 pandemic is today's major public health crisis, we have faced since the Second World War. The pandemic is spreading around the globe like a wave, and according to the World Health Organization's recent report, the number of confirmed cases and deaths are rising rapidly. COVID-19 pandemic has created severe social, economic, and political crises, which in turn will leave long-lasting scars. One of the countermeasures against controlling coronavirus outbreak is specific, accurate, reliable, and rapid detection technique to identify infected patients. The availability and affordability of RT-PCR kits remains a major bottleneck in many countries, while handling COVID-19 outbreak effectively. Recent findings indicate that chest radiography anomalies can characterize patients with COVID-19 infection. In this study, Corona-Nidaan, a lightweight deep convolutional neural network (DCNN), is proposed to detect COVID-19, Pneumonia, and Normal cases from chest X-ray image analysis; without any human intervention. We introduce a simple minority class oversampling method for dealing with imbalanced dataset problem. The impact of transfer learning with pre-trained CNNs on chest X-ray based COVID-19 infection detection is also investigated. Experimental analysis shows that Corona-Nidaan model outperforms prior works and other pre-trained CNN based models. The model achieved 95% accuracy for three-class classification with 94% precision and recall for COVID-19 cases. While studying the performance of various pre-trained models, it is also found that VGG19 outperforms other pre-trained CNN models by achieving 93% accuracy with 87% recall and 93% precision for COVID-19 infection detection. The model is evaluated by screening the COVID-19 infected Indian Patient chest X-ray dataset with good accuracy.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2021-11-13", "authors": ["MainakChakraborty", "Sunita VikrantDhavale", "JitendraIngole"], "doi": "10.1007/s10489-020-01978-9\n10.1001/jama.2017.14585\n10.1001/jama.2016.17216\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2017162326\n10.1038/s41586-020-2008-3"}
{"title": "Stacked-autoencoder-based model for COVID-19 diagnosis on CT images.", "abstract": "With the outbreak of COVID-19, medical imaging such as computed tomography (CT) based diagnosis is proved to be an effective way to fight against the rapid spread of the virus. Therefore, it is important to study computerized models for infectious detection based on CT imaging. New deep learning-based approaches are developed for CT assisted diagnosis of COVID-19. However, most of the current studies are based on a small size dataset of COVID-19 CT images as there are less publicly available datasets for patient privacy reasons. As a result, the performance of deep learning-based detection models needs to be improved based on a small size dataset. In this paper, a stacked autoencoder detector model is proposed to greatly improve the performance of the detection models such as precision rate and recall rate. Firstly, four autoencoders are constructed as the first four layers of the whole stacked autoencoder detector model being developed to extract better features of CT images. Secondly, the four autoencoders are cascaded together and connected to the dense layer and the softmax classifier to constitute the model. Finally, a new classification loss function is constructed by superimposing reconstruction loss to enhance the detection accuracy of the model. The experiment results show that our model is performed well on a small size COVID-2019 CT image dataset. Our model achieves the average accuracy, precision, recall, and F1-score rate of 94.7%, 96.54%, 94.1%, and 94.8%, respectively. The results reflect the ability of our model in discriminating COVID-19 images which might help radiologists in the diagnosis of suspected COVID-19 patients.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2021-11-13", "authors": ["DaqiuLi", "ZhangjieFu", "JunXu"], "doi": "10.1007/s10489-020-02002-w\n10.1038/s41564-020-0695-z\n10.1016/S1672-0229(03)01031-3\n10.1111/ajt.15876\n10.1093/cid/ciaa203\n10.1002/jmv.25762\n10.1038/s41423-020-0407-x\n10.1021/acsnano.0c02624\n10.1038/d41586-020-00983-9\n10.1038/s41591-020-0824-5\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2995508\n10.1016/j.eswa.2019.112957\n10.1016/j.neunet.2020.01.018\n10.1038/s41598-019-42557-4\n10.1145/3065386\n10.1016/j.amc.2005.09.016\n10.1038/s41583-020-0277-3\n10.1016/j.ipm.2009.03.002"}
{"title": "Automated diagnosis of COVID-19 with limited posteroanterior chest X-ray images using fine-tuned deep neural networks.", "abstract": "The novel coronavirus 2019 (COVID-19) is a respiratory syndrome that resembles pneumonia. The current diagnostic procedure of COVID-19 follows reverse-transcriptase polymerase chain reaction (RT-PCR) based approach which however is less sensitive to identify the virus at the initial stage. Hence, a more robust and alternate diagnosis technique is desirable. Recently, with the release of publicly available datasets of corona positive patients comprising of computed tomography (CT) and chest X-ray (CXR) imaging; scientists, researchers and healthcare experts are contributing for faster and automated diagnosis of COVID-19 by identifying pulmonary infections using deep learning approaches to achieve better cure and treatment. These datasets have limited samples concerned with the positive COVID-19 cases, which raise the challenge for unbiased learning. Following from this context, this article presents the random oversampling and weighted class loss function approach for unbiased fine-tuned learning (transfer learning) in various state-of-the-art deep learning approaches such as baseline ResNet, Inception-v3, Inception ResNet-v2, DenseNet169, and NASNetLarge to perform binary classification (as normal and COVID-19 cases) and also multi-class classification (as COVID-19, pneumonia, and normal case) of posteroanterior CXR images. Accuracy, precision, recall, loss, and area under the curve (AUC) are utilized to evaluate the performance of the models. Considering the experimental results, the performance of each model is scenario dependent; however, NASNetLarge displayed better scores in contrast to other architectures, which is further compared with other recently proposed approaches. This article also added the visual explanation to illustrate the basis of model classification and perception of COVID-19 in CXR images.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2021-11-13", "authors": ["Narinder SinghPunn", "SonaliAgarwal"], "doi": "10.1007/s10489-020-01900-3\n10.1164/ajrccm.150.5.7952571\n10.1007/s13246-020-00865-4\n10.1016/j.imavis.2009.04.012\n10.1148/radiol.2019181960\n10.3390/app10020559\n10.1016/j.jormas.2019.06.002\n10.1007/s11517-019-01965-4\n10.1186/s40537-019-0192-5\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2017162326\n10.1016/j.neucom.2016.12.038\n10.1016/j.futures.2017.03.006\n10.1109/TMI.2020.2993291\n10.1145/3376922\n10.1371/journal.pmed.1002686\n10.1146/annurev-bioeng-071516-044442"}
{"title": "Deep learning based detection and analysis of COVID-19 on chest X-ray images.", "abstract": "Covid-19 is a rapidly spreading viral disease that infects not only humans, but animals are also infected because of this disease. The daily life of human beings, their health, and the economy of a country are affected due to this deadly viral disease. Covid-19 is a common spreading disease, and till now, not a single country can prepare a vaccine for COVID-19. A clinical study of COVID-19 infected patients has shown that these types of patients are mostly infected from a lung infection after coming in contact with this disease. Chest x-ray (i.e., radiography) and chest CT are a more effective imaging technique for diagnosing lunge related problems. Still, a substantial chest x-ray is a lower cost process in comparison to chest CT. Deep learning is the most successful technique of machine learning, which provides useful analysis to study a large amount of chest x-ray images that can critically impact on screening of Covid-19. In this work, we have taken the PA view of chest x-ray scans for covid-19 affected patients as well as healthy patients. After cleaning up the images and applying data augmentation, we have used deep learning-based CNN models and compared their performance. We have compared Inception V3, Xception, and ResNeXt models and examined their accuracy. To analyze the model performance, 6432 chest x-ray scans samples have been collected from the Kaggle repository, out of which 5467 were used for training and 965 for validation. In result analysis, the Xception model gives the highest accuracy (i.e., 97.97%) for detecting Chest X-rays images as compared to other models. This work only focuses on possible methods of classifying covid-19 infected patients and does not claim any medical accuracy.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2021-11-13", "authors": ["RachnaJain", "MeenuGupta", "SohamTaneja", "D JudeHemanth"], "doi": "10.1007/s10489-020-01902-1\n10.1148/radiol.2303030853\n10.1148/radiol.2282030593\n10.1016/S0140-6736(20)30183-5\n10.1148/rg.2018170048\n10.1148/radiol.2462070712\n10.1093/ndt/gfaa069\n10.1016/j.idm.2020.02.002\n10.2214/AJR.14.13021\n10.3348/kjr.2016.17.1.166\n10.1148/radiol.2020200230\n10.1016/j.jfo.2020.02.001\n10.1016/j.clinimag.2020.04.001\n10.1016/S1473-3099(20)30086-4\n10.1186/s40537-019-0276-2\n10.4103/0301-4738.37595"}
{"title": "DenResCov-19: A deep transfer learning network for robust automatic classification of COVID-19, pneumonia, and tuberculosis from X-rays.", "abstract": "The global pandemic of coronavirus disease 2019 (COVID-19) is continuing to have a significant effect on the well-being of the global population, thus increasing the demand for rapid testing, diagnosis, and treatment. As COVID-19 can cause severe pneumonia, early diagnosis is essential for correct treatment, as well as to reduce the stress on the healthcare system. Along with COVID-19, other etiologies of pneumonia and Tuberculosis (TB) constitute additional challenges to the medical system. Pneumonia (viral as well as bacterial) kills about 2 million infants every year and is consistently estimated as one of the most important factor of childhood mortality (according to the World Health Organization). Chest X-ray (CXR) and computed tomography (CT) scans are the primary imaging modalities for diagnosing respiratory diseases. Although CT scans are the gold standard, they are more expensive, time consuming, and are associated with a small but significant dose of radiation. Hence, CXR have become more widespread as a first line investigation. In this regard, the objective of this work is to develop a new deep transfer learning pipeline, named DenResCov-19, to diagnose patients with COVID-19, pneumonia, TB or healthy based on CXR images. The pipeline consists of the existing DenseNet-121 and the ResNet-50 networks. Since the DenseNet and ResNet have orthogonal performances in some instances, in the proposed model we have created an extra layer with convolutional neural network (CNN) blocks to join these two models together to establish superior performance as compared to the two individual networks. This strategy can be applied universally in cases where two competing networks are observed. We have tested the performance of our proposed network on two-class (pneumonia and healthy), three-class (COVID-19 positive, healthy, and pneumonia), as well as four-class (COVID-19 positive, healthy, TB, and pneumonia) classification problems. We have validated that our proposed network has been able to successfully classify these lung-diseases on our four datasets and this is one of our novel findings. In particular, the AUC-ROC are 99.60, 96.51, 93.70, 96.40% and the F1 values are 98.21, 87.29, 76.09, 83.17% on our Dataset X-Ray 1, 2, 3, and 4 (DXR1, DXR2, DXR3, DXR4), respectively.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2021-11-12", "authors": ["MichailMamalakis", "Andrew JSwift", "BartVorselaars", "SurajitRay", "SimonneWeeks", "WeipingDing", "Richard HClayton", "Louise SMackenzie", "AbhirupBanerjee"], "doi": "10.1016/j.compmedimag.2021.102008\n10.1016/j.media.2020.101860\n10.1016/j.media.2020.101910\n10.1016/j.media.2020.101836\n10.1016/j.media.2021.102054\n10.1016/j.media.2020.101913\n10.1016/j.media.2021.101975\n10.1016/j.media.2021.101992\n10.1016/j.media.2020.101824"}
{"title": "Browser-based Data Annotation, Active Learning, and Real-Time Distribution of Artificial Intelligence Models: From Tumor Tissue Microarrays to COVID-19 Radiology.", "abstract": "Artificial intelligence (AI) is fast becoming the tool of choice for scalable and reliable analysis of medical images. However, constraints in sharing medical data outside the institutional or geographical space, as well as difficulties in getting AI models and modeling platforms to work across different environments, have led to a \"reproducibility crisis\" in digital medicine.\nThis study details the implementation of a web platform that can be used to mitigate these challenges by orchestrating a digital pathology AI pipeline, from raw data to model inference, entirely on the local machine. We discuss how this federated platform provides governed access to data by consuming the Application Program Interfaces exposed by cloud storage services, allows the addition of user-defined annotations, facilitates active learning for training models iteratively, and provides model inference computed directly in the web browser at practically zero cost. The latter is of particular relevance to clinical workflows because the code, including the AI model, travels to the user's data, which stays private to the governance domain where it was acquired.\nWe demonstrate that the web browser can be a means of democratizing AI and advancing data socialization in medical imaging backed by consumer-facing cloud infrastructure such as Box.com. As a case study, we test the accompanying platform end-to-end on a large dataset of digital breast cancer tissue microarray core images. We also showcase how it can be applied in contexts separate from digital pathology by applying it to a radiology dataset containing COVID-19 computed tomography images.\nThe platform described in this report resolves the challenges to the findable, accessible, interoperable, reusable stewardship of data and AI models by integrating with cloud storage to maintain user-centric governance over the data. It also enables distributed, federated computation for AI inference over those data and proves the viability of client-side AI in medical imaging.\nThe open-source application is publicly available at , with a short video demonstration at .", "journal": "Journal of pathology informatics", "date": "2021-11-12", "authors": ["Praphulla M SBhawsar", "MustaphaAbubakar", "Marjanka KSchmidt", "Nicola JCamp", "Melissa HCessna", "M\u00e1ire ADuggan", "MontserratGarc\u00eda-Closas", "Jonas SAlmeida"], "doi": "10.4103/jpi.jpi_100_20"}
{"title": "Results of the COVID-19 mental health international for the general population (COMET-G) study.", "abstract": "There are few published empirical data on the effects of COVID-19 on mental health, and until now, there is no large international study.\nDuring the COVID-19 pandemic, an online questionnaire gathered data from 55,589 participants from 40 countries (64.85% females aged 35.80\u00a0\u00b1\u00a013.61; 34.05% males aged 34.90\u00b113.29 and 1.10% other aged 31.64\u00b113.15). Distress and probable depression were identified with the use of a previously developed cut-off and algorithm respectively.\nDescriptive statistics were calculated. Chi-square tests, multiple forward stepwise linear regression analyses and Factorial Analysis of Variance (ANOVA) tested relations among variables.\nProbable depression was detected in 17.80% and distress in 16.71%. A significant percentage reported a deterioration in mental state, family dynamics and everyday lifestyle. Persons with a history of mental disorders had higher rates of current depression (31.82% vs. 13.07%). At least half of participants were accepting (at least to a moderate degree) a non-bizarre conspiracy. The highest Relative Risk (RR) to develop depression was associated with history of Bipolar disorder and self-harm/attempts (RR\u00a0=\u00a05.88). Suicidality was not increased in persons without a history of any mental disorder. Based on these results a model was developed.\nThe final model revealed multiple vulnerabilities and an interplay leading from simple anxiety to probable depression and suicidality through distress. This could be of practical utility since many of these factors are modifiable. Future research and interventions should specifically focus on them.", "journal": "European neuropsychopharmacology : the journal of the European College of Neuropsychopharmacology", "date": "2021-11-11", "authors": ["Konstantinos NFountoulakis", "GrigoriosKarakatsoulis", "SeriAbraham", "KristinaAdorjan", "Helal UddinAhmed", "Renato DAlarc\u00f3n", "KiyomiArai", "Sani SalihuAuwal", "MichaelBerk", "SarahBjedov", "JulioBobes", "TeresaBobes-Bascaran", "JulieBourgin-Duchesnay", "Cristina AnaBredicean", "LaurynasBukelskis", "AkakiBurkadze", "Indira Indiana CabreraAbud", "RubyCastilla-Puentes", "MarceloCetkovich", "HectorColon-Rivera", "RicardoCorral", "CarlaCortez-Vergara", "PiirikaCrepin", "DomenicoDe Berardis", "SergioZamora Delgado", "DavidDe Lucena", "AvinashDe Sousa", "Ramona DiStefano", "SeetalDodd", "Livia PriyankaElek", "AnnaElissa", "BertaErdelyi-Hamza", "GamzeErzin", "Martin JEtchevers", "PeterFalkai", "AdrianaFarcas", "IlyaFedotov", "ViktoriiaFilatova", "Nikolaos KFountoulakis", "IrynaFrankova", "FrancescoFranza", "PedroFrias", "TatianaGalako", "Cristian JGaray", "LeticiaGarcia-\u00c1lvarez", "Maria PazGarc\u00eda-Portilla", "XeniaGonda", "Tomasz MGondek", "Daniela MoreraGonz\u00e1lez", "HilaryGould", "PaoloGrandinetti", "ArturoGrau", "VioletaGroudeva", "MichalHagin", "TakayukiHarada", "M TasdikHasan", "Nurul AzreenHashim", "JanHilbig", "SahadatHossain", "RossitzaIakimova", "MonaIbrahim", "FeliciaIftene", "YuliaIgnatenko", "MatiasIrarrazaval", "ZalihaIsmail", "JamilaIsmayilova", "AsafJacobs", "MiroJakovljevi\u0107", "NenadJak\u0161i\u0107", "AfzalJaved", "Helin YilmazKafali", "SagarKaria", "OlgaKazakova", "DoaaKhalifa", "OlenaKhaustova", "SteveKoh", "SvetlanaKopishinskaia", "KorneliiaKosenko", "Sotirios AKoupidis", "IllesKovacs", "BarbaraKulig", "AlishaLalljee", "JustineLiewig", "AbdulMajid", "EvgeniiaMalashonkova", "KhameliaMalik", "Najma IqbalMalik", "GulayMammadzada", "BilveshMandalia", "DonatellaMarazziti", "DarkoMar\u010dinko", "StephanieMartinez", "EimantasMatiekus", "GabrielaMejia", "Roha SaeedMemon", "Xarah Elenne MezaMart\u00ednez", "DaliaMickevi\u010di\u016bt\u0117", "RoumenMilev", "MuftauMohammed", "AlejandroMolina-L\u00f3pez", "PetrMorozov", "Nuru SuleimanMuhammad", "FilipMusta\u010d", "Mika SNaor", "AmiraNassieb", "AlvydasNavickas", "TarekOkasha", "MilenaPandova", "Anca-LiviaPanfil", "LiliyaPanteleeva", "IonPapava", "Mikaella EPatsali", "AlexeyPavlichenko", "BojanaPejuskovic", "MarianaPinto Da Costa", "MikhailPopkov", "DinaPopovic", "Nor Jannah NasutionRaduan", "Francisca VargasRam\u00edrez", "ElmarsRancans", "SalmiRazali", "FedericoRebok", "AnnaRewekant", "Elena Ninoska ReyesFlores", "Mar\u00eda TeresaRivera-Encinas", "PilarSaiz", "Manuel S\u00e1nchezde Carmona", "David SaucedoMart\u00ednez", "Jo AnneSaw", "G\u00f6rkemSaygili", "PatriciaSchneidereit", "BhumikaShah", "TomohiroShirasaka", "KetevanSilagadze", "SattiSitanggang", "OlegSkugarevsky", "AnnaSpikina", "Sridevi SiraMahalingappa", "MariaStoyanova", "AnnaSzczegielniak", "Simona ClaudiaTamasan", "GiuseppeTavormina", "Maurilio Giuseppe MariaTavormina", "Pavlos NTheodorakis", "MauricioTohen", "Eva MariaTsapakis", "DinaTukhvatullina", "IrfanUllah", "RatnarajVaidya", "Johann MVega-Dienstmaier", "JelenaVrublevska", "OliveraVukovic", "OlgaVysotska", "NataliaWidiasih", "AnnaYashikhina", "Panagiotis EPrezerakos", "DariaSmirnova"], "doi": "10.1016/j.euroneuro.2021.10.004"}
{"title": "Chest computed tomography in the diagnosis of COVID-19 in patients with false negative RT-PCR.", "abstract": "To evaluate the role of chest computed tomography in patients with COVID-19 who presented initial negative result in reverse transcriptase-polymerase chain reaction (RT-PCR).\nA single-center, retrospective study that evaluated 39 patients with negative RT-PCR for COVID-19, who underwent chest computed tomography and had a final clinical or serological diagnosis of COVID-19. The visual tomographic classification was evaluated according to the Consensus of the Radiological Society of North America and software developed with artificial intelligence for automatic detection of findings and chance estimation of COVID-19.\nIn the visual tomographic analysis, only one of them (3%) presented computed tomography classified as negative, 69% were classified as typical and 28% as indeterminate. In the evaluation using the software, only four (about 10%) had a probability of COVID-19 <25%.\nComputed tomography can play an important role in management of suspected cases of COVID-19 with initial negative results in RT-PCR, especially considering those patients outside the ideal window for sample collection for RT-PCR.\nAvaliar o papel da tomografia computadorizada de t\u00f3rax em pacientes com COVID-19 que apresentaram rea\u00e7\u00e3o em cadeia da polimerase via transcriptase reversa (RT-PCR) inicial falsamente negativa.\nEstudo retrospectivo de centro \u00fanico que avaliou 39 pacientes com RT-PCR negativa para COVID-19, submetidos \u00e0 tomografia computadorizada de t\u00f3rax e que tiveram diagn\u00f3stico final cl\u00ednico ou serol\u00f3gico de COVID-19. A classifica\u00e7\u00e3o tomogr\u00e1fica visual foi avaliada de acordo com o Consenso da \nNa an\u00e1lise tomogr\u00e1fica visual, somente um deles (3%) apresentou tomografia computadorizada classificada como tendo resultado negativo, 69% foram classificados como t\u00edpicos e 28% como indeterminados. Na avalia\u00e7\u00e3o com uso \nA tomografia computadorizada pode desempenhar papel importante no manejo de casos suspeitos de COVID-19 com RT-PCR inicialmente negativa, principalmente levando-se em considera\u00e7\u00e3o os pacientes que est\u00e3o fora da janela ideal para coleta de amostra para RT-PCR.", "journal": "Einstein (Sao Paulo, Brazil)", "date": "2021-11-11", "authors": ["Eduardo Kaiser Ururahy NunesFonseca", "Lorena CarneiroFerreira", "Bruna Melo CoelhoLoureiro", "Daniel GiunchettiStrabelli", "Lucas de P\u00e1dua Gomes deFarias", "Gabriel Abrantes deQueiroz", "Jos\u00e9 Vitor RassiGarcia", "Renato de FreitasTeixeira", "Victor Arcanjo AlmeidaGama", "Rodrigo CarusoChate", "Antonildes NascimentoAssun\u00e7\u00e3o J\u00fanior", "M\u00e1rcio Valente YamadaSawamura", "Cesar HigaNomura"], "doi": "10.31744/einstein_journal/2021AO6363"}
{"title": "Classifying chest CT images as COVID-19 positive/negative using a convolutional neural network ensemble model and uniform experimental design method.", "abstract": "To classify chest computed tomography (CT) images as positive or negative for coronavirus disease 2019 (COVID-19) quickly and accurately, researchers attempted to develop effective models by using medical images.\nA convolutional neural network (CNN) ensemble model was developed for classifying chest CT images as positive or negative for COVID-19. To classify chest CT images acquired from COVID-19 patients, the proposed COVID19-CNN ensemble model combines the use of multiple trained CNN models with a majority voting strategy. The CNN models were trained to classify chest CT images by transfer learning from well-known pre-trained CNN models and by applying their algorithm hyperparameters as appropriate. The combination of algorithm hyperparameters for a pre-trained CNN model was determined by uniform experimental design. The chest CT images (405 from COVID-19 patients and 397 from healthy patients) used for training and performance testing of the COVID19-CNN ensemble model were obtained from an earlier study by Hu in 2020. Experiments showed that, the COVID19-CNN ensemble model achieved 96.7% accuracy in classifying CT images as COVID-19 positive or negative, which was superior to the accuracies obtained by the individual trained CNN models. Other performance measures (i.e., precision, recall, specificity, and F\nThe COVID19-CNN ensemble model had superior accuracy and excellent capability in classifying chest CT images as COVID-19 positive or negative.", "journal": "BMC bioinformatics", "date": "2021-11-10", "authors": ["Yao-MeiChen", "Yenming JChen", "Wen-HsienHo", "Jinn-TsongTsai"], "doi": "10.1186/s12859-021-04083-x\n10.1101/2020.04.24.20078998\n10.1148/radiol.2020200905\n10.1101/2020.02.23.20026930\n10.1101/2020.02.14.20023028\n10.1148/ryct.2020200028\n10.1007/s11263-015-0816-y"}
{"title": "A novel deep neuroevolution-based image classification method to diagnose coronavirus disease (COVID-19).", "abstract": "COVID-19 has had a detrimental impact on normal activities, public safety, and the global financial system. To identify the presence of this disease within communities and to commence the management of infected patients early, positive cases should be diagnosed as quickly as possible. New results from X-ray imaging indicate that images provide key information about COVID-19. Advanced deep-learning (DL) models can be applied to X-ray radiological images to accurately diagnose this disease and to mitigate the effects of a shortage of skilled medical personnel in rural areas. However, the performance of DL models strongly depends on the methodology used to design their architectures. Therefore, deep neuroevolution (DNE) techniques are introduced to automatically design DL architectures accurately. In this paper, a new paradigm is proposed for the automated diagnosis of COVID-19 from chest X-ray images using a novel two-stage improved DNE Algorithm. The proposed DNE framework is evaluated on a real-world dataset and the results demonstrate that it provides the highest classification performance in terms of different evaluation metrics.", "journal": "Computers in biology and medicine", "date": "2021-11-09", "authors": ["SajadAhmadian", "Seyed Mohammad JafarJalali", "Syed Mohammed ShamsulIslam", "AbbasKhosravi", "EbrahimFazli", "SaeidNahavandi"], "doi": "10.1016/j.compbiomed.2021.104994"}
{"title": "COVID-19 infection localization and severity grading from chest X-ray images.", "abstract": "The immense spread of coronavirus disease 2019 (COVID-19) has left healthcare systems incapable to diagnose and test patients at the required rate. Given the effects of COVID-19 on pulmonary tissues, chest radiographic imaging has become a necessity for screening and monitoring the disease. Numerous studies have proposed Deep Learning approaches for the automatic diagnosis of COVID-19. Although these methods achieved outstanding performance in detection, they have used limited chest X-ray (CXR) repositories for evaluation, usually with a few hundred COVID-19 CXR images only. Thus, such data scarcity prevents reliable evaluation of Deep Learning models with the potential of overfitting. In addition, most studies showed no or limited capability in infection localization and severity grading of COVID-19 pneumonia. In this study, we address this urgent need by proposing a systematic and unified approach for lung segmentation and COVID-19 localization with infection quantification from CXR images. To accomplish this, we have constructed the largest benchmark dataset with 33,920 CXR images, including 11,956 COVID-19 samples, where the annotation of ground-truth lung segmentation masks is performed on CXRs by an elegant human-machine collaborative approach. An extensive set of experiments was performed using the state-of-the-art segmentation networks, U-Net, U-Net++, and Feature Pyramid Networks (FPN). The developed network, after an iterative process, reached a superior performance for lung region segmentation with Intersection over Union (IoU) of 96.11% and Dice Similarity Coefficient (DSC) of 97.99%. Furthermore, COVID-19 infections of various shapes and types were reliably localized with 83.05% IoU and 88.21% DSC. Finally, the proposed approach has achieved an outstanding COVID-19 detection performance with both sensitivity and specificity values above 99%.", "journal": "Computers in biology and medicine", "date": "2021-11-09", "authors": ["Anas MTahir", "Muhammad E HChowdhury", "AmithKhandakar", "TawsifurRahman", "YazanQiblawey", "UzairKhurshid", "SerkanKiranyaz", "NabilIbtehaz", "M SohelRahman", "SomayaAl-Maadeed", "SakibMahmud", "MaymounaEzeddin", "KhaledHameed", "TahirHamid"], "doi": "10.1016/j.compbiomed.2021.105002\n10.1002/rmv.2112\n10.34740/KAGGLE/DSV/2759090"}
{"title": "[Forefront of AI Applications for COVID-19 Imaging Diagnosis].", "abstract": "The intra- and inter-observer variability in diagnosis of thoracic CT images may affect the diagnosis of COVID-19. Therefore, several studies have been reported to develop artificial intelligence (AI) approaches using deep learning (DL) and radiomics technologies. The difference between them is automatic feature extraction (DL) and hand-crafted one (radiomics). The advantages of the AI-based imaging approaches for the COVID-19 are fast throughput, non-invasion, quantification, and integration of PCR results, CT findings, and clinical information. To the best of my knowledge, three types of the AI approaches have been studied: detection, severity differentiation, and prognosis prediction of COVID-19. AI technologies on assessment of severity/prediction of prognosis for COVID-19 may be more crucial than detection of COVID-19 pneumonia after COVID-19 becomes one of common diseases.", "journal": "Igaku butsuri : Nihon Igaku Butsuri Gakkai kikanshi = Japanese journal of medical physics : an official journal of Japan Society of Medical Physics", "date": "2021-11-09", "authors": ["HidetakaArimura", "TakahiroIwasaki"], "doi": "10.11323/jjmp.41.3_82"}
{"title": "Accuracy of deep learning-based computed tomography diagnostic system for COVID-19: A consecutive sampling external validation cohort study.", "abstract": "Ali-M3, an artificial intelligence program, analyzes chest computed tomography (CT) and detects the likelihood of coronavirus disease (COVID-19) based on scores ranging from 0 to 1. However, Ali-M3 has not been externally validated. Our aim was to evaluate the accuracy of Ali-M3 for detecting COVID-19 and discuss its clinical value. We evaluated the external validity of Ali-M3 using sequential Japanese sampling data. In this retrospective cohort study, COVID-19 infection probabilities for 617 symptomatic patients were determined using Ali-M3. In 11 Japanese tertiary care facilities, these patients underwent reverse transcription-polymerase chain reaction (RT-PCR) testing. They also underwent chest CT to confirm a diagnosis of COVID-19. Of the 617 patients, 289 (46.8%) were RT-PCR-positive. The area under the curve (AUC) of Ali-M3 for predicting a COVID-19 diagnosis was 0.797 (95% confidence interval: 0.762\u20120.833) and the goodness-of-fit was P = 0.156. With a cut-off probability of a diagnosis of COVID-19 by Ali-M3 set at 0.5, the sensitivity and specificity were 80.6% and 68.3%, respectively. A cut-off of 0.2 yielded a sensitivity and specificity of 89.2% and 43.2%, respectively. Among the 223 patients who required oxygen, the AUC was 0.825. Sensitivity at a cut-off of 0.5% and 0.2% was 88.7% and 97.9%, respectively. Although the sensitivity was lower when the days from symptom onset were fewer, the sensitivity increased for both cut-off values after 5 days. We evaluated Ali-M3 using external validation with symptomatic patient data from Japanese tertiary care facilities. As Ali-M3 showed sufficient sensitivity performance, despite a lower specificity performance, Ali-M3 could be useful in excluding a diagnosis of COVID-19.", "journal": "PloS one", "date": "2021-11-05", "authors": ["TatsuyoshiIkenoue", "YukiKataoka", "YoshinoriMatsuoka", "JunichiMatsumoto", "JunjiKumasawa", "KentaroTochitatni", "HirakuFunakoshi", "TomohiroHosoda", "AikoKugimiya", "MichinoriShirano", "FumikoHamabe", "SachiyoIwata", "ShingoFukuma", "NoneNone"], "doi": "10.1371/journal.pone.0258760\n10.1016/j.chest.2020.03.063\n10.1111/anae.15072\n10.2214/AJR.20.22954\n10.2214/AJR.20.23034\n10.2214/ajr.20.22975\n10.1016/j.ejrad.2020.108941\n10.1148/radiol.2020200370\n10.1148/radiol.2020200823\n10.1186/s41747-018-0061-6\n10.1148/ryct.2020200075\n10.1007/s00330-020-06699-8\n10.1136/bmj.m689\n10.1136/bmj.m1328\n10.7326/M14-0697\n10.1515/cclm-2020-0285\n10.7326/M20-1495\n10.1093/cid/cis403\n10.1038/s41591-020-0897-1\n10.1148/radiol.2020201365\n10.1038/srep34921\n10.23736/S0031-0808.20.03938\u20135\n10.1038/s41598-020-74164-z\n10.1503/cmaj.050090\n10.1148/radiol.2020200642\n10.1001/jama.2020.6173"}
{"title": "Detection of COVID-19 from Chest CT Images Using CNN with MLP Hybrid Model.", "abstract": "COVID-19 when left undetected can lead to a hazardous infection spread, leading to an unfortunate loss of life. It's of utmost importance to diagnose COVID-19 in Infected patients at the earliest, to avoid further complications. RT-PCR, the gold standard method is routinely used for the diagnosis of COVID-19 infection. Yet, this method comes along with few limitations such as its time-consuming nature, a scarcity of trained manpower, sophisticated laboratory equipment and the possibility of false positive and negative results. Physicians and global health care centers use CT scan as an alternate for the diagnosis of COVID-19. But this process of detection too, might demand more manual work, effort and time. Thus, automating the detection of COVID-19 using an intelligent system has been a recent research topic, in the view of pandemic. This will also help in saving the physician's time for carrying out further treatment. In this paper, a hybrid learning model has been proposed to identify the COVID-19 infection using CT scan images. The Convolutional Neural Network (CNN) was used for feature extraction and Multilayer Perceptron was used for classification. This hybrid learning model's results were also compared with traditional CNN and MLP models in terms of Accuracy, F1-Score, Precision and Recall. This Hybrid CNN-MLP model showed an Accuracy of 94.89% when compared with CNN and MLP giving 86.95% and 80.77% respectively.", "journal": "Studies in health technology and informatics", "date": "2021-11-05", "authors": ["Sakthi Jaya SundarRajasekar", "VasumathiNarayanan", "VaralakshmiPerumal"], "doi": "10.3233/SHTI210617"}
{"title": "EpistoNet: an ensemble of Epistocracy-optimized mixture of experts for detecting COVID-19 on chest X-ray images.", "abstract": "The Coronavirus has spread across the world and infected millions of people, causing devastating damage to the public health and global economies. To mitigate the impact of the coronavirus a reliable, fast, and accurate diagnostic system should be promptly implemented. In this study, we propose EpistoNet, a decision tree-based ensemble model using two mixtures of discriminative experts to classify COVID-19 lung infection from chest X-ray images. To optimize the architecture and hyper-parameters of the designed neural networks, we employed Epistocracy algorithm, a recently proposed hyper-heuristic evolutionary method. Using 2500 chest X-ray images consisting of 1250 COVID-19 and 1250 non-COVID-19 cases, we left out 500 images for testing and partitioned the remaining 2000 images into 5 different clusters using K-means clustering algorithm. We trained multiple deep convolutional neural networks on each cluster to help build a mixture of strong discriminative experts from the top-performing models supervised by a gating network. The final ensemble model obtained 95% accuracy on COVID-19 images and 93% accuracy on non-COVID-19. The experimental results show that EpistoNet can accurately, and reliably be used to detect COVID-19 infection in the chest X-ray images, and Epistocracy algorithm can be effectively used to optimize the hyper-parameters of the proposed models.", "journal": "Scientific reports", "date": "2021-11-05", "authors": ["Seyed ZiaeMousavi Mojab", "SeyedmohammadShams", "FarshadFotouhi", "HamidSoltanian-Zadeh"], "doi": "10.1038/s41598-021-00524-y\n10.1016/j.meegid.2020.104211\n10.3389/fmed.2020.00515\n10.1038/s41598-020-76550-z\n10.7326/M20-1495\n10.7861/clinmedicine.13-4-349\n10.1148/radiol.2020201160\n10.1016/j.eswa.2018.04.021\n10.1109/TMI.2017.2655486\n10.1038/s41591-019-0447-x\n10.1080/07391102.2020.1767212\n10.1007/s12539-020-00393-5\n10.1109/ACCESS.2020.3016780\n10.1016/j.imu.2020.100360\n10.1155/2021/8829829\n10.1109/ACCESS.2020.2995597\n10.1016/j.eswa.2020.114054\n10.1007/s10462-012-9338-y"}
{"title": "Detection and characterization of COVID-19 findings in chest CT: Feasibility and applicability of an AI-based software tool.", "abstract": "The COVID-19 pandemic has challenged institutions' diagnostic processes worldwide. The aim of this study was to assess the feasibility of an artificial intelligence (AI)-based software tool that automatically evaluates chest computed tomography for findings of suspected COVID-19.Two groups were retrospectively evaluated for COVID-19-associated ground glass opacities of the lungs (group A: real-time polymerase chain reaction positive COVID patients, n\u200a=\u200a108; group B: asymptomatic pre-operative group, n\u200a=\u200a88). The performance of an AI-based software assessment tool for detection of COVID-associated abnormalities was compared with human evaluation based on COVID-19 reporting and data system (CO-RADS) scores performed by 3 readers.All evaluated variables of the AI-based assessment showed significant differences between the 2 groups (P\u200a<\u200a.01). The inter-reader reliability of CO-RADS scoring was 0.87. The CO-RADS scores were substantially higher in group A (mean 4.28) than group B (mean 1.50). The difference between CO-RADS scoring and AI assessment was statistically significant for all variables but showed good correlation with the clinical context of the CO-RADS score. AI allowed to predict COVID positive cases with an accuracy of 0.94.The evaluated AI-based algorithm detects COVID-19-associated findings with high sensitivity and may support radiologic workflows during the pandemic.", "journal": "Medicine", "date": "2021-11-04", "authors": ["AndiGashi", "Rahel AKubik-Huch", "VasilikiChatzaraki", "AnnaPotempa", "FranziskaRauch", "SasaGrbic", "BenediktWiggli", "Andr\u00e9eFriedl", "TiloNiemann"], "doi": "10.1097/MD.0000000000027478"}
{"title": "AIoT Used for COVID-19 Pandemic Prevention and Control.", "abstract": "The pandemic of COVID-19 is continuing to wreak havoc in 2021, with at least 170 million victims around the world. Healthcare systems are overwhelmed by the large-scale virus infection. Luckily, Internet of Things (IoT) is one of the most effective paradigms in the intelligent world, in which the technology of artificial intelligence (AI), like cloud computing and big data analysis, is playing a vital role in preventing the spread of the pandemic of COVID-19. AI and 5G technologies are advancing by leaps and bounds, further strengthening the intelligence and connectivity of IoT applications, and conventional IoT has been gradually upgraded to be more powerful AI\u2009+\u2009IoT (AIoT). For example, in terms of remote screening and diagnosis of COVID-19 patients, AI technology based on machine learning and deep learning has recently upgraded medical equipment significantly and has reshaped the workflow with minimal contact with patients, so medical specialists can make clinical decisions more efficiently, providing the best protection not only to patients but also to specialists themselves. This paper reviews the latest progress made in combating COVID-19 with both IoT and AI and also provides comprehensive details on how to combat the pandemic of COVID-19 as well as the technologies that may be applied in the future.", "journal": "Contrast media & molecular imaging", "date": "2021-11-04", "authors": ["Shu-WenChen", "Xiao-WeiGu", "Jia-JiWang", "Hui-ShengZhu"], "doi": "10.1155/2021/3257035\n10.1109/GCWkshps50303.2020.9367584\n10.1109/ICECCE49384.2020.9179284\n10.1109/JTEHM.2021.3058841\n10.2196/preprints.19033\n10.32604/cmes.2021.016386\n10.1109/JSEN.2021.3062442\n10.1109/JBHI.2020.3042523\n10.1016/j.bspc.2021.102656\n10.1109/JBHI.2020.3037127\n10.1109/TIP.2021.3058783\n10.1016/j.media.2020.101836\n10.1155/2021/5544742\n10.26599/bdma.2020.9020012\n10.32604/cmes.2021.015807\n10.1155/2021/6633755\n10.1007/s11390-020-0679-8.2\n10.1007/s42979-020-00300-1\n10.1109/I-SMAC49090.2020.9243576\n10.1109/ACCESS.2021.3058448\n10.1109/CONFLUENCE.2019.8776970\n10.1109/CBMS.2018.00087\n10.1109/ICECOCS50124.2020.9314459\n10.1109/BIBM49941.2020.9313088\n10.1109/IEMTRONICS51293.2020.9216437\n10.1109/IC2IE50715.2020.9274663\n10.1007/s42979-020-00400-y\n10.1007/s12063-020-00164-x\n10.1007/s11356-020-11676-1\n10.1007/978-3-030-62412-5_46\n10.1109/ICUEMS52408.2021.00026\n10.1007/s41062-020-00454-0\n10.1186/s13677-020-00215-5\n10.19850/j.cnki.2096-4706.2020.13.054\n10.1007/s42979-020-00248-2\n10.1007/s13167-020-00218-x\n10.1186/s11782-020-00087-1\n10.1057/s42214-021-00108-7\n10.3969/j.issn.1006-723X.2020.03.018\n10.14089/j.cnki.cn11-3664/f.2020.03.001\n10.1109/MNET.011.2000704\n10.1109/JIOT.2020.3041042\n10.1109/TMRB.2020.3036461\n10.3969/j.issn.1672-8270.2020.06.043\n10.1109/ICSTCEE49637.2020.9277223\n10.1109/ISMSIT50672.2020.9254906\n10.1109/NILES50944.2020.9257919"}
{"title": "A Transfer Learning-Based Approach with Deep CNN for COVID-19- and Pneumonia-Affected Chest X-ray Image Classification.", "abstract": "The COVID-19 pandemic creates a significant impact on everyone's life. One of the fundamental movements to cope with this challenge is identifying the COVID-19-affected patients as early as possible. In this paper, we classified COVID-19, Pneumonia, and Healthy cases from the chest X-ray images by applying the transfer learning approach on the pre-trained VGG-19 architecture. We use MongoDB as a database to store the original image and corresponding category. The analysis is performed on a public dataset of 3797 X-ray images, among them COVID-19 affected (1184 images), Pneumonia affected (1294 images), and Healthy (1319 images) (https://www.kaggle.com/tawsifurrahman/covid19-radiography-database/version/3). This research gained an accuracy of 97.11%, average precision of 97%, and average Recall of 97% on the test dataset.", "journal": "SN computer science", "date": "2021-11-02", "authors": ["SoarovChakraborty", "ShouravPaul", "K M AzharulHasan"], "doi": "10.1007/s42979-021-00881-5\n10.1109/ACCESS.2018.2798799\n10.1016/j.artmed.2019.07.009\n10.1016/j.media.2017.07.005\n10.1111/tmi.13383\n10.1093/cid/cir1053\n10.1145/1773912.1773922\n10.1145/1327452.1327492\n10.1145/1365815.1365816\n10.1016/j.crad.2018.12.015\n10.1371/journal.pmed.1002686\n10.1016/j.tranon.2018.10.012\n10.1007/s13755-018-0057-x\n10.3389/fnins.2018.00804\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1016/j.ijmedinf.2020.104284\n10.1109/TMI.2020.2993291\n10.1007/s00330-021-07715-1\n10.1155/2020/8828855"}
{"title": "Automated Diagnosis of Chest X-Ray for Early Detection of COVID-19 Disease.", "abstract": "In March 2020, the World Health Organization announced the COVID-19 pandemic, its dangers, and its rapid spread throughout the world. In March 2021, the second wave of the pandemic began with a new strain of COVID-19, which was more dangerous for some countries, including India, recording 400,000 new cases daily and more than 4,000 deaths per day. This pandemic has overloaded the medical sector, especially radiology. Deep-learning techniques have been used to reduce the burden on hospitals and assist physicians for accurate diagnoses. In our study, two models of deep learning, ResNet-50 and AlexNet, were introduced to diagnose X-ray datasets collected from many sources. Each network diagnosed a multiclass (four classes) and a two-class dataset. The images were processed to remove noise, and a data augmentation technique was applied to the minority classes to create a balance between the classes. The features extracted by convolutional neural network (CNN) models were combined with traditional Gray-level Cooccurrence Matrix (GLCM) and Local Binary Pattern (LBP) algorithms in a 1-D vector of each image, which produced more representative features for each disease. Network parameters were tuned for optimum performance. The ResNet-50 network reached accuracy, sensitivity, specificity, and Area Under the Curve (AUC) of 95%, 94.5%, 98%, and 97.10%, respectively, with the multiclasses (COVID-19, viral pneumonia, lung opacity, and normal), while it reached accuracy, sensitivity, specificity, and AUC of 99%, 98%, 98%, and 97.51%, respectively, with the binary classes (COVID-19 and normal).", "journal": "Computational and mathematical methods in medicine", "date": "2021-11-02", "authors": ["Ebrahim MohammedSenan", "AliAlzahrani", "Mohammed YAlzahrani", "NizarAlsharif", "Theyazn H HAldhyani"], "doi": "10.1155/2021/6919483\n10.1007/s00256-020-03582-x\n10.1016/j.ejrad.2020.109075\n10.2214/AJR.20.23530\n10.1148/radiol.2020200642\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103805\n10.1109/JBHI.2020.3037127\n10.1007/s00330-020-07044-9\n10.2196/19569\n10.1109/TMI.2020.2995965\n10.1109/JBHI.2020.3019505\n10.1007/s40846-020-00529-4\n10.1016/j.asoc.2020.106691\n10.1016/j.eswa.2020.113909\n10.1016/j.inffus.2020.10.004\n10.1016/j.gltp.2021.01.001\n10.3390/ijerph18063056\n10.3390/healthcare9050522\n10.1109/IIPHDW.2018.8388338\n10.1016/j.patcog.2017.10.013\n10.1016/S0893-6080(03)00115-1\n10.1016/j.bspc.2019.101734\n10.1109/TMI.2018.2791721\n10.1155/2021/1004767\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2021.104348\n10.1016/j.inffus.2021.02.013\n10.1016/j.eswa.2020.114054"}
{"title": "WOANet: Whale optimized deep neural network for the classification of COVID-19 from radiography images.", "abstract": "Coronavirus Diseases (COVID-19) is a new disease that will be declared a global pandemic in 2020. It is characterized by a constellation of traits like fever, dry cough, dyspnea, fatigue, chest pain, etc. Clinical findings have shown that the human chest Computed Tomography(CT) images can diagnose lung infection in most COVID-19 patients. Visual changes in CT scan due to COVID-19 is subjective and evaluated by radiologists for diagnosis purpose. Deep Learning (DL) can provide an automatic diagnosis tool to relieve radiologists' burden for quantitative analysis of CT scan images in patients. However, DL techniques face different training problems like mode collapse and instability. Deciding on training hyper-parameters to adjust the weight and biases of DL by a given CT image dataset is crucial for achieving the best accuracy. This paper combines the backpropagation algorithm and Whale Optimization Algorithm (WOA) to optimize such DL networks. Experimental results for the diagnosis of COVID-19 patients from a comprehensive COVID-CT scan dataset show the best performance compared to other recent methods. The proposed network architecture results were validated with the existing pre-trained network to prove the efficiency of the network.", "journal": "Biocybernetics and biomedical engineering", "date": "2021-11-02", "authors": ["RMurugan", "TriptiGoel", "SeyedaliMirjalili", "Deba KumarChakrabartty"], "doi": "10.1016/j.bbe.2021.10.004"}
{"title": "An ensemble learning method based on ordinal regression for COVID-19 diagnosis from chest CT.", "abstract": "Coronavirus disease 2019 (COVID-19) has brought huge losses to the world, and it remains a great threat to public health. X-ray computed tomography (CT) plays a central role in the management of COVID-19. Traditional diagnosis with pulmonary CT images is time-consuming and error-prone, which could not meet the need for precise and rapid COVID-19 screening. Nowadays, deep learning (DL) has been successfully applied to CT image analysis, which assists radiologists in workflow scheduling and treatment planning for patients with COVID-19. Traditional methods use cross-entropy as the loss function with a Softmax classifier following a fully-connected layer. Most DL-based classification methods target intraclass relationships in a certain class (early, progressive, severe, or dissipative phases), ignoring the natural order of different phases of the disease progression,", "journal": "Physics in medicine and biology", "date": "2021-10-30", "authors": ["XiaodongGuo", "YimingLei", "PengHe", "WenbingZeng", "RanYang", "YinjinMa", "PengFeng", "QingLyu", "GeWang", "HongmingShan"], "doi": "10.1088/1361-6560/ac34b2"}
{"title": "Radiologist-supervised Transfer Learning: Improving Radiographic Localization of Pneumonia and Prognostication of Patients With COVID-19.", "abstract": "To assess the potential of a transfer learning strategy leveraging radiologist supervision to enhance convolutional neural network-based (CNN) localization of pneumonia on radiographs and to further assess the prognostic value of CNN severity quantification on patients evaluated for COVID-19 pneumonia, for whom severity on the presenting radiograph is a known predictor of mortality and intubation.\nWe obtained an initial CNN previously trained to localize pneumonia along with 25,684 radiographs used for its training. We additionally curated 1466 radiographs from patients who had a computed tomography (CT) performed on the same day. Regional likelihoods of pneumonia were then annotated by cardiothoracic radiologists, referencing these CTs. Combining data, a preexisting CNN was fine-tuned using transfer learning. Whole-image and regional performance of the updated CNN was assessed using receiver-operating characteristic area under the curve and Dice. Finally, the value of CNN measurements was assessed with survival analysis on 203 patients with COVID-19 and compared against modified radiographic assessment of lung edema (mRALE) score.\nPneumonia detection area under the curve improved on both internal (0.756 to 0.841) and external (0.864 to 0.876) validation data. Dice overlap also improved, particularly in the lung bases (R: 0.121 to 0.433, L: 0.111 to 0.486). There was strong correlation between radiologist mRALE score and CNN fractional area of involvement (\u03c1=0.85). Survival analysis showed similar, strong prognostic ability of the CNN and mRALE for mortality, likelihood of intubation, and duration of hospitalization among patients with COVID-19.\nRadiologist-supervised transfer learning can enhance the ability of CNNs to localize and quantify the severity of disease. Closed-loop systems incorporating radiologists may be beneficial for continued improvement of artificial intelligence algorithms.", "journal": "Journal of thoracic imaging", "date": "2021-10-29", "authors": ["BrianHurt", "Meagan ARubel", "Evan MMasutani", "KathleenJacobs", "LewisHahn", "MichaelHorowitz", "SethKligerman", "AlbertHsiao"], "doi": "10.1097/RTI.0000000000000618\n10.1148/ryai.2020200079\n10.1016/j.cmpb.2020.105581\n10.1007/s00330-020-07044-9\n10.1016/j.media.2020.101794\n10.1109/TMI.2020.2993291\n10.1007/s13246-020-00865-4\n10.1007/s00264-020-04609-7\n10.1007/s40846-020-00529-4\n10.1007/s13246-020-00888-x\n10.1097/RTI.0000000000000541\n10.1148/radiol.2020201365\n10.1136/thoraxjnl-2017-211280\n10.1007/s11739-020-02509-7\n10.1001/jamainternmed.2020.0994\n10.1016/j.annonc.2020.03.296\n10.1371/journal.pmed.1002686\n10.1371/journal.pmed.1002683\n10.1002/widm.1312\n10.1148/ryai.2020190043\n10.1097/RTI.0000000000000505\n10.1109/TMI.2016.2528162\n10.1002/emp2.12297\n10.1148/ryai.2019180041\n10.1002/1097-0142(1950)3:1<32::aid-cncr2820030106>3.0.co;2-3\n10.1007/978-3-319-24574-4_28\n10.2214/AJR.19.22145\n10.1148/radiol.2018180921\n10.1109/42.414618\n10.3390/s21020369\n10.1109/ACCESS.2019.2941511"}
{"title": "CO-IRv2: Optimized InceptionResNetV2 for COVID-19 detection from chest CT images.", "abstract": "This paper focuses on the application of deep learning (DL) in the diagnosis of coronavirus disease (COVID-19). The novelty of this work is in the introduction of optimized InceptionResNetV2 for COVID-19 (CO-IRv2) method. A part of the CO-IRv2 scheme is derived from the concepts of InceptionNet and ResNet with hyperparameter tuning, while the remaining part is a new architecture consisting of a global average pooling layer, batch normalization, dense layers, and dropout layers. The proposed CO-IRv2 is applied to a new dataset of 2481 computed tomography (CT) images formed by collecting two independent datasets. Data resizing and normalization are performed, and the evaluation is run up to 25 epochs. Various performance metrics, including precision, recall, accuracy, F1-score, area under the receiver operating characteristics (AUC) curve are used as performance metrics. The effectiveness of three optimizers known as Adam, Nadam and RMSProp are evaluated in classifying suspected COVID-19 patients and normal people. Results show that for CO-IRv2 and for CT images, the obtained accuracies of Adam, Nadam and RMSProp optimizers are 94.97%, 96.18% and 96.18%, respectively. Furthermore, it is shown here that for the case of CT images, CO-IRv2 with Nadam optimizer has better performance than existing DL algorithms in the diagnosis of COVID-19 patients. Finally, CO-IRv2 is applied to an X-ray dataset of 1662 images resulting in a classification accuracy of 99.40%.", "journal": "PloS one", "date": "2021-10-29", "authors": ["M Rubaiyat HossainMondal", "SubratoBharati", "PrajoyPodder"], "doi": "10.1371/journal.pone.0259179\n10.1111/tmi.13383\n10.1016/j.imu.2020.100374\n10.1056/NEJMc2001272\n10.1056/NEJMc2013020\n10.1001/jama.2020.2783\n10.7326/M20-1495\n10.3233/HIS-210008\n10.1016/j.imu.2020.100391\n10.1007/s11045-020-00756-7\n10.1097/RLI.0000000000000672\n10.1016/j.compbiomed.2020.103792\n10.2174/1573405617666210713113439\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200236\n10.1148/radiol.2020200230\n10.1038/s41591-019-0447-x\n10.3390/s18082521\n10.1164/rccm.201705-0860OC\n10.1038/s41591-018-0177-5\n10.1007/s12194-017-0406-5\n10.1016/j.ejrad.2020.109041\n10.1016/j.eng.2020.04.010\n10.1016/j.bspc.2021.102588\n10.1007/s10140-020-01886-y\n10.1007/s00330-020-07108-w\n10.1016/j.jpha.2020.03.004\n10.1109/TCBB.2021.3065361\n10.1016/j.asoc.2020.106885\n10.1007/s11548-020-02286-w\n10.1038/s41746-020-00373-5\n10.1016/j.compbiomed.2020.104037\n10.1016/j.media.2020.101824\n10.1007/s10044-021-00984-y\n10.1007/s13246-020-00865-4\n10.6084/m9.figshare.14818116.v1\n10.1371/journal.pone.0228422"}
{"title": "Potential of artificial intelligence to accelerate diagnosis and drug discovery for COVID-19.", "abstract": "The coronavirus disease (COVID-19) pandemic has caused havoc worldwide. The tests currently used to diagnose COVID-19 are based on real time reverse transcription polymerase chain reaction (RT-PCR), computed tomography medical imaging techniques and immunoassays. It takes 2 days to obtain results from the RT-PCR test and also shortage of test kits creating a requirement for alternate and rapid methods to accurately diagnose COVID-19. Application of artificial intelligence technologies such as the Internet of Things, machine learning tools and big data analysis to COVID-19 diagnosis could yield rapid and accurate results. The neural networks and machine learning tools can also be used to develop potential drug molecules. Pharmaceutical companies face challenges linked to the costs of drug molecules, research and development efforts, reduced efficiency of drugs, safety concerns and the conduct of clinical trials. In this review, relevant features of artificial intelligence and their potential applications in COVID-19 diagnosis and drug development are highlighted.", "journal": "PeerJ", "date": "2021-10-29", "authors": ["IndiraMikkili", "Abraham PeeleKarlapudi", "T CVenkateswarulu", "Vidya PrabhakarKodali", "Deepika Sri SinghMacamdas", "KrupanidhiSreerama"], "doi": "10.7717/peerj.12073\n10.1016/j.heliyon.2018.e00938\n10.1109/ACCESS.2019.2945545\n10.4172/2329-6887.1000e173\n10.1016/B978-0-12-817133-2.00018-5\n10.7717/peerj.7702\n10.3390/ai1020009\n10.1080/07391102.2020.1780946\n10.3390/ijms20112783\n10.1016/j.drudis.2020.12.009\n10.1021/acscentsci.0c00501\n10.1148/radiol.2020201237\n10.1016/j.tips.2019.06.004\n10.1021/ci300367a\n10.7717/peerj.10083\n10.7861/futurehosp.6-2-94\n10.7717/peerj.10180\n10.7717/peerj-cs.358\n10.1038/d41586-018-05267-x\n10.1002/aisy.202000070\n10.1038/s41568-018-0016-5\n10.1016/j.imu.2020.100378\n10.1016/j.arth.2020.04.055\n10.1136/svn-2017-000101\n10.7717/peerj-cs.564\n10.1016/j.chaos.2020.110059\n10.1148/radiol.2020200905\n10.2214/AJR.20.22954\n10.1038/s41390-019-0498-1\n10.1038/s41591-020-0931-3\n10.1016/j.dsx.2020.06.068\n10.1016/j.trci.2017.10.005\n10.1007/s10462-018-09679-z\n10.1016/j.compbiomed.2020.103792\n10.1016/j.drudis.2020.10.010\n10.7717/peerj.10801\n10.1109/RBME.2020.2987975\n10.1007/s12098-020-03263-6\n10.1016/j.cell.2020.01.021\n10.1080/14737159.2020.1757437\n10.1021/acsnano.0c02624\n10.1016/j.dsx.2020.04.012\n10.1038/s41573-019-0024-5\n10.1148/radiol.2020203511\n10.1007/s40495-020-00216-7\n10.1007/s00330-020-06934-2\n10.3389/fchem.2018.00030\n10.1021/acs.chemrev.8b00728\n10.1002/cpt.1795"}
{"title": "Predictive usefulness of RT-PCR testing in different patterns of Covid-19 symptomatology: analysis of a French cohort of 12,810 outpatients.", "abstract": "Reverse transcriptase polymerase chain reaction (RT-PCR) is a key tool to diagnose Covid-19. Yet it may not be the most efficient test in all patients. In this paper, we develop a clinical strategy for prescribing RT-PCR to patients based on data from COVIDOM, a French cohort of 54,000 patients with clinically suspected Covid-19, including 12,810 patients tested by RT-PCR. We use a machine-learning algorithm (decision tree) in order to predict RT-PCR results based on the clinical presentation. We show that symptoms alone are sufficient to predict RT-PCR outcome with a mean average precision of 86%. We identify combinations of symptoms that are predictive of RT-PCR positivity (90% for anosmia/ageusia) or negativity (only 30% of RT-PCR+\u2009for a subgroup with cardiopulmonary symptoms): in both cases, RT-PCR provides little added diagnostic value. We propose a prescribing strategy based on clinical presentation that can improve the global efficiency of RT-PCR testing.", "journal": "Scientific reports", "date": "2021-10-29", "authors": ["NoneNone"], "doi": "10.1038/s41598-021-99991-6\n10.1016/S0140-6736(20)30211-7\n10.1007/s00330-020-06934-2\n10.1128/mbio.00722-20\n10.1515/cclm-2020-0285\n10.1148/radiol.2020201343\n10.1001/jama.2020.8259\n10.1038/s41591-020-0931-3\n10.1016/j.medmal.2020.04.006\n10.1007/s00405-020-05965-1\n10.1101/2020.04.18.20071134\n10.1007/s00405-020-05999-5\n10.1093/chemse/bjr102\n10.1136/bmjopen-2016-013246\n10.1038/s41591-020-0891-7\n10.1093/infdis/jiaa206"}
{"title": "Quantification of pulmonary involvement in COVID-19 pneumonia by means of a cascade of two U-nets: training and assessment on multiple datasets using different annotation criteria.", "abstract": "This study aims at exploiting artificial intelligence (AI) for the identification, segmentation and quantification of COVID-19 pulmonary lesions. The limited data availability and the annotation quality are relevant factors in training AI-methods. We investigated the effects of using multiple datasets, heterogeneously populated and annotated according to different criteria.\nWe developed an automated analysis pipeline, the LungQuant system, based on a cascade of two U-nets. The first one (U-net[Formula: see text]) is devoted to the identification of the lung parenchyma; the second one (U-net[Formula: see text]) acts on a bounding box enclosing the segmented lungs to identify the areas affected by COVID-19 lesions. Different public datasets were used to train the U-nets and to evaluate their segmentation performances, which have been quantified in terms of the Dice Similarity Coefficients. The accuracy in predicting the CT-Severity Score (CT-SS) of the LungQuant system has been also evaluated.\nBoth the volumetric DSC (vDSC) and the accuracy showed a dependency on the annotation quality of the released data samples. On an independent dataset (COVID-19-CT-Seg), both the vDSC and the surface DSC (sDSC) were measured between the masks predicted by LungQuant system and the reference ones. The vDSC (sDSC) values of 0.95\u00b10.01 and 0.66\u00b10.13 (0.95\u00b10.02 and 0.76\u00b10.18, with 5 mm tolerance) were obtained for the segmentation of lungs and COVID-19 lesions, respectively. The system achieved an accuracy of 90% in CT-SS identification on this benchmark dataset.\nWe analysed the impact of using data samples with different annotation criteria in training an AI-based quantification system for pulmonary involvement in COVID-19 pneumonia. In terms of vDSC measures, the U-net segmentation strongly depends on the quality of the lesion annotations. Nevertheless, the CT-SS can be accurately predicted on independent test sets, demonstrating the satisfactory generalization ability of the LungQuant.", "journal": "International journal of computer assisted radiology and surgery", "date": "2021-10-27", "authors": ["FrancescaLizzi", "AbramoAgosti", "FrancescaBrero", "Raffaella FiammaCabini", "Maria EvelinaFantacci", "SilviaFigini", "AlessandroLascialfari", "FrancescoLaruina", "PiernicolaOliva", "StefanoPiffer", "IanPostuma", "LisaRinaldi", "CinziaTalamonti", "AlessandraRetico"], "doi": "10.1007/s11548-021-02501-2\n10.1007/s11547-020-01237-4\n10.1109/TMI.2020.3001036\n10.1002/mp.14424\n10.1007/s10278-021-00460-3\n10.1148/RADIOL.2020202439"}
{"title": "Outbreak COVID-19 in Medical Image Processing Using Deep Learning: A State-of-the-Art Review.", "abstract": "From the month of December-19, the outbreak of Coronavirus (COVID-19) triggered several deaths and overstated every aspect of individual health. COVID-19 has been designated as a pandemic by World Health Organization. The circumstances placed serious trouble on every country worldwide, particularly with health arrangements and time-consuming responses. The increase in the positive cases of COVID-19 globally spread every day. The quantity of accessible diagnosing kits is restricted because of complications in detecting the existence of the illness. Fast and correct diagnosis of COVID-19 is a timely requirement for the prevention and controlling of the pandemic through suitable isolation and medicinal treatment. The significance of the present work is to discuss the outline of the deep learning techniques with medical imaging such as outburst prediction, virus transmitted indications, detection and treatment aspects, vaccine availability with remedy research. Abundant image resources of medical imaging as X-rays, Computed Tomography Scans, Magnetic Resonance imaging, formulate deep learning high-quality methods to fight against the pandemic COVID-19. The review presents a comprehensive idea of deep learning and its related applications in healthcare received over the past decade. At the last, some issues and confrontations to control the health crisis and outbreaks have been introduced. The progress in technology has contributed to developing individual's lives. The problems faced by the radiologists during medical imaging techniques and deep learning approaches for diagnosing the COVID-19 infections have been also discussed.", "journal": "Archives of computational methods in engineering : state of the art reviews", "date": "2021-10-26", "authors": ["JaspreetKaur", "PrabhpreetKaur"], "doi": "10.1007/s11831-021-09667-7\n10.1016/j.inffus.2017.10.006\n10.1016/j.scs.2020.102018\n10.1109/ACCESS.2020.3006172\n10.1016/j.cell.2018.02.010\n10.3390/app8101715\n10.1016/j.compbiomed.2020.103792\n10.1007/s10489-020-01826-w\n10.1007/s13246-020-00865-4\n10.1109/TMI.2020.2996645\n10.1016/j.asoc.2020.106742\n10.1109/ACCESS.2020.3005510\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2020.105532\n10.1016/j.chaos.2020.110190\n10.1007/s00330-020-07044-9\n10.1016/j.compbiomed.2021.104575\n10.1016/j.asoc.2020.106912\n10.1007/s10489-021-02393-4\n10.1007/s12652-021-03306-6\n10.1016/j.media.2021.102205\n10.1155/2021/6680455\n10.1007/s10489-020-01831-z\n10.1016/j.scs.2021.103252\n10.1109/ACCESS.2020.3009328\n10.1038/s41591-020-0820-9\n10.1016/j.jare.2020.03.005\n10.1016/j.jhin.2020.01.022\n10.3390/electronics9020274\n10.1016/j.ajem.2020.09.013\n10.1186/s12880-020-00485-0\n10.1016/j.neucom.2019.04.086\n10.1007/s12065-020-00403-x\n10.1109/TMI.2016.2553401\n10.1007/s10278-019-00182-7\n10.1016/j.cmpb.2019.105268\n10.1002/itl2.187\n10.1016/j.scs.2020.102582\n10.1016/j.inffus.2014.09.004\n10.1109/TMI.2016.2538802\n10.1016/j.media.2017.07.005\n10.1109/ACCESS.2017.2788044\n10.1007/s00138-020-01060-x\n10.1016/j.media.2018.11.010\n10.1002/mp.13620\n10.1109/TMI.2017.2743464\n10.1109/TMI.2019.2903562\n10.1109/trpms.2018.2890359\n10.1109/TMI.2019.2919951\n10.1109/ACCESS.2020.3005152\n10.1109/ACCESS.2020.2981337\n10.1109/TMI.2015.2508280\n10.1109/TMI.2018.2872031\n10.1109/TMI.2015.2458702\n10.1109/TMI.2016.2528120\n10.1109/TMI.2016.2528162\n10.1109/TNNLS.2019.2892409\n10.1109/TMI.2019.2894322\n10.1109/TPAMI.2012.277\n10.1109/TMI.2013.2256922\n10.1167/iovs.16-19964\n10.1109/ACCESS.2020.2993937\n10.3390/app11010371\n10.1038/s41598-020-80261-w\n10.1016/j.scs.2020.102589\n10.1148/radiol.2020200432\n10.1001/jama.2020.2648\n10.1111/tmi.13383\n10.1007/s12194-017-0406-5\n10.1016/j.zemedi.2018.11.002\n10.1007/s10916-018-1088-1\n10.1016/j.gpb.2017.07.003\n10.1038/s41591-020-0824-5\n10.1016/j.media.2020.101693\n10.1016/j.ijid.2020.03.004\n10.1016/S0140-6736(20)30183-5\n10.1016/j.ijid.2020.03.021\n10.1016/S0140-6736(20)30627-9\n10.2196/23996\n10.1148/radiol.2020200463\n10.1016/j.compbiomed.2019.103387\n10.1016/j.cogsys.2019.09.007\n10.1007/s11042-018-5714-1\n10.1038/s41598-018-22437-z\n10.1016/j.patrec.2020.03.011\n10.1038/nature14539\n10.1016/j.patcog.2019.01.006\n10.1007/s11263-015-0816-y\n10.1371/journal.pone.0207982\n10.1007/s11548-018-1843-2\n10.1016/S0140-6736(20)30260-9\n10.1016/j.jaut.2020.102433\n10.1016/S0140-6736(20)30251-8\n10.1007/s11427-020-1637-5\n10.1186/s12942-020-00202-8\n10.1016/j.ijantimicag.2020.105948\n10.1016/j.scs.2020.102372\n10.1001/jama.2020.4169\n10.1148/radiol.2020200642\n10.1148/radiol.2020200823\n10.1016/S0140-6736(20)30211-7\n10.1007/s00134-020-05996-6\n10.1148/radiol.2020200370\n10.1007/s00259-020-04734-w\n10.1148/radiol.2020200770\n10.1016/j.eng.2020.04.010\n10.5812/archcid.103232\n10.1016/j.dsx.2020.04.012\n10.1007/s12539-020-00376-6\n10.1016/j.csbj.2020.03.025\n10.1007/s00264-020-04609-7\n10.1016/j.ijmedinf.2020.104284\n10.1109/JBHI.2020.3019505\n10.1007/s10489-020-01900-3\n10.1038/s41598-020-76550-z\n10.1109/TCBB.2021.3065361\n10.1016/j.imu.2020.100360\n10.1006/viro.1995.0056"}
{"title": "A comparative analysis of eleven neural networks architectures for small datasets of lung images of COVID-19 patients toward improved clinical decisions.", "abstract": "The 2019 novel severe acute respiratory syndrome coronavirus 2-SARS-CoV2, commonly known as COVID-19, is a highly infectious disease that has endangered the health of many people around the world. COVID-19, which infects the lungs, is often diagnosed and managed using X-ray or computed tomography (CT) images. For such images, rapid and accurate classification and diagnosis can be performed using deep learning methods that are trained using existing neural network models. However, at present, there is no standardized method or uniform evaluation metric for image classification, which makes it difficult to compare the strengths and weaknesses of different neural network models. This paper used eleven well-known convolutional neural networks, including VGG-16, ResNet-18, ResNet-50, DenseNet-121, DenseNet-169, Inception-v3, Inception-v4, SqueezeNet, MobileNet, ShuffeNet, and EfficientNet-b0, to classify and distinguish COVID-19 and non-COVID-19 lung images. These eleven models were applied to different batch sizes and epoch cases, and their overall performance was compared and discussed. The results of this study can provide decision support in guiding research on processing and analyzing small medical datasets to understand which model choices can yield better outcomes in lung image classification, diagnosis, disease management and patient care.", "journal": "Computers in biology and medicine", "date": "2021-10-25", "authors": ["YuanYang", "LinZhang", "MingyuDu", "JingyuBo", "HaoleiLiu", "LeiRen", "XiaoheLi", "M JamalDeen"], "doi": "10.1016/j.compbiomed.2021.104887\n10.1142/S1793962320410032\n10.1142/S1793962321410014\n10.1142/S1793962321410014\n10.1016/j.ijsu.2020.02.034\n10.1016/j.ejrad.2020.109041\n10.1101/2020.02.23.20026930\n10.1109/CVPR.2016.90\n10.1007/s10489-020-01714-3\n10.1101/2020.03.19.20039354\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.308\n10.1007/978-3-030-01264-9_8\n10.33889/IJMEMS.2020.5.4.052\n10.1186/s40537-019-0197-0\n10.1186/s40537-019-0197-0\n10.1109/ICCV.2019.00140\n10.2200/S00010ED1V01Y200508IVM003\n10.2200/S00010ED1V01Y200508IVM003\n10.4236/jcc.2019.73002\n10.1109/TIP.2005.854492\n10.1109/TIP.2012.2214050\n10.1007/s11263-019-01228-7\n10.1007/978-3-319-10590-1_53\n10.1109/WACV.2018.00097"}
{"title": "Highlights of the 16th annual scientific meeting of the society of cardiovascular computed tomography.", "abstract": "The 16th Society of Cardiovascular Computed Tomography (SCCT) annual scientific meeting welcomed 781 digital attendees from 55 countries. The program included 27 sessions across three simultaneously streaming channels, 11 exhibitors, 153 poster presentations, and 32\u00a0\u200bhours of on demand videos. The main themes of the meeting included coronary artery disease, valvular heart disease, structural heart disease, and advanced analytics including machine learning. This article summaries the main themes of the meeting and some of the key presentations, which will shape the future of cardiovascular computed tomography in clinical practice.", "journal": "Journal of cardiovascular computed tomography", "date": "2021-10-25", "authors": ["Michelle CWilliams", "MarosFerencik", "Kelley RBranch", "KoenNieman", "Brian BGhoshhajra", "Andrew DChoi", "Edward DNicol", "EricWilliamson"], "doi": "10.1016/j.jcct.2021.10.002"}
{"title": "A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19.", "abstract": "The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.", "journal": "Computers in biology and medicine", "date": "2021-10-24", "authors": ["NedaAzouji", "AshkanSami", "MohammadTaheri", "HenningM\u00fcller"], "doi": "10.1016/j.compbiomed.2021.104927"}
{"title": "Deep learning for lung disease segmentation on CT: Which reconstruction kernel should be used?", "abstract": "The purpose of this study was to determine whether a single reconstruction kernel or both high and low frequency kernels should be used for training deep learning models for the segmentation of diffuse lung disease on chest computed tomography (CT).\nTwo annotated datasets of COVID-19 pneumonia (323,960 slices) and interstitial lung disease (ILD) (4,284 slices) were used. Annotated CT images were used to train a U-Net architecture to segment disease. All CT slices were reconstructed using both a lung kernel (LK) and a mediastinal kernel (MK). Three different trainings, resulting in three different models were compared for each disease: training on LK only, MK only or LK+MK images. Dice similarity scores (DSC) were compared using the Wilcoxon signed-rank test.\nModels only trained on LK images performed better on LK images than on MK images (median DSC\u00a0=\u00a00.62 [interquartile range (IQR): 0.54, 0.69] vs. 0.60 [IQR: 0.50, 0.70], P < 0.001 for COVID-19 and median DSC = 0.62 [IQR: 0.56, 0.69] vs. 0.50 [IQR 0.43, 0.57], P < 0.001 for ILD). Similarly, models only trained on MK images performed better on MK images (median DSC = 0.62 [IQR: 0.53, 0.68] vs. 0.54 [IQR: 0.47, 0.63], P < 0.001 for COVID-19 and 0.69 [IQR: 0.61, 0.73] vs. 0.63 [IQR: 0.53, 0.70], P < 0.001 for ILD). Models trained on both kernels performed better or similarly than those trained on only one kernel. For COVID-19, median DSC was 0.67 (IQR: =0.59, 0.73) when applied on LK images and 0.67 (IQR: 0.60, 0.74) when applied on MK images (P < 0.001 for both). For ILD, median DSC was 0.69 (IQR: 0.63, 0.73) when applied on LK images (P\u00a0=\u00a00.006) and 0.68 (IQR: 0.62, 0.72) when applied on MK images (P > 0.99).\nReconstruction kernels impact the performance of deep learning-based models for lung disease segmentation. Training on both LK and MK images improves the performance.", "journal": "Diagnostic and interventional imaging", "date": "2021-10-24", "authors": ["Trieu-NghiHoang-Thi", "MariaVakalopoulou", "StergiosChristodoulidis", "NikosParagios", "Marie-PierreRevel", "GuillaumeChassagnon"], "doi": "10.1016/j.diii.2021.10.001"}
{"title": "A Promising and Challenging Approach: Radiologists' Perspective on Deep Learning and Artificial Intelligence for Fighting COVID-19.", "abstract": "Chest X-rays (CXR) and computed tomography (CT) are the main medical imaging modalities used against the increased worldwide spread of the 2019 coronavirus disease (COVID-19) epidemic. Machine learning (ML) and artificial intelligence (AI) technology, based on medical imaging fully extracting and utilizing the hidden information in massive medical imaging data, have been used in COVID-19 research of disease diagnosis and classification, treatment decision-making, efficacy evaluation, and prognosis prediction. This review article describes the extensive research of medical image-based ML and AI methods in preventing and controlling COVID-19, and summarizes their characteristics, differences, and significance in terms of application direction, image collection, and algorithm improvement, from the perspective of radiologists. The limitations and challenges faced by these systems and technologies, such as generalization and robustness, are discussed to indicate future research directions.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-10-24", "authors": ["TianmingWang", "ZhuChen", "QuanliangShang", "CongMa", "XiangyuChen", "EnhuaXiao"], "doi": "10.3390/diagnostics11101924\n10.1016/j.bios.2020.112752\n10.1148/radiol.2020200463\n10.1016/j.diii.2020.10.001\n10.1038/nbt.4233\n10.7150/thno.38065\n10.1016/j.ejrad.2020.109236\n10.1007/s00259-020-04795-x\n10.1002/14651858.CD013639.pub4\n10.1007/s00259-020-04953-1\n10.1148/radiol.2020200905\n10.1007/s00330-021-07937-3\n10.1038/s41467-020-18685-1\n10.1038/s41467-020-17971-2\n10.1016/j.cell.2020.04.045\n10.1038/s41551-021-00704-1\n10.1148/radiol.2020203511\n10.1007/s00330-021-08050-1\n10.1016/j.acra.2020.09.004\n10.1183/13993003.00775-2020\n10.21037/atm-20-3026\n10.1186/s12879-021-05839-9\n10.3389/fmed.2021.699984\n10.1186/s12967-021-02992-2\n10.1148/radiol.2020201874\n10.1371/journal.pone.0252440\n10.1148/radiol.2020201491\n10.1002/mp.14609\n10.1148/ryai.2020200079\n10.1148/radiol.2020202439\n10.1016/j.media.2021.102096\n10.1038/s41598-021-95114-3\n10.7150/thno.46465\n10.1038/s41467-020-17280-8\n10.3233/XST-200685\n10.2147/TCRM.S280726\n10.3389/fimmu.2020.585647\n10.1038/s41598-021-86735-9\n10.3348/kjr.2020.0146\n10.1038/s41591-020-0931-3\n10.3348/kjr.2020.1104\n10.3390/jpm11060501\n10.1016/S2589-7500(21)00039-X\n10.1007/s00330-020-07225-6\n10.1186/s12911-021-01588-6\n10.1016/j.media.2020.101913\n10.1007/s00259-020-05075-4\n10.3233/XST-200757\n10.1109/ACCESS.2020.2994762\n10.1016/j.comcom.2021.06.011\n10.3389/frai.2021.612914\n10.1016/j.imu.2020.100378\n10.1080/03014460.2020.1839132\n10.1016/j.chaos.2020.109864\n10.1016/j.chaos.2020.109853\n10.1016/j.chaos.2020.109850\n10.3390/ijerph17155330\n10.1155/2021/6668985\n10.1146/annurev-biophys-062920-063711\n10.1016/S2589-7500(20)30192-8\n10.3389/fimmu.2020.01581\n10.1016/j.bj.2020.05.001\n10.1016/j.chest.2020.11.026\n10.1016/j.diii.2020.11.008"}
{"title": "Predicting Mechanical Ventilation and Mortality in COVID-19 Using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study.", "abstract": "In this study, we aimed to predict mechanical ventilation requirement and mortality using computational modeling of chest radiographs (CXRs) for coronavirus disease 2019 (COVID-19) patients. This two-center, retrospective study analyzed 530 deidentified CXRs from 515 COVID-19 patients treated at Stony Brook University Hospital and Newark Beth Israel Medical Center between March and August 2020. Linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), and random forest (RF) machine learning classifiers to predict mechanical ventilation requirement and mortality were trained and evaluated using radiomic features extracted from patients' CXRs. Deep learning (DL) approaches were also explored for the clinical outcome prediction task and a novel radiomic embedding framework was introduced. All results are compared against radiologist grading of CXRs (zone-wise expert severity scores). Radiomic classification models had mean area under the receiver operating characteristic curve (mAUCs) of 0.78 \u00b1 0.05 (sensitivity = 0.72 \u00b1 0.07, specificity = 0.72 \u00b1 0.06) and 0.78 \u00b1 0.06 (sensitivity = 0.70 \u00b1 0.09, specificity = 0.73 \u00b1 0.09), compared with expert scores mAUCs of 0.75 \u00b1 0.02 (sensitivity = 0.67 \u00b1 0.08, specificity = 0.69 \u00b1 0.07) and 0.79 \u00b1 0.05 (sensitivity = 0.69 \u00b1 0.08, specificity = 0.76 \u00b1 0.08) for mechanical ventilation requirement and mortality prediction, respectively. Classifiers using both expert severity scores and radiomic features for mechanical ventilation (mAUC = 0.79 \u00b1 0.04, sensitivity = 0.71 \u00b1 0.06, specificity = 0.71 \u00b1 0.08) and mortality (mAUC = 0.83 \u00b1 0.04, sensitivity = 0.79 \u00b1 0.07, specificity = 0.74 \u00b1 0.09) demonstrated improvement over either artificial intelligence or radiologist interpretation alone. Our results also suggest instances in which the inclusion of radiomic features in DL improves model predictions over DL alone. The models proposed in this study and the prognostic information they provide might aid physician decision making and efficient resource allocation during the COVID-19 pandemic.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-10-24", "authors": ["JosephBae", "SaarthakKapse", "GagandeepSingh", "RishabhGattu", "SyedAli", "NealShah", "ColinMarshall", "JonathanPierce", "TejPhatak", "AmitGupta", "JeremyGreen", "NikhilMadan", "PrateekPrasanna"], "doi": "10.3390/diagnostics11101812\n10.1016/S1473-3099(20)30120-1\n10.1148/radiol.2020201754\n10.2196/24018\n10.1001/jamainternmed.2020.2033\n10.1148/ryct.2020200047\n10.1007/s00330-020-07270-1\n10.3390/jcm9124129\n10.1148/radiol.2020201160\n10.1148/radiol.2020200642\n10.1038/s42256-020-0180-7\n10.1371/journal.pone.0233328\n10.7717/peerj.11205\n10.1080/23808993.2019.1585805\n10.1016/j.compbiomed.2020.103792\n10.1016/j.media.2020.101860\n10.1016/j.crad.2021.02.005\n10.1148/ryai.2020200098\n10.1007/s11036-020-01672-7\n10.1109/ACCESS.2021.3086020\n10.1007/s10278-021-00421-w\n10.1109/LGRS.2018.2802944\n10.1109/TSMC.1973.4309314\n10.1016/0031-3203(91)90143-S\n10.1109/34.709601\n10.1109/TPAMI.2005.159\n10.1038/s41598-019-47765-6\n10.1038/s41598-021-88538-4"}
{"title": "Stratifying the early radiologic trajectory in dyspneic patients with COVID-19 pneumonia.", "abstract": "This study aimed to stratify the early pneumonia trajectory on chest radiographs and compare patient characteristics in dyspneic patients with coronavirus disease 2019 (COVID-19).\nWe retrospectively included 139 COVID-19 patients with dyspnea (87 men, 62.7\u00b116.3 years) and serial chest radiographs from January to September 2020. Radiographic pneumonia extent was quantified as a percentage using a previously-developed deep learning algorithm. A group-based trajectory model was used to categorize the pneumonia trajectory after symptom onset during hospitalization. Clinical findings, and outcomes were compared, and Cox regression was performed for survival analysis.\nRadiographic pneumonia trajectories were categorized into four groups. Group 1 (n = 83, 59.7%) had negligible pneumonia, and group 2 (n = 29, 20.9%) had mild pneumonia. Group 3 (n = 13, 9.4%) and group 4 (n = 14, 10.1%) showed similar considerable pneumonia extents at baseline, but group 3 had decreasing pneumonia extent at 1-2 weeks, while group 4 had increasing pneumonia extent. Intensive care unit admission and mortality were significantly more frequent in groups 3 and 4 than in groups 1 and 2 (P < .05). Groups 3 and 4 shared similar clinical and laboratory findings, but thrombocytopenia (<150\u00d7103/\u03bcL) was exclusively observed in group 4 (P = .016). When compared to groups 1 and 2, group 4 (hazard ratio, 63.3; 95% confidence interval, 7.9-504.9) had a two-fold higher risk for mortality than group 3 (hazard ratio, 31.2; 95% confidence interval, 3.5-280.2), and this elevated risk was maintained after adjusting confounders.\nMonitoring the early radiologic trajectory beyond baseline further prognosticated at-risk COVID-19 patients, who potentially had thrombo-inflammatory responses.", "journal": "PloS one", "date": "2021-10-23", "authors": ["Jin YoungKim", "Keum JiJung", "Seung-JinYoo", "Soon HoYoon"], "doi": "10.1371/journal.pone.0259010\n10.1148/radiol.2021203998\n10.1001/jama.2020.2648\n10.3348/kjr.2020.0132\n10.1148/radiol.2020200370\n10.1148/radiol.2020203496\n10.3348/kjr.2020.0564\n10.1148/radiol.2020201365\n10.1148/radiol.2020203173\n10.1016/j.jinf.2020.04.021\n10.3346/jkms.2020.35.e316\n10.3346/jkms.2020.35.e413\n10.1148/radiol.2020201160\n10.1186/s12889-019-7077-6\n10.1177/0049124101029003005\n10.1146/annurev.clinpsy.121208.131413\n10.1093/aje/kwt179\n10.2214/AJR.20.22976\n10.1016/j.mayocp.2020.04.006\n10.1016/S1473-3099(20)30086-4\n10.1159/000512007\n10.1111/jth.14975\n10.1161/CIRCRESAHA.120.317703\n10.1080/09537104.2020.1754383\n10.1016/j.cca.2020.03.022\n10.1093/labmed/lmaa067\n10.1016/j.thromres.2020.11.017\n10.1186/s13613-020-00706-3"}
{"title": "COVID-19 Diagnosis Using Capsule Network and Fuzzy ", "abstract": "The COVID-19 epidemic is spreading day by day. Early diagnosis of this disease is essential to provide effective preventive and therapeutic measures. This process can be used by a computer-aided methodology to improve accuracy. In this study, a new and optimal method has been utilized for the diagnosis of COVID-19. Here, a method based on fuzzy ", "journal": "BioMed research international", "date": "2021-10-23", "authors": ["AliFarki", "ZahraSalekshahrezaee", "Arash MohammadiTofigh", "RezaGhanavati", "BehdadArandian", "AmirahmadChapnevis"], "doi": "10.1155/2021/2295920\n10.1155/2021/5544742\n10.1016/j.measurement.2019.107086\n10.1038/s41598-021-90428-8\n10.2174/1573405616666200129095242\n10.1016/j.cmpb.2020.105532\n10.1016/j.media.2020.101794\n10.1007/s12539-020-00403-6\n10.1371/journal.pone.0235187\n10.1109/MITP.2020.3042379\n10.1101/2020.04.13.20063461v1\n10.1007/978-3-030-56689-0_12\n10.1016/j.compmedimag.2020.101716\n10.1016/j.cie.2020.106559\n10.1007/s40313-016-0242-6\n10.1007/s11042-020-08699-8\n10.1109/ICCS45141.2019.9065537\n10.1038/s41598-020-71294-2\n10.1109/ACCESS.2020.3016780"}
{"title": "GACDN: generative adversarial feature completion and diagnosis network for COVID-19.", "abstract": "The outbreak of coronavirus disease 2019 (COVID-19) causes tens of million infection world-wide. Many machine learning methods have been proposed for the computer-aided diagnosis between COVID-19 and community-acquired pneumonia (CAP) from chest computed tomography (CT) images. Most of these methods utilized the location-specific handcrafted features based on the segmentation results to improve the diagnose performance. However, the prerequisite segmentation step is time-consuming and needs the intervention by lots of expert radiologists, which cannot be achieved in the areas with limited medical resources.\nWe propose a generative adversarial feature completion and diagnosis network (GACDN) that simultaneously generates handcrafted features by radiomic counterparts and makes accurate diagnoses based on both original and generated features. Specifically, we first calculate the radiomic features from the CT images. Then, in order to fast obtain the location-specific handcrafted features, we use the proposed GACDN to generate them by its corresponding radiomic features. Finally, we use both radiomic features and location-specific handcrafted features for COVID-19 diagnosis.\nFor the performance of our generated location-specific handcrafted features, the results of four basic classifiers show that it has an average of 3.21% increase in diagnoses accuracy. Besides, the experimental results on COVID-19 dataset show that our proposed method achieved superior performance in COVID-19 vs. community acquired pneumonia (CAP) classification compared with the state-of-the-art methods.\nThe proposed method significantly improves the diagnoses accuracy of COVID-19 vs. CAP in the condition of incomplete location-specific handcrafted features. Besides, it is also applicable in some regions lacking of expert radiologists and high-performance computing resources.", "journal": "BMC medical imaging", "date": "2021-10-23", "authors": ["QiZhu", "HaizhouYe", "LiangSun", "ZhongnianLi", "RanWang", "FengShi", "DinggangShen", "DaoqiangZhang"], "doi": "10.1186/s12880-021-00681-6\n10.1148/radiol.2020201343\n10.1016/j.compbiomed.2020.103792\n10.1016/S0140-6736(20)30260-9\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200274\n10.1148/radiol.2020200432\n10.1016/j.ejrad.2020.108961\n10.2214/AJR.20.22954\n10.1007/s00330-020-06731-x\n10.1109/JBHI.2020.3019505\n10.1109/TMI.2020.2992546\n10.1109/TMI.2016.2582386\n10.1088/1361-6560/abe838\n10.29080/jhsp.v4i2.375\n10.1016/j.neuroimage.2011.06.064\n10.1109/MSP.2017.2765202\n10.1145/3301282\n10.1142/S0218488598000094\n10.1109/TMM.2020.3013408"}
{"title": "Quantitative chest CT combined with plasma cytokines predict outcomes in COVID-19 patients.", "abstract": "Despite extraordinary international efforts to dampen the spread and understand the mechanisms behind SARS-CoV-2 infections, accessible predictive biomarkers directly applicable in the clinic are yet to be discovered. Recent studies have revealed that diverse types of assays bear limited predictive power for COVID-19 outcomes. Here, we harness the predictive power of chest CT in combination with plasma cytokines using a machine learning approach for predicting death during hospitalization and maximum severity degree in COVID-19 patients. Patients (n=152) from the Mount Sinai Health System in New York with plasma cytokine assessment and a chest CT within 5 days from admission were included. Demographics, clinical, and laboratory variables, including plasma cytokines (IL-6, IL-8, and TNF-\u03b1) were collected from the electronic medical record. We found that chest CT combined with plasma cytokines were good predictors of death (AUC 0.78) and maximum severity (AUC 0.82), whereas CT quantitative was better at predicting severity (AUC 0.81 vs 0.70) while cytokine measurements better predicted death (AUC 0.70 vs 0.66). Finally, we provide a simple scoring system using plasma IL-6, IL-8, TNF-\u03b1, GGO to aerated lung ratio and age as novel metrics that may be used to monitor patients upon hospitalization and help physicians make critical decisions and considerations for patients at high risk of death for COVID-19.", "journal": "medRxiv : the preprint server for health sciences", "date": "2021-10-22", "authors": ["GuillermoCarbonell", "Diane MarieDel Valle", "EdgarGonzalez-Kozlova", "BrettMarinelli", "EmmaKlein", "MariaEl Homsi", "DanielStocker", "MichaelChung", "AdamBernheim", "Nicole WSimons", "JianiXiang", "SharonNirenberg", "PatriciaKovatch", "SaraLewis", "MiriamMerad", "SachaGnjatic", "BachirTaouli"], "doi": "10.1101/2021.10.11.21264709"}
{"title": "Deep Radiomic Analysis for Predicting Coronavirus Disease 2019 in Computerized Tomography and X-Ray Images.", "abstract": "This article proposes to encode the distribution of features learned from a convolutional neural network (CNN) using a Gaussian mixture model (GMM). These parametric features, called GMM-CNN, are derived from chest computed tomography (CT) and X-ray scans of patients with coronavirus disease 2019 (COVID-19). We use the proposed GMM-CNN features as input to a robust classifier based on random forests (RFs) to differentiate between COVID-19 and other pneumonia cases. Our experiments assess the advantage of GMM-CNN features compared with standard CNN classification on test images. Using an RF classifier (80% samples for training; 20% samples for testing), GMM-CNN features encoded with two mixture components provided a significantly better performance than standard CNN classification ( ). Specifically, our method achieved an accuracy in the range of 96.00%-96.70% and an area under the receiver operator characteristic (ROC) curve in the range of 99.29%-99.45%, with the best performance obtained by combining GMM-CNN features from both CT and X-ray images. Our results suggest that the proposed GMM-CNN features could improve the prediction of COVID-19 in chest CT and X-ray scans.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-10-21", "authors": ["AhmadChaddad", "LamaHassan", "ChristianDesrosiers"], "doi": "10.1109/TNNLS.2021.3119071"}
{"title": "Identification of most important features based on a fuzzy ensemble technique: Evaluation on joint space narrowing progression in knee osteoarthritis patients.", "abstract": "Feature selection (FS) is a crucial and at the same time challenging processing step that aims to reduce the dimensionality of complex classification or regression problems. Various techniques have been proposed in the literature to address this challenge with emphasis to medical applications. However, each one of the existing FS algorithms come with its own advantages and disadvantages introducing a certain level of bias.\nTo avoid bias and alleviate the defectiveness of single feature selection results, an ensemble FS methodology is proposed in this paper that aggregates the results of several FS algorithms (filter, wrapper and embedded ones). Fuzzy logic is employed to combine multiple feature importance scores thus leading to a more robust selection of informative features. The proposed fuzzy ensemble FS methodology was applied on the problem of knee osteoarthritis (KOA) prediction with special emphasis on the progression of joint space narrowing (JSN). The proposed FS methodology was integrated into an end-to-end machine learning pipeline and a thorough experimental evaluation was conducted using data from the Osteoarthritis Initiative (OAI) database. Several classifiers were investigated for their suitability in the task of JSN prediction and the best performing model was then post-hoc analyzed by using the SHAP method.\nThe results showed that the proposed method presented a better and more stable performance in contrast to other competitive feature selection methods, leading to an average accuracy of 78.14% using XG Boost at 31 selected features. The post-hoc explainability highlighted the important features that contribute to the classification of patients with JSN progression.\nThe proposed fuzzy feature selection approach improves the performance of the predictive models by selecting a small optimal subset of features compared to popular feature selection methods.", "journal": "International journal of medical informatics", "date": "2021-10-19", "authors": ["CharisNtakolia", "ChristosKokkotis", "SerafeimMoustakidis", "DimitriosTsaopoulos"], "doi": "10.1016/j.ijmedinf.2021.104614"}
{"title": "Early prediction of in-hospital death of COVID-19 patients: a machine-learning model based on age, blood analyses, and chest x-ray score.", "abstract": "An early-warning model to predict in-hospital mortality on admission of COVID-19 patients at an emergency department (ED) was developed and validated using a machine-learning model. In total, 2782 patients were enrolled between March 2020 and December 2020, including 2106 patients (first wave) and 676 patients (second wave) in the COVID-19 outbreak in Italy. The first-wave patients were divided into two groups with 1474 patients used to train the model, and 632 to validate it. The 676 patients in the second wave were used to test the model. Age, 17 blood analytes, and Brescia chest X-ray score were the variables processed using a random forests classification algorithm to build and validate the model. Receiver operating characteristic (ROC) analysis was used to assess the model performances. A web-based death-risk calculator was implemented and integrated within the Laboratory Information System of the hospital. The final score was constructed by age (the most powerful predictor), blood analytes (the strongest predictors were lactate dehydrogenase, D-dimer, neutrophil/lymphocyte ratio, C-reactive protein, lymphocyte %, ferritin std, and monocyte %), and Brescia chest X-ray score (https://bdbiomed.shinyapps.io/covid19score/). The areas under the ROC curve obtained for the three groups (training, validating, and testing) were 0.98, 0.83, and 0.78, respectively. The model predicts in-hospital mortality on the basis of data that can be obtained in a short time, directly at the ED on admission. It functions as a web-based calculator, providing a risk score which is easy to interpret. It can be used in the triage process to support the decision on patient allocation.", "journal": "eLife", "date": "2021-10-19", "authors": ["EmirenaGarrafa", "MarikaVezzoli", "MarcoRavanelli", "DavideFarina", "AndreaBorghesi", "StefanoCalza", "RobertoMaroldi"], "doi": "10.7554/eLife.70640\n10.1007/s11606-021-06626-7\n10.1016/S2665-9913(21)00059-X\n10.1515/cclm-2020-0722\n10.1515/cclm-2020-0459\n10.1007/s11547-020-01200-3\n10.1016/j.ijid.2020.05.021\n10.1007/s11547-020-01202-1\n10.3389/fimmu.2020.584241\n10.1023/A:1010933404324\n10.1136/annrheumdis-2020-218323\n10.1285/i20705948v5n1p89\n10.1613/jair.953\n10.1016/j.ejcb.2016.04.005\n10.1136/bmj.g7594\n10.1007/978-3-319-00032-9_13\n10.3171/2020.9.FOCUS20681\n10.1001/jamasurg.2020.2713\n10.1214/aos/1013203451\n10.4155/bio-2020-0109\n10.1016/j.healthpol.2020.10.006\n10.1159/000514481\n10.1080/23744235.2020.1784457\n10.1186/s12874-020-01080-1\n10.1136/bmj.m3339\n10.7554/eLife.66125\n10.1016/j.cmi.2021.03.002\n10.1016/j.blre.2020.100745\n10.1001/jamainternmed.2020.2033\n10.7554/eLife.63195\n10.1515/cclm-2020-1121\n10.1093/gerona/glaa291\n10.1007/s00330-020-07504-2\n10.1515/cclm-2020-1529\n10.1016/j.bbagen.2019.02.008\n10.1186/s13049-020-00764-3\n10.3892/ijmm.2019.4222\n10.1148/radiol.2020202723\n10.1136/bmj.m1464\n10.1285/i20705948v4n1p23\n10.1038/s41598-017-11104-4\n10.1016/S2665-9913(20)30343-X\n10.1136/bmj.m1328"}
{"title": "Current limitations to identify covid-19 using artificial intelligence with chest x-ray imaging (part ii). The shortcut learning problem.", "abstract": "Since the outbreak of the COVID-19 pandemic, computer vision researchers have been working on automatic identification of this disease using radiological images. The results achieved by automatic classification methods far exceed those of human specialists, with sensitivity as high as 100% being reported. However, prestigious radiology societies have stated that the use of this type of imaging alone is not recommended as a diagnostic method. According to some experts the patterns presented in these images are unspecific and subtle, overlapping with other viral pneumonias. This report seeks to evaluate the analysis the robustness and generalizability of different approaches using artificial intelligence, deep learning and computer vision to identify COVID-19 using chest X-rays images. We also seek to alert researchers and reviewers to the issue of \"shortcut learning\". Recommendations are presented to identify whether COVID-19 automatic classification models are being affected by shortcut learning. Firstly, papers using explainable artificial intelligence methods are reviewed. The results of applying external validation sets are evaluated to determine the generalizability of these methods. Finally, studies that apply traditional computer vision methods to perform the same task are considered. It is evident that using the whole chest X-Ray image or the bounding box of the lungs, the image regions that contribute most to the classification appear outside of the lung region, something that is not likely possible. In addition, although the investigations that evaluated their models on data sets external to the training set, the effectiveness of these models decreased significantly, it may provide a more realistic representation as how the model will perform in the clinic. The results indicate that, so far, the existing models often involve shortcut learning, which makes their use less appropriate in the clinical setting.", "journal": "Health and technology", "date": "2021-10-19", "authors": ["Jos\u00e9 DanielL\u00f3pez-Cabrera", "Rub\u00e9nOrozco-Morales", "Jorge ArmandoPortal-D\u00edaz", "OrlandoLovelle-Enr\u00edquez", "Marl\u00e9nP\u00e9rez-D\u00edaz"], "doi": "10.1007/s12553-021-00609-8\n10.1016/j.ijantimicag.2020.105924\n10.1016/j.cca.2020.03.009\n10.1148/radiol.2020200642\n10.7326/M20-1495\n10.1016/j.ijid.2020.03.071\n10.1016/j.chest.2020.04.003\n10.1177/0846537120924606\n10.1109/ACCESS.2021.3058537\n10.1007/s12553-021-00520-2\n10.1148/radiol.2020200527\n10.1148/ryct.2020200034\n10.1148/radiol.2020201160\n10.3348/kjr.2020.0132\n10.1016/j.ejrad.2020.109092\n10.1016/j.crad.2020.03.008\n10.1109/JBHI.2020.3037127\n10.1016/S2589-7500(20)30079-0\n10.1109/ACCESS.2020.3027685\n10.1016/j.jiph.2020.06.028\n10.15212/bioi-2020-0015\n10.1007/s10462-021-09985-z\n10.1016/j.bspc.2020.102365\n10.1016/j.mehy.2020.109761\n10.1007/s12559-020-09795-5\n10.1613/jair.1.12162\n10.1016/j.csbj.2020.08.003\n10.1148/ryai.2019180031\n10.1371/journal.pmed.1002683\n10.1016/j.inffus.2021.04.008\n10.1109/ACCESS.2020.3044858\n10.3892/etm.2020.8797\n10.1007/s11548-020-02305-w\n10.1007/s13755-020-00119-3\n10.1007/s00521-020-05636-6\n10.1016/j.patrec.2020.09.010\n10.1109/ACCESS.2021.3079716\n10.1023/B:VISI.0000029664.99615.94\n10.1109/TPAMI.2002.1017623\n10.1371/journal.pone.0235187\n10.1016/j.ins.2020.09.041\n10.20944/preprints202003.0300.v1"}
{"title": "COVID-19 diagnosis from chest x-rays: developing a simple, fast, and accurate neural network.", "abstract": "Chest x-rays are a fast and inexpensive test that may potentially diagnose COVID-19, the disease caused by the novel coronavirus. However, chest imaging is not a first-line test for COVID-19 due to low diagnostic accuracy and confounding with other viral pneumonias. Recent research using deep learning may help overcome this issue as convolutional neural networks (CNNs) have demonstrated high accuracy of COVID-19 diagnosis at an early stage.\nWe used the COVID-19 Radiography database [36], which contains x-ray images of COVID-19, other viral pneumonia, and normal lungs. We developed a CNN in which we added a dense layer on top of a pre-trained baseline CNN (EfficientNetB0), and we trained, validated, and tested the model on 15,153 X-ray images. We used data augmentation to avoid overfitting and address class imbalance; we used fine-tuning to improve the model's performance. From the external test dataset, we calculated the model's accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and F1-score.\nOur model differentiated COVID-19 from normal lungs with 95% accuracy, 90% sensitivity, and 97% specificity; it differentiated COVID-19 from other viral pneumonia and normal lungs with 93% accuracy, 94% sensitivity, and 95% specificity.\nOur parsimonious CNN shows that it is possible to differentiate COVID-19 from other viral pneumonia and normal lungs on x-ray images with high accuracy. Our method may assist clinicians with making more accurate diagnostic decisions and support chest X-rays as a valuable screening tool for the early, rapid diagnosis of COVID-19.\nThe online version contains supplementary material available at 10.1007/s13755-021-00166-4.", "journal": "Health information science and systems", "date": "2021-10-19", "authors": ["VasilisNikolaou", "SebastianoMassaro", "MasoudFakhimi", "LamprosStergioulas", "WolfgangGarn"], "doi": "10.1007/s13755-021-00166-4\n10.36416/1806-3756/e20200226\n10.1148/radiol.2020200642\n10.1007/s13755-020-00135-3\n10.1016/j.imu.2020.100360\n10.1016/j.chaos.2020.109944\n10.1016/j.diii.2020.11.008\n10.1016/j.cmpb.2020.105608\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103792\n10.1007/s12559-020-09751-3\n10.1007/s00264-020-04609-7\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103805\n10.1016/j.cmpb.2020.105581\n10.18517/ijaseit.10.2.11446\n10.1109/ACCESS.2020.2994762\n10.1016/j.cmpb.2020.105532\n10.1007/s13246-020-00865-4\n10.1371/journal.pone.0235187\n10.1097/RTI.0000000000000532\n10.1007/s13246-020-00888-x\n10.1016/S2589-7500(21)00039-X"}
{"title": "COVID-19 Diagnosis from CT Images with Convolutional Neural Network Optimized by Marine Predator Optimization Algorithm.", "abstract": "In recent years, almost every country in the world has struggled against the spread of Coronavirus Disease 2019. If governments and public health systems do not take action against the spread of the disease, it will have a severe impact on human life. A noteworthy technique to stop this pandemic is diagnosing COVID-19 infected patients and isolating them instantly. The present study proposes a method for the diagnosis of COVID-19 from CT images. The method is a hybrid method based on convolutional neural network which is optimized by a newly introduced metaheuristic, called marine predator optimization algorithm. This optimization method is performed to improve the system accuracy. The method is then implemented on the chest CT scans with the COVID-19-related findings (MosMedData) dataset, and the results are compared with three other methods from the literature to indicate the method's performance. The final results indicate that the proposed method with 98.11% accuracy, 98.13% precision, 98.66% sensitivity, and 97.26% ", "journal": "BioMed research international", "date": "2021-10-16", "authors": ["HuapingJia", "JunlongZhao", "AliArshaghi"], "doi": "10.1155/2021/5122962\n10.1007/s10489-020-01826-w\n10.1117/12.2588672\n10.1016/j.media.2020.101794\n10.1016/j.imu.2020.100412\n10.1109/ACCESS.2020.3022366\n10.1002/ima.22608\n10.1016/j.energy.2018.10.153\n10.1515/med-2018-0002\n10.1007/s13369-021-05688-3\n10.1016/j.egyr.2019.11.013\n10.1016/j.egyr.2019.10.029\n10.1016/j.egyr.2020.03.010\n10.1016/j.egyr.2020.04.012\n10.1016/j.egyr.2019.09.039\n10.1016/j.cma.2020.113609\n10.1016/j.knosys.2019.105190\n10.1016/j.eswa.2020.113377\n10.1038/44831\n10.1109/ACCESS.2020.3016780\n10.1007/s00330-020-06817-6"}
{"title": "Development and prospective validation of COVID-19 chest X-ray screening model for patients attending emergency departments.", "abstract": "Chest X-rays (CXRs) are the first-line investigation in patients presenting to emergency departments (EDs) with dyspnoea and are a valuable adjunct to clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to facilitate rapid triage of CXRs for further patient testing and/or isolation. In this work we develop an AI algorithm, CovIx, to differentiate normal, abnormal, non-COVID-19 pneumonia, and COVID-19 CXRs using a multicentre cohort of 293,143 CXRs. The algorithm is prospectively validated in 3289 CXRs acquired from patients presenting to ED with symptoms of COVID-19 across four sites in NHS Greater Glasgow and Clyde. CovIx achieves area under receiver operating characteristic curve for COVID-19 of 0.86, with sensitivity and F1-score up to 0.83 and 0.71 respectively, and performs on-par with four board-certified radiologists. AI-based algorithms can identify CXRs with COVID-19 associated pneumonia, as well as distinguish non-COVID pneumonias in symptomatic patients presenting to ED. Pre-trained models and inference scripts are freely available at https://github.com/beringresearch/bravecx-covid .", "journal": "Scientific reports", "date": "2021-10-16", "authors": ["IgnatDrozdov", "BenjaminSzubert", "ElainaReda", "PeterMakary", "DanielForbes", "Sau LeeChang", "AbinayaEzhil", "SrikanthPuttagunta", "MarkHall", "ChrisCarlin", "David JLowe"], "doi": "10.1038/s41598-021-99986-3\n10.1186/s40249-019-0617-6\n10.1056/NEJMoa2002032\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.2648\n10.1001/jama.2020.3786\n10.1136/postgradmedj-2020-138029\n10.1016/j.ejrad.2020.108961\n10.1093/cid/ciaa722\n10.1128/JCM.00512-20\n10.1002/14651858.CD013705.pub2\n10.1007/s00330-020-06827-4\n10.1136/bmj.m2426\n10.1148/radiol.2020201365\n10.1136/thoraxjnl-2020-214916\n10.1016/j.crad.2020.03.008\n10.1148/radiol.2020201160\n10.1148/radiol.2020204238\n10.1038/s41598-020-76550-z\n10.1016/j.cmpb.2020.105581\n10.1148/radiol.2020202944\n10.1038/s41467-020-18685-1\n10.1148/radiol.2020203511\n10.1038/s42256-021-00307-0\n10.1097/RTI.0000000000000532\n10.1186/s40537-019-0192-5\n10.1371/journal.pone.0229963\n10.1371/journal.pmed.1002697\n10.1109/TMI.2020.2993291\n10.1177/0962280214541852\n10.2307/2531595\n10.1148/radiol.2020201874\n10.1007/s12652-021-02917-3\n10.1007/s00134-017-4683-6\n10.1109/TNNLS.2013.2293637\n10.1038/s41598-020-70479-z\n10.1007/s13246-020-00888-x\n10.1007/s12559-020-09775-9\n10.1016/j.jbi.2021.103820\n10.1109/TKDE.2008.239\n10.1016/S2468-2667(20)30282-6\n10.1016/j.ejro.2020.100231\n10.1148/radiol.2020200823\n10.1186/s41747-020-00203-z\n10.2214/AJR.19.21512\n10.1007/s10916-021-01747-2\n10.1007/s00330-020-07270-1\n10.1148/ryct.2020200337\n10.1016/j.jinf.2020.03.005\n10.1371/journal.pone.0237297\n10.1136/bmjopen-2020-040129\n10.1016/S2214-109X(20)30464-2\n10.1016/j.crad.2020.04.005\n10.1148/ryct.2020200280\n10.1136/bmj.m1808\n10.1007/s10916-020-01562-1"}
{"title": "Diagnostic Test Accuracy of Deep Learning Detection of COVID-19: A Systematic Review and Meta-Analysis.", "abstract": "To perform a meta-analysis to compare the diagnostic test accuracy (DTA) of deep learning (DL) in detecting coronavirus disease 2019 (COVID-19), and to investigate how network architecture and type of datasets affect DL performance.\nWe searched PubMed, Web of Science and Inspec from January 1, 2020, to December 3, 2020, for retrospective and prospective studies on deep learning detection with at least reported sensitivity and specificity. Pooled DTA was obtained using random-effect models. Sub-group analysis between studies was also carried out for data source and network architectures.\nThe pooled sensitivity and specificity were 91% (95% confidence interval [CI]: 88%, 93%; I\nThe diagnosis of COVID-19 via deep learning has achieved incredible performance, and the source of datasets, as well as network architectures, strongly affect DL performance.", "journal": "Academic radiology", "date": "2021-10-16", "authors": ["Temitope EmmanuelKomolafe", "YuzhuCao", "Benedictor AlexanderNguchu", "PatriceMonkam", "Ebenezer ObaloluwaOlaniyi", "HaotianSun", "JianZheng", "XiaodongYang"], "doi": "10.1016/j.acra.2021.08.008\n10.1148/radiol.2020200905\n10.4103/0970-1591.91444\n10.1002/sim.3631\n10.1002/sim.1186\n10.1007/s11548-020-02286-w\n10.1186/s43163-020-00039-9\n10.1001/jama.1994.0351033008103"}
{"title": "COVID-19 detection from lung CT-Scans using a fuzzy integral-based CNN ensemble.", "abstract": "The COVID-19 pandemic has collapsed the public healthcare systems, along with severely damaging the economy of the world. The SARS-CoV-2 virus also known as the coronavirus, led to community spread, causing the death of more than a million people worldwide. The primary reason for the uncontrolled spread of the virus is the lack of provision for population-wise screening. The apparatus for RT-PCR based COVID-19 detection is scarce and the testing process takes 6-9\u00a0h. The test is also not satisfactorily sensitive (71% sensitive only). Hence, Computer-Aided Detection techniques based on deep learning methods can be used in such a scenario using other modalities like chest CT-scan images for more accurate and sensitive screening. In this paper, we propose a method that uses a Sugeno fuzzy integral ensemble of four pre-trained deep learning models, namely, VGG-11, GoogLeNet, SqueezeNet v1.1 and Wide ResNet-50-2, for classification of chest CT-scan images into COVID and Non-COVID categories. The proposed framework has been tested on a publicly available dataset for evaluation and it achieves 98.93% accuracy and 98.93% sensitivity on the same. The model outperforms state-of-the-art methods on the same dataset and proves to be a reliable COVID-19 detector. The relevant source codes for the proposed approach can be found at: https://github.com/Rohit-Kundu/Fuzzy-Integral-Covid-Detection.", "journal": "Computers in biology and medicine", "date": "2021-10-15", "authors": ["RohitKundu", "Pawan KumarSingh", "SeyedaliMirjalili", "RamSarkar"], "doi": "10.1016/j.compbiomed.2021.104895\n10.1088/2632-2153/abf22c\n10.1109/TPAMI.2019.2918284\n10.1007/s11042-021-11319-8\n10.1148/radiol.2020200370\n10.1109/ICCV.2017.74\n10.25540/e3y2-aqye"}
{"title": "Artificial intelligence on COVID-19 pneumonia detection using chest xray images.", "abstract": "Recent studies show the potential of artificial intelligence (AI) as a screening tool to detect COVID-19 pneumonia based on chest x-ray (CXR) images. However, issues on the datasets and study designs from medical and technical perspectives, as well as questions on the vulnerability and robustness of AI algorithms have emerged. In this study, we address these issues with a more realistic development of AI-driven COVID-19 pneumonia detection models by generating our own data through a retrospective clinical study to augment the dataset aggregated from external sources. We optimized five deep learning architectures, implemented development strategies by manipulating data distribution to quantitatively compare study designs, and introduced several detection scenarios to evaluate the robustness and diagnostic performance of the models. At the current level of data availability, the performance of the detection model depends on the hyperparameter tuning and has less dependency on the quantity of data. InceptionV3 attained the highest performance in distinguishing pneumonia from normal CXR in two-class detection scenario with sensitivity (Sn), specificity (Sp), and positive predictive value (PPV) of 96%. The models attained higher general performance of 91-96% Sn, 94-98% Sp, and 90-96% PPV in three-class compared to four-class detection scenario. InceptionV3 has the highest general performance with accuracy, F1-score, and g-mean of 96% in the three-class detection scenario. For COVID-19 pneumonia detection, InceptionV3 attained the highest performance with 86% Sn, 99% Sp, and 91% PPV with an AUC of 0.99 in distinguishing pneumonia from normal CXR. Its capability of differentiating COVID-19 pneumonia from normal and non-COVID-19 pneumonia attained 0.98 AUC and a micro-average of 0.99 for other classes.", "journal": "PloS one", "date": "2021-10-15", "authors": ["Lei RigiBaltazar", "Mojhune GabrielManzanillo", "JoverlynGaudillo", "Ethel DominiqueViray", "MarioDomingo", "BeatriceTiangco", "JasonAlbia"], "doi": "10.1371/journal.pone.0257884\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020200642\n10.1038/s41598-020-76550-z\n10.1016/j.clinimag.2020.04.001\n10.1259/bjr/20276974\n10.1016/j.ejrad.2004.03.010\n10.1016/j.ejro.2020.100231\n10.1007/s10489-020-01888-w\n10.1016/j.bspc.2021.102583\n10.1016/j.asoc.2020.106885\n10.1016/j.media.2020.101794\n10.1109/ACCESS.2020.3010287\n10.1007/s10489-020-02076-6\n10.1007/s13246-020-00865-4\n10.1016/j.imu.2020.100360\n10.1007/s40846-020-00529-4\n10.1007/s10044-021-00984-y\n10.1007/s10489-020-01900-3\n10.1101/2020.03.20.20039834\n10.1371/journal.pmed.1002686\n10.1148/radiol.2020201491\n10.1016/S2589-7500(20)30003-0"}
{"title": "Lessons learned in transitioning to AI in the medical imaging of COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has wreaked havoc across the world. It also created a need for the urgent development of efficacious predictive diagnostics, specifically, artificial intelligence (AI) methods applied to medical imaging. This has led to the convergence of experts from multiple disciplines to solve this global pandemic including clinicians, medical physicists, imaging scientists, computer scientists, and informatics experts to bring to bear the best of these fields for solving the challenges of the COVID-19 pandemic. However, such a convergence over a very brief period of time has had unintended consequences and created its own challenges. As part of Medical Imaging Data and Resource Center initiative, we discuss the lessons learned from career transitions across the three involved disciplines (radiology, medical imaging physics, and computer science) and draw recommendations based on these experiences by analyzing the challenges associated with each of the three associated transition types: (1)\u00a0AI of non-imaging data to AI of medical imaging data, (2)\u00a0medical imaging clinician to AI of medical imaging, and (3)\u00a0AI of medical imaging to AI of COVID-19 imaging. The lessons learned from these career transitions and the diffusion of knowledge among them could be accomplished more effectively by recognizing their associated intricacies. These lessons learned in the transitioning to AI in the medical imaging of COVID-19 can inform and enhance future AI applications, making the whole of the transitions more than the sum of each discipline, for confronting an emergency like the COVID-19 pandemic or solving emerging problems in biomedicine.", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-10-15", "authors": ["IssamEl Naqa", "HuiLi", "JordanFuhrman", "QiyuanHu", "NaveenaGorre", "WeijieChen", "Maryellen LGiger"], "doi": "10.1117/1.JMI.8.S1.010902\n10.1001/jama.2021.3284\n10.1001/jama.2021.3760\n10.1016/S0140-6736(21)00193-8\n10.1001/jama.2021.2974\n10.1001/jama.2021.2828\n10.1001/jama.2021.1046\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020201160\n10.1148/rg.2020200159\n10.1016/S1473-3099(20)30086-4\n10.2214/AJR.20.22976\n10.1183/23120541.00539-2020\n10.1002/jum.15284\n10.1186/s13613-020-00799-w\n10.2214/AJR.20.23513\n10.1148/radiol.2020204267\n10.5114/pjr.2021.102609\n10.1148/radiol.2020203702\n10.1148/radiol.2020202439\n10.5114/pjr.2021.103858\n10.1148/radiol.2021204141\n10.1148/radiol.2020202326\n10.1056/NEJMc2030450\n10.1148/radiol.2020203627\n10.1148/radiol.2020202944\n10.1148/radiol.2020201491\n10.1148/radiol.2020204238\n10.1148/radiol.2020203511\n10.1148/radiol.2020204226\n10.1038/s41598-021-89553-1\n10.1016/j.ejrad.2020.109236\n10.1259/bjr.20190855\n10.1007/s12553-021-00520-2\n10.3322/caac.21552\n10.1038/s42256-021-00307-0\n10.1136/bmj.m1328\n10.1093/jnci/95.1.14\n10.1109/TRPMS.2018.2832609\n10.1093/scan/nsp053\n10.21037/jtd.2017.05.34\n10.1177/0962280206079046\n10.1007/978-3-030-33128-3_1\n10.1148/radiol.2020200038\n10.1111/biom.13379\n10.1007/s10278-019-00180-9\n10.1371/journal.pmed.1002683\n10.1109/ACCESS.2021.3079716\n10.1038/s41571-021-00541-w\n10.1117/1.JMI.5.4.044505\n10.1146/annurev-bioeng-071812-152416\n10.1148/radiol.2020191145\n10.1148/radiol.2019190613\n10.1038/s41598-020-69250-1\n10.1136/bmj.304.6840.1491\n10.21037/cdt.2017.03.12\n10.2214/ajr.167.4.8819370\n10.1007/s00330-018-5695-5\n10.1007/s10554-009-9501-y\n10.1148/radiol.13122665\n10.1148/radiol.2015142700\n10.1148/radiol.2020200432\n10.1148/radiol.2020201343\n10.1177/0846537120938328\n10.1148/ryct.2020200312\n10.1007/s00330-020-07042-x\n10.1007/s00134-020-06212-1\n10.1371/journal.pone.0242301\n10.1613/jair.953\n10.1148/radiol.2020201473\n10.1148/radiol.2020203173\n10.1007/s00330-020-06898-3\n10.1148/ryct.2020200214\n10.1148/radiol.2020201365\n10.1148/ryct.2020200152\n10.1016/j.ejrad.2020.109008\n10.1002/widm.1131\n10.1016/j.ijrobp.2021.01.042\n10.1002/mp.13678\n10.1002/mp.12811\n10.1016/j.ijrobp.2018.08.022\n10.1109/TBME.2015.2477688\n10.1117/12.2217098\n10.1148/radiol.14131320\n10.1148/radiol.2222010506\n10.3348/kjr.2020.0132\n10.1038/s41591-020-0931-3\n10.1038/s41569-020-0360-5\n10.1117/12.2581977\n10.1088/1361-6560/abbf9e\n10.1007/s10140-020-01808-y\n10.1117/1.JMI.8.S1.014501\n10.1148/radiol.2020200463\n10.1126/science.abe2813\n10.3174/ajnr.A7072\n10.1001/jamaneurol.2020.1127\n10.1038/d41586-020-02599-5\n10.1148/radiol.2015151169\n10.1038/srep17787\n10.1007/s00428-020-02886-6\n10.1016/j.jacr.2020.09.060\n10.3390/jcm9113697\n10.1016/j.tacc.2021.02.007\n10.1016/S0140-6736(19)30037-6\n10.1038/s41591-020-1041-y\n10.1016/S2589-7500(20)30219-3\n10.1038/s41591-020-0941-1\n10.1002/mp.15170\n10.1148/ryai.2020200029"}
{"title": "Estimated pulse wave velocity improves risk stratification for all-cause mortality in patients with COVID-19.", "abstract": "Accurate risk stratification in COVID-19 patients consists a major clinical need to guide therapeutic strategies. We sought to evaluate the prognostic role of estimated pulse wave velocity (ePWV), a marker of arterial stiffness which reflects overall arterial integrity and aging, in risk stratification of hospitalized patients with COVID-19. This retrospective, longitudinal cohort study, analyzed a total population of 1671 subjects consisting of 737 hospitalized COVID-19 patients consecutively recruited from two tertiary centers (Newcastle cohort: n\u2009=\u2009471 and Pisa cohort: n\u2009=\u2009266) and a non-COVID control cohort (n\u2009=\u2009934). Arterial stiffness was calculated using validated formulae for ePWV. ePWV progressively increased across the control group, COVID-19 survivors and deceased patients (adjusted mean increase per group 1.89\u00a0m/s, P\u2009<\u20090.001). Using a machine learning approach, ePWV provided incremental prognostic value and improved reclassification for mortality over the core model including age, sex and comorbidities [AUC (core model\u2009+\u2009ePWV vs. core model)\u2009=\u20090.864 vs. 0.755]. ePWV provided similar prognostic value when pulse pressure or hs-Troponin were added to the core model or over its components including age and mean blood pressure (p\u2009<\u20090.05 for all). The optimal prognostic ePWV value was 13.0\u00a0m/s. ePWV conferred additive discrimination (AUC: 0.817 versus 0.779, P\u2009<\u20090.001) and reclassification value (NRI\u2009=\u20090.381, P\u2009<\u20090.001) over the 4C Mortality score, a validated score for predicting mortality in COVID-19 and the Charlson comorbidity index. We suggest that calculation of ePWV, a readily applicable estimation of arterial stiffness, may serve as an additional clinical tool to refine risk stratification of hospitalized patients with COVID-19 beyond established risk factors and scores.", "journal": "Scientific reports", "date": "2021-10-14", "authors": ["KimonStamatelopoulos", "GeorgiosGeorgiopoulos", "Kenneth FBaker", "GiusyTiseo", "DimitriosDelialis", "CharalamposLazaridis", "GretaBarbieri", "StefanoMasi", "Nikolaos IVlachogiannis", "KaterynaSopova", "AlessandroMengozzi", "LorenzoGhiadoni", "Ina Schimvan der Loeff", "Aidan THanrath", "BajramAjdini", "CharalambosVlachopoulos", "Meletios ADimopoulos", "Christopher J ADuncan", "MarcoFalcone", "KonstantinosStellos", "NoneNone", "NoneNone"], "doi": "10.1038/s41598-021-99050-0\n10.1001/jamainternmed.2020.2033\n10.1161/hypertensionaha.120.15324\n10.1016/j.jcv.2020.104371\n10.1111/eci.13362\n10.1111/eci.13378\n10.1093/eurheartj/ehy339\n10.1016/j.jacc.2009.10.061\n10.1016/j.jacc.2013.09.063\n10.1093/eurheartj/ehq165\n10.1097/hjh.0000000000000935\n10.1001/jamanetworkopen.2019.12831\n10.1161/hypertensionaha.120.16563\n10.1136/bmj.m3339\n10.1038/nrcardio.2017.155\n10.1161/01.cir.103.7.987\n10.1016/j.jacc.2019.07.012\n10.1161/hypertensionaha.107.090464\n10.1016/j.jacc.2012.07.054\n10.3109/08037051.2011.617045\n10.1161/HYPERTENSIONAHA.114.03617\n10.1161/hypertensionaha.119.14307\n10.1161/circulationaha.105.535435\n10.1007/s00421-013-2648-1\n10.1126/sciadv.abe4724\n10.1111/joim.13275\n10.1097/hjh.0000000000000644\n10.1097/01.hjh.0000249701.49854.21\n10.1161/jaha.117.007003\n10.1042/bsr20190015\n10.1183/13993003.01157-2020\n10.1007/s11906-018-0867-x\n10.1097/HJH.0b013e32834fa8b0\n10.1016/s1473-3099(20)30120-1\n10.3390/medsci9010006\n10.1038/s41598-021-85646-z\n10.1161/circresaha.119.314862\n10.1016/j.jacc.2014.12.035\n10.1111/jch.13954\n10.1161/hypertensionaha.119.14088\n10.1002/sim.4085"}
{"title": "Conditional GAN based augmentation for predictive modeling of respiratory signals.", "abstract": "Respiratory illness is the primary cause of mortality and impairment in the life span of an individual in the current COVID-19 pandemic scenario. The inability to inhale and exhale is one of the difficult conditions for a person suffering from respiratory disorders. Unfortunately, the diagnosis of respiratory disorders with the presently available imaging and auditory screening modalities are sub-optimal and the accuracy of diagnosis varies with different medical experts. At present, deep neural nets demand a massive amount of data suitable for precise models. In reality, the respiratory data set is quite limited, and therefore, data augmentation (DA) is employed to enlarge the data set. In this study, conditional generative adversarial networks (cGAN) based DA is utilized for synthetic generation of signals. The publicly available repository such as ICBHI 2017 challenge, RALE and Think Labs Lung Sounds Library are considered for classifying the respiratory signals. To assess the efficacy of the artificially created signals by the DA approach, similarity measures are calculated between original and augmented signals. After that, to quantify the performance of augmentation in classification, scalogram representation of generated signals are fed as input to different pre-trained deep learning architectures viz Alexnet, GoogLeNet and ResNet-50. The experimental results are computed and performance results are compared with existing classical approaches of augmentation. The research findings conclude that the proposed cGAN method of augmentation provides better accuracy of 92.50% and 92.68%, respectively for both the two data sets using ResNet 50 model.", "journal": "Computers in biology and medicine", "date": "2021-10-13", "authors": ["SJayalakshmy", "Gnanou FlorenceSudha"], "doi": "10.1016/j.compbiomed.2021.104930"}
{"title": "Comprehensive literature review on the radiographic findings, imaging modalities, and the role of radiology in the COVID-19 pandemic.", "abstract": "Since the outbreak of the coronavirus disease 2019 (COVID-19) pandemic, over 103214008 cases have been reported, with more than 2231158 deaths as of January 31, 2021. Although the gold standard for diagnosis of this disease remains the reverse-transcription polymerase chain reaction of nasopharyngeal and oropharyngeal swabs, its false-negative rates have ignited the use of medical imaging as an important adjunct or alternative. Medical imaging assists in identifying the pathogenesis, the degree of pulmonary damage, and the characteristic features in each imaging modality. This literature review collates the characteristic radiographic findings of COVID-19 in various imaging modalities while keeping the preliminary focus on chest radiography, computed tomography (CT), and ultrasound scans. Given the higher sensitivity and greater proficiency in detecting characteristic findings during the early stages, CT scans are more reliable in diagnosis and serve as a practical method in following up the disease time course. As research rapidly expands, we have emphasized the CO-RADS classification system as a tool to aid in communicating the likelihood of COVID-19 suspicion among healthcare workers. Additionally, the utilization of other scoring systems such as MuLBSTA, Radiological Assessment of Lung Edema, and Brixia in this pandemic are reviewed as they integrate the radiographic findings into an objective scoring system to risk stratify the patients and predict the severity of disease. Furthermore, current progress in the utilization of artificial intelligence ", "journal": "World journal of radiology", "date": "2021-10-12", "authors": ["AmanPal", "AbulhassanAli", "Timothy RYoung", "JuanOostenbrink", "AkulPrabhakar", "AmoghPrabhakar", "NinaDeacon", "AmarArnold", "AhmedEltayeb", "CharlesYap", "David MYoung", "AlanTang", "SubramanianLakshmanan", "Ying YiLim", "MarthaPokarowski", "PramathKakodkar"], "doi": "10.4329/wjr.v13.i9.258"}
{"title": "Comparison of pirfenidone and corticosteroid treatments at the COVID-19 pneumonia with the guide of artificial intelligence supported thoracic computed tomography.", "abstract": "We aimed to investigate the effect of short-term pirfenidone treatment on prolonged COVID-19 pneumonia.\nHospital files of patients hospitalised with a diagnosis of critical COVID-19 pneumonia from November 2020 to March 2021 were retrospectively reviewed. Chest computed tomography images taken both before treatment and 2\u00a0months after treatment, demographic characteristics and laboratory parameters of patients receiving pirfenidone\u00a0+\u00a0methylprednisolone (n\u00a0=\u00a013) and only methylprednisolones (n\u00a0=\u00a09) were recorded. Pulmonary function tests were performed after the second month of the treatment. CT involvement rates were determined by machine learning.\nA total of 22 patients, 13 of whom (59.1%) were using methylprednisolone\u00a0+\u00a0pirfenidone and 9 of whom (40.9%) were using only methylprednisolone were included. When the blood gas parameters and pulmonary function tests of the patients were compared at the end of the second month, it was found that the FEV1, FEV1%, FVC and FVC% values were statistically significantly higher in the methylprednisolone\u00a0+\u00a0pirfenidone group compared with the methylprednisolone group (P\u00a0=\u00a0.025, P\u00a0=\u00a0.012, P\u00a0=\u00a0.026 and P\u00a0=\u00a0.017, respectively). When the rates of change in CT scans at diagnosis and second month of treatment were examined, it was found that the involvement rates in the methylprednisolone\u00a0+\u00a0pirfenidone group were statistically significantly decreased (P\u00a0<\u00a0.001).\nAntifibrotic agents can reduce fibrosis that may develop in the future. These can also help dose reduction and/or non-use strategy for methylprednisolone therapy, which has many side effects. Further large series and randomised controlled studies are needed on this subject.", "journal": "International journal of clinical practice", "date": "2021-10-09", "authors": ["MuratAcat", "PinarYildiz Gulhan", "SerkanOner", "Muhammed KamilTuran"], "doi": "10.1111/ijcp.14961\n10.1109/CICT48419.2019.9066263\n10.1007/978-3-642-03683-5_15\n10.3389/fmed.2020.00539"}
{"title": "AANet: Adaptive Attention Network for COVID-19 Detection From Chest X-Ray Images.", "abstract": "Accurate and rapid diagnosis of COVID-19 using chest X-ray (CXR) plays an important role in large-scale screening and epidemic prevention. Unfortunately, identifying COVID-19 from the CXR images is challenging as its radiographic features have a variety of complex appearances, such as widespread ground-glass opacities and diffuse reticular-nodular opacities. To solve this problem, we propose an adaptive attention network (AANet), which can adaptively extract the characteristic radiographic findings of COVID-19 from the infected regions with various scales and appearances. It contains two main components: an adaptive deformable ResNet and an attention-based encoder. First, the adaptive deformable ResNet, which adaptively adjusts the receptive fields to learn feature representations according to the shape and scale of infected regions, is designed to handle the diversity of COVID-19 radiographic features. Then, the attention-based encoder is developed to model nonlocal interactions by self-attention mechanism, which learns rich context information to detect the lesion regions with complex shapes. Extensive experiments on several public datasets show that the proposed AANet outperforms state-of-the-art methods.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-10-07", "authors": ["ZhijieLin", "ZhaoshuiHe", "ShengliXie", "XuWang", "JiTan", "JunLu", "BeihaiTan"], "doi": "10.1109/TNNLS.2021.3114747"}
{"title": "Radiology Implementation Considerations for Artificial Intelligence (AI) Applied to COVID-19, From the ", "abstract": "Hundreds of imaging-based artificial intelligence (AI) models have been developed in response to the COVID-19 pandemic. AI systems that incorporate imaging have shown promise in primary detection, severity grading, and prognostication of outcomes in COVID-19, and have enabled integration of imaging with a broad range of additional clinical and epidemiologic data. However, systematic reviews of AI models applied to COVID-19 medical imaging have highlighted problems in the field, including methodologic issues and problems in real-world deployment. Clinical use of such models should be informed by both the promise and potential pitfalls of implementation. How does a practicing radiologist make sense of this complex topic, and what factors should be considered in the implementation of AI tools for imaging of COVID-19? This critical review aims to help the radiologist understand the nuances that impact the clinical deployment of AI for imaging of COVID-19. We review imaging use cases for AI models in COVID-19 (e.g., diagnosis, severity assessment, and prognostication) and explore considerations for AI model development and testing, deployment infrastructure, clinical user interfaces, quality control, and institutional review board and regulatory approvals, with a practical focus on what a radiologist should consider when implementing an AI tool for COVID-19.", "journal": "AJR. American journal of roentgenology", "date": "2021-10-07", "authors": ["Matthew DLi", "KenChang", "XueyanMei", "AdamBernheim", "MichaelChung", "SharonSteinberger", "JayashreeKalpathy-Cramer", "Brent PLittle"], "doi": "10.2214/AJR.21.26717"}
{"title": "Detection and analysis of COVID-19 in medical images using deep learning techniques.", "abstract": "The main purpose of this work is to investigate and compare several deep learning enhanced techniques applied to X-ray and CT-scan medical images for the detection of COVID-19. In this paper, we used four powerful pre-trained CNN models, VGG16, DenseNet121, ResNet50,and ResNet152, for the COVID-19 CT-scan binary classification task. The proposed Fast.AI ResNet framework was designed to find out the best architecture, pre-processing, and training parameters for the models largely automatically. The accuracy and F1-score were both above 96% in the diagnosis of COVID-19 using CT-scan images. In addition, we applied transfer learning techniques to overcome the insufficient data and to improve the training time. The binary and multi-class classification of X-ray images tasks were performed by utilizing enhanced VGG16 deep transfer learning architecture. High accuracy of 99% was achieved by enhanced VGG16 in the detection of X-ray images from COVID-19 and pneumonia. The accuracy and validity of the algorithms were assessed on X-ray and CT-scan well-known public datasets. The proposed methods have better results for COVID-19 diagnosis than other related in literature. In our opinion, our work can help virologists and radiologists to make a better and faster diagnosis in the struggle against the outbreak of COVID-19.", "journal": "Scientific reports", "date": "2021-10-06", "authors": ["DandiYang", "CristhianMartinez", "LaraVisu\u00f1a", "HardevKhandhar", "ChintanBhatt", "JesusCarretero"], "doi": "10.1038/s41598-021-99015-3\n10.1016/j.ijsu.2020.02.034\n10.1038/s41598-020-79139-8\n10.1038/s41598-021-91305-0\n10.1001/jama.2017.14585\n10.1016/j.future.2018.04.065\n10.3390/ijms17081313\n10.1016/j.media.2017.07.005\n10.1007/s11548-021-02335-y\n10.1016/j.patrec.2019.11.013\n10.1007/s11042-020-10010-8\n10.1371/journal.pone.0242535\n10.3390/info11020108\n10.1016/j.cell.2018.02.010\n10.1109/ACCESS.2020.3025164\n10.1016/j.bspc.2021.102588\n10.1109/ACCESS.2020.3025010\n10.1016/j.eswa.2020.114054\n10.3390/electronics9091388\n10.1016/j.compbiomed.2020.103792"}
{"title": "Artificial Intelligence against COVID-19 Pandemic: A Comprehensive Insight.", "abstract": "COVID-19 is a pandemic initially identified in Wuhan, China, which is caused by a novel coronavirus, also recognized as the Severe Acute Respiratory Syndrome (SARS-nCoV-2). Unlike other coronaviruses, this novel pathogen may cause unusual contagious pain, which results in viral pneumonia, serious heart problems, and even death. Researchers worldwide are continuously striving to develop a cure for this highly infectious disease, yet there are no well-defined absolute treatments available at present. Several vaccination drives using emergency use authorisation vaccines have been held across many countries; however, their long-term efficacy and side-effects studies are yet to be studied. Various analytical and statistical models have been developed, however, their outcome rate is prolonged. Thus, modern science stresses the application of state-of-the-art methods to combat COVID-19. This paper aims to provide a deep insight into the comprehensive literature about AI and AI-driven tools in the battle against the COVID-19 pandemic. The high efficacy of these AI systems can be observed in terms of highly accurate results, i.e., > 95%, as reported in various studies. The extensive literature reviewed in this paper is divided into five sections, each describing the application of AI against COVID-19 viz. COVID-19 prevention, diagnostic, infection spread trend prediction, therapeutic and drug repurposing. The application of Artificial Intelligence (AI) and AI-driven tools are proving to be useful in managing and fighting against the COVID-19 pandemic, especially by analysing the X-Ray and CT-Scan imaging data of infected subjects, infection trend predictions, etc.", "journal": "Current medical imaging", "date": "2021-10-06", "authors": ["AzharEqubal", "SarfarazMasood", "IftekharEqubal", "ShafiAhmad", "Noor ZamanKhan", "Zahid AKhan"], "doi": "10.2174/1573405617666211004115208"}
{"title": "Integrating Domain Knowledge Into Deep Networks for Lung Ultrasound With Applications to COVID-19.", "abstract": "Lung ultrasound (LUS) is a cheap, safe and non-invasive imaging modality that can be performed at patient bed-side. However, to date LUS is not widely adopted due to lack of trained personnel required for interpreting the acquired LUS frames. In this work we propose a framework for training deep artificial neural networks for interpreting LUS, which may promote broader use of LUS. When using LUS to evaluate a patient's condition, both anatomical phenomena (e.g., the pleural line, presence of consolidations), as well as sonographic artifacts (such as A- and B-lines) are of importance. In our framework, we integrate domain knowledge into deep neural networks by inputting anatomical features and LUS artifacts in the form of additional channels containing pleural and vertical artifacts masks along with the raw LUS frames. By explicitly supplying this domain knowledge, standard off-the-shelf neural networks can be rapidly and efficiently finetuned to accomplish various tasks on LUS data, such as frame classification or semantic segmentation. Our framework allows for a unified treatment of LUS frames captured by either convex or linear probes. We evaluated our proposed framework on the task of COVID-19 severity assessment using the ICLUS dataset. In particular, we finetuned simple image classification models to predict per-frame COVID-19 severity score. We also trained a semantic segmentation model to predict per-pixel COVID-19 severity annotations. Using the combined raw LUS frames and the detected lines for both tasks, our off-the-shelf models performed better than complicated models specifically designed for these tasks, exemplifying the efficacy of our framework.", "journal": "IEEE transactions on medical imaging", "date": "2021-10-05", "authors": ["OzFrank", "NirSchipper", "MordehayVaturi", "GinoSoldati", "AndreaSmargiassi", "RiccardoInchingolo", "ElenaTorri", "TizianoPerrone", "FedericoMento", "LibertarioDemi", "MeiravGalun", "Yonina CEldar", "ShaiBagon"], "doi": "10.1109/TMI.2021.3117246"}
{"title": "The potential and challenges of Health 4.0 to face COVID-19 pandemic: a rapid review.", "abstract": "The COVID-19 pandemic has generated the need to evolve health services to reduce the risk of contagion and promote a collaborative environment even remotely. Advances in Industry 4.0, including the internet of things, mobile networks, cloud computing, and artificial intelligence make Health 4.0 possible to connect patients with healthcare professionals. Hence, the focus of this work is analyzing the potentiality, and challenges of state-of-the-art Health 4.0 applications to face the COVID-19 pandemic including augmented environments, diagnosis of the virus, forecasts, medical robotics, and remote clinical services. It is concluded that Health 4.0 can be applied in the prevention of contagion, improve diagnosis, promote virtual learning environments, and offer remote services. However, there are still ethical, technical, security, and legal challenges to be addressed. Additionally, more imaging datasets for COVID-19 detection need to be made available to the scientific community. Working in the areas of opportunity will help to address the new normal. Likewise, Health 4.0 can be applied not only in the COVID-19 pandemic, but also in future global viruses and natural disasters.", "journal": "Health and technology", "date": "2021-10-05", "authors": ["Cecilia-IreneLoeza-Mej\u00eda", "EddyS\u00e1nchez-DelaCruz", "PilarPozos-Parra", "Luis-AlfonsoLandero-Hern\u00e1ndez"], "doi": "10.1007/s12553-021-00598-8\n10.1016/j.ijsu.2020.02.034\n10.1109/ACCESS.2020.2992341\n10.3389/fmed.2020.00429\n10.1016/j.dsx.2020.06.029\n10.1038/s41746-020-0306-7\n10.1016/j.compeleceng.2020.106765\n10.3389/fphy.2020.00336\n10.1109/OJEMB.2020.3026928\n10.1002/hbe2.237\n10.1007/s13755-018-0049-x\n10.1016/j.future.2019.10.043\n10.1016/j.comnet.2017.05.018\n10.1016/j.procs.2020.03.447\n10.1016/j.glohj.2019.07.001\n10.1016/j.compbiomed.2020.103792\n10.1016/j.jradnu.2019.01.008\n10.1016/j.jaad.2019.07.008\n10.1080/10833196.2018.1447256\n10.1186/s12903-019-0937-8\n10.1016/j.heliyon.2019.e02205\n10.1093/gastro/gov027\n10.1126/science.abc0473\n10.1016/j.idm.2020.08.001\n10.1016/j.aju.2018.06.006\n10.1016/j.jor.2019.04.020\n10.1016/j.neunet.2018.06.018\n10.1016/j.soard.2020.01.006\n10.1016/j.wneu.2019.10.142\n10.1080/13645706.2019.1584116\n10.1080/24699322.2018.1560097\n10.1016/j.ymssp.2018.05.050\n10.1186/s12889-020-09301-4\n10.1057/hs.2016.2\n10.1007/s40735-017-0078-z\n10.2147/IEH.S133518\n10.1016/j.jbiomech.2020.109690\n10.1109/TBME.2015.2422751\n10.1080/10798587.2017.1329245\n10.1080/17517575.2015.1053416\n10.3390/s18061714\n10.1016/j.chemolab.2020.104054\n10.3390/ijerph17113819\n10.1016/S2589-7500(19)30109-8\n10.1016/j.media.2020.101794"}
{"title": "Public Covid-19 X-ray datasets and their impact on model bias - A systematic review of a significant problem.", "abstract": "Computer-aided-diagnosis and stratification of COVID-19 based on chest X-ray suffers from weak bias assessment and limited quality-control. Undetected bias induced by inappropriate use of datasets, and improper consideration of confounders prevents the translation of prediction models into clinical practice. By adopting established tools for model evaluation to the task of evaluating datasets, this study provides a systematic appraisal of publicly available COVID-19 chest X-ray datasets, determining their potential use and evaluating potential sources of bias. Only 9 out of more than a hundred identified datasets met at least the criteria for proper assessment of risk of bias and could be analysed in detail. Remarkably most of the datasets utilised in 201 papers published in peer-reviewed journals, are not among these 9 datasets, thus leading to models with high risk of bias. This raises concerns about the suitability of such models for clinical use. This systematic review highlights the limited description of datasets employed for modelling and aids researchers to select the most suitable datasets for their task.", "journal": "Medical image analysis", "date": "2021-10-02", "authors": ["BeatrizGarcia Santa Cruz", "Mat\u00edas Nicol\u00e1sBossa", "JanS\u00f6lter", "Andreas DominikHusch"], "doi": "10.1016/j.media.2021.102225\n10.2174/1573405616666201123120417\n10.1136/thorax-2020-BTSabstracts.404\n10.3390/app11020672\n10.3390/diagnostics10040231\n10.1016/j.ophtha.2020.09.009\n10.1038/s41467-020-17478-w\n10.1016/S0140-6736(19)30037-6\n10.1101/2020.09.13.20193565\n10.1167/tvst.9.2.7\n10.1148/radiol.2019191586\n10.1214/ss/1009211805\n10.1038/s41467-020-19478-2\n10.1016/S2589-7500(19)30063-9\n10.1016/S2589-7500(20)30223-5\n10.1016/S0140-6736(19)31819-7\n10.1038/s42256-020-00239-1\n10.1371/journal.pmed.1001744\n10.7326/M18-1377\n10.1136/bmj.m689\n10.1016/j.bja.2020.07.040\n10.1001/jama.2019.18058\n10.1038/s42256-021-00307-0\n10.1007/s10489-020-01862-6\n10.1038/s41591-020-0941-1\n10.1161/CIRCOUTCOMES.120.006556\n10.1007/978-3-030-16399-0\n10.1136/bmj.l6927\n10.1136/bmj.m1328\n10.1016/j.ijid.2020.03.017"}
{"title": "COVID\u201119 pathology imaging: A one-year perspective.", "abstract": "The first cases of coronavirus disease 2019 (COVID\u201119) were reported in Wuhan, China, in December 2019. Five months later, the World Health Organization (WHO) announced a pandemic. The symptoms are nonspecific, and include breathing difficulties, cough, fever, and the loss of smell and taste. The diagnosis is confirmed by real-time reverse transcriptase-polymerase chain reaction (RT-PCR) testing. Medical imaging has been mainly used to estimate the range of disease or potential complications.The aim of this study was to present the radiographic features of COVID\u201119 reported in published papers. This investigation includes the scientific work concerning chest radiography (chest X-ray - CXR) and computed tomography (CT) in COVID\u201119 patients. The most common pathologies are described, and the classification of COVID\u201119 appearance in CT and other radiology reports is summarized. The usage of lung ultrasound (LUS) was taken into consideration. This study emphasizes the role of artificial intelligence (AI) in the COVID\u201119 pandemic. The algorithms developed to detect the disease are discussed. The role of medical imaging is not limited to the respiratory system; it can also be used in searching for and monitoring complications (cardiac, vascular or brain damage). Due to the significant role of radiology in the current pandemic, a review of the latest medical literature was performed to help clarify the upcoming data.", "journal": "Dental and medical problems", "date": "2021-10-02", "authors": ["MartynaHajac", "CyprianOlchowy", "Rafa\u0142Por\u0119ba", "Pawe\u0142Ga\u0107"], "doi": "10.17219/dmp/135814"}
{"title": "Role of standard and soft tissue chest radiography images in deep-learning-based early diagnosis of COVID-19.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-10-02", "authors": ["QiyuanHu", "KarenDrukker", "Maryellen LGiger"], "doi": "10.1117/1.JMI.8.S1.014503\n10.1371/journal.pone.0242958\n10.1136/bmj.m1808\n10.1016/j.chest.2020.04.003\n10.1016/j.clinimag.2020.04.001\n10.1148/ryct.2020200034\n10.1148/radiol.2020201874\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020203511\n10.1136/bmj.m1328\n10.1109/CVPR.2017.369\n10.1117/12.2581977\n10.1109/CVPR.2009.5206848\n10.1371/journal.pmed.1002686\n10.1038/s41598-021-87994-2\n10.1117/1.JMI.4.4.041307\n10.3978/j.issn.2223-4292.2014.11.20\n10.1038/s41598-020-67441-4\n10.1080/01621459.1987.10478410\n10.1006/jmps.1998.1218\n10.2307/2531595\n10.1148/radiol.12120725\n10.1109/ICCV.2017.74\n10.2214/AJR.19.21512"}
{"title": "Cardiac involvement in hospitalized patients with COVID-19 and its incremental value in outcomes prediction.", "abstract": "Recent reports linked acute COVID-19 infection in hospitalized patients to cardiac abnormalities. Studies have not evaluated presence of abnormal cardiac structure and function before scanning in setting of COVD-19 infection. We sought to examine cardiac abnormalities in consecutive group of patients with acute COVID-19 infection according to the presence or absence of cardiac disease based on review of health records and cardiovascular imaging studies. We looked at independent contribution of imaging findings to clinical outcomes. After excluding patients with previous left ventricular (LV) systolic dysfunction (global and/or segmental), 724 patients were included. Machine learning identified predictors of in-hospital mortality and in-hospital mortality\u2009+\u2009ECMO. In patients without previous cardiovascular disease, LV EF\u2009<\u200950% occurred in 3.4%, abnormal LV\u00a0global longitudinal strain (<\u200916%) in 24%, and diastolic dysfunction in 20%. Right ventricular systolic dysfunction (RV free wall strain\u2009<\u200920%) was noted in 18%. Moderate and large pericardial effusion were uncommon with an incidence of 0.4% for each category. Forty patients received ECMO support, and 79 died (10.9%). A stepwise increase in AUC was observed with addition of vital signs and laboratory measurements to baseline clinical characteristics, and a further significant increase (AUC 0.91) was observed when echocardiographic measurements were added. The performance of an optimized prediction model was similar to the model including baseline characteristics\u2009+\u2009vital signs and laboratory results\u2009+\u2009echocardiographic measurements.", "journal": "Scientific reports", "date": "2021-10-02", "authors": ["PayamPournazari", "Alison LSpangler", "FawziAmeer", "Kobina KHagan", "Mauricio ETano", "MohammedChamsi-Pasha", "Lakshmi HChebrolu", "William AZoghbi", "KhurramNasir", "Sherif FNagueh"], "doi": "10.1038/s41598-021-98773-4\n10.1016/j.jcmg.2020.04.014\n10.1161/CIRCULATIONAHA.120.047971\n10.1007/s00392-020-01683-0\n10.1186/s13054-020-02958-8\n10.1093/ehjci/jeaa178\n10.1111/echo.14825\n10.1007/s10554-020-01968-5\n10.1111/echo.14869\n10.1016/j.amjcard.2020.07.010\n10.1016/j.echo.2020.05.030\n10.1007/s00392-020-01727-5\n10.1111/echo.14835\n10.1016/j.jacc.2020.05.068\n10.1016/j.amjcard.2020.06.053\n10.1002/ehf2.13044\n10.1007/s10554-020-02010-4\n10.1016/j.echo.2020.06.009\n10.1016/j.jcmg.2020.05.010\n10.1016/j.jcmg.2020.06.004\n10.1186/s40560-020-00516-6\n10.1016/j.cjca.2020.05.030\n10.1016/j.jacc.2020.08.069\n10.1016/j.echo.2021.05.010\n10.14814/phy2.14628\n10.1016/j.echo.2014.10.003\n10.1016/j.echo.2016.01.011\n10.1093/ehjci/jey042\n10.1016/j.echo.2017.01.007\n10.3390/jcm9103263\n10.1097/MCA.0000000000000914\n10.1097/MCA.0000000000000934"}
{"title": "Development of smart camera systems based on artificial intelligence network for social distance detection to fight against COVID-19.", "abstract": "In this work, an artificial intelligence network-based smart camera system prototype, which tracks social distance using a bird's-eye perspective, has been developed. \"MobileNet SSD-v3\", \"Faster-R-CNN Inception-v2\", \"Faster-R-CNN ResNet-50\" models have been utilized to identify people in video sequences. The final prototype based on the Faster R-CNN model is an integrated embedded system that detects social distance with the camera. The software developed using the \"Nvidia Jetson Nano\" development kit and Raspberry Pi camera module calculates all necessary actions in itself, detects social distance violations, makes audible and light warnings, and reports the results to the server. It is predicted that the developed smart camera prototype can be integrated into public spaces within the \"sustainable smart cities,\" the scope that the world is on the verge of a change.", "journal": "Applied soft computing", "date": "2021-10-01", "authors": ["OnurKaraman", "AdiAlhudhaif", "KemalPolat"], "doi": "10.1016/j.asoc.2021.107610\n10.1016/j.ijantimicag.2020.105951\n10.1007/s11427-020-1637-5\n10.1001/jama.2020.1585\n10.1056/NEJMoa2001191\n10.3906/sag-2004-172\n10.1542/peds.2020-0702\n10.3345/cep.2020.00493\n10.1016/S0140-6736(20)30313-5\n10.1136/bmj.m1066\n10.1016/S0140-6736(20)30679-6\n10.1093/jtm/taaa039\n10.1093/jtm/taaa020\n10.1136/bmj.m3223\n10.1016/j.neucom.2018.01.092\n10.1016/j.scitotenv.2020.138858\n10.3390/make1030044\n10.1016/j.procs.2018.10.335\n10.1145/2184319.2184337\n10.1016/j.sysarc.2020.101896\n10.1093/eurpub/ckn107"}
{"title": "MTU-COVNet: A hybrid methodology for diagnosing the COVID-19 pneumonia with optimized features from multi-net.", "abstract": "The aim of this study was to establish and evaluate a fully automatic deep learning system for the diagnosis of COVID-19 using thoracic computed tomography (CT).\nIn this retrospective study, a novel hybrid model (MTU-COVNet) was developed to extract visual features from volumetric thoracic CT scans for the detection of COVID-19. The collected dataset consisted of 3210 CT scans from 953 patients. Of the total 3210 scans in the final dataset, 1327 (41%) were obtained from the COVID-19 group, 929 (29%) from the CAP group, and 954 (30%) from the Normal CT group. Diagnostic performance was assessed with the area under the receiver operating characteristic (ROC) curve, sensitivity, and specificity.\nThe proposed approach with the optimized features from concatenated layers reached an overall accuracy of 97.7% for the CT-MTU dataset. The rest of the total performance metrics, such as; specificity, sensitivity, precision, F1 score, and Matthew Correlation Coefficient were 98.8%, 97.6%, 97.8%, 97.7%, and 96.5%, respectively. This model showed high diagnostic performance in detecting COVID-19 pneumonia (specificity: 98.0% and sensitivity: 98.2%) and CAP (specificity: 99.1% and sensitivity: 97.1%). The areas under the ROC curves for COVID-19 and CAP were 0.997 and 0.996, respectively.\nA deep learning-based AI system built on the CT imaging can detect COVID-19 pneumonia with high diagnostic efficiency and distinguish it from CAP and normal CT. AI applications can have beneficial effects in the fight against COVID-19.", "journal": "Clinical imaging", "date": "2021-10-01", "authors": ["G\u00fcrkanKavuran", "Erdal\u0130n", "Ay\u015feg\u00fcl Alt\u0131ntopGe\u00e7kil", "Mahmut\u015eahin", "Nurcan K\u0131r\u0131c\u0131Berber"], "doi": "10.1016/j.clinimag.2021.09.007\n10.1016/j.rmed.2020.106239\n10.1097/MCP.0000000000000671\n10.1001/jama.2019.21118\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200490\n10.1016/j.ejrad.2019.108774\n10.21037/jtd.2018.02.57\n10.1016/j.diii.2020.10.004\n10.3390/app8101715\n10.1148/radiol.2020200905\n10.1101/2020.03.20.20039834\n10.1101/2020.02.14.20023028\n10.1183/13993003.00775-2020\n10.1101/2020.03.12.20027185\n10.1061/(ASCE)GT.1943-5606.0001284\n10.1109/CVPR.2016.90\n10.3390/electronics8101130\n10.1023/A:1022627411411\n10.1007/978-1-4615-5703-6_3\n10.1023/A:1009715923555\n10.1080/03007995.2020.1830050\n10.1148/radiol.2020200230\n10.1155/2020/9756518\n10.1148/ryct.2020200034\n10.1007/s11547-021-01370-8\n10.1016/j.mtcomm.2021.102198"}
{"title": "Determining Top Fully Connected Layer's Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19.", "abstract": "Deep convolutional neural network (CNN)-assisted classification of images is one of the most discussed topics in recent years. Continuously innovation of neural network architectures is making it more correct and efficient every day. But training a neural network from scratch is very time-consuming and requires a lot of sophisticated computational equipment and power. So, using some pre-trained neural network as feature extractor for any image classification task or \"transfer learning\" is a very popular approach that saves time and computational power for practical use of CNNs. In this paper, an efficient way of building full model from any pre-trained model with high accuracy and low memory is proposed using knowledge distillation. Using the distilled knowledge of the last layer of pre-trained networks passes through fully connected layers with different hidden layers, followed by Softmax layer. The accuracies of student networks are mildly lesser than the whole models, but accuracy of student models clearly indicates the accuracy of the real network. In this way, the best number of hidden layers for dense layer for that pre-trained network with best accuracy and no-overfitting can be found with less time. Here, VGG16 and VGG19 (pre-trained upon \"ImageNet\" dataset) is tested upon chest X-rays (pneumonia and COVID-19). For finding the best total number of hidden layers, it saves nearly 44\u00a0min for VGG19 and 36\u00a0min and 37\u00a0s for VGG16 feature extractor.", "journal": "Journal of digital imaging", "date": "2021-10-01", "authors": ["RitwickGhosh"], "doi": "10.1007/s10278-021-00518-2\n10.1093/cid/cir1051\n10.1109/42.34715\n10.1109/42.845178\n10.1109/TMI.2003.815900\n10.1007/978-3-319-10590-1_53\n10.1145/3065386\n10.1109/TMI.2016.2553401\n10.1109/TMI.2016.2528129\n10.1109/TMI.2016.2536809\n10.1109/TMI.2016.2528162\n10.1038/nature14539\n10.1109/MSP.2017.2765695\n10.1016/j.cell.2018.02.010"}
{"title": "COVID-view: Diagnosis of COVID-19 using Chest CT.", "abstract": "Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.", "journal": "IEEE transactions on visualization and computer graphics", "date": "2021-09-30", "authors": ["ShreerajJadhav", "GaofengDeng", "MarleneZawin", "Arie EKaufman"], "doi": "10.1109/TVCG.2021.3114851\n10.1109/TVCG.2020.3020958"}
{"title": "AI-Based Quantitative CT Analysis of Temporal Changes According to Disease Severity in COVID-19 Pneumonia.", "abstract": "To quantitatively evaluate computed tomography (CT) parameters of coronavirus disease 2019 (COVID-19) pneumonia an artificial intelligence (AI)-based software in different clinical severity groups during the disease course.\nFrom March 11 to April 15, 2020, 51 patients (age, 18-84 years; 28 men) diagnosed and hospitalized with COVID-19 pneumonia with a total of 116 CT scans were enrolled in the study. Patients were divided into mild (n = 12), moderate (n = 31), and severe (n = 8) groups based on clinical severity. An AI-based quantitative CT analysis, including lung volume, opacity score, opacity volume, percentage of opacity, and mean lung density, was performed in initial and follow-up CTs obtained at different time points. Receiver operating characteristic analysis was performed to find the diagnostic ability of quantitative CT parameters for discriminating severe from nonsevere pneumonia.\nIn baseline assessment, the severe group had significantly higher opacity score, opacity volume, higher percentage of opacity, and higher mean lung density than the moderate group (all P \u2264 0.001). Through consecutive time points, the severe group had a significant decrease in lung volume (P = 0.006), a significant increase in total opacity score (P = 0.003), and percentage of opacity (P = 0.007). A significant increase in total opacity score was also observed for the mild group (P = 0.011). Residual opacities were observed in all groups. The involvement of more than 4 lobes (sensitivity, 100%; specificity, 65.26%), total opacity score greater than 4 (sensitivity, 100%; specificity, 64.21), total opacity volume greater than 337.4 mL (sensitivity, 80.95%; specificity, 84.21%), percentage of opacity greater than 11% (sensitivity, 80.95%; specificity, 88.42%), total high opacity volume greater than 10.5 mL (sensitivity, 95.24%; specificity, 66.32%), percentage of high opacity greater than 0.8% (sensitivity, 85.71%; specificity, 80.00%) and mean lung density HU greater than -705 HU (sensitivity, 57.14%; specificity, 90.53%) were related to severe pneumonia.\nAn AI-based quantitative CT analysis is an objective tool in demonstrating disease severity and can also assist the clinician in follow-up by providing information about the disease course and prognosis according to different clinical severity groups.", "journal": "Journal of computer assisted tomography", "date": "2021-09-29", "authors": ["SelinArdali Duzgun", "GamzeDurhan", "FigenBasaran Demirkazik", "IlimIrmak", "JaleKarakaya", "ErhanAkpinar", "MeltemGulsun Akpinar", "Ahmet CagkanInkaya", "SerpilOcal", "ArzuTopeli", "Orhan MacitAriyurek"], "doi": "10.1097/RCT.0000000000001224"}
{"title": "COVID Mortality Prediction with Machine Learning Methods: A Systematic Review and Critical Appraisal.", "abstract": "More than a year has passed since the report of the first case of coronavirus disease 2019 (COVID), and increasing deaths continue to occur. Minimizing the time required for resource allocation and clinical decision making, such as triage, choice of ventilation modes and admission to the intensive care unit is important. Machine learning techniques are acquiring an increasingly sought-after role in predicting the outcome of COVID patients. Particularly, the use of baseline machine learning techniques is rapidly developing in COVID mortality prediction, since a mortality prediction model could rapidly and effectively help clinical decision-making for COVID patients at imminent risk of death. Recent studies reviewed predictive models for SARS-CoV-2 diagnosis, severity, length of hospital stay, intensive care unit admission or mechanical ventilation modes outcomes; however, systematic reviews focused on prediction of COVID mortality outcome with machine learning methods are lacking in the literature. The present review looked into the studies that implemented machine learning, including deep learning, methods in COVID mortality prediction thus trying to present the existing published literature and to provide possible explanations of the best results that the studies obtained. The study also discussed challenging aspects of current studies, providing suggestions for future developments.", "journal": "Journal of personalized medicine", "date": "2021-09-29", "authors": ["FrancescaBottino", "EmanuelaTagliente", "LucaPasquini", "Alberto DiNapoli", "MartinaLucignani", "LorenzoFig\u00e0-Talamanca", "AntonioNapolitano"], "doi": "10.3390/jpm11090893\n10.1038/s41418-020-00720-9\n10.3934/mbe.2021039\n10.3390/jpm11040290\n10.1183/13993003.00775-2020\n10.21037/atm-20-3026\n10.1148/radiol.2020202723\n10.1136/bmj.m1328\n10.1016/j.jiph.2020.06.028\n10.1109/RBME.2020.2987975\n10.1109/ACCESS.2021.3058537\n10.1038/s42256-021-00307-0\n10.1016/j.patcog.2009.06.009\n10.1002/cem.3226\n10.3390/biom10101460\n10.1038/s41598-021-86327-7\n10.1136/bmjresp-2017-000240\n10.1111/j.2517-6161.1996.tb02080.x\n10.3389/fpubh.2020.587937\n10.1038/s41551-020-00633-5\n10.1371/journal.pone.0243262\n10.1007/s11548-020-02299-5\n10.2196/24018\n10.2196/20259\n10.1186/s12911-020-01316-6\n10.2196/25442\n10.1038/s41379-020-00700-x\n10.1007/s00521-020-05592-1\n10.1002/emp2.12205\n10.1371/journal.pone.0249285\n10.1038/s41467-020-18684-2\n10.1038/s41598-020-75767-2\n10.1080/07853890.2020.1868564\n10.2196/24207\n10.1093/ije/dyaa171\n10.1038/s42256-020-0180-7\n10.2196/23458\n10.1038/s41746-021-00456-x\n10.1016/j.mayocpiqo.2021.05.001\n10.3390/jpm11050343\n10.1136/bmjhci-2020-100235\n10.2214/AJR.20.22954\n10.1080/01621459.1994.10476866\n10.3390/jcm8060799\n10.1145/1577069.1577078\n10.1016/j.jocs.2016.05.005\n10.1017/dmp.2021.82\n10.1186/s12889-020-09721-2\n10.1007/s00330-020-07270-1\n10.1016/j.ijid.2020.05.021\n10.1007/s00330-020-07269-8\n10.5808/GI.2019.17.4.e41\n10.1155/2020/2836236\n10.1016/j.dss.2012.01.016\n10.1109/TSMCB.2008.2002909\n10.1155/2013/239628\n10.1109/IJCNN.2016.7727770\n10.1016/j.imu.2020.100449\n10.1016/j.asoc.2020.106885\n10.1109/TKDE.2019.2912815\n10.3389/fpubh.2017.00307\n10.1186/1471-2288-14-40\n10.1016/j.jclinepi.2014.09.007\n10.7326/0003-4819-130-6-199903160-00016\n10.1097/EDE.0b013e3181c30fb2\n10.1016/j.jclinepi.2019.09.016\n10.1016/j.jbi.2017.10.008\n10.1373/clinchem.2016.255539\n10.1371/journal.pone.0245384\n10.1016/S2589-7500(20)30217-X\n10.2196/23128\n10.1002/jmv.26699\n10.18632/aging.103770\n10.1186/s13098-020-00565-9\n10.1016/S2352-4642(21)00066-3"}
{"title": "Application of Artificial Intelligence in COVID-19 Diagnosis and Therapeutics.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic began at the end of December 2019, giving rise to a high rate of infections and causing COVID-19-associated deaths worldwide. It was first reported in Wuhan, China, and since then, not only global leaders, organizations, and pharmaceutical/biotech companies, but also researchers, have directed their efforts toward overcoming this threat. The use of artificial intelligence (AI) has recently surged internationally and has been applied to diverse aspects of many problems. The benefits of using AI are now widely accepted, and many studies have shown great success in medical research on tasks, such as the classification, detection, and prediction of disease, or even patient outcome. In fact, AI technology has been actively employed in various ways in COVID-19 research, and several clinical applications of AI-equipped medical devices for the diagnosis of COVID-19 have already been reported. Hence, in this review, we summarize the latest studies that focus on medical imaging analysis, drug discovery, and therapeutics such as vaccine development and public health decision-making using AI. This survey clarifies the advantages of using AI in the fight against COVID-19 and provides future directions for tackling the COVID-19 pandemic using AI techniques.", "journal": "Journal of personalized medicine", "date": "2021-09-29", "authors": ["KenAsada", "MasaakiKomatsu", "RyoShimoyama", "KenTakasawa", "NorioShinkai", "AkiraSakai", "AminaBolatkan", "MasayoshiYamada", "SatoshiTakahashi", "HidenoriMachino", "KazumaKobayashi", "SyuzoKaneko", "RyujiHamamoto"], "doi": "10.3390/jpm11090886\n10.1016/S0140-6736(20)30154-9\n10.1111/eci.13323\n10.1615/CritRevBiomedEng.2021036595\n10.1016/S0140-6736(21)00306-8\n10.1007/s13577-021-00512-4\n10.3390/v13071192\n10.3390/ijerph18157799\n10.1126/scitranslmed.abj6984\n10.3390/ijerph18157752\n10.3390/ijms22147425\n10.1016/j.lfs.2021.119580\n10.12968/hmed.2021.0112\n10.1136/bmj.m2980\n10.1001/jama.2020.22813\n10.3390/cancers12123532\n10.3389/fonc.2021.666937\n10.3390/biom10040524\n10.3390/biom10010062\n10.3390/biom10081123\n10.3390/biom10091249\n10.3390/biom10101460\n10.3390/cancers13061415\n10.3390/biom11040565\n10.1038/s41598-019-50567-5\n10.1016/j.bpg.2021.101745\n10.3390/biom10111526\n10.3390/biom11010090\n10.3390/cancers13143611\n10.1038/s41598-021-90555-2\n10.3390/biomedicines9070720\n10.3390/app11010371\n10.3390/biom10121691\n10.3390/app11031127\n10.1148/radiol.2020203173\n10.1016/j.media.2020.101794\n10.1109/TMI.2020.2993291\n10.1016/j.media.2021.102046\n10.1136/bmjinnov-2020-000593\n10.1038/s41598-021-90411-3\n10.1016/S2213-2600(20)30166-1\n10.1378/chest.09-0001\n10.1109/TMI.2020.2994459\n10.1016/j.compbiomed.2021.104296\n10.1016/j.media.2021.101975\n10.1001/jama.2020.15088\n10.1016/j.cell.2020.04.045\n10.1038/s41467-020-17971-2\n10.1038/s41467-020-18685-1\n10.1016/j.media.2020.101844\n10.1016/j.media.2020.101913\n10.1016/j.media.2021.101978\n10.1038/s41467-020-20657-4\n10.1016/j.media.2021.102054\n10.1007/s00330-021-08049-8\n10.1016/j.neucom.2021.06.012\n10.1016/j.clinimag.2021.06.036\n10.1056/NEJMoa2027906\n10.1038/s41586-021-03693-y\n10.1038/s41586-021-03696-9\n10.1038/s41591-021-01377-8\n10.1056/NEJMoa2035389\n10.1001/jama.2021.7563\n10.1056/NEJMc2103916\n10.1126/scitranslmed.abi9915\n10.1161/CIRCULATIONAHA.121.055913\n10.1038/s41591-021-01270-4\n10.1056/NEJMc2100362\n10.1038/s41586-021-03681-2\n10.1056/NEJMoa2105000\n10.1016/j.cell.2021.02.037\n10.1038/s41551-021-00699-9\n10.1038/s41587-019-0280-2\n10.1101/2020.02.19.955484\n10.1371/journal.pcbi.1008864\n10.1038/s42256-021-00335-w\n10.1016/j.heliyon.2020.e04639\n10.1371/journal.pone.0238907\n10.1038/s41467-020-18709-w\n10.1038/s41586-020-2286-9\n10.1038/s41467-021-23328-0\n10.1038/s41598-021-88153-3\n10.1016/j.bj.2020.05.001\n10.1038/s41598-020-75767-2\n10.1038/s41467-020-18684-2\n10.7717/peerj.10337\n10.1038/s42256-020-0180-7\n10.1038/s41598-020-78392-1\n10.1101/2020.10.29.339317\n10.1007/BF01024874\n10.1039/D1CC00050K\n10.1126/science.51.1306.23\n10.1038/s41746-021-00459-8\n10.1038/d41586-020-00822-x\n10.1542/peds.2020-0702\n10.1001/jama.2020.20717\n10.1098/rstb.2018.0365\n10.1186/s12992-021-00677-5\n10.1101/2020.04.09.20059055\n10.1101/2020.04.03.20052084\n10.1101/2020.04.17.20059535\n10.1101/2020.04.19.20068072\n10.1371/journal.pcbi.1008837\n10.1101/2020.04.24.20078477\n10.9781/ijimai.2020.02.002\n10.1371/journal.pone.0246120\n10.3390/jcm9030674\n10.1101/2020.05.10.20097527\n10.2196/19446\n10.1101/2020.05.05.20083436\n10.1101/2020.04.19.20069948\n10.1101/2020.04.06.20039909\n10.1371/journal.pone.0232391\n10.1101/2020.04.09.20059840\n10.1016/j.imu.2020.100378\n10.1016/j.medj.2020.10.002\n10.1016/j.ijid.2020.02.058\n10.1186/s40560-021-00557-5\n10.1126/science.aba9757\n10.1016/S1473-3099(20)30162-6\n10.1016/S0140-6736(20)30260-9\n10.1056/NEJMoa2006100\n10.1049/iet-spr.2016.0341\n10.1016/j.imu.2020.100313\n10.1038/s41598-020-75912-x\n10.1109/MNET.011.2000713\n10.3390/electronics10141626\n10.1613/jair.1.12632\n10.2196/26227\n10.3389/fpsyg.2021.596582\n10.1016/j.ijmedinf.2021.104442\n10.1109/JBHI.2020.3032060\n10.1109/JBHI.2019.2942429\n10.1088/1361-6560/ab9fca\n10.1055/s-0040-1702009"}
{"title": "A two-tier feature selection method using Coalition game and Nystrom sampling for screening COVID-19 from chest X-Ray images.", "abstract": "The world is still under the threat of different strains of the coronavirus and the pandemic situation is far from over. The method, that is widely used for the detection of COVID-19 is Reverse Transcription Polymerase chain reaction (RT-PCR), which is a time-consuming method and is prone to manual errors, and has poor precision. Although many nations across the globe have begun the mass immunization procedure, the COVID-19 vaccine will take a long time to reach everyone. The application of artificial intelligence (AI) and computer-aided diagnosis (CAD) has been used in the domain of medical imaging for a long period. It is quite evident that the use of CAD in the detection of COVID-19 is inevitable. The main objective of this paper is to use convolutional neural network (CNN) and a novel feature selection technique to analyze Chest X-Ray (CXR) images for the detection of COVID-19. We propose a novel two-tier feature selection method, which increases the accuracy of the overall classification model used for screening COVID-19 CXRs. Filter feature selection models are often more effective than wrapper methods as wrapper methods tend to be computationally more expensive and are not useful for large datasets dealing with a large number of features. However, most filter methods do not take into consideration how a group of features would work together, rather they just look at the features individually and decide on a score. We have used approximate Shapley value, a concept of Coalition game theory, to deal with this problem. Further, in the case of a large dataset, it is important to work with shorter embeddings of the features. We have used CUR decomposition and Nystrom sampling to further reduce the feature space. To check the efficacy of this two-tier feature selection method, we have applied it to the features extracted by three standard deep learning models, namely ", "journal": "Journal of ambient intelligence and humanized computing", "date": "2021-09-28", "authors": ["PratikBhowal", "SubhankarSen", "RamSarkar"], "doi": "10.1007/s12652-021-03491-4\n10.1016/j.matpr.2017.11.298\n10.1109/ACCESS.2020.3025164\n10.3390/diagnostics11020315\n10.1098/rsif.2017.0387\n10.1016/j.compbiomed\n10.1109/ACCESS.2020.3028012\n10.1007/s11042-019-07811-x\n10.1007/s00500-020-05183-1\n10.3390/diagnostics11050895\n10.1109/TMI.2020.2993291\n10.1038/s41551-018-0195-0\n10.1109/TMI.2020.2994459\n10.1002/mp.13264\n10.1146/annurev-bioeng-071516-044442\n10.1007/s10489-020-02149-6\n10.1142/S0218001421510046\n10.1128/CMR.00133-20\n10.1007/s10489-020-01888-w\n10.1016/j.mehy.2020.109761\n10.1038/s41598-019-56847-4\n10.1109/JBHI.2020.3023246\n10.1038/s41598-018-34455-y"}
{"title": "Detection and classification of lung diseases for pneumonia and Covid-19 using machine and deep learning techniques.", "abstract": "Since the arrival of the novel Covid-19, several types of researches have been initiated for its accurate prediction across the world. The earlier lung disease pneumonia is closely related to Covid-19, as several patients died due to high chest congestion (pneumonic condition). It is challenging to differentiate Covid-19 and pneumonia lung diseases for medical experts. The chest X-ray imaging is the most reliable method for lung disease prediction. In this paper, we propose a novel framework for the lung disease predictions like pneumonia and Covid-19 from the chest X-ray images of patients. The framework consists of dataset acquisition, image quality enhancement, adaptive and accurate region of interest (ROI) estimation, features extraction, and disease anticipation. In dataset acquisition, we have used two publically available chest X-ray image datasets. As the image quality degraded while taking X-ray, we have applied the image quality enhancement using median filtering followed by histogram equalization. For accurate ROI extraction of chest regions, we have designed a modified region growing technique that consists of dynamic region selection based on pixel intensity values and morphological operations. For accurate detection of diseases, robust set of features plays a vital role. We have extracted visual, shape, texture, and intensity features from each ROI image followed by normalization. For normalization, we formulated a robust technique to enhance the detection and classification results. Soft computing methods such as artificial neural network (ANN), support vector machine (SVM), K-nearest neighbour (KNN), ensemble classifier, and deep learning classifier are used for classification. For accurate detection of lung disease, deep learning architecture has been proposed using recurrent neural network (RNN) with long short-term memory (LSTM). Experimental results show the robustness and efficiency of the proposed model in comparison to the existing state-of-the-art methods.", "journal": "Journal of ambient intelligence and humanized computing", "date": "2021-09-28", "authors": ["ShimpyGoyal", "RajivSingh"], "doi": "10.1007/s12652-021-03464-7\n10.1007/s10489-020-01829-7\n10.1155/2018/4168538\n10.1007/s13246-020-00865-4\n10.1007/s11042-019-08394-3\n10.1109/TMI.2010.2095026\n10.1007/s42399-020-00383-0\n10.1007/s10489-020-01714-3\n10.1007/s00415-020-10067-3\n10.1007/s00500-020-05275-y\n10.1007/s00405-020-06319-7\n10.1007/s11042-019-08260-2\n10.1007/s12652-020-02669-6\n10.1007/s42979-020-00373-y\n10.3390/diagnostics10060417\n10.1007/s10489-020-02010-w\n10.1109/tmi.2013.2284099\n10.1007/s10489-020-01902-1\n10.1016/j.cell.2018.02.010\n10.1049/iet-ipr.2016.1014\n10.5373/JARDCS/V11I9/20193162\n10.1007/s12652-020-02502-0\n10.1016/j.procs.2017.12.016\n10.1007/s42399-020-00527-2\n10.1109/icsec.2016.7859887\n10.1007/s13755-020-00135-3\n10.1186/s12890-020-01286-5\n10.1007/s00405-020-06284-1\n10.1142/s0218001421510046\n10.1007/s10489-020-02149-6\n10.1007/s42399-020-00603-7\n10.1007/s10140-020-01869-z\n10.1007/s10489-020-01888-w\n10.1186/s43055-020-00296-x\n10.1007/s00330-020-07201-0\n10.1016/s0933-3657(01)00094-x"}
{"title": "A Fine-tuned deep convolutional neural network for chest radiography image classification on COVID-19 cases.", "abstract": "The outbreak of coronavirus disease 2019 (COVID-19) continues to have a catastrophic impact on the living standard of people worldwide. To fight against COVID-19, many countries are using a combination of containment and mitigation activities. Effective screening of contaminated patients is a critical step in the battle against COVID-19. During the early medical examination, it was observed that patient having abnormalities in chest radiography images shows the symptoms of COVID-19 infection. Motivated by this, in this article, we proposed a unique framework to diagnose the COVID-19 infection. Here, we removed the fully connected layers of an already proven model VGG-16 and placed a new simplified fully connected layer set that is initialized with some random weights on top of this deep convolutional neural network, which has already learned discriminative features, namely, edges, colors, geometric changes,shapes, and objects. To avoid the risk of destroying the rich features, we warm up our FC head by seizing all layers in the body of our network and then unfreeze all the layers in the network body to be fine-tuned.The suggested classification model achieved an accuracy of 97.12% with 99.2% sensitivity and 99.6% specificity for COVID-19 identification. This classification model is superior to the other classification model used to classify COVID-19 infected patients.", "journal": "Multimedia tools and applications", "date": "2021-09-28", "authors": ["Amiya KumarDash", "PuspanjaliMohapatra"], "doi": "10.1007/s11042-021-11388-9\n10.1007/s13246-020-00865-4\n10.1016/j.asoc.2019.105773\n10.1016/S0140-6736(20)30211-7\n10.1007/s11042-018-5714-1\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/j.compbiomed.2020.103792\n10.1007/s11042-020-08905-7\n10.1016/j.idm.2020.02.002\n10.1021/acsnano.0c02624\n10.1038/s41598-019-56847-4"}
{"title": "Weakly Supervised Segmentation of COVID19 Infection with Scribble Annotation on CT Images.", "abstract": "Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.", "journal": "Pattern recognition", "date": "2021-09-28", "authors": ["XiaomingLiu", "QuanYuan", "YaozongGao", "KeleiHe", "ShuoWang", "XiaoTang", "JinshanTang", "DinggangShen"], "doi": "10.1016/j.patcog.2021.108341"}
{"title": "Radiology Stereotypes, Application Barriers, and Hospital Integration: A Mixed-methods Study of Medical Student Perceptions of Radiology.", "abstract": "Limited exposure to radiology by medical students can perpetuate negative stereotypes and hamper recruitment efforts. The purpose of this study is to understand medical students' perceptions of radiology and how they change based on medical education and exposure.\nA single-institution mixed-methods study included four groups of medical students with different levels of radiology exposure. All participants completed a 16-item survey regarding demographics, opinions of radiology, and perception of radiology stereotypes. Ten focus groups were administered to probe perceptions of radiology. Focus groups were coded to identify specific themes in conjunction with the survey results.\nForty-nine participants were included. Forty-two percent of participants had positive opinions of radiology. Multiple radiology stereotypes were identified, and false stereotypes were diminished with increased radiology exposure. Opinions of the impact of artificial intelligence on radiology closely aligned with positive or negative views of the field overall. Multiple barriers to applying for a radiology residency position were identified including board scores and lack of mentorship. COVID-19 did not affect perceptions of radiology. There was broad agreement that students do not enter medical school with many preconceived notions of radiology, but that subsequent exposure was generally positive. Exposure both solidified and eliminated various stereotypes. Finally, there was general agreement that radiology is integral to the health system with broad exposure on all services.\nMedical student perceptions of radiology are notably influenced by exposure and radiology programs should take active steps to engage in medical student education.", "journal": "Academic radiology", "date": "2021-09-27", "authors": ["Lars JGrimm", "Laura JFish", "Caroline WtCarrico", "Jonathan GMartin", "Vesta CNwankwo", "SamanthaFarley", "Carolyn CMeltzer", "Charles MMaxfield"], "doi": "10.1016/j.acra.2021.08.020"}
{"title": "Feasibility of machine integrated point of care lung ultrasound automatic B-lines tool in the Corona-virus 2019 critical care unit.", "abstract": null, "journal": "Critical care (London, England)", "date": "2021-09-26", "authors": ["GalTsaban", "OriGalante", "YanivAlmog", "YuvalUllman", "LiorFuchs"], "doi": "10.1186/s13054-021-03770-8\n10.1186/cc5668\n10.1016/S2213-2600(20)30120-X\n10.1016/j.ajem.2015.06.002\n10.21037/jtd.2016.04.55\n10.1007/s10877-019-00440-7"}
{"title": "Evaluating Deep Neural Network Architectures with Transfer Learning for Pneumonitis Diagnosis.", "abstract": "Pneumonitis is an infectious disease that causes the inflammation of the air sac. It can be life-threatening to the very young and elderly. Detection of pneumonitis from X-ray images is a significant challenge. Early detection and assistance with diagnosis can be crucial. Recent developments in the field of deep learning have significantly improved their performance in medical image analysis. The superior predictive performance of the deep learning methods makes them ideal for pneumonitis classification from chest X-ray images. However, training deep learning models can be cumbersome and resource-intensive. Reusing knowledge representations of public models trained on large-scale datasets through transfer learning can help alleviate these challenges. In this paper, we compare various image classification models based on transfer learning with well-known deep learning architectures. The Kaggle chest X-ray dataset was used to evaluate and compare our models. We apply basic data augmentation and fine-tune our feed-forward classification head on the models pretrained on the ImageNet dataset. We observed that the DenseNet201 model outperforms other models with an AUROC score of 0.966 and a recall score of 0.99. We also visualize the class activation maps from the DenseNet201 model to interpret the patterns recognized by the model for prediction.", "journal": "Computational and mathematical methods in medicine", "date": "2021-09-24", "authors": ["SuryaKrishnamurthy", "KathiravanSrinivasan", "Saeed MianQaisar", "P M Durai RajVincent", "Chuan-YuChang"], "doi": "10.1155/2021/8036304\n10.1109/ICC.2007.637\n10.1007/s40747-021-00324-x\n10.1007/s11554-020-00987-8\n10.1016/j.imed.2021.05.005\n10.1007/s12065-020-00540-3\n10.1016/j.media.2017.07.005\n10.1007/s11684-019-0726-4\n10.1016/S2589-7500(19)30123-2\n10.1038/s41746-020-00376-2\n10.1007/978-3-030-35445-9_20\n10.1016/j.compeleceng.2019.08.004\n10.1155/2020/8876798\n10.3390/diagnostics10090649\n10.1016/j.measurement.2020.108046\n10.3390/diagnostics11030530\n10.1016/j.asoc.2020.106859\n10.1016/j.asoc.2020.106744\n10.1007/s12652-021-03075-2\n10.1155/2019/4180949\n10.4108/eai.28-5-2020.166290\n10.1038/s41467-020-17971-2\n10.1016/j.chaos.2020.110495\n10.3390/s21144749\n10.3390/s21082852\n10.17632/rscbjbr9sj.2\n10.32604/cmc.2021.016736\n10.1109/TCSVT.2017.2718622\n10.1109/ICCE-China.2017.7990985\n10.1155/2021/5541134"}
{"title": "Non-melanoma skin cancer diagnosis: a comparison between dermoscopic and smartphone images by unified visual and sonification deep learning algorithms.", "abstract": "Non-melanoma skin cancer (NMSC) is the most frequent keratinocyte-origin skin tumor. It is confirmed that dermoscopy of NMSC confers a diagnostic advantage as compared to visual face-to-face assessment. COVID-19 restrictions diagnostics by telemedicine photos, which are analogous to visual inspection, displaced part of in-person visits. This study evaluated by a dual convolutional neural network (CNN) performance metrics in dermoscopic (DI) versus smartphone-captured images (SI) and tested if artificial intelligence narrows the proclaimed gap in diagnostic accuracy.\nA CNN that receives a raw image and predicts malignancy, overlaid by a second independent CNN which processes a sonification (image-to-sound mapping) of the original image, were combined into a unified malignancy classifier. All images were histopathology-verified in a comparison between NMSC and benign skin lesions excised as suspected NMSCs. Study criteria outcomes were sensitivity and specificity for the unified output.\nImages acquired by DI (n\u2009=\u2009132 NMSC, n\u2009=\u200933 benign) were compared to SI (n\u2009=\u2009170 NMSC, n\u2009=\u200928 benign). DI and SI analysis metrics resulted in an area under the curve (AUC) of the receiver operator characteristic curve of 0.911 and 0.821, respectively. Accuracy was increased by DI (0.88; CI 81.9-92.4) as compared to SI (0.75; CI 68.1-80.6, p\u2009<\u20090.005). Sensitivity of DI was higher than SI (95.3%, CI 90.4-98.3 vs 75.3%, CI 68.1-81.6, p\u2009<\u20090.001), but not specificity (p\u2009=\u2009NS).\nTelemedicine use of smartphone images might result in a substantial decrease in diagnostic performance as compared to dermoscopy, which needs to be considered by both healthcare providers and patients.", "journal": "Journal of cancer research and clinical oncology", "date": "2021-09-22", "authors": ["ADascalu", "B NWalker", "YOron", "E ODavid"], "doi": "10.1007/s00432-021-03809-x\n10.1002/cncr.32969\n10.1177/1357633X19874200\n10.1016/J.ESWA.2012.07.021\n10.1016/j.ebiom.2019.04.055\n10.1002/14651858.CD011901.pub2\n10.1111/jdv.12434\n10.1684/ejd.2012.1727\n10.1016/J.JAAD.2016.10.041\n10.1136/bmj.m127\n10.1200/CCI.17.00159\n10.1146/annurev-psych-120709-145346\n10.3389/fmed.2020.598903\n10.1007/S00432-018-02834-7\n10.1200/JCO.19.02031\n10.1055/S-0039-1677897\n10.1016/j.jaad.2003.07.029\n10.1001/jamadermatol.2015.0173\n10.1056/NEJMRA1708701\n10.1016/j.dib.2020.106221\n10.1111/jdv.14782\n10.1016/j.jaad.2019.08.012\n10.1001/jamadermatol.2015.1187\n10.1001/jamadermatol.2013.2139\n10.1200/JCO.19.03350\n10.1038/s41591-018-0300-7\n10.1038/sdata.2018.161\n10.1001/jamadermatol.2018.4378\n10.1001/archdermatol.2012.893\n10.1016/j.ebiom.2019.01.028\n10.1111/bjd.16730"}
{"title": "Reliable and Interpretable Mortality Prediction With Strong Foresight in COVID-19 Patients: An International Study From China and Germany.", "abstract": "Cohort-independent robust mortality prediction model in patients with COVID-19 infection is not yet established. To build up a reliable, interpretable mortality prediction model with strong foresight, we have performed an international, bi-institutional study from China (Wuhan cohort, collected from January to March) and Germany (W\u00fcrzburg cohort, collected from March to September). A Random Forest-based machine learning approach was applied to 1,352 patients from the Wuhan cohort, generating a mortality prediction model based on their clinical features. The results showed that five clinical features at admission, including lymphocyte (%), neutrophil count, C-reactive protein, lactate dehydrogenase, and \u03b1-hydroxybutyrate dehydrogenase, could be used for mortality prediction of COVID-19 patients with more than 91% accuracy and 99% AUC. Additionally, the time-series analysis revealed that the predictive model based on these clinical features is very robust over time when patients are in the hospital, indicating the strong association of these five clinical features with the progression of treatment as well. Moreover, for different preexisting diseases, this model also demonstrated high predictive power. Finally, the mortality prediction model has been applied to the independent W\u00fcrzburg cohort, resulting in high prediction accuracy (with above 90% accuracy and 85% AUC) as well, indicating the robustness of the model in different cohorts. In summary, this study has established the mortality prediction model that allowed early classification of COVID-19 patients, not only at admission but also along the treatment timeline, not only cohort-independent but also highly interpretable. This model represents a valuable tool for triaging and optimizing the resources in COVID-19 patients.", "journal": "Frontiers in artificial intelligence", "date": "2021-09-21", "authors": ["TaoBai", "XueZhu", "XiangZhou", "DeniseGrathwohl", "PengshuoYang", "YuguoZha", "YuJin", "HuiChong", "QingyangYu", "NoraIsberner", "DongkeWang", "LeiZhang", "K MartinKort\u00fcm", "JunSong", "LeoRasche", "HermannEinsele", "KangNing", "XiaohuaHou"], "doi": "10.3389/frai.2021.672050\n10.1177/0885066620951426\n10.36660/ijcs.20200150\n10.1111/tbed.13651\n10.3760/cma.j.issn.0254-6450.2020.02.003\n10.1111/ijlh.13229\n10.1109/jbhi.2021.3052134\n10.1001/jamainternmed.2020.3596\n10.1038/s41430-020-0652-1\n10.2196/23458\n10.1038/s41440-020-0497-y\n10.1007/s00408-013-9530-0\n10.1016/j.cub.2019.02.034\n10.1016/j.jaci.2020.04.006\n10.1038/s41467-020-17280-8\n10.1109/18.61115\n10.1515/cclm-2020-0240\n10.1016/j.jjcc.2020.07.013\n10.1016/s0140-6736(20)30923-5\n10.1093/aje/kwaa093\n10.2174/157436212800376681\n10.1016/s0140-6736(21)00632-2\n10.1038/209912a0\n10.3390/ijerph17092986\n10.1038/s41592-019-0431-x\n10.1016/s0140-6736(21)01095-3\n10.1038/160845a0\n10.1097/00054725-200409000-00026\n10.1016/j.chest.2020.09.070\n10.1007/s00259-020-05075-4\n10.1016/s0140-6736(21)00895-3\n10.1038/s41586-020-2521-4\n10.1016/j.xinn.2020.100022\n10.1016/j.ebiom.2020.102880\n10.1038/s41590-020-0736-z\n10.1136/bmj.m606\n10.1038/s42256-020-0180-7\n10.1007/s00277-020-04184-2\n10.1016/s0140-6736(20)30566-3\n10.3390/cancers12030603"}
{"title": "Experimental Technologies in the Diagnosis and Treatment of COVID-19 in Patients with Comorbidities.", "abstract": "The COVID-19 pandemic has impacted the whole world and raised concerns about its effects on different human organ systems. Early detection of COVID-19 may significantly increase the rate of survival; thus, it is critical that the disease is detected early. Emerging technologies have been used to prevent, diagnose, and manage COVID-19 among the populace in the USA and globally. Numerous studies have revealed the growing implementation of novel engineered systems during the intervention at various points of the disease's pathogenesis, especially as it relates to comorbidities and complications related to cardiovascular and respiratory organ systems. In this review, we provide a succinct, but extensive, review of the pathogenesis of COVID-19, particularly as it relates to angiotensin-converting enzyme 2 (ACE2) as a viral entry point. This is followed by a comprehensive analysis of cardiovascular and respiratory comorbidities of COVID-19 and novel technologies that are used to diagnose and manage hospitalized patients. Continuous cardiorespiratory monitoring systems, novel machine learning algorithms for rapidly triaging patients, various imaging modalities, wearable immunosensors, hotspot tracking systems, and other emerging technologies are reviewed. COVID-19 effects on the immune system, associated inflammatory biomarkers, and innovative therapies are also assessed. Finally, with emphasis on the impact of wearable and non-wearable systems, this review highlights future technologies that could help diagnose, monitor, and mitigate disease progression. Technologies that account for an individual's health conditions, comorbidities, and even socioeconomic factors can drastically reduce the high mortality seen among many COVID-19 patients, primarily via disease prevention, early detection, and pertinent management.", "journal": "Journal of healthcare informatics research", "date": "2021-09-21", "authors": ["Md ShahnoorAmin", "MarcinWozniak", "LidijaBarbaric", "ShanelPickard", "Rahul SYerrabelli", "AntonChristensen", "Olivia CCoiado"], "doi": "10.1007/s41666-021-00106-7\n10.3345/cep.2020.00493\n10.1001/jamacardio.2020.1286\n10.1186/s40249-020-00646-x\n10.1126/science.1116480\n10.5144/0256-4947.2013.427\n10.1186/s40779-020-00240-0\n10.1126/science.abb2762\n10.1128/JVI.02205-08\n10.1016/j.cell.2020.02.052\n10.1128/JVI.01542-10\n10.1038/s41591-020-0868-6\n10.1038/s41586-020-2012-7\n10.1056/NEJMoa2002032\n10.1080/22221751.2020.1719902\n10.1126/science.abb2507\n10.1210/en.2004-0443\n10.1016/j.ejim.2020.04.037\n10.1161/CIRCULATIONAHA.120.047022\n10.1038/s41440-020-0455-8\n10.2174/1871530320666200427112902\n10.1002/ddr.21656\n10.1016/S2213-2600(20)30116-8\n10.1001/jama.2020.6775\n10.1016/j.dsx.2020.03.016\n10.1001/jamacardio.2020.1017\n10.1161/CIRCULATIONAHA.120.046941\n10.1007/s00395-020-0791-5\n10.1016/S0140-6736(20)30566-3\n10.1093/cvr/cvaa106\n10.1038/s41569-020-0360-5\n10.1161/CIRCULATIONAHA.120.047164\n10.1001/jamacardio.2020.0950\n10.1016/j.cjca.2020.03.028\n10.1001/jama.2020.1585\n10.1111/jth.14768\n10.1016/S0140-6736(20)31129-6\n10.1016/S0140-6736(20)31103-X\n10.1111/irv.12470\n10.1161/CIRCRESAHA.120.317055\n10.1109/TBME.2019.2912407\n10.3390/vibration2010005\n10.1161/CIRCHEARTFAILURE.117.004313\n10.1186/s12873-019-0243-4\n10.1001/jama.2020.8570\n10.5811/westjem.2020.4.47372\n10.1038/s41586-020-2008-3\n10.1056/NEJMoa2001017\n10.1183/09031936.06.00131905\n10.1016/j.rmed.2020.105941\n10.1097/CM9.0000000000000775\n10.1111/all.14238\n10.18332/tid/119324\n10.1001/jama.2020.3786\n10.1016/S0140-6736(20)30183-5\n10.1038/s41598-020-76550-z\n10.1186/s13054-018-2105-y\n10.1016/j.jcmg.2018.06.023\n10.4103/sja.SJA_73_18\n10.1164/rccm.201802-0236CI\n10.1093/eurheartj/ehw164\n10.1021/acs.jproteome.6b01038\n10.1371/journal.pone.0187545\n10.1038/s41577-020-0311-8\n10.4414/SMW.2002.10054\n10.1007/s00277-020-04019-0\n10.1001/jamainternmed.2020.0994\n10.1016/j.cca.2020.04.020\n10.1001/jama.2020.2648\n10.1128/MMBR.05015-11\n10.32604/cmc.2020.010691\n10.1136/bmj.m1328\n10.1371/journal.pone.0198144\n10.1007/978-1-4939-6625-7_8\n10.1038/s41368-020-0074-x\n10.1186/s12911-020-01266-z\n10.1093/clinchem/hvaa200\n10.2196/24048\n10.1007/s10916-020-01597-4\n10.1038/s41598-021-82885-y\n10.1016/j.imu.2020.100378\n10.1080/15459624.2017.1411598\n10.1136/bmj.317.7161.798\n10.2105/AJPH.2009.166082\n10.1177/002214650905000301\n10.1353/dem.0.0017\n10.1016/j.socscimed.2010.11.002\n10.15585/mmwr.mm6915e3\n10.1001/jama.2020.6548\n10.15585/mmwr.mm7003e1\n10.1007/s00431-020-03684-7\n10.1001/jamapediatrics.2020.3651\n10.1001/jama.2021.2091"}
{"title": "Automatic deep learning system for COVID-19 infection quantification in chest CT.", "abstract": "The paper proposes an automatic deep learning system for COVID-19 infection areas segmentation in chest CT scans. CT imaging proved its ability to detect the COVID-19 disease even for asymptotic patients, which make it a trustworthy alternative for PCR. Coronavirus disease spread globally and PCR screening is the adopted diagnostic testing method for COVID-19 detection. However, PCR is criticized due its low sensitivity ratios, also, it is time-consuming and manual complicated process. The proposed framework includes different steps; it starts to prepare the region of interest by segmenting the lung organ, which then undergoes edge enhancing diffusion filtering (EED) to improve the infection areas contrast and intensity homogeneity. The proposed FCN is implemented using U-net architecture with modified residual block to include concatenation skip connection. The block improves the learning of gradient values by forwarding the infection area features through the network. The proposed system is evaluated using different measures and achieved dice overlapping score of 0.961 and 0.780 for lung and infection areas segmentation, respectively. The proposed system is trained and tested using many 2D CT slices extracted from diverse datasets from different sources, which demonstrate the system generalization and effectiveness. The use of more datasets from different sources helps to enhance the system accuracy and generalization, which can be accomplished based on the data availability in in the future.", "journal": "Multimedia tools and applications", "date": "2021-09-21", "authors": ["Omar IbrahimAlirr"], "doi": "10.1007/s11042-021-11299-9\n10.1109/TMI.2020.2996645\n10.1148/radiol.2020200432\n10.1007/s10278-019-00227-x\n10.4236/jcc.2015.311023\n10.1148/radiol.2020200905\n10.1109/TMI.2009.2022368\n10.1016/j.jinf.2020.04.004\n10.1016/j.chest.2020.04.003\n10.1016/j.ijsu.2020.02.034"}
{"title": "Software system to predict the infection in COVID-19 patients using deep learning and web of things.", "abstract": "Since the end of 2019, computed tomography (CT) images have been used as an important substitute for the time-consuming Reverse Transcriptase polymerase chain reaction (RT-PCR) test; a new coronavirus 2019 (COVID-19) disease has been detected and has quickly spread through many countries across the world. Medical imaging such as computed tomography provides great potential due to growing skepticism toward the sensitivity of RT-PCR as a screening tool. For this purpose, automated image segmentation is highly desired for a clinical decision aid and disease monitoring. However, there is limited publicly accessible COVID-19 image knowledge, leading to the overfitting of conventional approaches. To address this issue, the present paper focuses on data augmentation techniques to create synthetic data. Further, a framework has been proposed using WoT and traditional U-Net with EfficientNet B0 to segment the COVID Radiopedia and Medseg datasets automatically. The framework achieves an ", "journal": "Software: practice & experience", "date": "2021-09-21", "authors": ["AshimaSingh", "AmritaKaur", "ArwinderDhillon", "SahilAhuja", "HarpreetVohra"], "doi": "10.1002/spe.3011\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1016/j.radi.2005.02.003\n10.1016/j.media.2017.07.005\n10.1109/rbme.2020.2987975\n10.1016/j.chest.2020.04.003\n10.14358/PERS.80.2.000\n10.1080/17517575.2020.1820583\n10.1148/radiol.2020201343\n10.1101/2020.03.19.20039354\n10.1148/radiol.2020201237\n10.1148/radiol.2020200905\n10.1007/s11831-021-09547-0\n10.1101/2020.03.19.20038315\n10.1101/2020.04.22\n10.1148/radiol.2020200642\n10.5281/zenodo.3757476\n10.1038/nature14539\n10.1007/978-3-319-46976-8_19\n10.1016/j.media.2019.01.012\n10.1109/tmi.2018.2845918"}
{"title": "DR-MIL: deep represented multiple instance learning distinguishes COVID-19 from community-acquired pneumonia in CT images.", "abstract": "Given that the novel coronavirus disease 2019 (COVID-19) has become a pandemic, a method to accurately distinguish COVID-19 from community-acquired pneumonia (CAP) is urgently needed. However, the spatial uncertainty and morphological diversity of COVID-19 lesions in the lungs, and subtle differences with respect to CAP, make differential diagnosis non-trivial.\nWe propose a deep represented multiple instance learning (DR-MIL) method to fulfill this task. A 3D volumetric CT scan of one patient is treated as one bag and ten CT slices are selected as the initial instances. For each instance, deep features are extracted from the pre-trained ResNet-50 with fine-tuning and represented as one deep represented instance score (DRIS). Each bag with a DRIS for each initial instance is then input into a citation k-nearest neighbor search to generate the final prediction. A total of 141 COVID-19 and 100 CAP CT scans were used. The performance of DR-MIL is compared with other potential strategies and state-of-the-art models.\nDR-MIL displayed an accuracy of 95% and an area under curve of 0.943, which were superior to those observed for comparable methods. COVID-19 and CAP exhibited significant differences in both the DRIS and the spatial pattern of lesions (p<0.001). As a means of content-based image retrieval, DR-MIL can identify images used as key instances, references, and citers for visual interpretation.\nDR-MIL can effectively represent the deep characteristics of COVID-19 lesions in CT images and accurately distinguish COVID-19 from CAP in a weakly supervised manner. The resulting DRIS is a useful supplement to visual interpretation of the spatial pattern of lesions when screening for COVID-19.", "journal": "Computer methods and programs in biomedicine", "date": "2021-09-19", "authors": ["ShouliangQi", "CaiwenXu", "ChenLi", "BinTian", "ShuyueXia", "JigangRen", "LimingYang", "HanlinWang", "HuiYu"], "doi": "10.1016/j.cmpb.2021.106406\n10.1148/radiol.2020201491\n10.1109/RBME.2020.2990959\n10.1109/RBME.2020.2987975\n10.1109/CVPR.2016.90"}
{"title": "COVID-19 early detection for imbalanced or low number of data using a regularized cost-sensitive CapsNet.", "abstract": "With the presence of novel coronavirus disease at the end of 2019, several approaches were proposed to help physicians detect the disease, such as using deep learning to recognize lung involvement based on the pattern of pneumonia. These approaches rely on analyzing the CT images and exploring the COVID-19 pathologies in the lung. Most of the successful methods are based on the deep learning technique, which is state-of-the-art. Nevertheless, the big drawback of the deep approaches is their need for many samples, which is not always possible. This work proposes a combined deep architecture that benefits both employed architectures of DenseNet and CapsNet. To more generalize the deep model, we propose a regularization term with much fewer parameters. The network convergence significantly improved, especially when the number of training data is small. We also propose a novel Cost-sensitive loss function for imbalanced data that makes our model feasible for the condition with a limited number of positive data. Our novelties make our approach more intelligent and potent in real-world situations with imbalanced data, popular in hospitals. We analyzed our approach on two publicly available datasets, HUST and COVID-CT, with different protocols. In the first protocol of HUST, we followed the original paper setup and outperformed it. With the second protocol of HUST, we show our approach superiority concerning imbalanced data. Finally, with three different validations of the COVID-CT, we provide evaluations in the presence of a low number of data along with a comparison with state-of-the-art.", "journal": "Scientific reports", "date": "2021-09-18", "authors": ["MaliheJavidi", "SaeidAbbaasi", "SaraNaybandi Atashi", "MahdiJampour"], "doi": "10.1038/s41598-021-97901-4\n10.1038/s41598-020-76282-0\n10.1038/s41598-020-76740-9\n10.1038/s41467-020-17971-2\n10.1038/s41598-020-74539-2\n10.1038/s41598-021-84219-4\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-70479-z\n10.1109/CVPR.2017.243\n10.1109/TPAMI.2019.2913372\n10.3390/sym12040651\n10.1007/s00521-020-05437-x\n10.1016/j.media.2020.101910\n10.1038/s41598-020-71294-2\n10.1016/j.patrec.2020.10.001\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200343\n10.1155/2020/9756518\n10.1148/radiol.2020201491\n10.2196/19569\n10.1016/j.imu.2020.100427\n10.1038/s41598-021-93658-y\n10.1038/s41598-020-74164-z\n10.1007/s00500-020-05424-3\n10.32604/cmc.2021.012955\n10.3390/s18093153\n10.1016/j.patcog.2021.107851\n10.1038/s41551-020-00633-5\n10.1109/JBHI.2020.3023246\n10.1016/j.media.2021.101978\n10.1038/s41467-020-20657-4\n10.1038/s41551-021-00704-1"}
{"title": "CARes-UNet: Content-aware residual UNet for lesion segmentation of COVID-19 from chest CT images.", "abstract": "Coronavirus disease 2019 (COVID-19) has caused a serious global health crisis. It has been proven that the deep learning method has great potential to assist doctors in diagnosing COVID-19 by automatically segmenting the lesions in computed tomography (CT) slices. However, there are still several challenges restricting the application of these methods, including high variation in lesion characteristics and low contrast between lesion areas and healthy tissues. Moreover, the lack of high-quality labeled samples and large number of patients lead to the urgency to develop a high accuracy model, which performs well not only under supervision but also with semi-supervised methods.\nWe propose a content-aware lung infection segmentation deep residual network (content-aware residual UNet (CARes-UNet)) to segment the lesion areas of COVID-19 from the chest CT slices. In our CARes-UNet, the residual connection was used in the convolutional block, which alleviated the degradation problem during the training. Then, the content-aware upsampling modules were introduced to improve the performance of the model while reducing the computation cost. Moreover, to achieve faster convergence, an advanced optimizer named Ranger was utilized to update the model's parameters during training. Finally, we employed a semi-supervised segmentation framework to deal with the problem of lacking pixel-level labeled data.\nWe evaluated our approach using three public datasets with multiple metrics and compared its performance to several models. Our method outperforms other models in multiple indicators, for instance in terms of Dice coefficient on COVID-SemiSeg Dataset, CARes-UNet got the score 0.731, and semi-CARes-UNet further boosted it to 0.776. More ablation studies were done and validated the effectiveness of each key component of our proposed model.\nCompared with the existing neural network methods applied to the COVID-19 lesion segmentation tasks, our CARes-UNet can gain more accurate segmentation results, and semi-CARes-UNet can further improve it using semi-supervised learning methods while presenting a possible way to solve the problem of lack of high-quality annotated samples. Our CARes-UNet and semi-CARes-UNet can be used in artificial intelligence-empowered computer-aided diagnosis system to improve diagnostic accuracy in this ongoing COVID-19 pandemic.", "journal": "Medical physics", "date": "2021-09-17", "authors": ["XinhuaXu", "YuhangWen", "LuZhao", "YiZhang", "YoujunZhao", "ZixuanTang", "ZiduoYang", "Calvin Yu-ChianChen"], "doi": "10.1002/mp.15231\n10.1109/CVPR42600.2020.01070"}
{"title": "Federated learning for predicting clinical outcomes in patients with COVID-19.", "abstract": "Federated learning (FL) is a method used for training artificial intelligence models with data from multiple sources while maintaining data anonymity, thus removing many barriers to data sharing. Here we used data from 20\u2009institutes across the globe to train a FL model, called EXAM (electronic medical record (EMR) chest X-ray AI model), that predicts the future oxygen requirements of symptomatic patients with COVID-19 using inputs of vital signs, laboratory data and chest X-rays. EXAM achieved an average area under the curve (AUC) >0.92 for predicting outcomes at 24 and 72\u2009h from the time of initial presentation to the emergency room, and it provided 16% improvement in average AUC measured across all participating sites and an average increase in generalizability of 38% when compared with models trained at a single site using that site's data. For prediction of mechanical ventilation treatment or death at 24\u2009h at the largest independent test site, EXAM achieved a sensitivity of 0.950 and specificity of 0.882. In this study, FL facilitated rapid data science collaboration without data exchange and generated a model that generalized across heterogeneous, unharmonized datasets for prediction of clinical outcomes in patients with COVID-19, setting the stage for the broader use of FL in healthcare.", "journal": "Nature medicine", "date": "2021-09-17", "authors": ["IttaiDayan", "Holger RRoth", "AoxiaoZhong", "AhmedHarouni", "AmilcareGentili", "Anas ZAbidin", "AndrewLiu", "Anthony BeardsworthCosta", "Bradford JWood", "Chien-SungTsai", "Chih-HungWang", "Chun-NanHsu", "C KLee", "PeiyingRuan", "DaguangXu", "DufanWu", "EddieHuang", "Felipe CamposKitamura", "GriffinLacey", "Gustavo C\u00e9sarde Ant\u00f4nio Corradi", "GustavoNino", "Hao-HsinShin", "HirofumiObinata", "HuiRen", "Jason CCrane", "JesseTetreault", "JiahuiGuan", "John WGarrett", "Joshua DKaggie", "Jung GilPark", "KeithDreyer", "KrishnaJuluru", "KristopherKersten", "Marcio Aloisio Bezerra CavalcantiRockenbach", "Marius GeorgeLinguraru", "Masoom AHaider", "MeenaAbdelMaseeh", "NicolaRieke", "Pablo FDamasceno", "Pedro Mario CruzE Silva", "PochuanWang", "ShengXu", "ShuichiKawano", "SiraSriswasdi", "Soo YoungPark", "Thomas MGrist", "VarunBuch", "WatsamonJantarabenjakul", "WeichungWang", "Won YoungTak", "XiangLi", "XihongLin", "Young JoonKwon", "AboodQuraini", "AndrewFeng", "Andrew NPriest", "BarisTurkbey", "BenjaminGlicksberg", "BernardoBizzo", "Byung SeokKim", "CarlosTor-D\u00edez", "Chia-ChengLee", "Chia-JungHsu", "ChinLin", "Chiu-LingLai", "Christopher PHess", "ColinCompas", "DeepekshaBhatia", "Eric KOermann", "EvanLeibovitz", "HisashiSasaki", "HitoshiMori", "IsaacYang", "Jae HoSohn", "Krishna Nand KeshavaMurthy", "Li-ChenFu", "Matheus Ribeiro Furtadode Mendon\u00e7a", "MikeFralick", "Min KyuKang", "MohammadAdil", "NatalieGangai", "PeeraponVateekul", "PierreElnajjar", "SarahHickman", "SharmilaMajumdar", "Shelley LMcLeod", "SheridanReed", "StefanGr\u00e4f", "StephanieHarmon", "TatsuyaKodama", "ThanyaweePuthanakit", "TonyMazzulli", "Vitor Limade Lavor", "YothinRakvongthai", "Yu RimLee", "YuhongWen", "Fiona JGilbert", "Mona GFlores", "QuanzhengLi"], "doi": "10.1038/s41591-021-01506-3\n10.1016/j.media.2021.101992\n10.1186/s13244-019-0738-2\n10.1017/ice.2020.461\n10.1002/emp2.12071\n10.1007/978-3-030-60548-3_17\n10.1101/2020.05.10.20096073\n10.1093/jamia/ocaa172\n10.1145/2810103.2813677\n10.1007/978-3-030-32692-0_16\n10.1109/allerton.2015.7447103\n10.1007/978-3-030-32245-8_1\n10.1109/cvpr.2016.90\n10.1145/3124749.3124754\n10.1007/978-1-4842-6699-1_1"}
{"title": "Evaluating risk stratification scoring systems to predict mortality in patients with COVID-19.", "abstract": "The COVID-19 pandemic has necessitated efficient and accurate triaging of patients for more effective allocation of resources and treatment.\nThe objectives are to investigate parameters and risk stratification tools that can be applied to predict mortality within 90 days of hospital admission in patients with COVID-19.\nA literature search of original studies assessing systems and parameters predicting mortality of patients with COVID-19 was conducted using MEDLINE and EMBASE.\n589 titles were screened, and 76 studies were found investigating the prognostic ability of 16 existing scoring systems (area under the receiving operator curve (AUROC) range: 0.550-0.966), 38 newly developed COVID-19-specific prognostic systems (AUROC range: 0.6400-0.9940), 15 artificial intelligence (AI) models (AUROC range: 0.840-0.955) and 16 studies on novel blood parameters and imaging.\nCurrent scoring systems generally underestimate mortality, with the highest AUROC values found for APACHE II and the lowest for SMART-COP. Systems featuring heavier weighting on respiratory parameters were more predictive than those assessing other systems. Cardiac biomarkers and CT chest scans were the most commonly studied novel parameters and were independently associated with mortality, suggesting potential for implementation into model development. All types of AI modelling systems showed high abilities to predict mortality, although none had notably higher AUROC values than COVID-19-specific prediction models. All models were found to have bias, including lack of prospective studies, small sample sizes, single-centre data collection and lack of external validation.\nThe single parameters established within this review would be useful to look at in future prognostic models in terms of the predictive capacity their combined effect may harness.", "journal": "BMJ health & care informatics", "date": "2021-09-16", "authors": ["KellyChu", "BatoolAlharahsheh", "NaveenGarg", "PayalGuha"], "doi": "10.1136/bmjhci-2021-100389\n10.1186/s12879-020-04994-9\n10.1007/s11036-017-0932-8\n10.7717/peerj.10018\n10.1016/j.eng.2020.10.013\n10.2196/24018\n10.1561/2000000039\n10.1016/j.annemergmed.2020.09.236\n10.1016/j.amjcard.2020.08.040\n10.1016/S0140-6736(20)30566-3\n10.1183/13993003.03498-2020\n10.1186/s12967-020-02505-7\n10.1017/S0950268820002903\n10.1136/bmjopen-2020-040729\n10.1136/heartjnl-2020-317322\n10.1136/bmjopen-2020-044028\n10.7150/ijms.48396\n10.7150/thno.47980\n10.14218/JCTH.2020.00043\n10.1017/S0950268820001442\n10.3390/molecules25235725\n10.1371/journal.pone.0238680\n10.1007/s00330-020-07033-y\n10.7150/thno.46833\n10.1038/s41467-020-18684-2\n10.1371/journal.pone.0243262\n10.1111/ijcp.13926\n10.1371/journal.pone.0243262\n10.1109/JBHI.2020.3034296"}
{"title": "Densely connected attention network for diagnosing COVID-19 based on chest CT.", "abstract": "To fully enhance the feature extraction capabilities of deep learning models, so as to accurately diagnose coronavirus disease 2019 (COVID-19) based on chest CT images, a densely connected attention network (DenseANet) was constructed by utilizing the self-attention mechanism in deep learning.\nDuring the construction of the DenseANet, we not only densely connected attention features within and between the feature extraction blocks with the same scale, but also densely connected attention features with different scales at the end of the deep model, thereby further enhancing the high-order features. In this way, as the depth of the deep model increases, the spatial attention features generated by different layers can be densely connected and gradually transferred to deeper layers. The DenseANet takes CT images of the lung fields segmented by an improved U-Net as inputs and outputs the probability of the patients suffering from COVID-19.\nCompared with exiting attention networks, DenseANet can maximize the utilization of self-attention features at different depths in the model. A five-fold cross-validation experiment was performed on a dataset containing 2993 CT scans of 2121 patients, and experiments showed that the DenseANet can effectively locate the lung lesions of patients infected with SARS-CoV-2, and distinguish COVID-19, common pneumonia and normal controls with an average of 96.06% Acc and 0.989 AUC.\nThe DenseANet we proposed can generate strong attention features and achieve the best diagnosis results. In addition, the proposed method of densely connecting attention features can be easily extended to other advanced deep learning methods to improve their performance in related tasks.", "journal": "Computers in biology and medicine", "date": "2021-09-15", "authors": ["YuFu", "PengXue", "EnqingDong"], "doi": "10.1016/j.compbiomed.2021.104857\n10.1109/JBHI.2021.3094578\n10.1016/j.cmpb.2021.106381\n10.1016/j.media.2020.101913\n10.1016/j.compbiomed.2021.104744\n10.3389/fmed.2020.608525\n10.1038/s41467-020-17971-2\n10.1002/mp.15044\n10.1109/CVPR.2016.90\n10.1007/978-3-319-24574-4_28"}
{"title": "Predicting clinical outcomes in COVID-19 using radiomics on chest radiographs.", "abstract": "For optimal utilization of healthcare resources, there is a critical need for early identification of COVID-19 patients at risk of poor prognosis as defined by the need for intensive unit care and mechanical ventilation. We tested the feasibility of chest X-ray (CXR)-based radiomics metrics to develop machine-learning algorithms for predicting patients with poor outcomes.\nIn this Institutional Review Board (IRB) approved, Health Insurance Portability and Accountability Act (HIPAA) compliant, retrospective study, we evaluated CXRs performed around the time of admission from 167 COVID-19 patients. Of the 167 patients, 68 (40.72%) required intensive care during their stay, 45 (26.95%) required intubation, and 25 (14.97%) died. Lung opacities were manually segmented using ITK-SNAP (open-source software). CaPTk (open-source software) was used to perform 2D radiomics analysis.\nOf all the algorithms considered, the AdaBoost classifier performed the best with AUC = 0.72\u2009to predict the need for intubation, AUC = 0.71\u2009to predict death, and AUC = 0.61\u2009to predict the need for admission to the intensive care unit (ICU). AdaBoost had similar performance with ElasticNet in predicting the need for admission to ICU. Analysis of the key radiomic metrics that drive model prediction and performance showed the importance of first-order texture metrics compared to other radiomics panel metrics. Using a Venn-diagram analysis, two first-order texture metrics and one second-order texture metric that consistently played an important role in driving model performance in all three outcome predictions were identified.\nConsidering the quantitative nature and reliability of radiomic metrics, they can be used prospectively as prognostic markers to individualize treatment plans for COVID-19 patients and also assist with healthcare resource management.\nWe report on the performance of CXR-based imaging metrics extracted from RT-PCR positive COVID-19 patients at admission to develop machine-learning algorithms for predicting the need for ICU, the need for intubation, and mortality, respectively.", "journal": "The British journal of radiology", "date": "2021-09-15", "authors": ["Bino AbelVarghese", "HeeseopShin", "BhushanDesai", "AliGholamrezanezhad", "XiaomengLei", "MelissaPerkins", "AssadOberai", "NehaNanda", "StevenCen", "VinayDuddalwar"], "doi": "10.1259/bjr.20210221\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1016/S0140-6736(20)30183-5\n10.1128/JCM.00512-20\n10.1136/bmj.m1201\n10.1007/s00134-020-06092-5\n10.1148/radiol.2020203511\n10.1155/2020/8889023\n10.1007/s11547-020-01232-9\n10.1148/radiol.2020201754\n10.1007/s00330-020-07270-1\n10.1007/s00330-020-07504-2\n10.1371/journal.pmed.1003379\n10.1001/jamanetworkopen.2020.21892\n10.1117/1.JMI.5.1.011018\n10.1007/978-3-030-46643-5_38\n10.1148/radiol.2020191145\n10.1214/aos/1016218223\n10.1214/09-AOAS260\n10.1093/oxfordjournals.pan.a004868\n10.1016/j.jacr.2020.03.025\n10.1002/acm2.12666\n10.1002/acm2.13162\n10.1056/NEJMoa2015432\n10.2214/AJR.18.20624\n10.21203/rs.3.rs-37657/v1\n10.1148/ryct.2020200322\n10.21037/atm-20-3026\n10.1136/bmjopen-2020-042946\n10.2967/jnumed.117.199935\n10.1038/s41598-021-83967-7"}
{"title": "Genetic-based adaptive momentum estimation for predicting mortality risk factors for COVID-19 patients using deep learning.", "abstract": "The mortality risk factors for coronavirus disease (COVID-19) must be early predicted, especially for severe cases, to provide intensive care before they develop to critically ill immediately. This paper aims to develop an optimized convolution neural network (CNN) for predicting mortality risk factors for COVID-19 patients. The proposed model supports two types of input data clinical variables and the computed tomography (CT) scans. The features are extracted from the optimized CNN phase and then applied to the classification phase. The CNN model's hyperparameters were optimized using a proposed genetic-based adaptive momentum estimation (GB-ADAM) algorithm. The GB-ADAM algorithm employs the genetic algorithm (GA) to optimize Adam optimizer's configuration parameters, consequently improving the classification accuracy. The model is validated using three recent cohorts from New York, Mexico, and Wuhan, consisting of 3055, 7497,504 patients, respectively. The results indicated that the most significant mortality risk factors are: CD ", "journal": "International journal of imaging systems and technology", "date": "2021-09-15", "authors": ["Sally MElghamrawy", "Aboul EllaHassanien", "Athanasios VVasilakos"], "doi": "10.1002/ima.22644\n10.1016/j.ijid.2020.01.009\n10.1016/S0140-6736(20)30566-3\n10.1038/s41467-020-17280-8\n10.1016/j.measurement.2019.107459\n10.1101/2020.02.27.20028027v2\n10.1101/2020.04.21.20074591v1\n10.1101/2020.04.11.20056523v1\n10.1101/2020.02.27.20028027v3\n10.1101/2020.02.20.20025510v1"}
{"title": "A novel and efficient deep learning approach for COVID-19 detection using X-ray imaging modality.", "abstract": "With the exponential growth of COVID-19 cases, medical practitioners are searching for accurate and quick automated detection methods to prevent Covid from spreading while trying to reduce the computational requirement of devices. In this research article, a deep learning Convolutional Neural Network (CNN) based accurate and efficient ensemble model using deep learning is being proposed with 2161 COVID-19, 2022 pneumonia, and 5863 normal chest X-ray images that has been collected from previous publications and other online resources. To improve the detection accuracy contrast enhancement and image normalization have been done to produce better quality images at the pre-processing level. Further data augmentation methods are used by creating modified versions of images in the dataset to train the four efficient CNN models (Inceptionv3, DenseNet121, Xception, InceptionResNetv2) Experimental results provide 98.33% accuracy for binary class and 92.36% for multiclass. The performance evaluation metrics reveal that this tool can be very helpful for early disease diagnosis.", "journal": "International journal of imaging systems and technology", "date": "2021-09-15", "authors": ["PrashantBhardwaj", "AmanpreetKaur"], "doi": "10.1002/ima.22627\n10.1016/S2213-2600(20)30056-4\n10.1016/j.scs.2020.102589\n10.1002/jmv.25681\n10.1016/j.ijsu.2020.04.001\n10.1001/jama.2020.1585\n10.3390/app10020559\n10.1109/IVCNZ.2018.8634671\n10.1016/j.compbiomed.2020.103795\n10.1016/j.irbm.2020.05.003\n10.1016/S1473-3099(20)30134-1\n10.1016/j.inffus.2017.10.006\n10.1016/j.scs.2020.102018\n10.1007/s11042-017-4637-6\n10.1148/radiol.2020200642\n10.1016/j.clinimag.2020.04.001\n10.1038/s41598-020-76550-z\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103792\n10.1109/JBHI.2016.2636665\n10.1109/ACCESS.2020.3010287\n10.1016/j.cmpb.2020.105581\n10.1007/s11042-019-07820-w\n10.1016/j.media.2020.101794\n10.1016/j.chaos.2020.109944\n10.1109/CVPR.2017.243\n10.1109/CVPR.2018.00474\n10.1007/s11263-015-0816-y\n10.1016/j.cell.2018.02.010\n10.20944/preprints202003.0300.v1\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105608\n10.1109/JSEN.2018.2807245\n10.1016/j.imu.2020.100360\n10.1109/TII.2021.3057683\n10.3390/sym13010113\n10.1109/CIBCB48159.2020.9277695\n10.1109/EBBT.2019.8742050\n10.1007/s00521-020-05410-8\n10.1016/j.patrec.2019.11.013\n10.1101/2020.04.13.20063461\n10.1109/JIOT.2020.3047662\n10.1007/s11042-017-5537-5\n10.1016/j.media.2021.101993\n10.1016/j.patrec.2018.08.010\n10.1007/s10462-020-09825-6\n10.1109/ICPR.1996.547205\n10.35940/ijeat.b3957.129219\n10.1007/s13202-020-00839-y\n10.1016/j.measurement.2019.106965\n10.1007/s13246-020-00888-x\n10.1167/9.8.1037\n10.1007/s12652-020-02669-6\n10.1016/j.asoc.2020.106580"}
{"title": "Application of machine learning in CT images and X-rays of COVID-19 pneumonia.", "abstract": "Coronavirus disease (COVID-19) has spread worldwide. X-ray and computed tomography (CT) are 2 technologies widely used in image acquisition, segmentation, diagnosis, and evaluation. Artificial intelligence can accurately segment infected parts in X-ray and CT images, assist doctors in improving diagnosis efficiency, and facilitate the subsequent assessment of the severity of the patient infection. The medical assistant platform based on machine learning can help radiologists make clinical decisions and helper in screening, diagnosis, and treatment. By providing scientific methods for image recognition, segmentation, and evaluation, we summarized the latest developments in the application of artificial intelligence in COVID-19 lung imaging, and provided guidance and inspiration to researchers and doctors who are fighting the COVID-19 virus.", "journal": "Medicine", "date": "2021-09-14", "authors": ["FengjunZhang"], "doi": "10.1097/MD.0000000000026855"}
{"title": "A Novel Multicolor-thresholding Auto-detection Method to Detect the Location and Severity of Inflammation in Confirmed SARS-COV-2 Cases using Chest X-Ray Images.", "abstract": "Since late 2019, Coronavirus Disease 2019 (COVID-19) has spread around the world. It has been determined that the disease is very contagious and can cause Acute Respiratory Distress (ARD). Medical imaging has the potential to help identify, detect, and quantify the severity of this infection. This work seeks to develop a novel auto-detection technique for verified COVID-19 cases that can detect aberrant alterations in traditional X-ray pictures.\nNineteen separately colored layers were created from X-ray scans of patients diagnosed with COVID-19. Each layer represents objects that have a similar contrast and can be represented by a single color. In a single layer, objects with similar contrasts are formed. A single color image was created by extracting all the objects from all the layers. The prototype model could recognize a wide range of abnormal changes in the image texture based on color differentiation. This was true even when the contrast values of the detected unclear abnormalities varied slightly.\nThe results indicate that the proposed novel method is 91% accurate in detecting and grading COVID-19 lung infections compared to the opinions of three experienced radiologists evaluating chest X-ray images. Additionally, the method can be used to determine the infection site and severity of the disease by categorizing X-rays into five severity levels.\nBy comparing affected tissue to healthy tissue, the proposed COVID-19 auto-detection method can identify locations and indicate the severity of the disease, as well as predict where the disease may spread.", "journal": "Current medical imaging", "date": "2021-09-14", "authors": ["Mohammed SAlqahtani", "Mohamed AAbbas", "Ali MAlqahtani", "Mohammad YAlshahrani", "Abdulhadi JAlkulib", "Magbool AAlelyani", "Awad MAlmarhaby"], "doi": "10.2174/1573405617666210910150119"}
{"title": "Classification of Lung Disease in Children by Using Lung Ultrasound Images and Deep Convolutional Neural Network.", "abstract": "Bronchiolitis is the most common cause of hospitalization of children in the first year of life and pneumonia is the leading cause of infant mortality worldwide. Lung ultrasound technology (LUS) is a novel imaging diagnostic tool for the early detection of respiratory distress and offers several advantages due to its low-cost, relative safety, portability, and easy repeatability. More precise and efficient diagnostic and therapeutic strategies are needed. Deep-learning-based computer-aided diagnosis (CADx) systems, using chest X-ray images, have recently demonstrated their potential as a screening tool for pulmonary disease (such as COVID-19 pneumonia). We present the first computer-aided diagnostic scheme for LUS images of pulmonary diseases in children. In this study, we trained from scratch four state-of-the-art deep-learning models (VGG19, Xception, Inception-v3 and Inception-ResNet-v2) for detecting children with bronchiolitis and pneumonia. In our experiments we used a data set consisting of 5,907 images from 33 healthy infants, 3,286 images from 22 infants with bronchiolitis, and 4,769 images from 7 children suffering from bacterial pneumonia. Using four-fold cross-validation, we implemented one binary classification (healthy vs. bronchiolitis) and one three-class classification (healthy vs. bronchiolitis vs. bacterial pneumonia) out of three classes. Affine transformations were applied for data augmentation. Hyperparameters were optimized for the learning rate, dropout regularization, batch size, and epoch iteration. The Inception-ResNet-v2 model provides the highest classification performance, when compared with the other models used on test sets: for healthy vs. bronchiolitis, it provides 97.75% accuracy, 97.75% sensitivity, and 97% specificity whereas for healthy vs. bronchiolitis vs. bacterial pneumonia, the Inception-v3 model provides the best results with 91.5% accuracy, 91.5% sensitivity, and 95.86% specificity. We performed a gradient-weighted class activation mapping (Grad-CAM) visualization and the results were qualitatively evaluated by a pediatrician expert in LUS imaging: heatmaps highlight areas containing diagnostic-relevant LUS imaging-artifacts, e.g., A-, B-, pleural-lines, and consolidations. These complex patterns are automatically learnt from the data, thus avoiding hand-crafted features usage. By using LUS imaging, the proposed framework might aid in the development of an accessible and rapid decision support-method for diagnosing pulmonary diseases in children using LUS imaging.", "journal": "Frontiers in physiology", "date": "2021-09-14", "authors": ["SilviaMagrelli", "PieroValentini", "CristinaDe Rose", "RosaMorello", "DaniloBuonsenso"], "doi": "10.3389/fphys.2021.693448\n10.1007/s11548-015-1181-6\n10.1007/s13246-020-00865-4\n10.1186/s12887-015-0380-1\n10.1121/1.1903488\n10.1117/12.2581865\n10.1038/s41598-019-54499-y\n10.1007/978-3-319-66179-7_30\n10.1016/j.ultrasmedbio.2020.07.005\n10.3390/app11020672\n10.1093/cid/cir625\n10.1016/0165-0173(94)00016-I\n10.1002/jum.15147\n10.1007/s40477-021-00600-z\n10.1186/s12890-019-0925-4\n10.1016/S2213-2600(20)30120-X\n10.1002/uog.22055\n10.1007/s40477-020-00520-4\n10.1002/jum.15347\n10.1007/s00247-020-04750-w\n10.1109/TUFFC.2020.3005512\n10.1145/1541880.1541882\n10.1016/j.emc.2011.10.009\n10.1109/CVPR.2017.195\n10.1056/NEJMp1500523\n10.1186/1476-7120-6-16\n10.1371/journal.pone.0206410\n10.1109/CVPR.2009.5206848\n10.3390/diagnostics9040172\n10.1121/1.1903489\n10.1121/1.393818\n10.1088/0031-9155/5/4/302\n10.1038/s41591-018-0316-z\n10.1016/j.ejheart.2007.10.009\n10.1109/TMI.2016.2538802\n10.1109/TMI.2016.2553401\n10.1136/thoraxjnl-2011-200598\n10.1117/12.2254581\n10.1007/978-3-030-05318-5\n10.1016/j.ajog.2020.04.020\n10.1016/j.amjcard.2004.02.012\n10.1109/ACCESS.2017.2788044\n10.1097/PEC.0000000000001050\n10.1007/978-3-030-01045-4_8\n10.1038/nature14539\n10.1016/j.cmpb.2019.06.023\n10.1164/ajrccm.156.5.96-07096\n10.1378/chest.08-2281\n10.1183/23120541.00539-2020\n10.1016/j.media.2017.07.005\n10.1016/S0140-6736(14)61698-6\n10.1016/j.eng.2018.11.020\n10.1002/mp.12134\n10.1016/j.acra.2018.02.018\n10.3389/fdata.2021.612561\n10.1016/S0301-5629(02)00561-6\n10.1007/978-3-319-24571-3_14\n10.1016/S0140-6736(20)31875-4\n10.1002/ppul.25255\n10.1002/ppul.24426\n10.1097/RUQ.0000000000000411\n10.1007/s10044-021-00984-y\n10.1186/1757-7241-22-23\n10.1016/j.compbiomed.2020.103792\n10.5811/westjem.2020.5.47743\n10.3389/fmed.2020.00375\n10.1016/0301-5629(86)90220-6\n10.1117/1.JMI.4.1.014502\n10.1038/s41551-018-0195-0\n10.1542/peds.2014-2742\n10.7863/jum.2003.22.2.173\n10.1002/jum.15306\n10.1093/pch/20.2.67\n10.5220/0007404301120119\n10.1016/j.media.2019.01.010\n10.1109/ICCV.2017.74\n10.1001/jama.2017.9039\n10.1146/annurev-bioeng-071516-044442\n10.1111/anae.15082\n10.7863/ultra.15.08023\n10.1080/17476348.2019.1565997\n10.1002/jum.15285\n10.1378/chest.130.2.533\n10.1109/EMBC.2017.8037577\n10.1542/peds.2006-2223\n10.1007/s00431-019-03335-6\n10.1109/CVPR.2016.308\n10.1002/jum.15468\n10.1016/j.arcped.2017.11.005\n10.1016/j.ultrasmedbio.2020.07.003\n10.1109/JBHI.2019.2936151\n10.1007/s00134-012-2513-4\n10.1007/s00134-021-06373-7\n10.1007/s00134-020-06048-9\n10.1016/j.ajem.2006.02.013\n10.1155/2018/7068349\n10.1038/s41598-020-76550-z\n10.1109/ICCVW.2017.71\n10.1016/j.compbiomed.2019.02.002\n10.1016/j.ultrasmedbio.2017.07.013"}
{"title": "Nature-inspired solution for coronavirus disease detection and its impact on existing healthcare systems.", "abstract": "Coronavirus is an infectious life-threatening disease and is mainly transmitted through infected person coughs, sneezes, or exhales. This disease is a global challenge that demands advanced solutions to address multiple dimensions of this pandemic for health and wellbeing.\u00a0 Different types of medical and technological-based solutions have been proposed to control and treat COVID-19. Machine learning is one of the technologies used in Magnetic Resonance Imaging (MRI) classification whereas nature-inspired algorithms are also adopted for image optimization. In this paper, we combined the machine learning and nature-inspired algorithm for brain MRI images of COVID-19 patients namely Machine Learning and Nature Inspired Model for Coronavirus (MLNI-COVID-19). This model improves the MRI image classification and optimization for better diagnosis. This model will improve the overall performance especially the area of brain images that is neglected due to the unavailability of the dataset. COVID-19 has a serious impact on the patient brain. The proposed model will help to improve the diagnosis process for better medical decisions and performance. The proposed model is evaluated with existing algorithms and achieved better performance in terms of sensitivity, specificity, and accuracy.", "journal": "Computers & electrical engineering : an international journal", "date": "2021-09-14", "authors": ["Kashif NaseerQureshi", "AdiAlhudhaif", "Maria AhmedQureshi", "GwanggilJeon"], "doi": "10.1016/j.compeleceng.2021.107411"}
{"title": "Rapidly deploying a COVID-19 decision support system in one of the largest Brazilian hospitals.", "abstract": "The COVID-19 pandemic generated research interest in automated models to perform classification and segmentation from medical imaging of COVID-19 patients, However, applications in real-world scenarios are still needed. We describe the development and deployment of COVID-19 decision support and segmentation system. A partnership with a Brazilian radiologist consortium, gave us access to 1000s of labeled computed tomography (CT) and X-ray images from S\u00e3o Paulo Hospitals. The system used EfficientNet and EfficientDet networks, state-of-the-art convolutional neural networks for natural images classification and segmentation, in a real-time scalable scenario in communication with a Picture Archiving and Communication System (PACS). Additionally, the system could reject non-related images, using header analysis and classifiers. We achieved CT and X-ray classification accuracies of 0.94 and 0.98, respectively, and Dice coefficient for lung and covid findings segmentations of 0.98 and 0.73, respectively. The median response time was 7\u2009s for X-ray and 4\u2009min for CT.", "journal": "Health informatics journal", "date": "2021-09-14", "authors": ["DiedreCarmo", "IsraelCampiotti", "L\u00edviaRodrigues", "IreneFantini", "GustavoPinheiro", "DanielMoraes", "RodrigoNogueira", "LeticiaRittner", "RobertoLotufo"], "doi": "10.1177/14604582211033017"}
{"title": "Novel ensemble of optimized CNN and dynamic selection techniques for accurate Covid-19 screening using chest CT images.", "abstract": "The world is significantly affected by infectious coronavirus disease (covid-19). Timely prognosis and treatment are important to control the spread of this infection. Unreliable screening systems and limited number of clinical facilities are the major hurdles in controlling the spread of covid-19. Nowadays, many automated detection systems based on deep learning techniques using computed tomography (CT) images have been proposed to detect covid-19. However, these systems have the following drawbacks: (i) limited data problem poses a major hindrance to train the deep neural network model to provide accurate diagnosis, (ii) random choice of hyperparameters of Convolutional Neural Network (CNN) significantly affects the classification performance, since the hyperparameters have to be application dependent and, (iii) the generalization ability using CNN classification is usually not validated. To address the aforementioned issues, we propose two models: (i) based on a transfer learning approach, and (ii) using novel strategy to optimize the CNN hyperparameters using Whale optimization-based BAT algorithm\u00a0+\u00a0AdaBoost classifier built using dynamic ensemble selection techniques. According to our second method depending on the characteristics of test sample, the classifier is chosen, thereby reducing the risk of overfitting and simultaneously produced promising results. Our proposed methodologies are developed using 746 CT images. Our method obtained a sensitivity, specificity, accuracy, F-1 score, and precision of 0.98, 0.97, 0.98, 0.98, and 0.98, respectively with five-fold cross-validation strategy. Our developed prototype is ready to be tested with huge chest CT images database before its real-world application.", "journal": "Computers in biology and medicine", "date": "2021-09-12", "authors": ["SameenaPathan", "P CSiddalingaswamy", "PreethamKumar", "ManoharaPai M M", "TanweerAli", "U RajendraAcharya"], "doi": "10.1016/j.compbiomed.2021.104835"}
{"title": "A multi-scale gated multi-head attention depthwise separable CNN model for recognizing COVID-19.", "abstract": "Coronavirus 2019 (COVID-19) is a new acute respiratory disease that has spread rapidly throughout the world. In this paper, a lightweight convolutional neural network (CNN) model named multi-scale gated multi-head attention depthwise separable CNN (MGMADS-CNN) is proposed, which is based on attention mechanism and depthwise separable convolution. A multi-scale gated multi-head attention mechanism is designed to extract effective feature information from the COVID-19 X-ray and CT images for classification. Moreover, the depthwise separable convolution layers are adopted as MGMADS-CNN's backbone to reduce the model size and parameters. The LeNet-5, AlexNet, GoogLeNet, ResNet, VGGNet-16, and three MGMADS-CNN models are trained, validated and tested with tenfold cross-validation on X-ray and CT images. The results show that MGMADS-CNN with three attention layers (MGMADS-3) has achieved accuracy of 96.75% on X-ray images and 98.25% on CT images. The specificity and sensitivity are 98.06% and 96.6% on X-ray images, and 98.17% and 98.05% on CT images. The size of MGMADS-3 model is only 43.6\u00a0M bytes. In addition, the detection speed of MGMADS-3 on X-ray images and CT images are 6.09\u00a0ms and 4.23\u00a0ms for per image, respectively. It is proved that the MGMADS-3 can detect and classify COVID-19 faster with higher accuracy and efficiency.", "journal": "Scientific reports", "date": "2021-09-12", "authors": ["GengHong", "XiaoyanChen", "JianyongChen", "MiaoZhang", "YumengRen", "XinyuZhang"], "doi": "10.1038/s41598-021-97428-8\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200230\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200274\n10.1016/j.clinimag.2020.10.035\n10.1148/radiol.2020200463\n10.1148/radiol.2020200343\n10.1007/s10489-020-01829-7\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1038/s41598-019-56847-4\n10.1007/s10096-020-03901-z\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2995965\n10.1109/TCBB.2021.3065361\n10.1016/j.neunet.2019.12.024\n10.1016/j.neunet.2020.01.034\n10.1016/j.neucom.2019.11.049\n10.1007/s40846-020-00529-4"}
{"title": "Diagnostic classification of coronavirus disease 2019 (COVID-19) and other pneumonias using radiomics features in CT chest images.", "abstract": "We propose a classification method using the radiomics features of CT chest images to identify patients with coronavirus disease 2019 (COVID-19) and other pneumonias. The chest CT images of two groups of participants (90 COVID-19 patients who were confirmed as positive by nucleic acid test of RT-PCR and 90 other pneumonias patients) were collected, and the two groups of data were manually drawn to outline the region of interest (ROI) of pneumonias. The radiomics method was used to extract textural features and histogram features of the ROI and obtain a radiomics features vector from each sample. Then, we divided the data into two independent radiomic cohorts for training (70 COVID-19 patients and 70 other pneumonias patients), and validation (20 COVID-19 patients and 20 other pneumonias patients) by using support vector machine (SVM). This model used 20 rounds of tenfold cross-validation for training. Finally, single-shot testing of the final model was performed on the independent validation cohort. In the COVID-19 patients, correlation analysis (multiple comparison correction-Bonferroni correction, P\u2009<\u20090.05/7) was also conducted to determine whether the textural and histogram features were correlated with the laboratory test index of blood, i.e., blood oxygen, white blood cell, lymphocytes, neutrophils, C-reactive protein, hypersensitive C-reactive protein, and erythrocyte sedimentation rate. The final model showed good discrimination on the independent validation cohort, with an accuracy of 89.83%, sensitivity of 94.22%, specificity of 85.44%, and AUC of 0.940. This proved that the radiomics features were highly distinguishable, and this SVM model can effectively identify and diagnose patients with COVID-19 and other pneumonias. The correlation analysis results showed that some textural features were positively correlated with WBC, and NE, and also negatively related to SPO2H and NE. Our results showed that radiomic features can classify COVID-19 patients and other pneumonias patients. The SVM model can achieve an excellent diagnosis of COVID-19.", "journal": "Scientific reports", "date": "2021-09-11", "authors": ["NingYang", "FamingLiu", "ChunlongLi", "WenqingXiao", "ShuangcongXie", "ShuyiYuan", "WeiZuo", "XiaofenMa", "GuihuaJiang"], "doi": "10.1038/s41598-021-97497-9\n10.21037/atm.2020.02.06\n10.1056/NEJMoa2001017\n10.1148/radiol.2020200236\n10.1148/radiol.2020200230\n10.1148/radiol.2020200343\n10.1001/jama.2020.1585\n10.1148/radiol.2020200370\n10.21037/jtd.2020.02.64\n10.1038/bjc.2012.581\n10.1148/radiol.2015151169\n10.1109/TSMC.1973.4309314\n10.1109/21.44046\n10.1158/0008-5472.CAN-18-0125\n10.1016/j.media.2020.101910\n10.3390/diagnostics11010041\n10.1016/j.acra.2020.09.004\n10.1186/s12967-020-02692-3\n10.1007/s12539-020-00410-7"}
{"title": "Recognition of COVID-19 from CT Scans Using Two-Stage Deep-Learning-Based Approach: CNR-IEMN.", "abstract": "Since the appearance of the COVID-19 pandemic (at the end of 2019, Wuhan, China), the recognition of COVID-19 with medical imaging has become an active research topic for the machine learning and computer vision community. This paper is based on the results obtained from the 2021 COVID-19 SPGC challenge, which aims to classify volumetric CT scans into normal, COVID-19, or community-acquired pneumonia (Cap) classes. To this end, we proposed a deep-learning-based approach (CNR-IEMN) that consists of two main stages. In the first stage, we trained four deep learning architectures with a multi-tasks strategy for slice-level classification. In the second stage, we used the previously trained models with an XG-boost classifier to classify the whole CT scan into normal, COVID-19, or Cap classes. Our approach achieved a good result on the validation set, with an overall accuracy of 87.75% and 96.36%, 52.63%, and 95.83% sensitivities for COVID-19, Cap, and normal, respectively. On the other hand, our approach achieved fifth place on the three test datasets of SPGC in the COVID-19 challenge, where our approach achieved the best result for COVID-19 sensitivity. In addition, our approach achieved second place on two of the three testing sets.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-09-11", "authors": ["FaresBougourzi", "RiccardoContino", "CosimoDistante", "AbdelmalikTaleb-Ahmed"], "doi": "10.3390/s21175878\n10.1007/s10238-020-00648-x\n10.3390/s21051742\n10.7326/M20-1495\n10.21203/rs.3.rs-491375/v1\n10.1109/ICASSP39728.2021.9414185\n10.1038/s41597-021-00900-3\n10.1148/radiol.2020200236\n10.1016/j.eswa.2020.113459\n10.3390/jimaging7030051\n10.1109/TBDATA.2021.3056564\n10.1109/TMI.2021.3066161\n10.3390/s21020455\n10.1007/s10140-020-01886-y\n10.1080/07391102.2020.1788642\n10.1038/s41467-020-20657-4\n10.1109/TIP.2021.3058783\n10.1101/2020.03.12.20027185\n10.1101/2020.10.11.20211052\n10.1109/ICASSP39728.2021.9414947\n10.1109/ICASSP39728.2021.9414745\n10.1109/ICASSP39728.2021.9414007\n10.1109/ICASSP39728.2021.9414426\n10.1109/ICASSP39728.2021.9413707\n10.1145/3065386"}
{"title": "On the Use of Deep Learning for Imaging-Based COVID-19 Detection Using Chest X-rays.", "abstract": "The global COVID-19 pandemic that started in 2019 and created major disruptions around the world demonstrated the imperative need for quick, inexpensive, accessible and reliable diagnostic methods that would allow the detection of infected individuals with minimal resources. Radiography, and more specifically, chest radiography, is a relatively inexpensive medical imaging modality that can potentially offer a solution for the diagnosis of COVID-19 cases. In this work, we examined eleven deep convolutional neural network architectures for the task of classifying chest X-ray images as belonging to healthy individuals, individuals with COVID-19 or individuals with viral pneumonia. All the examined networks are established architectures that have been proven to be efficient in image classification tasks, and we evaluated three different adjustments to modify the architectures for the task at hand by expanding them with additional layers. The proposed approaches were evaluated for all the examined architectures on a dataset with real chest X-ray images, reaching the highest classification accuracy of 98.04% and the highest F1-score of 98.22% for the best-performing setting.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-09-11", "authors": ["Gabriel IluebeOkolo", "StamosKatsigiannis", "TurkeAlthobaiti", "NaeemRamzan"], "doi": "10.3390/s21175702\n10.1016/j.idm.2020.02.002\n10.1056/NEJMoa2002032\n10.1001/jama.2020.2648\n10.7326/M20-1382\n10.1001/jama.2020.3786\n10.1148/radiol.2020200432\n10.1016/j.ijid.2020.04.023\n10.2807/1560-7917.ES.2020.25.50.2000568\n10.1148/ryct.2020200034\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200642\n10.1148/radiol.2020203173\n10.1001/jama.2020.4326\n10.1148/ryct.2020209004\n10.1148/radiol.2020201160\n10.1016/j.jinf.2020.03.007\n10.1016/j.acra.2020.03.002\n10.1007/s00117-013-2537-y\n10.1016/j.crad.2020.03.008\n10.1016/j.inffus.2021.04.008\n10.1016/S2589-7500(19)30123-2\n10.1109/TMI.2019.2928790\n10.1109/TMI.2019.2936500\n10.1145/3065386\n10.1148/radiol.2017162326\n10.1148/radiol.2019181960\n10.3390/s20123482\n10.1007/s00345-019-03059-0\n10.1038/s41551-018-0301-3\n10.3390/app10020559\n10.1007/s11263-015-0816-y\n10.3390/app9194130\n10.1007/s00330-021-07715-1\n10.1109/CVPR.2016.308\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.3010287\n10.1016/j.imu.2020.100405\n10.1016/j.bbe.2020.08.008\n10.1038/s41598-020-74539-2\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-71294-2\n10.1109/JSEN.2020.3028494\n10.1016/j.cell.2018.02.010\n10.1016/j.icte.2020.04.010\n10.1007/s10916-018-0932-7\n10.1021/ci00027a006\n10.1016/j.geoderma.2019.06.016\n10.25835/0090041\n10.1016/j.asoc.2020.106912\n10.32628/IJSRST207614\n10.1101/2020.11.08.20227819"}
{"title": "Adoption of Digital Pathology in Developing Countries: From Benefits to Challenges.", "abstract": "Digital pathology and the use of artificial intelligence constitute undisputedly the future of modern pathology. The outcomes and benefits of the whole slide imaging are beyond the scope of traditional microscopy, which the pathologists were using for decades. COVID-19 pandemic has further highlighted the importance of digital pathology as it offers the pathologists to work from their place of comfort and bridges the gap of physical barriers. In addition to the many advantages, there are certain limitations and challenges, which have to be overcomed particularly in the developing world. The major issue is the cost of scanners and technical support and training of staff. However, despite all these problems and challenges that exist, these can be resolved with the passage of time, where the role of world leader organisations will be of great importance in resolving these challenges. Key Words: Digital pathology, Artificial intelligence, Whole slide imaging.", "journal": "Journal of the College of Physicians and Surgeons--Pakistan : JCPSP", "date": "2021-09-11", "authors": ["TalatZehra", "AsmaShabbir"], "doi": "10.29271/jcpsp.2021.09.1120"}
{"title": "Machine learning-based CT radiomics model distinguishes COVID-19 from non-COVID-19 pneumonia.", "abstract": "To develop a machine learning-based CT radiomics model is critical for the accurate diagnosis of the rapid spreading coronavirus disease 2019 (COVID-19).\nIn this retrospective study, a total of 326 chest CT exams from 134 patients (63 confirmed COVID-19 patients and 71 non-COVID-19 patients) were collected from January 20 to February 8, 2020. A semi-automatic segmentation procedure was used to delineate the volume of interest (VOI), and radiomic features were extracted. The Support Vector Machine (SVM) model was built on the combination of 4 groups of features, including radiomic features, traditional radiological features, quantifying features, and clinical features. By repeating cross-validation procedure, the performance on the time-independent testing cohort was evaluated by the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity.\nFor the SVM model built on the combination of 4 groups of features (integrated model), the per-exam AUC was 0.925 (95% CI 0.856 to 0.994) for differentiating COVID-19 on the testing cohort, and the sensitivity and specificity were 0.816 (95% CI 0.651 to 0.917) and 0.923 (95% CI 0.621 to 0.996), respectively. As for the SVM models built on radiomic features, radiological features, quantifying features, and clinical features, individually, the AUC on the testing cohort reached 0.765, 0.818, 0.607, and 0.739, respectively, significantly lower than the integrated model, except for the radiomic model.\nThe machine learning-based CT radiomics models may accurately classify COVID-19, helping clinicians and radiologists to identify COVID-19 positive cases.", "journal": "BMC infectious diseases", "date": "2021-09-10", "authors": ["Hui JuanChen", "LiMao", "YangChen", "LiYuan", "FeiWang", "XiuliLi", "QinleiCai", "JieQiu", "FengChen"], "doi": "10.1186/s12879-021-06614-6\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMc2001272\n10.1056/NEJMc2001573\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30154-9\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200823\n10.1186/s12916-019-1422-6\n10.1016/j.ejca.2011.11.036\n10.1200/JCO.2015.65.9128\n10.1093/annonc/mdz001\n10.7150/thno.46465\n10.1038/s41598-021-83237-6\n10.1007/s00330-020-07032-z\n10.1038/s41598-020-76141-y\n10.7150/thno.46428\n10.1186/s12967-020-02692-3\n10.1086/511159\n10.1007/s00330-020-07044-9\n10.1007/s00330-020-07022-1\n10.1158/0008-5472.CAN-17-0339\n10.1023/A:1010933404324\n10.1148/radiol.2020200274\n10.1007/s00330-020-06978-4\n10.1016/j.compbiomed.2021.104304\n10.1016/j.media.2020.101844\n10.1007/s11307-020-01487-8\n10.1016/j.wneu.2019.08.232\n10.1016/j.compbiomed.2020.104135\n10.3389/fonc.2015.00272"}
{"title": "AIforCOVID: Predicting the clinical outcomes in patients with COVID-19 applying AI to chest-X-rays. An Italian multicentre study.", "abstract": "Recent epidemiological data report that worldwide more than 53 million people have been infected by SARS-CoV-2, resulting in 1.3 million deaths. The disease has been spreading very rapidly and few months after the identification of the first infected, shortage of hospital resources quickly became a problem. In this work we investigate whether artificial intelligence working with chest X-ray (CXR) scans and clinical data can be used as a possible tool for the early identification of patients at risk of severe outcome, like intensive care or death. Indeed, further to induce lower radiation dose than computed tomography (CT), CXR is a simpler and faster radiological technique, being also more widespread. In this respect, we present three approaches that use features extracted from CXR images, either handcrafted or automatically learnt by convolutional neuronal networks, which are then integrated with the clinical data. As a further contribution, this work introduces a repository that collects data from 820 patients enrolled in six Italian hospitals in spring 2020 during the first COVID-19 emergency. The dataset includes CXR images, several clinical attributes and clinical outcomes. Exhaustive evaluation shows promising performance both in 10-fold and leave-one-centre-out cross-validation, suggesting that clinical data and images have the potential to provide useful information for the management of patients and hospital resources.", "journal": "Medical image analysis", "date": "2021-09-08", "authors": ["PaoloSoda", "Natascha ClaudiaD'Amico", "JacopoTessadori", "GiovanniValbusa", "ValerioGuarrasi", "ChandraBortolotto", "Muhammad UsmanAkbar", "RosaSicilia", "ErmannoCordelli", "DeborahFazzini", "MichaelaCellina", "GiancarloOliva", "GiovanniCallea", "SilviaPanella", "MaurizioCariati", "DilettaCozzi", "VittorioMiele", "ElviraStellato", "GianpaoloCarrafiello", "GiuliaCastorani", "AnnalisaSimeone", "LorenzoPreda", "GiulioIannello", "AlessioDel Bue", "FabioTedoldi", "MarcoAl\u00ed", "DiegoSona", "SergioPapa"], "doi": "10.1016/j.media.2021.102216\n10.1371/journal.pone.0087357"}
{"title": "Pneumonia detection in chest X-ray images using an ensemble of deep learning models.", "abstract": "Pneumonia is a respiratory infection caused by bacteria or viruses; it affects many individuals, especially in developing and underdeveloped nations, where high levels of pollution, unhygienic living conditions, and overcrowding are relatively common, together with inadequate medical infrastructure. Pneumonia causes pleural effusion, a condition in which fluids fill the lung, causing respiratory difficulty. Early diagnosis of pneumonia is crucial to ensure curative treatment and increase survival rates. Chest X-ray imaging is the most frequently used method for diagnosing pneumonia. However, the examination of chest X-rays is a challenging task and is prone to subjective variability. In this study, we developed a computer-aided diagnosis system for automatic pneumonia detection using chest X-ray images. We employed deep transfer learning to handle the scarcity of available data and designed an ensemble of three convolutional neural network models: GoogLeNet, ResNet-18, and DenseNet-121. A weighted average ensemble technique was adopted, wherein the weights assigned to the base learners were determined using a novel approach. The scores of four standard evaluation metrics, precision, recall, f1-score, and the area under the curve, are fused to form the weight vector, which in studies in the literature was frequently set experimentally, a method that is prone to error. The proposed approach was evaluated on two publicly available pneumonia X-ray datasets, provided by Kermany et al. and the Radiological Society of North America (RSNA), respectively, using a five-fold cross-validation scheme. The proposed method achieved accuracy rates of 98.81% and 86.85% and sensitivity rates of 98.80% and 87.02% on the Kermany and RSNA datasets, respectively. The results were superior to those of state-of-the-art methods and our method performed better than the widely used ensemble techniques. Statistical analyses on the datasets using McNemar's and ANOVA tests showed the robustness of the approach. The codes for the proposed work are available at https://github.com/Rohit-Kundu/Ensemble-Pneumonia-Detection.", "journal": "PloS one", "date": "2021-09-08", "authors": ["RohitKundu", "RitachetaDas", "Zong WooGeem", "Gi-TaeHan", "RamSarkar"], "doi": "10.1371/journal.pone.0256630\n10.1002/jhm.955\n10.1002/ppul.22806\n10.3390/s21113922\n10.1007/s00779-020-01494-0\n10.7717/peerj-cs.495\n10.3390/app10093233\n10.1016/j.cmpb.2019.06.023\n10.1007/s12559-020-09787-5\n10.1186/s12911-019-0792-1\n10.21037/atm-20-3026\n10.1155/2019/4180949\n10.1016/j.chemolab.2021.104256\n10.1016/j.measurement.2019.05.076\n10.2214/AJR.19.21512\n10.1038/s41598-021-93658-y\n10.1038/s41598-021-93783-8\n10.1016/j.compbiomed.2020.103869\n10.1162/089976698300017197\n10.1016/j.csda.2003.10.021"}
{"title": "Cardiovascular CT and MRI in 2020: Review of Key Articles.", "abstract": "Despite the global coronavirus pandemic, cardiovascular imaging continued to evolve throughout 2020. It was an important year for cardiac CT and MRI, with increasing prominence in cardiovascular research, use in clinical decision making, and in guidelines. This review summarizes key publications in 2020 relevant to current and future clinical practice. In cardiac CT, these have again predominated in assessment of patients with chest pain and structural heart diseases, although more refined CT techniques, such as quantitative plaque analysis and CT perfusion, are also maturing. In cardiac MRI, the major developments have been in patients with cardiomyopathy and myocarditis, although coronary artery disease applications remain well represented. Deep learning applications in cardiovascular imaging have continued to advance in both CT and MRI, and these are now closer than ever to routine clinical adoption. Perhaps most important has been the rapid deployment of MRI in enhancing understanding of the impact of COVID-19 infection on the heart. Although this review focuses primarily on articles published in ", "journal": "Radiology", "date": "2021-09-08", "authors": ["Gaurav SGulsin", "NiallMcVeigh", "Jonathon ALeipsic", "Jonathan DDodd"], "doi": "10.1148/radiol.2021211002"}
{"title": "The human-AI scoring system: A new method for CT-based assessment of COVID-19 severity.", "abstract": "Chest computed tomography (CT) plays an important role in the diagnosis and assessment of coronavirus disease 2019 (COVID-19).\nTo evaluate the value of an artificial intelligence (AI) scoring system for radiologically assessing the severity of COVID-19.\nChest CT images of 81 patients (61 of normal type and 20 of severe type) with confirmed COVID-19 were used. The test data were anonymized. The scores achieved by four methods (junior radiologists; AI scoring system; human-AI segmentation system; human-AI scoring system) were compared with that by two experienced radiologists (reference score). The mean absolute errors (MAEs) between the four methods and experienced radiologists were calculated separately. The Wilcoxon test is used to predict the significance of the severity of COVID-19. Then use Spearman correlation analysis ROC analysis was used to evaluate the performance of different scores.\nThe AI score had a relatively low MAE (1.67-2.21). Score of human-AI scoring system had the lowest MAE (1.67), a diagnostic value almost equal to reference score (r= 0.97), and a strongest correlation with clinical severity (r= 0.59, p< 0.001). The AUCs of reference score, score of junior radiologists, AI score, score of human-AI segmentation system, and score of human-AI scoring system were 0.874, 0.841, 0.852, 0.857 and 0.865, respectively.\nThe human-AI scoring system can help radiologists to improve the accuracy of COVID-19 severity assessment.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2021-09-07", "authors": ["MingzhuLiu", "WeifuLv", "BaocaiYin", "YaqiongGe", "WeiWei"], "doi": "10.3233/THC-213199"}
{"title": "Risk factors and disease profile of post-vaccination SARS-CoV-2 infection in UK users of the COVID Symptom Study app: a prospective, community-based, nested, case-control study.", "abstract": "COVID-19 vaccines show excellent efficacy in clinical trials and effectiveness in real-world data, but some people still become infected with SARS-CoV-2 after vaccination. This study aimed to identify risk factors for post-vaccination SARS-CoV-2 infection and describe the characteristics of post-vaccination illness.\nThis prospective, community-based, nested, case-control study used self-reported data (eg, on demographics, geographical location, health risk factors, and COVID-19 test results, symptoms, and vaccinations) from UK-based, adult (\u226518 years) users of the COVID Symptom Study mobile phone app. For the risk factor analysis, cases had received a first or second dose of a COVID-19 vaccine between Dec 8, 2020, and July 4, 2021; had either a positive COVID-19 test at least 14 days after their first vaccination (but before their second; cases 1) or a positive test at least 7 days after their second vaccination (cases 2); and had no positive test before vaccination. Two control groups were selected (who also had not tested positive for SARS-CoV-2 before vaccination): users reporting a negative test at least 14 days after their first vaccination but before their second (controls 1) and users reporting a negative test at least 7 days after their second vaccination (controls 2). Controls 1 and controls 2 were matched (1:1) with cases 1 and cases 2, respectively, by the date of the post-vaccination test, health-care worker status, and sex. In the disease profile analysis, we sub-selected participants from cases 1 and cases 2 who had used the app for at least 14 consecutive days after testing positive for SARS-CoV-2 (cases 3 and cases 4, respectively). Controls 3 and controls 4 were unvaccinated participants reporting a positive SARS-CoV-2 test who had used the app for at least 14 consecutive days after the test, and were matched (1:1) with cases 3 and 4, respectively, by the date of the positive test, health-care worker status, sex, body-mass index (BMI), and age. We used univariate logistic regression models (adjusted for age, BMI, and sex) to analyse the associations between risk factors and post-vaccination infection, and the associations of individual symptoms, overall disease duration, and disease severity with vaccination status.\nBetween Dec 8, 2020, and July 4, 2021, 1\u2009240\u2009009 COVID Symptom Study app users reported a first vaccine dose, of whom 6030 (0\u00b75%) subsequently tested positive for SARS-CoV-2 (cases 1), and 971\u2009504 reported a second dose, of whom 2370 (0\u00b72%) subsequently tested positive for SARS-CoV-2 (cases 2). In the risk factor analysis, frailty was associated with post-vaccination infection in older adults (\u226560 years) after their first vaccine dose (odds ratio [OR] 1\u00b793, 95% CI 1\u00b750-2\u00b748; p<0\u00b70001), and individuals living in highly deprived areas had increased odds of post-vaccination infection following their first vaccine dose (OR 1\u00b711, 95% CI 1\u00b701-1\u00b723; p=0\u00b7039). Individuals without obesity (BMI <30 kg/m\nTo minimise SARS-CoV-2 infection, at-risk populations must be targeted in efforts to boost vaccine effectiveness and infection control measures. Our findings might support caution around relaxing physical distancing and other personal protective measures in the post-vaccination era, particularly around frail older adults and individuals living in more deprived areas, even if these individuals are vaccinated, and might have implications for strategies such as booster vaccinations.\nZOE, the UK Government Department of Health and Social Care, the Wellcome Trust, the UK Engineering and Physical Sciences Research Council, UK Research and Innovation London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare, the UK National Institute for Health Research, the UK Medical Research Council, the British Heart Foundation, and the Alzheimer's Society.", "journal": "The Lancet. Infectious diseases", "date": "2021-09-05", "authors": ["MichelaAntonelli", "Rose SPenfold", "JordiMerino", "Carole HSudre", "ErikaMolteni", "SarahBerry", "Liane SCanas", "Mark SGraham", "KerstinKlaser", "MarcModat", "BenjaminMurray", "EricKerfoot", "LiyuanChen", "JieDeng", "Marc F\u00d6sterdahl", "Nathan JCheetham", "David ADrew", "Long HNguyen", "Joan CapdevilaPujol", "ChristinaHu", "SomeshSelvachandran", "LorenzoPolidori", "AnnaMay", "JonathanWolf", "Andrew TChan", "AlexanderHammers", "Emma LDuncan", "Tim DSpector", "SebastienOurselin", "Claire JSteves"], "doi": "10.1016/S1473-3099(21)00460-6\n10.1093/cid/ciab581"}
{"title": "The Clinical Information Systems Response to the COVID-19 Pandemic.", "abstract": "The year 2020 was predominated by the coronavirus disease 2019 (COVID-19) pandemic. The objective of this article is to review the areas in which clinical information systems (CIS) can be and have been utilized to support and enhance the response of healthcare systems to pandemics, focusing on COVID-19.\nPubMed/MEDLINE, Google Scholar, the tables of contents of major informatics journals, and the bibliographies of articles were searched for studies pertaining to CIS, pandemics, and COVID-19 through October 2020. The most informative and detailed studies were highlighted, while many others were referenced.\nCIS were heavily relied upon by health systems and governmental agencies worldwide in response to COVID-19. Technology-based screening tools were developed to assist rapid case identification and appropriate triaging. Clinical care was supported by utilizing the electronic health record (EHR) to onboard frontline providers to new protocols, offer clinical decision support, and improve systems for diagnostic testing. Telehealth became the most rapidly adopted medical trend in recent history and an essential strategy for allowing safe and effective access to medical care. Artificial intelligence and machine learning algorithms were developed to enhance screening, diagnostic imaging, and predictive analytics - though evidence of improved outcomes remains limited. Geographic information systems and big data enabled real-time dashboards vital for epidemic monitoring, hospital preparedness strategies, and health policy decision making. Digital contact tracing systems were implemented to assist a labor-intensive task with the aim of curbing transmission. Large scale data sharing, effective health information exchange, and interoperability of EHRs remain challenges for the informatics community with immense clinical and academic potential. CIS must be used in combination with engaged stakeholders and operational change management in order to meaningfully improve patient outcomes.\nManaging a pandemic requires widespread, timely, and effective distribution of reliable information. In the past year, CIS and informaticists made prominent and influential contributions in the global response to the COVID-19 pandemic.", "journal": "Yearbook of medical informatics", "date": "2021-09-04", "authors": ["J JefferyReeves", "Natalie MPageler", "Elizabeth CWick", "Genevieve BMelton", "Yu-Heng GamalielTan", "Brian JClay", "Christopher ALonghurst"], "doi": "10.1055/s-0041-1726513"}
{"title": "COVID-19 Imaging-based AI Research - A Literature Review.", "abstract": "The new coronavirus disease 2019 (COVID-19) is spreading rapidly around the world. Artificial Intelligence (AI) assisted identification and detection of diseases is an effective method of medical diagnosis.\nTo present recent advances in AI-assisted diagnosis of COVID-19, we introduce major aspects of AI in the process of diagnosing COVID-19.\nIn this paper, we firstly cover the latest collection and processing methods of datasets of COVID-19. The processing methods mainly include building public datasets, transfer learning, unsupervised learning and weakly supervised learning, semi-supervised learning methods and so on. Secondly, we introduce the algorithm application and evaluation metrics of AI in medical imaging segmentation and automatic screening. Then, we introduce the quantification and severity assessment of infection in COVID-19 patients based on image segmentation and automatic screening. Finally, we analyze and point out the current AI-assisted diagnosis of COVID-19 problems, which may provide useful clues for future work.\nAI is critical for COVID-19 diagnosis. Combining chest imaging with AI can not only save time and effort, but also provide more accurate and efficient medical diagnosis results.", "journal": "Current medical imaging", "date": "2021-09-03", "authors": ["ChengGe", "LiliZhang", "LiangxuXie", "RenKong", "HongZhang", "ShanChang"], "doi": "10.2174/1573405617666210902103729"}
{"title": "COVID-19 detection method based on SVRNet and SVDNet in lung x-rays.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-09-03", "authors": ["KedongRao", "KaiXie", "ZiqiHu", "XiaolongGuo", "ChangWen", "JianbiaoHe"], "doi": "10.1117/1.JMI.8.S1.017504\n10.1148/radiol.2020200642\n10.1148/radiol.2020200241\n10.1148/radiol.2020200463\n10.1038/s41586-020-2008-3\n10.1148/radiol.2020200490\n10.1613/jair.1.12162\n10.7507/1001-5515.201710060\n10.7507/1001-5515.202005056\n10.1038/s41598-020-76550-z\n10.1007/s10044-021-00984-y\n10.1007/s13246-020-00865-4\n10.1148/radiol.2020200905\n10.1016/j.cmpb.2020.105581\n10.1016/j.asoc.2020.106897\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.195\n10.1109/ACCESS.2020.2971225\n10.1016/j.cmpb.2020.105608\n10.1016/j.chaos.2020.110245\n10.1016/j.compbiomed.2020.103792"}
{"title": "Deep learning for distinguishing normal versus abnormal chest radiographs and generalization to two unseen diseases tuberculosis and COVID-19.", "abstract": "Chest radiography (CXR) is the most widely-used thoracic clinical imaging modality and is crucial for guiding the management of cardiothoracic conditions. The detection of specific CXR findings has been the main focus of several artificial intelligence (AI) systems. However, the wide range of possible CXR abnormalities makes it impractical to detect every possible condition by building multiple separate systems, each of which detects one or more pre-specified conditions. In this work, we developed and evaluated an AI system to classify CXRs as normal or abnormal. For training and tuning the system, we used a de-identified dataset of 248,445 patients from a multi-city hospital network in India. To assess generalizability, we evaluated our system using 6 international datasets from India, China, and the United States. Of these datasets, 4 focused on diseases that the AI was not trained to detect: 2 datasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our results suggest that the AI system trained using a large dataset containing a diverse array of CXR abnormalities generalizes to new patient populations and unseen diseases. In a simulated workflow where the AI system prioritized abnormal cases, the turnaround time for abnormal cases reduced by 7-28%. These results represent an important step towards evaluating whether AI can be safely used to flag cases in a general setting where previously unseen abnormalities exist. Lastly, to facilitate the continued development of AI models for CXR, we release our collected labels for the publicly available dataset.", "journal": "Scientific reports", "date": "2021-09-03", "authors": ["ZaidNabulsi", "AndrewSellergren", "ShaharJamshy", "CharlesLau", "EdwardSantos", "Atilla PKiraly", "WenxingYe", "JieYang", "RoryPilgrim", "SaharKazemzadeh", "JinYu", "Sreenivasa RajuKalidindi", "MozziyarEtemadi", "FlorenciaGarcia-Vicente", "DavidMelnick", "Greg SCorrado", "LilyPeng", "KrishEswaran", "DanielTse", "NeeralBeladia", "YunLiu", "Po-Hsuan CameronChen", "ShravyaShetty"], "doi": "10.1038/s41598-021-93967-2\n10.1007/s11604-008-0259-2\n10.4103/2156-7514.97747\n10.1148/radiol.2019191293\n10.1371/journal.pmed.1002686\n10.1148/radiol.2017162326\n10.1148/radiol.2018180237\n10.1016/S2589-7500(20)30162-X\n10.1186/s12916-019-1426-2\n10.1056/NEJMoa2002032\n10.1371/journal.pone.0242301\n10.2214/AJR.09.2950\n10.1111/j.1525-1497.2006.00427.x\n10.1148/radiol.2019192515\n10.1016/j.crad.2018.05.015\n10.1148/radiol.2018180921\n10.1148/radiol.2019191225\n10.1038/s41746-020-0273-z\n10.1097/RLI.0000000000000341\n10.1148/radiol.2018181422\n10.1109/TMI.2013.2284099\n10.1109/TMI.2013.2290491\n10.7326/M20-1495\n10.1016/S0893-6080(98)00116-6\n10.1002/sim.1012\n10.1136/bmj.310.6973.170"}
{"title": "Auto informing COVID-19 detection result from x-ray/CT images based on deep learning.", "abstract": "It is no secret to all that the corona pandemic has caused a decline in all aspects of the world. Therefore, offering an accurate automatic diagnostic system is very important. This paper proposed an accurate COVID-19 system by testing various deep learning models for x-ray/computed tomography (CT) medical images. A deep preprocessing procedure was done with two filters and segmentation to increase classification results. According to the results obtained, 99.94% of accuracy, 98.70% of sensitivity, and 100% of specificity scores were obtained by the Xception model in the x-ray dataset and the InceptionV3 model for CT scan images. The compared results have demonstrated that the proposed model is proven to be more successful than the deep learning algorithms in previous studies. Moreover, it has the ability to automatically notify the examination results to the patients, the health authority, and the community after taking any x-ray or CT images.", "journal": "The Review of scientific instruments", "date": "2021-09-03", "authors": ["Ahlam FadhilMahmood", "Saja WaleedMahmood"], "doi": "10.1063/5.0059829"}
{"title": "An AI-based radiomics nomogram for disease prognosis in patients with COVID-19 pneumonia using initial CT images and clinical indicators.", "abstract": "This study utilized a comprehensive nomogram to evaluate the prognosis of patients with COVID-19 pneumonia.\nCOVID-19 pneumonia data was divided into training set (256 of 321, 80%), internal validation set (65 of 321, 20%) and independent external validation set (n\u00a0=\u00a0188). After image processing, lesion segmentation, feature extraction and feature selection, radiomics signatures and clinical indicators were used to develop a radiomics model and a clinical model respectively. Combining radiomics signatures and clinical indicators, a radiomics nomogram was built. The performance of proposed models was evaluated by the receiver operating characteristic curve (AUC). Calibration curves and decision curve analysis were used to assess the performance of the radiomics nomogram.\nTwo clinical indicators that were age and chronic lung disease or asthma and 21 radiomics features were selected to build the radiomics nomogram. The radiomics nomogram yielded an Area Under The Curve\nThe radiomics nomogram may be used to assess the deterioration of COVID-19 pneumonia.", "journal": "International journal of medical informatics", "date": "2021-09-01", "authors": ["MudanZhang", "XianchunZeng", "ChencuiHuang", "JunLiu", "XinfengLiu", "XingzhiXie", "RongpinWang"], "doi": "10.1016/j.ijmedinf.2021.104545\n10.1038/s41587-020-0513-4\n10.1016/S0140-6736(20)30211-7\n10.1017/S0950268820002010\n10.1148/radiol.2020200642\n10.1148/radiol.2020200323\n10.1148/radiol.2020200370\n10.1148/rg.2018170048\n10.1016/S1473-3099(20)30134-1\n10.1016/j.cell.2020.04.045\n10.7150/ijms.58889\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200370\n10.7150/thno.46428\n10.1007/s00330-020-07032-z\n10.7326/M14-0698\n10.1016/S0140-6736(10)61459-6\n10.1001/jama.2020.3072\n10.1111/eci.13209"}
{"title": "CORONA-Net: Diagnosing COVID-19 from X-ray Images Using Re-Initialization and Classification Networks.", "abstract": "The COVID-19 pandemic has been deemed a global health pandemic. The early detection of COVID-19 is key to combating its outbreak and could help bring this pandemic to an end. One of the biggest challenges in combating COVID-19 is accurate testing for the disease. Utilizing the power of Convolutional Neural Networks (CNNs) to detect COVID-19 from chest X-ray images can help radiologists compare and validate their results with an automated system. In this paper, we propose a carefully designed network, dubbed CORONA-Net, that can accurately detect COVID-19 from chest X-ray images. CORONA-Net is divided into two phases: (1) The reinitialization phase and (2) the classification phase. In the reinitialization phase, the network consists of encoder and decoder networks. The objective of this phase is to train and initialize the encoder and decoder networks by a distribution that comes out of medical images. In the classification phase, the decoder network is removed from CORONA-Net, and the encoder network acts as a backbone network to fine-tune the classification phase based on the learned weights from the reinitialization phase. Extensive experiments were performed on a publicly available dataset, COVIDx, and the results show that CORONA-Net significantly outperforms the current state-of-the-art networks with an overall accuracy of 95.84%.", "journal": "Journal of imaging", "date": "2021-08-31", "authors": ["SherifElbishlawi", "Mohamed HAbdelpakey", "Mohamed SShehata", "Mostafa MMohamed"], "doi": "10.3390/jimaging7050081\n10.1038/nature14539\n10.1148/radiol.2020200905\n10.1101/2020.04.14.20065722\n10.1016/j.mehy.2020.109761\n10.3390/ijerph17186933\n10.1016/j.media.2020.101794\n10.1101/2020.02.14.20023028\n10.1007/s10489-020-02076-6\n10.1101/2020.05.01.20088211\n10.36227/techrxiv.12464402.v1\n10.1016/j.cmpb.2020.105532\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105608\n10.1109/ACCESS.2020.3005510\n10.1016/j.chaos.2020.110245\n10.3389/fmed.2020.00427\n10.1016/j.ejrad.2020.108940\n10.33889/IJMEMS.2020.5.4.052\n10.1016/j.eng.2020.04.010\n10.1101/2020.02.23.20026930\n10.1145/3065386"}
{"title": "PM\u2082.\u2085 Monitoring: Use Information Abundance Measurement and Wide and Deep Learning.", "abstract": "This article devises a photograph-based monitoring model to estimate the real-time PM", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-08-31", "authors": ["KeGu", "HongyanLiu", "ZhifangXia", "JunfeiQiao", "WeisiLin", "DanielThalmann"], "doi": "10.1109/TNNLS.2021.3105394"}
{"title": "X-Ray Equipped with Artificial Intelligence: Changing the COVID-19 Diagnostic Paradigm during the Pandemic.", "abstract": "Due to the excessive use of raw materials in diagnostic tools and equipment during the COVID-19 pandemic, there is a dire need for cheaper and more effective methods in the healthcare system. With the development of artificial intelligence (AI) methods in medical sciences as low-cost and safer diagnostic methods, researchers have turned their attention to the use of imaging tools with AI that have fewer complications for patients and reduce the consumption of healthcare resources. Despite its limitations, X-ray is suggested as the first-line diagnostic modality for detecting and screening COVID-19 cases.\nThis systematic review assessed the current state of AI applications and the performance of algorithms in X-ray image analysis. The search strategy yielded 322 results from four databases and google scholar, 60 of which met the inclusion criteria. The performance statistics included the area under the receiver operating characteristics (AUC) curve, accuracy, sensitivity, and specificity.\nThe average sensitivity and specificity of CXR equipped with AI algorithms for COVID-19 diagnosis were >96% (83%-100%) and 92% (80%-100%), respectively. For common X-ray methods in COVID-19 detection, these values were 0.56 (95% CI 0.51-0.60) and 0.60 (95% CI 0.54-0.65), respectively. AI has substantially improved the diagnostic performance of X-rays in COVID-19.\nX-rays equipped with AI can serve as a tool to screen the cases requiring CT scans. The use of this tool does not waste time or impose extra costs, has minimal complications, and can thus decrease or remove unnecessary CT slices and other healthcare resources.", "journal": "BioMed research international", "date": "2021-08-31", "authors": ["MustafaGhaderzadeh", "MehradAria", "FarkhondehAsadi"], "doi": "10.1155/2021/9942873\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1001/jama.2020.3786\n10.1016/j.eng.2020.04.010\n10.33889/ijmems.2020.5.4.052\n10.1148/radiol.2020201160\n10.1016/j.clinimag.2020.04.001\n10.1016/S1076-6332(00)80380-3\n10.1016/j.scs.2020.102589\n10.2196/27468\n10.1155/2021/9933481\n10.1371/journal.pmed.1001744\n10.1371/journal.pmed.1000097\n10.1371/journal.pone.0242899\n10.1016/j.procs.2020.09.258\n10.1038/s41598-021-88807-2\n10.1007/s42600-020-00091-7\n10.1016/j.ins.2020.09.041\n10.1007/s13755-020-00116-6\n10.1016/j.jksuci.2020.12.010\n10.1016/j.mehy.2020.109761\n10.1109/JAS.2020.1003393\n10.31661/jbpe.v0i0.2008-1153\n10.1016/j.imu.2020.100505\n10.3390/diagnostics10060358\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103869\n10.1109/ACCESS.2020.2994762\n10.18517/ijaseit.10.2.11446\n10.1016/j.measurement.2020.108288\n10.1007/s10489-020-01888-w\n10.1016/j.bbe.2020.08.008\n10.1109/ACCESS.2020.3016780\n10.1016/j.compbiomed.2020.103805\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100506\n10.1007/s00264-020-04609-7\n10.1007/s12539-020-00393-5\n10.1016/j.cmpb.2020.105581\n10.9781/ijimai.2020.04.003\n10.2106/JBJS.20.00715\n10.1371/journal.pone.0242301\n10.1016/j.compbiomed.2020.103792\n10.1371/journal.pone.0235187\n10.1155/2021/6677314\n10.1007/s13246-020-00888-x\n10.1016/j.patrec.2020.09.010\n10.1371/journal.pone.0242535\n10.1016/j.imu.2020.100405\n10.1007/s40846-020-00529-4\n10.1016/j.chaos.2020.110495\n10.1016/j.asoc.2020.106859\n10.1007/s12652-020-02688-3\n10.25046/aj050522\n10.1016/j.patcog.2020.107747\n10.1016/j.asoc.2020.106642\n10.7717/peerj-cs.303\n10.1016/j.eswa.2020.114142\n10.1109/TMI.2020.2996645\n10.3791/61737\n10.1016/j.asoc.2020.107052\n10.1016/j.cmpb.2020.105608\n10.1016/j.cmpb.2020.105532\n10.1080/07391102.2020.1767212\n10.1097/RTI.0000000000000532\n10.3390/sym12040651\n10.1016/j.chaos.2020.109944\n10.1016/j.ijmedinf.2020.104284\n10.1109/JBHI.2020.3037127\n10.1155/2020/8889412\n10.32604/cmc.2020.011326\n10.3390/ijerph17186933\n10.1016/j.bbe.2020.08.005\n10.32604/cmes.2020.011920\n10.1016/j.eswa.2020.113909\n10.1016/j.media.2020.101794\n10.1186/s12938-020-00831-x\n10.3233/XST-200715\n10.1002/ima.22469\n10.1016/j.chaos.2020.110071\n10.1016/j.chemolab.2020.104054\n10.1016/j.imu.2020.100378\n10.1109/TMI.2020.2995965\n10.1186/s40537-019-0197-0\n10.1613/jair.953\n10.1136/bmjopen-2020-042946\n10.2807/1560-7917.ES.2020.25.10.2000180\n10.1056/NEJMoa2008457\n10.1148/radiol.2020200642\n10.1007/s00330-020-07347-x\n10.1007/s11604-020-00958-w\n10.2214/AJR.20.22959\n10.1148/radiol.2020201237"}
{"title": "Research on Classification of COVID-19 Chest X-Ray Image Modal Feature Fusion Based on Deep Learning.", "abstract": "Most detection methods of coronavirus disease 2019 (COVID-19) use classic image classification models, which have problems of low recognition accuracy and inaccurate capture of modal features when detecting chest X-rays of COVID-19. This study proposes a COVID-19 detection method based on image modal feature fusion. This method first performs small-sample enhancement processing on chest X-rays, such as rotation, translation, and random transformation. Five classic pretraining models are used when extracting modal features. A global average pooling layer reduces training parameters and prevents overfitting. The model is trained and fine-tuned, the machine learning evaluation standard is used to evaluate the model, and the receiver operating characteristic (ROC) curve is drawn. Experiments show that compared with the classic model, the classification method in this study can more effectively detect COVID-19 image modal information, and it achieves the expected effect of accurately detecting cases.", "journal": "Journal of healthcare engineering", "date": "2021-08-31", "authors": ["DongshengJi", "ZhujunZhang", "YanzhongZhao", "QianchuanZhao"], "doi": "10.1155/2021/6799202\n10.1097/cm9.0000000000000866\n10.1007/s00330-021-07715-1\n10.1148/radiol.2020200905\n10.3390/sym12040651\n10.1016/j.imu.2020.100360\n10.5455/jjee.204-158531224\n10.1007/s13246-020-00865-4\n10.1109/TKDE.2009.191\n10.1109/cvpr.2017.195\n10.1016/j.eswa.2020.114054\n10.1007/s10044-021-00970-4"}
{"title": "ANFIS-Net for automatic detection of COVID-19.", "abstract": "Among the most leading causes of mortality across the globe are infectious diseases which have cost tremendous lives with the latest being coronavirus (COVID-19) that has become the most recent challenging issue. The extreme nature of this infectious virus and its ability to spread without control has made it mandatory to find an efficient auto-diagnosis system to assist the people who work in touch with the patients. As fuzzy logic is considered a powerful technique for modeling vagueness in medical practice, an Adaptive Neuro-Fuzzy Inference System (ANFIS) was proposed in this paper as a key rule for automatic COVID-19 detection from chest X-ray images based on the characteristics derived by texture analysis using gray level co-occurrence matrix (GLCM) technique. Unlike the proposed method, especially deep learning-based approaches, the proposed ANFIS-based method can work on small datasets. The results were promising performance accuracy, and compared with the other state-of-the-art techniques, the proposed method gives the same performance as the deep learning with complex architectures using many backbone.", "journal": "Scientific reports", "date": "2021-08-29", "authors": ["AfnanAl-Ali", "OmarElharrouss", "UvaisQidwai", "SomayaAl-Maaddeed"], "doi": "10.1038/s41598-021-96601-3\n10.1016/j.chaos.2020.109947\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00383-w\n10.14257/ijbsbt.2016.8.3.21\n10.1002/ima.22170\n10.1016/j.procs.2019.12.134\n10.1002/ima.22257\n10.1002/ima.22329\n10.14419/ijet.v7i3.27.17763\n10.31557/APJCP.2018.19.11.3203\n10.1007/s10586-018-2160-9\n10.3390/math8060890\n10.1007/s42979-020-00401-x\n10.1007/s42979-019-0007-y\n10.1007/s42979-019-0007-y\n10.1016/j.compbiomed.2020.103792\n10.1007/s10489-020-01823-z\n10.1007/s10462-017-9610-2\n10.1109/ACCESS.2019.2893141\n10.1016/j.measurement.2016.10.010"}
{"title": "Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers.", "abstract": "An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented.\nIt is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue.\nFor this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios.\nThe techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated).\nAmong the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-08-29", "authors": ["IamPalatnik de Sousa", "Marley M B RVellasco", "EduardoCosta da Silva"], "doi": "10.3390/s21165657\n10.1038/s41598-020-76550-z\n10.1016/j.eswa.2020.113909\n10.3389/fmed.2020.00427\n10.1109/ACCESS.2021.3065456\n10.1016/j.inffus.2019.12.012\n10.1101/2020.04.13.20063941\n10.1007/s10489-020-01867-1\n10.1109/TPAMI.2012.120\n10.3390/s19132969"}
{"title": "Pulmonary COVID-19: Learning Spatiotemporal Features Combining CNN and LSTM Networks for Lung Ultrasound Video Classification.", "abstract": "Deep Learning is a very active and important area for building Computer-Aided Diagnosis (CAD) applications. This work aims to present a hybrid model to classify lung ultrasound (LUS) videos captured by convex transducers to diagnose COVID-19. A Convolutional Neural Network (CNN) performed the extraction of spatial features, and the temporal dependence was learned using a Long Short-Term Memory (LSTM). Different types of convolutional architectures were used for feature extraction. The hybrid model (CNN-LSTM) hyperparameters were optimized using the Optuna framework. The best hybrid model was composed of an Xception pre-trained on ImageNet and an LSTM containing 512 units, configured with a dropout rate of 0.4, two fully connected layers containing 1024 neurons each, and a sequence of 20 frames in the input layer (20\u00d72018). The model presented an average accuracy of 93% and sensitivity of 97% for COVID-19, outperforming models based purely on spatial approaches. Furthermore, feature extraction using transfer learning with models pre-trained on ImageNet provided comparable results to models pre-trained on LUS images. The results corroborate with other studies showing that this model for LUS classification can be an important tool in the fight against COVID-19 and other lung diseases.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-08-29", "authors": ["BrunoBarros", "PauloLacerda", "C\u00e9lioAlbuquerque", "AuraConci"], "doi": "10.3390/s21165486\n10.1056/NEJMoa2001017\n10.1101/2021.03.19.21253946\n10.1101/2020.12.30.20249034\n10.1016/S0140-6736(21)00183-5\n10.1038/d41586-021-01274-7\n10.1155/2020/5714714\n10.1016/j.chaos.2020.110152\n10.1007/S00521-020-05626-8\n10.1016/j.chaos.2020.109945\n10.1016/j.chaos.2020.110182\n10.1038/s41746-021-00453-0\n10.3892/etm.2020.8797\n10.1016/j.chaos.2020.110027\n10.3390/ijerph17103730\n10.1155/2020/1846926\n10.1371/journal.pntd.0008280\n10.1136/bmj.m1091\n10.1016/S2213-2600(20)30120-X\n10.3390/s21062174\n10.1590/s1678-9946202062044\n10.1136/bmj.m1808\n10.1136/bmjresp-2018-000354\n10.1186/s12245-018-0170-2\n10.1016/j.jemermed.2021.01.041\n10.1590/0100-3984.2020.0051\n10.1093/cid/ciaa1408\n10.1016/j.pulmoe.2021.02.004\n10.1016/j.eng.2020.09.007\n10.1121/10.0002183\n10.1016/j.ultrasmedbio.2020.07.003\n10.1097/00000542-200401000-00006\n10.1016/j.cjca.2020.05.008\n10.2214/ajr.159.1.1609716\n10.1016/j.ultrasmedbio.2020.04.033\n10.1016/j.afjem.2020.04.007\n10.3389/fdata.2021.612561\n10.1109/TUFFC.2021.3068190\n10.1016/j.eng.2018.11.020\n10.1109/JPROC.2021.3054390\n10.1002/emp2.12018\n10.1590/0100-3984.2020.53.5e3\n10.1016/S2589-7500(19)30123-2\n10.1038/s41746-020-00376-2\n10.1016/j.scs.2020.102589\n10.1007/s12065-020-00540-3\n10.1155/2018/5137904\n10.1016/j.ibmed.2020.100013\n10.1016/j.media.2020.101913\n10.1016/j.asoc.2020.106912\n10.1109/ACCESS.2020.3016780\n10.2196/23811\n10.1016/j.chaos.2020.109947\n10.1016/j.chaos.2020.110338\n10.1016/j.inffus.2020.11.005\n10.1007/s10044-020-00950-0\n10.1159/000509763\n10.1002/14651858.CD013639.PUB4\n10.1007/BF02551274\n10.1148/radiol.2017171183\n10.1002/mp.13764\n10.1007/3-540-46805-6_19\n10.1109/CVPR.2017.369\n10.3390/s21155192\n10.1038/nature14539\n10.1016/j.patcog.2017.10.013\n10.1109/ICCSRE.2019.8807741\n10.1007/s10462-020-09825-6\n10.1109/CVPR.2016.90\n10.1109/CVPR.2016.308\n10.1109/CVPR.2017.195\n10.1038/323533a0\n10.1142/S0218488598000094\n10.1016/j.physd.2019.132306\n10.1162/neco.1997.9.8.1735\n10.1162/089976600300015015\n10.3115/v1/d14-1179\n10.1007/s11263-015-0816-y\n10.1049/iet-ipr.2019.0561\n10.1007/978-3-662-38527-2_55\n10.1007/978-3-642-25566-3_40\n10.21105/joss.00431\n10.1001/jama.2016.17216\n10.1038/nature21056\n10.1001/jama.2017.14585\n10.1007/978-3-030-01045-4_8\n10.1109/TUFFC.2020.3002249\n10.1016/j.ejmp.2021.02.023\n10.1016/j.compbiomed.2021.104296\n10.1109/TMI.2020.2994459\n10.1109/CCISP51026.2020.9273469\n10.1016/j.inffus.2021.02.013\n10.1136/bmjopen-2020-045120\n10.3390/app11020672\n10.1016/j.rcae.2015.04.008\n10.1145/3292500.3330701"}
{"title": "Precise Segmentation of COVID-19 Infected Lung from CT Images Based on Adaptive First-Order Appearance Model with Morphological/Anatomical Constraints.", "abstract": "A new segmentation technique is introduced for delineating the lung region in 3D computed tomography (CT) images. To accurately model the distribution of Hounsfield scale values within both chest and lung regions, a new probabilistic model is developed that depends on a linear combination of Gaussian (LCG). Moreover, we modified the conventional expectation-maximization (EM) algorithm to be run in a sequential way to estimate both the dominant Gaussian components (one for the lung region and one for the chest region) and the subdominant Gaussian components, which are used to refine the final estimated joint density. To estimate the marginal density from the mixed density, a modified k-means clustering approach is employed to classify the Gaussian subdominant components to determine which components belong properly to a lung and which components belong to a chest. The initial segmentation, based on the LCG-model, is then refined by the imposition of 3D morphological constraints based on a 3D Markov-Gibbs random field (MGRF) with analytically estimated potentials. The proposed approach was tested on CT data from 32 coronavirus disease 2019 (COVID-19) patients. Segmentation quality was quantitatively evaluated using four metrics: ", "journal": "Sensors (Basel, Switzerland)", "date": "2021-08-29", "authors": ["AhmedSharafeldeen", "MohamedElsharkawy", "Norah SalehAlghamdi", "AhmedSoliman", "AymanEl-Baz"], "doi": "10.3390/s21165482\n10.1007/s11481-020-09944-5\n10.1001/jamainternmed.2020.3596\n10.17762/turcomat.v12i2.1102\n10.1109/iembs.2007.4353317\n10.1016/j.acra.2006.02.039\n10.1016/S1076-6332(03)00380-5\n10.1109/42.929615\n10.1118/1.598898\n10.1109/42.650879\n10.1118/1.3147146\n10.1016/j.dsp.2014.09.002\n10.1155/2014/479154\n10.1109/TMI.2012.2219881\n10.1007/978-3-319-10404-1_100\n10.1016/j.patcog.2020.107747\n10.1118/1.3003066\n10.1118/1.3222872\n10.1016/j.compmedimag.2007.03.002\n10.1016/j.media.2012.08.002\n10.1109/TMI.2014.2337057\n10.1007/s00521-021-06273-3\n10.1155/2013/942353\n10.1186/s12880-020-00529-5\n10.1109/TPAMI.2016.2644615\n10.1007/978-3-319-24574-4_28\n10.1016/j.compbiomed.2020.104037\n10.1109/TMI.2020.2996645\n10.1016/j.knosys.2020.106647\n10.1109/TPAMI.2019.2938758\n10.1016/j.patcog.2021.108071\n10.1109/3dv.2016.79\n10.1148/radiol.2020200905\n10.1109/cvpr.2016.90\n10.3390/diagnostics11020158\n10.1002/mp.14609\n10.1038/s41598-020-80936-4\n10.1038/s41598-020-80261-w\n10.1002/mp.14676\n10.1016/j.media.2020.101693\n10.1109/isit.2004.1365067\n10.1007/3-540-45468-3_62\n10.1063/1.4825026\n10.1055/a-1388-8147\n10.1007/978-3-319-46723-8_49"}
{"title": "Radiomics-based machine learning differentiates \"ground-glass\" opacities due to COVID-19 from acute non-COVID-19 lung disease.", "abstract": "Ground-glass opacities (GGOs) are a non-specific high-resolution computed tomography (HRCT) finding tipically observed in early Coronavirus disesase 19 (COVID-19) pneumonia. However, GGOs are also seen in other acute lung diseases, thus making challenging the differential diagnosis. To this aim, we investigated the performance of a radiomics-based machine learning method to discriminate GGOs due to COVID-19 from those due to other acute lung diseases. Two sets of patients were included: a first set of 28 patients (COVID) diagnosed with COVID-19 infection confirmed by real-time polymerase chain reaction (RT-PCR) between March and April 2020 having (a) baseline HRCT at hospital admission and (b) predominant GGOs pattern on HRCT; a second set of 30 patients (nCOVID) showing (a) predominant GGOs pattern on HRCT performed between August 2019 and April 2020 and (b) availability of final diagnosis. Two readers independently segmented GGOs on HRCTs using a semi-automated approach, and radiomics features were extracted using a standard open source software (PyRadiomics). Partial least square (PLS) regression was used as the multivariate machine-learning algorithm. A leave-one-out nested cross-validation was implemented. PLS \u03b2-weights of radiomics features, including the 5% features with the largest \u03b2-weights in magnitude (top 5%), were obtained. The diagnostic performance of the radiomics model was assessed through receiver operating characteristic (ROC) analysis. The Youden's test assessed sensitivity and specificity of the classification. A null hypothesis probability threshold of 5% was chosen (p\u2009<\u20090.05). The predictive model delivered an AUC of 0.868 (Youden's index\u2009=\u20090.68, sensitivity\u2009=\u200993%, specificity 75%, p\u2009=\u20094.2\u2009\u00d7\u200910", "journal": "Scientific reports", "date": "2021-08-28", "authors": ["AndreaDelli Pizzi", "Antonio MariaChiarelli", "PieroChiacchiaretta", "CristinaValdesi", "PierpaoloCroce", "DomenicoMastrodicasa", "MichelaVillani", "StefanoTrebeschi", "Francesco LorenzoSerafini", "ConsueloRosa", "GiulioCocco", "RiccardoLuberti", "SabrinaConte", "LuciaMazzamurro", "ManuelaMereu", "Rosa LuciaPatea", "ValentinaPanara", "StefanoMarinari", "JacopoVecchiet", "MassimoCaulo"], "doi": "10.1038/s41598-021-96755-0\n10.4081/monaldi.2020.1298\n10.1016/j.jaut.2020.102433\n10.1111/joim.13091\n10.2214/AJR.20.22976\n10.3906/sag-2004-160\n10.1016/j.diii.2020.03.014\n10.1148/radiol.2462070712\n10.1371/journal.pone.0152505\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020202504\n10.1016/j.crad.2020.07.025\n10.1148/rg.2020200159\n10.2214/ajr.20.23391\n10.1148/radiol.2020201237\n10.1148/rg.236035101\n10.1016/j.ejrad.2020.109217\n10.1186/s13244-020-00933-z\n10.1148/radiol.2020200905\n10.2214/ajr.18.20624\n10.1038/s41598-021-86735-9\n10.1371/journal.pone.0246582\n10.1186/s12880-021-00564-w\n10.1016/j.ejrad.2021.109552\n10.1038/s41598-021-83237-6\n10.5152/dir.2019.19321\n10.1038/s41746-021-00431-6\n10.1038/s41467-020-18685-1\n10.1038/s41598-020-76141-y\n10.1002/mco2.14\n10.2214/ajr.169.2.9242736\n10.1148/radiol.2462070712\n10.2214/ajr.184.2.01840613\n10.2214/ajr.180.4.1800965\n10.1097/RCT.0b013e31820ccf18\n10.2214/ajr.175.5.1751329\n10.1006/cbmr.1996.0014\n10.1158/0008-5472.CAN-17-0339\n10.1007/s00330-020-07174-0\n10.1016/j.ijrobp.2018.05.053\n10.1038/s41598-021-84816-3\n10.1137/0905052\n10.1016/j.neuroimage.2013.07.021\n10.1142/s0129065720500677\n10.1162/089976699300016304\n10.1016/j.patcog.2015.11.015\n10.1007/s00261-019-02321-8\n10.21147/j.issn.1000-9604.2018.04.04\n10.1155/2018/6803971\n10.1136/thx.2008.101691\n10.1016/S1473-3099(20)30706-4\n10.1016/j.ejrad.2021.109602\n10.1007/s00330-020-07012-3"}
{"title": "Stress Echo 2030: The Novel ABCDE-(FGLPR) Protocol to Define the Future of Imaging.", "abstract": "With stress echo (SE) 2020 study, a new standard of practice in stress imaging was developed and disseminated: the ABCDE protocol for functional testing within and beyond CAD. ABCDE protocol was the fruit of SE 2020, and is the seed of SE 2030, which is articulated in 12 projects: 1-SE in coronary artery disease (SECAD); 2-SE in diastolic heart failure (SEDIA); 3-SE in hypertrophic cardiomyopathy (SEHCA); 4-SE post-chest radiotherapy and chemotherapy (SERA); 5-Artificial intelligence SE evaluation (AI-SEE); 6-Environmental stress echocardiography and air pollution (ESTER); 7-SE in repaired Tetralogy of Fallot (SETOF); 8-SE in post-COVID-19 (SECOV); 9: Recovery by stress echo of conventionally unfit donor good hearts (RESURGE); 10-SE for mitral ischemic regurgitation (SEMIR); 11-SE in valvular heart disease (SEVA); 12-SE for coronary vasospasm (SESPASM). The study aims to recruit in the next 5 years (2021-2025) \u226510,000 patients followed for \u22655 years (up to 2030) from \u226520 quality-controlled laboratories from \u226510 countries. In this COVID-19 era of sustainable health care delivery, SE2030 will provide the evidence to finally recommend SE as the optimal and versatile imaging modality for functional testing anywhere, any time, and in any patient.", "journal": "Journal of clinical medicine", "date": "2021-08-28", "authors": ["EugenioPicano", "QuirinoCiampi", "LauroCortigiani", "Adelaide MArruda-Olson", "ClarissaBorguezan-Daros", "Jos\u00e9 Luisde Castro E Silva Pretto", "RosangelaCocchia", "EduardoBossone", "ElisaMerli", "Garvan CKane", "AlbertVarga", "GergelyAgoston", "Maria ChiaraScali", "DoralisaMorrone", "IanaSimova", "MartinaSamardjieva", "AllaBoshchenko", "TamaraRyabova", "AlexanderVrublevsky", "AttilaPalinkas", "Eszter DPalinkas", "RobertSepp", "Marco A RTorres", "Hector RVillarraga", "Tamara Kova\u010devi\u0107Preradovi\u0107", "RodolfoCitro", "MiguelAmor", "HugoMosto", "MichaelSalam\u00e8", "PaulLeeson", "CristinaMangia", "NicolaGaibazzi", "DomenicoTuttolomondo", "CostantinaProta", "JesusPeteiro", "Caroline MVan De Heyning", "AntonelloD'Andrea", "FaustoRigo", "AleksandraNikolic", "MiodragOstojic", "JorgeLowenstein", "RosinaArbucci", "Diego M LowensteinHaber", "Pablo MMerlo", "KarinaWierzbowska-Drabik", "Jaroslaw DKasprzak", "MaciejHaberka", "Ana CristinaCamarozano", "NithimaRatanasit", "FabioMori", "Maria GraziaD'Alfonso", "LuigiTassetti", "AlessandraMilazzo", "IacopoOlivotto", "AlbertoMarchi", "HugoRodriguez-Zanella", "AngelaZagatina", "RatnasariPadang", "MilicaDekleva", "AnaDjordievic-Dikic", "NikolaBoskovic", "MiloradTesic", "VojislavGiga", "BrankoBeleslin", "GiovanniDi Salvo", "ValentinaLorenzoni", "MatteoCameli", "Giulia ElenaMandoli", "ToninoBombardini", "PioCaso", "JelenaCelutkiene", "AndreaBarbieri", "GiovanniBenfari", "YleniaBartolacelli", "AlessandroMalagoli", "FrancescaBursi", "FrancescaMantovani", "BrunoVillari", "AntonelloRusso", "MicheleDe Nes", "ClaraCarpeggiani", "InesMonte", "FedericaRe", "CarlosCotrim", "GiuseppeBilardo", "Ariel KSaad", "ArnasKaruzas", "DovydasMatuliauskas", "PaoloColonna", "FrancescoAntonini-Canterin", "MauroPepi", "Patricia APellikka", "NoneThe Stress Echo Study Group Of The Italian Society Of Echocardiography And Cardiovascular Imaging Siecvi"], "doi": "10.3390/jcm10163641\n10.1186/s12947-016-0092-1\n10.1186/s12947-018-0141-z\n10.1007/s10554-020-01789-6\n10.1016/j.echo.2013.02.003\n10.1016/j.jcmg.2020.06.001\n10.1016/j.ijcard.2017.09.172\n10.1093/eurheartj/ehn492\n10.1093/eurheartj/ehz425\n10.1016/j.echo.2019.07.001\n10.1016/j.jcmg.2020.04.020\n10.3390/jcm10132906\n10.1016/j.jacc.2019.08.1046\n10.1016/j.amjcard.2019.06.017\n10.1093/ehjci/jev222\n10.1093/eurheartj/eht350\n10.1016/j.echo.2016.10.016\n10.1093/eurheartj/ehw128\n10.1093/eurheartj/ehz641\n10.1161/CIRCULATIONAHA.118.034646\n10.1161/CIRCULATIONAHA.116.024822\n10.1016/j.jchf.2015.05.010\n10.1093/eurheartj/ehy531\n10.1016/j.jchf.2013.03.008\n10.1161/JAHA.115.002530\n10.1007/s10554-020-02071-5\n10.1093/ehjci/jez029\n10.1016/j.jacc.2010.04.040\n10.1161/CIRCIMAGING.117.006894\n10.1161/CIRCHEARTFAILURE.119.006769\n10.1002/ejhf.1604\n10.1093/eurheartj/ehu284\n10.1093/ehjci/jeu291\n10.1016/j.jacc.2020.08.045\n10.1016/j.echo.2011.11.005\n10.1016/j.amjcard.2008.08.023\n10.3390/jcm10071347\n10.1016/j.ijcard.2016.06.044\n10.1038/nrcardio.2016.140\n10.1093/ehjci/jet123\n10.1148/rg.2019180061\n10.1056/NEJMoa1209825\n10.1089/lrb.2014.0012\n10.1152/ajpheart.00124.2017\n10.1161/CIRCULATIONAHA.116.025434\n10.5114/wo.2014.40108\n10.1007/s11886-018-1010-y\n10.1016/j.jacc.2019.07.006\n10.1016/j.acvd.2016.10.003\n10.1016/j.echo.2014.07.012\n10.1002/ejhf.1920\n10.1016/j.annonc.2019.10.023\n10.1161/CIRCIMAGING.119.009727\n10.1080/17434440.2018.1497482\n10.1016/S0735-1097(10)80182-2\n10.1016/0735-1097(95)00483-1\n10.1053/euhj.1999.1541\n10.1016/j.jacc.2007.11.012\n10.1093/ehjci/jeu039\n10.1093/eurheartj/ehi408\n10.1016/j.jacc.2018.12.054\n10.1016/j.jcmg.2019.02.024\n10.1530/ERP-18-0056\n10.1093/eurheartj/ehz135\n10.1016/j.ijcard.2017.12.058\n10.1177/2047487320928450\n10.1093/eurheartj/ehaa409\n10.1093/eurheartj/ehaa411\n10.1093/eurheartj/ehaa962\n10.1161/01.CIR.0000027561.41736.3C\n10.3390/jcm8020274\n10.1093/eurheartj/ehu458\n10.1161/CIR.0000000000000931\n10.1016/j.jacc.2015.03.553\n10.1038/d41586-019-03809-5\n10.1007/s11356-021-13622-1\n10.1093/eurheartj/eht394\n10.1016/j.jacc.2018.02.016\n10.1371/journal.pone.0050168\n10.1016/j.icrp.2012.06.009\n10.1161/JAHA.117.007104\n10.1016/j.echo.2020.01.007\n10.1161/CIRCULATIONAHA.117.029138\n10.1016/j.ijcard.2015.05.080\n10.1093/ehjci/jev159\n10.1088/1361-6498/aba66d\n10.1016/j.echo.2018.08.008\n10.1093/ehjci/jet130\n10.1007/s00246-018-1962-0\n10.1007/s00246-004-0648-y\n10.1007/s10554-019-01753-z\n10.1093/ehjci/jeaa178\n10.3233/CH-200895\n10.1007/s12265-020-10031-6\n10.1016/j.jcmg.2020.06.004\n10.1016/j.hrtlng.2020.05.004\n10.4081/monaldi.2020.1358\n10.1016/j.healun.2010.05.034\n10.1093/ehjci/jev139\n10.1053/j.semtcvs.2019.05.010\n10.1016/j.healun.2009.05.029\n10.1016/j.echo.2010.11.014\n10.1186/1476-7120-12-20\n10.1186/1476-7120-11-27\n10.1016/j.transproceed.2015.12.036\n10.1016/j.echo.2011.02.006\n10.1016/j.acvd.2017.12.001\n10.23736/S0026-4725.20.05093-8\n10.1016/S0002-9149(00)01119-X\n10.1161/01.CIR.0000151097.30779.04\n10.1093/eurheartj/ehx391\n10.1161/CIR.0000000000000503\n10.1161/01.CIR.0000087599.49332.05\n10.1161/JAHA.119.012212\n10.1016/j.jacc.2017.12.017\n10.1016/j.jacc.2019.02.003\n10.1111/echo.14804\n10.1016/j.echo.2017.04.005\n10.1093/eurheartj/ehu152\n10.1016/j.ijcard.2017.08.068\n10.1016/0002-8703(84)90406-X\n10.1016/j.jcmg.2020.03.008\n10.1161/01.cir.95.1.265\n10.1161/CIRCEP.110.959809\n10.1016/j.hrthm.2017.10.035\n10.1016/j.jacc.2018.05.051\n10.1093/eurheartj/14.8.1088\n10.1378/chest.119.1.155\n10.1161/01.CIR.74.6.1255\n10.1253/circj.CJ-66-0098\n10.1093/eurheartj/ehv351\n10.1136/bcr-2019-229766\n10.1080/AC.71.4.3159696\n10.1016/0002-9149(88)91204-0\n10.1093/ehjci/ehaa946.0018\n10.1093/eurheartj/ehab493\n10.3390/jcm10143020"}
{"title": "The Applications of Artificial Intelligence in Chest Imaging of COVID-19 Patients: A Literature Review.", "abstract": "Diagnostic imaging is regarded as fundamental in the clinical work-up of patients with a suspected or confirmed COVID-19 infection. Recent progress has been made in diagnostic imaging with the integration of artificial intelligence (AI) and machine learning (ML) algorisms leading to an increase in the accuracy of exam interpretation and to the extraction of prognostic information useful in the decision-making process. Considering the ever expanding imaging data generated amid this pandemic, COVID-19 has catalyzed the rapid expansion in the application of AI to combat disease. In this context, many recent studies have explored the role of AI in each of the presumed applications for COVID-19 infection chest imaging, suggesting that implementing AI applications for chest imaging can be a great asset for fast and precise disease screening, identification and characterization. However, various biases should be overcome in the development of further ML-based algorithms to give them sufficient robustness and reproducibility for their integration into clinical practice. As a result, in this literature review, we will focus on the application of AI in chest imaging, in particular, deep learning, radiomics and advanced imaging as quantitative CT.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-08-28", "authors": ["Maria ElenaLaino", "AngelaAmmirabile", "AlessandroPosa", "PierandreaCancian", "SherifShalaby", "VictorSavevski", "EmanueleNeri"], "doi": "10.3390/diagnostics11081317\n10.1148/radiol.2020204267\n10.1186/s13244-021-00962-2\n10.1148/radiol.2020203173\n10.1016/j.clinimag.2020.04.001\n10.1186/s12967-020-02324-w\n10.1016/j.jacr.2020.03.006\n10.1002/jmv.25822\n10.26355/eurrev_202003_20549\n10.1016/j.radi.2020.04.005\n10.1002/jum.15284\n10.1007/s11547-020-01135-9\n10.1007/s11547-020-01197-9\n10.1007/s11547-020-01305-9\n10.1007/s11547-021-01389-x\n10.1590/0100-3984.2019.0049\n10.1155/2021/6677314\n10.1109/RBME.2020.2987975\n10.1148/ryct.2020200082\n10.1148/ryct.2020200075\n10.21037/atm-20-3026\n10.1148/ryct.2020200044\n10.1002/mp.14609\n10.1109/TKDE.2009.191\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3010287\n10.1016/j.imu.2020.100412\n10.1016/j.asoc.2020.106580\n10.1109/TMI.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105532\n10.1038/s41598-020-76550-z\n10.3348/kjr.2020.0132\n10.1148/radiol.2020201160\n10.1016/j.ejro.2020.100231\n10.12788/fp.0045\n10.1007/s13755-020-00119-3\n10.1016/j.chaos.2020.110122\n10.1016/j.chaos.2020.110245\n10.1016/j.mehy.2020.109577\n10.1016/j.compbiomed.2020.103805\n10.1007/s42600-021-00132-9\n10.1007/s00138-020-01128-8\n10.1007/s13755-020-00116-6\n10.1016/j.chaos.2020.110170\n10.1038/s41598-020-71294-2\n10.11604/pamj.supp.2020.35.2.24258\n10.1148/radiol.2020201874\n10.7150/ijbs.53982\n10.1007/s00330-020-06863-0\n10.1148/radiol.2020201754\n10.1016/j.acra.2021.01.016\n10.1148/ryai.2020200079\n10.1007/s00330-020-07269-8\n10.1371/journal.pone.0236621\n10.1016/j.ins.2020.09.041\n10.1148/radiol.2020200823\n10.1016/j.compbiomed.2021.104252\n10.1155/2020/8889023\n10.3892/etm.2020.8797\n10.3348/kjr.2020.0146\n10.1109/RBME.2020.2990959\n10.1016/j.ejrad.2020.109233\n10.21037/atm.2020.03.132\n10.1038/s41467-020-17971-2\n10.1007/s00330-020-07044-9\n10.1038/s41598-020-76282-0\n10.1007/s00330-020-06801-0\n10.1007/s00259-020-04953-1\n10.1007/s10140-020-01856-4\n10.21037/qims-20-531\n10.1148/radiol.2020201473\n10.1148/radiol.2020202439\n10.1186/s12967-020-02692-3\n10.1007/s00330-020-07032-z\n10.7150/ijms.48432\n10.1101/2020.05.08.20094664\n10.1016/j.cmpb.2021.106004\n10.1186/s12880-020-00529-5\n10.1007/s10044-020-00950-0\n10.1007/s10489-020-01943-6\n10.1016/j.ejrad.2020.109402\n10.1038/s41591-020-0931-3\n10.1007/s10140-020-01821-1\n10.1007/s00330-020-07033-y\n10.2214/AJR.20.22976\n10.1097/RLI.0000000000000672\n10.1164/rccm.201908-1581ST\n10.3390/ijerph18062842\n10.1016/j.media.2020.101824\n10.1183/13993003.00775-2020\n10.1109/JBHI.2020.3034296\n10.2196/21604\n10.2196/24973\n10.21037/jtd-20-1584\n10.1007/s00330-020-07042-x\n10.1371/journal.pone.0236858\n10.1038/s41598-020-80261-w\n10.1016/j.jrid.2020.04.004\n10.1007/s00330-020-07271-0\n10.1016/j.ejro.2020.100272\n10.1007/s00330-020-07013-2\n10.24875/RIC.20000451\n10.3390/jcm9051514\n10.7150/thno.45985\n10.1016/j.accpm.2020.10.014\n10.5152/dir.2020.20407\n10.21037/atm-20-3554\n10.1007/s12539-020-00410-7\n10.1097/RCT.0000000000001094\n10.1097/RTI.0000000000000544\n10.1259/bjr.20200634\n10.7150/thno.46428\n10.1109/JBHI.2020.3036722\n10.1186/s12880-020-00521-z\n10.1016/j.ijid.2020.03.017\n10.1007/s00592-020-01654-x\n10.18632/aging.103000\n10.1186/s40001-020-00450-1\n10.1007/s00259-020-04929-1\n10.1016/j.chaos.2020.110153\n10.1186/s12938-020-00809-9\n10.3233/XST-200735\n10.1148/radiol.2020201491\n10.1148/radiol.2020200905\n10.1007/s00330-020-07087-y\n10.1002/mco2.14\n10.1038/s42256-021-00307-0\n10.1007/s10479-021-04006-2\n10.1109/ACCESS.2021.3085418\n10.1109/TCBB.2021.3066331\n10.1007/s12553-021-00520-2"}
{"title": "Microscopic segmentation and classification of COVID-19 infection with ensemble convolutional neural network.", "abstract": "The detection of biological RNA from sputum has a comparatively poor positive rate in the initial/early stages of discovering COVID-19, as per the World Health Organization. It has a different morphological structure as compared to healthy images, manifested by computer tomography (CT). COVID-19 diagnosis at an early stage can aid in the timely cure of patients, lowering the mortality rate. In this reported research, three-phase model is proposed for COVID-19 detection. In Phase I, noise is removed from CT images using a denoise convolutional neural network (DnCNN). In the Phase II, the actual lesion region is segmented from the enhanced CT images by using deeplabv3 and ResNet-18. In Phase III, segmented images are passed to the stack sparse autoencoder (SSAE) deep learning model having two stack auto-encoders (SAE) with the selected hidden layers. The designed SSAE model is based on both SAE and softmax layers for COVID19 classification. The proposed method is evaluated on actual patient data of Pakistan Ordinance Factories and other public benchmark data sets with different scanners/mediums. The proposed method achieved global segmentation accuracy of 0.96 and 0.97 for classification.", "journal": "Microscopy research and technique", "date": "2021-08-27", "authors": ["JaveriaAmin", "Muhammad AlmasAnjum", "MuhammadSharif", "AmjadRehman", "TanzilaSaba", "RidaZahra"], "doi": "10.1002/jemt.23913\n10.1007/s11042-019-7324-y\n10.1109/ACCESS.2020.3016627\n10.1002/jemt\n10.1109/MITP.2020.3036820\n10.1109/MITP.2020.3042379\n10.1002/jemt.23326\n10.1002/jemt.23702"}
{"title": "Detection of COVID-19 from chest x-ray images using transfer learning.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-08-27", "authors": ["JenitaManokaran", "FatemehZabihollahy", "AndrewHamilton-Wright", "ErangaUkwatta"], "doi": "10.1117/1.JMI.8.S1.017503\n10.1063/5.0015626\n10.1001/jama.2020.3786\n10.1016/j.measurement.2019.05.076\n10.1148/radiol.2019194005\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2995965\n10.2196/19569\n10.1016/j.ajem.2012.08.041\n10.1016/j.clinimag.2020.04.001\n10.1109/ACCESS.2018.2810849\n10.3233/JIFS-190861\n10.1007/s11045-019-00686-z\n10.1016/j.compbiomed.2020.103792\n10.3389/fmed.2020.00427\n10.1007/s00330-020-07044-9\n10.1038/s41598-020-76282-0\n10.1007/s13246-020-00865-4\n10.33889/IJMEMS.2020.5.4.02\n10.1016/j.eswa.2020.114054\n10.1038/s41598-020-76550-z\n10.1016/j.chaos.2020.109944\n10.1016/j.bbe.2020.08.008\n10.1007/s10044-021-00984-y\n10.1080/07391102.2020.1767212\n10.1117/12.2581314\n10.1007/978-3-642-35289-8_26\n10.1007/978-3-030-50420-5_47\n10.3390/jimaging6120131"}
{"title": "PAM-DenseNet: A Deep Convolutional Neural Network for Computer-Aided COVID-19 Diagnosis.", "abstract": "Currently, several convolutional neural network (CNN)-based methods have been proposed for computer-aided COVID-19 diagnosis based on lung computed tomography (CT) scans. However, the lesions of pneumonia in CT scans have wide variations in appearances, sizes, and locations in the lung regions, and the manifestations of COVID-19 in CT scans are also similar to other types of viral pneumonia, which hinders the further improvement of CNN-based methods. Delineating infection regions manually is a solution to this issue, while excessive workload of physicians during the epidemic makes it difficult for manual delineation. In this article, we propose a CNN called dense connectivity network with parallel attention module (PAM-DenseNet), which can perform well on coarse labels without manually delineated infection regions. The parallel attention module automatically learns to strengthen informative features from both channelwise and spatialwise simultaneously, which can make the network pay more attention to the infection regions without any manual delineation. The dense connectivity structure performs feature maps reuse by introducing direct connections from previous layers to all subsequent layers, which can extract representative features from fewer CT slices. The proposed network is first trained on 3530 lung CT slices selected from 382 COVID-19 lung CT scans, 372 lung CT scans infected by other pneumonia, and 200 normal lung CT scans to obtain a pretrained model for slicewise prediction. We then apply this pretrained model to a CT scans dataset containing 94 COVID-19 CT scans, 93 other pneumonia CT scans, and 93 normal lung scans, and achieve patientwise prediction through a voting mechanism. The experimental results show that the proposed network achieves promising results with an accuracy of 94.29%, a precision of 93.75%, a sensitivity of 95.74%, and a specificity of 96.77%, which is comparable to the methods that are based on manually delineated infection regions.", "journal": "IEEE transactions on cybernetics", "date": "2021-08-25", "authors": ["BinXiao", "ZeyuYang", "XiaomingQiu", "JingjingXiao", "GuoyinWang", "WenbingZeng", "WeishengLi", "YongjianNian", "WeiChen"], "doi": "10.1109/TCYB.2020.3042837\n10.1109/JIOT.2020.3012452\n10.1109/TMM.2020.3016122\n10.1109/TCYB.2020.2985398\n10.1101/2020.02.23.20026930\n10.1101/2020.02.25.20021568\n10.1101/2020.02.14.20023028\n10.1101/2020.03.12.20027185\n10.1101/2020.03.20.20039834\n10.1101/2020.03.19.20039354"}
{"title": "Application of deep learning to identify COVID-19 infection in posteroanterior chest X-rays.", "abstract": "The objective of this study was to assess seven configurations of six convolutional deep neural network architectures for classification of chest X-rays (CXRs) as COVID-19 positive or negative.\nThe primary dataset consisted of 294 COVID-19 positive and 294 COVID-19 negative CXRs, the latter comprising roughly equally many pneumonia, emphysema, fibrosis, and healthy images. We used six common convolutional neural network architectures, VGG16, DenseNet121, DenseNet201, MobileNet, NasNetMobile and InceptionV3. We studied six models (one for each architecture) which were pre-trained on a vast repository of generic (non-CXR) images, as well as a seventh DenseNet121 model, which was pre-trained on a repository of CXR images. For each model, we replaced the output layers with custom fully connected layers for the task of binary classification of images as COVID-19 positive or negative. Performance metrics were calculated on a hold-out test set with CXRs from patients who were not included in the training/validation set.\nWhen pre-trained on generic images, the VGG16, DenseNet121, DenseNet201, MobileNet, NasNetMobile, and InceptionV3 architectures respectively produced hold-out test set areas under the receiver operating characteristic (AUROCs) of 0.98, 0.95, 0.97, 0.95, 0.99, and 0.96 for the COVID-19 classification of CXRs. The X-ray pre-trained DenseNet121 model, in comparison, had a test set AUROC of 0.87.\nCommon convolutional neural network architectures with parameters pre-trained on generic images yield high-performance and well-calibrated COVID-19 CXR classification.", "journal": "Clinical imaging", "date": "2021-08-24", "authors": ["JenishMaharjan", "JacobCalvert", "EmilyPellegrini", "AbigailGreen-Saxena", "JanaHoffman", "AndreaMcCoy", "QingqingMao", "RitankarDas"], "doi": "10.1016/j.clinimag.2021.07.004\n10.1016/j.molmed.2020.02.008\n10.1001/jama.2020.2565\n10.1007/s11427-020-1661-4\n10.2807/1560-7917.ES.2020.25.10.2000180\n10.1148/radiol.2020200642\n10.1097/RLI.0000000000000670\n10.1148/radiol.2020200490\n10.3760/cma.j.issn.1005-1201.2020.0001\n10.1109/TMI.2020.2995965\n10.1148/radiol.2020200241\n10.1148/ryct.2020200034\n10.1148/radiol.2020200236\n10.1148/radiol.2020200230\n10.1016/j.patcog.2020.107613\n10.1038/s41598-020-76550-z\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.3389/fmed.2020.00427\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2020.103805\n10.1007/s10916-020-01562-1\n10.1007/s13246-020-00888-x\n10.1007/s12559-020-09775-9\n10.1007/s10489-020-01943-6\n10.1109/JBHI.2021.3069169\n10.1145/3431804\n10.1016/j.media.2021.102046\n10.1097/RTI.0000000000000347\n10.1007/s12098-020-03263-6\n10.1016/j.asoc.2020.106897\n10.1007/s13755-020-00119-3\n10.3390/ijerph17186933\n10.1016/j.irbm.2019.10.006\n10.20944/preprints202003.0300.v1\n10.1101/2020.03.19.20039354\n10.1101/2020.02.23.20026930\n10.1101/2020.03.26.20044610\n10.1007/s13755-020-00116-6"}
{"title": "COVID-19 lung infection segmentation with a novel two-stage cross-domain transfer learning framework.", "abstract": "With the global outbreak of COVID-19 in early 2020, rapid diagnosis of COVID-19 has become the urgent need to control the spread of the epidemic. In clinical settings, lung infection segmentation from computed tomography (CT) images can provide vital information for the quantification and diagnosis of COVID-19. However, accurate infection segmentation is a challenging task due to (i) the low boundary contrast between infections and the surroundings, (ii) large variations of infection regions, and, most importantly, (iii) the shortage of large-scale annotated data. To address these issues, we propose a novel two-stage cross-domain transfer learning framework for the accurate segmentation of COVID-19 lung infections from CT images. Our framework consists of two major technical innovations, including an effective infection segmentation deep learning model, called nCoVSegNet, and a novel two-stage transfer learning strategy. Specifically, our nCoVSegNet\u00a0conducts effective infection segmentation by taking advantage of attention-aware feature fusion and large receptive fields, aiming to resolve the issues related to low boundary contrast and large infection variations. To alleviate the shortage of the data, the nCoVSegNet\u00a0is pre-trained using a two-stage cross-domain transfer learning strategy, which makes full use of the knowledge from natural images (i.e., ImageNet) and medical images (i.e., LIDC-IDRI) to boost the final training on CT images with COVID-19 infections. Extensive experiments demonstrate that our framework achieves superior segmentation accuracy and outperforms the cutting-edge models, both quantitatively and qualitatively.", "journal": "Medical image analysis", "date": "2021-08-24", "authors": ["JiannanLiu", "BoDong", "ShuaiWang", "HuiCui", "Deng-PingFan", "JiquanMa", "GengChen"], "doi": "10.1016/j.media.2021.102205"}
{"title": "Deep transfer learning for COVID-19 detection and infection localization with superpixel based segmentation.", "abstract": "The evolution the novel corona virus disease (COVID-19) as a pandemic has inflicted several thousand deaths per day endangering the lives of millions of people across the globe. In addition to thermal scanning mechanisms, chest imaging examinations provide valuable insights to the detection of this virus, diagnosis and prognosis of the infections. Though Chest CT and Chest X-ray imaging are common in the clinical protocols of COVID-19 management, the latter is highly preferred, attributed to its simple image acquisition procedure and mobility of the imaging mechanism. However, Chest X-ray images are found to be less sensitive compared to Chest CT images in detecting infections in the early stages. In this paper, we propose a deep learning based framework to enhance the diagnostic values of these images for improved clinical outcomes. It is realized as a variant of the conventional SqueezeNet classifier with segmentation capabilities, which is trained with deep features extracted from the Chest X-ray images of a standard dataset for binary and multi class classification. The binary classifier achieves an accuracy of 99.53% in the discrimination of COVID-19 and Non COVID-19 images. Similarly, the multi class classifier performs classification of COVID-19, Viral Pneumonia and Normal cases with an accuracy of 99.79%. This model called the COVID-19 Super pixel SqueezNet (COVID-SSNet) performs super pixel segmentation of the activation maps to extract the regions of interest which carry perceptual image features and constructs an overlay of the Chest X-ray images with these regions. The proposed classifier model adds significant value to the Chest X-rays for an integral examination of the image features and the image regions influencing the classifier decisions to expedite the COVID-19 treatment regimen.", "journal": "Sustainable cities and society", "date": "2021-08-24", "authors": ["N BPrakash", "MMurugappan", "G RHemalakshmi", "MJayalakshmi", "MuftiMahmud"], "doi": "10.1016/j.scs.2021.103252\n10.1007/s12559-020-09774-w"}
{"title": "Caution, \"normal\" BMI: health risks associated with potentially masked individual underweight-EPMA Position Paper 2021.", "abstract": "An increasing interest in a healthy lifestyle raises questions about optimal body weight. Evidently, it should be clearly discriminated between the standardised \"normal\" body weight and individually optimal weight. To this end, the basic principle of personalised medicine \"one size does not fit all\" has to be applied. Contextually, \"normal\" but e.g. borderline body mass index might be optimal for one person but apparently suboptimal for another one strongly depending on the individual genetic predisposition, geographic origin, cultural and nutritional habits and relevant lifestyle parameters-all included into comprehensive individual patient profile. Even if only slightly deviant, both overweight and underweight are acknowledged risk factors for a shifted metabolism which, if being not optimised, may strongly contribute to the development and progression of severe pathologies. Development of innovative screening programmes is essential to promote population health by application of health risks assessment, individualised patient profiling and multi-parametric analysis, further used for cost-effective targeted prevention and treatments tailored to the person. The following healthcare areas are considered to be potentially strongly benefiting from the above proposed measures: suboptimal health conditions,\u00a0sports medicine, stress overload and associated complications, planned pregnancies, periodontal health and dentistry, sleep medicine, eye health and disorders, inflammatory disorders, healing and pain management, metabolic disorders, cardiovascular disease, cancers, psychiatric and neurologic disorders, stroke of known and unknown aetiology, improved individual and population outcomes under pandemic conditions such as COVID-19. In a long-term way, a significantly improved healthcare economy is one of benefits of the proposed paradigm shift from reactive to Predictive, Preventive and Personalised Medicine (PPPM/3PM). A tight collaboration between all stakeholders including scientific community, healthcare givers, patient organisations, policy-makers and educators is essential for the smooth implementation of 3PM concepts in daily practice.", "journal": "The EPMA journal", "date": "2021-08-24", "authors": ["OlgaGolubnitschaja", "AlenaLiskova", "LenkaKoklesova", "MarekSamec", "KamilBiringer", "DietrichB\u00fcsselberg", "HalinaPodbielska", "Anatolij AKunin", "Maria EEvsevyeva", "NivaShapira", "FriedemannPaul", "CarlErb", "Detlef EDietrich", "DieterFelbel", "AlexanderKarabatsiakis", "RostyslavBubnov", "JiriPolivka", "JiriPolivka", "ColinBirkenbihl", "HolgerFr\u00f6hlich", "MartinHofmann-Apitius", "PeterKubatka"], "doi": "10.1007/s13167-021-00251-4\n10.1186/1471-2458-14-806\n10.1017/S1368980013002541\n10.1136/bmjopen-2017-018241\n10.1016/S2213-8587(18)30288-2\n10.1007/s13167-020-00214-1\n10.1186/1878-5085-5-11\n10.1007/s13167-018-0127-9\n10.1007/s13167-017-0086-6\n10.1007/s13167-018-0131-0\n10.1007/s13167-018-0145-7\n10.1007/s13167-019-00162-5\n10.1007/s13167-019-00164-3\n10.1007/s13167-020-00217-y\n10.1007/s13167-020-00229-8\n10.1016/S1470-2045(04)01597-9\n10.1007/s13167-019-00194-x\n10.1007/s13167-017-0081-y\n10.1038/s41416-021-01309-w\n10.1371/journal.pone.0175125\n10.1007/BF02982710\n10.1371/journal.pone.0068660\n10.1016/j.clnu.2016.09.032\n10.1093/ageing/afq073\n10.1016/j.ygyno.2020.12.009\n10.1002/jcsm.12398\n10.1046/j.1467-789X.2002.00065.x\n10.1016/j.puhe.2014.06.019\n10.1111/dom.13466\n10.1016/j.ypmed.2008.03.010\n10.7150/jca.38567\n10.1007/s10549-018-05091-x\n10.1007/s13167-017-0092-8\n10.1016/j.jprot.2017.07.017\n10.1016/j.artd.2016.03.005\n10.1111/j.1743-6109.2006.00100.x\n10.1016/j.metabol.2020.154229\n10.5935/1518-0557.20180021\n10.1023/a:1026477628723\n10.1093/humrep/den017\n10.1007/s10875-018-0492-0\n10.1093/infdis/jix241\n10.1097/PEC.0b013e3182a21a23\n10.1017/S0033291714000142\n10.1055/s-0041-111802\n10.1093/eurheartj/ehv423\n10.1371/journal.pmed.1001998\n10.1016/j.amjcard.2015.02.024\n10.1016/j.pcad.2016.01.008\n10.1002/ehf2.12120\n10.35366/99145\n10.1016/j.jacc.2017.03.558\n10.1002/mas.21612\n10.1136/annrheumdis-2019-eular.5533\n10.1136/annrheumdis-2020-eular.1962\n10.1111/j.1524-475X.2009.00543.x\n10.1093/aje/kwu111\n10.1177/1941738119854763\n10.1097/BCO.0000000000000524\n10.1016/j.jor.2018.02.016\n10.1002/pbc.23129\n10.1371/journal.pone.0195118\n10.1016/j.jtho.2019.05.031\n10.1055/s-0035-1554964\n10.3892/mco.2016.964\n10.1158/1055-9965.EPI-15-1336\n10.1186/s12885-016-2891-z\n10.1007/s13167-017-0089-3\n10.1089/jwh.2019.7739\n10.1007/s13167-020-00206-1\n10.1186/s12884-020-03509-3\n10.1111/irv.12618\n10.1093/cid/ciq144\n10.1093/aje/kwv300\n10.1519/JSC.0000000000002449\n10.1093/ajcn/77.4.857\n10.1038/sj.ijo.0800620\n10.1007/s11940-009-0046-0\n10.1089/jop.2012.0198\n10.1038/nature11234\n10.1038/ctg.2015.16\n10.1038/nrneph.2015.191\n10.1186/s13167-015-0036-0\n10.3920/BM2016.0222\n10.1126/science.aad9379\n10.1186/s12967-017-1175-y\n10.1186/1750-1326-9-36\n10.1016/j.trsl.2016.08.002\n10.1136/gutjnl-2013-306541\n10.1016/j.cell.2016.11.003\n10.1007/s13167-018-0132-z\n10.1007/s00787-017-0945-7\n10.1016/j.clnu.2013.11.006\n10.1042/cs0730197\n10.1016/j.psyneuen.2014.09.031\n10.1016/S0306-4530(02)00021-5\n10.1007/s13167-017-0094-6\n10.4274/tnd.37132\n10.1016/j.biopha.2018.03.059\n10.1111/head.13217\n10.1007/s40279-014-0147-0\n10.1016/S1087-0792(02)00122-3\n10.1002/erv.2461\n10.3390/nu12040936\n10.1016/j.burns.2016.08.030\n10.1002/oby.21160\n10.1016/j.puhe.2018.02.027\n10.1016/j.diabet.2016.05.009\n10.3390/healthcare6030073\n10.1186/1878-5085-3-14\n10.1186/1878-5085-4-12\n10.1186/1878-5085-5-6\n10.1186/s13167-015-0026-2\n10.1007/s13167-020-00216-z\n10.1038/s41598-020-76200-4\n10.1186/s40303-015-0007-3\n10.1038/s41598-018-21763-6\n10.1007/s13167-019-00188-9\n10.1007/s13167-020-00221-2\n10.1002/trc2.12102\n10.1186/s12916-018-1122-7\n10.1016/j.mayocp.2013.07.001"}
{"title": "Computer-aided detection of COVID-19 from CT scans using an ensemble of CNNs and KSVM classifier.", "abstract": "Corona Virus Disease-2019 (COVID-19) is a global pandemic which is spreading briskly across the globe. The gold standard for the diagnosis of COVID-19 is viral nucleic acid detection with real-time polymerase chain reaction (RT-PCR). However, the sensitivity of RT-PCR in the diagnosis of early-stage COVID-19 is less. Recent research works have shown that computed tomography (CT) scans of the chest are effective for the early diagnosis of COVID-19. Convolutional neural networks (CNNs) are proven successful for diagnosing various lung diseases from CT scans. CNNs are composed of multiple layers which represent a hierarchy of features at each level. CNNs require a big number of labeled instances for training from scratch. In medical imaging tasks like the detection of COVID-19 where there is a difficulty in acquiring a large number of labeled CT scans, pre-trained CNNs trained on a huge number of natural images can be employed for extracting features. Feature representation of each CNN varies and an ensemble of features generated from various pre-trained CNNs can increase the diagnosis capability significantly. In this paper, features extracted from an ensemble of 5 different CNNs (MobilenetV2, Shufflenet, Xception, Darknet53 and EfficientnetB0) in combination with kernel support vector machine is used for the diagnosis of COVID-19 from CT scans. The method was tested using a public dataset and it attained an area under the receiver operating characteristic curve of 0.963, accuracy of 0.916, kappa score of 0.8305, F-score of 0.91, sensitivity of 0.917 and positive predictive value of 0.904 in the prediction of COVID-19.", "journal": "Signal, image and video processing", "date": "2021-08-24", "authors": ["BejoyAbraham", "Madhu SNair"], "doi": "10.1007/s11760-021-01991-6\n10.3233/JIFS-169913\n10.1016/j.bbe.2020.08.005\n10.1145/1961189.1961199\n10.1148/radiol.2020200432\n10.1183/09031936.00047908\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2995508\n10.3389/fphy.2020.00127\n10.1007/s00330-020-06956-w\n10.1016/j.ejrad.2020.109041\n10.1016/j.eng.2020.04.010"}
{"title": "Exploiting Shared Knowledge From Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation.", "abstract": "The novel Coronavirus disease (COVID-19) is a highly contagious virus and has spread all over the world, posing an extremely serious threat to all countries. Automatic lung infection segmentation from computed tomography (CT) plays an important role in the quantitative analysis of COVID-19. However, the major challenge lies in the inadequacy of annotated COVID-19 datasets. Currently, there are several public non-COVID lung lesion segmentation datasets, providing the potential for generalizing useful information to the related COVID-19 segmentation task. In this paper, we propose a novel relation-driven collaborative learning model to exploit shared knowledge from non-COVID lesions for annotation-efficient COVID-19 CT lung infection segmentation. The model consists of a general encoder to capture general lung lesion features based on multiple non-COVID lesions, and a target encoder to focus on task-specific features based on COVID-19 infections. We develop a collaborative learning scheme to regularize feature-level relation consistency of given input and encourage the model to learn more general and discriminative representation of COVID-19 infections. Extensive experiments demonstrate that trained with limited COVID-19 data, exploiting shared knowledge from non-COVID lesions can further improve state-of-the-art performance with up to 3.0% in dice similarity coefficient and 4.2% in normalized surface dice. In addition, experimental results on large scale 2D dataset with CT slices show that our method significantly outperforms cutting-edge segmentation methods metrics. Our method promotes new insights into annotation-efficient deep learning and illustrates strong potential for real-world applications in the global fight against COVID-19 in the absence of sufficient high-quality annotations.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-08-21", "authors": ["YichiZhang", "QingchengLiao", "LinYuan", "HeZhu", "JiezhenXing", "JicongZhang"], "doi": "10.1109/JBHI.2021.3106341\n10.1109/TPAMI.2021.3100536\n10.21203/rs.3.rs-571332/v1"}
{"title": "An explainable AI system for automated COVID-19 assessment and lesion categorization from CT-scans.", "abstract": "COVID-19 infection caused by SARS-CoV-2 pathogen has been a catastrophic pandemic outbreak all over the world, with exponential increasing of confirmed cases and, unfortunately, deaths. In this work we propose an AI-powered pipeline, based on the deep-learning paradigm, for automated COVID-19 detection and lesion categorization from CT scans. We first propose a new segmentation module aimed at automatically identifying lung parenchyma and lobes. Next, we combine the segmentation network with classification networks for COVID-19 identification and lesion categorization. We compare the model's classification results with those obtained by three expert radiologists on a dataset of 166 CT scans. Results showed a sensitivity of 90.3% and a specificity of 93.5% for COVID-19 detection, at least on par with those yielded by the expert radiologists, and an average lesion categorization accuracy of about 84%. Moreover, a significant role is played by prior lung and lobe segmentation, that allowed us to enhance classification performance by over 6 percent points. The interpretation of the trained AI models reveals that the most significant areas for supporting the decision on COVID-19 identification are consistent with the lesions clinically associated to the virus, i.e., crazy paving, consolidation and ground glass. This means that the artificial models are able to discriminate a positive patient from a negative one (both controls and patients with interstitial pneumonia tested negative to COVID) by evaluating the presence of those lesions into CT scans. Finally, the AI models are integrated into a user-friendly GUI to support AI explainability for radiologists, which is publicly available at http://perceivelab.com/covid-ai. The whole AI system is unique since, to the best of our knowledge, it is the first AI-based software, publicly available, that attempts to explain to radiologists what information is used by AI methods for making decisions and that proactively involves them in the decision loop to further improve the COVID-19 understanding.", "journal": "Artificial intelligence in medicine", "date": "2021-08-21", "authors": ["MatteoPennisi", "IsaakKavasidis", "ConcettoSpampinato", "VincenzoSchinina", "SimonePalazzo", "Federica ProiettoSalanitri", "GiovanniBellitto", "FrancescoRundo", "MarcoAldinucci", "MassimoCristofaro", "PaoloCampioni", "ElisaPianura", "FedericaDi Stefano", "AdaPetrone", "FabrizioAlbarello", "GiuseppeIppolito", "SalvatoreCuzzocrea", "SabrinaConoci"], "doi": "10.1016/j.artmed.2021.102114\n10.1109/TCYB.2020.2990162\n10.1118/1.3528204\n10.1145/3203217.3205340"}
{"title": "Quantitative CT for detecting COVID\u201119 pneumonia in suspected cases.", "abstract": "Corona Virus Disease 2019 (COVID-19) is currently a worldwide pandemic and has a huge impact on public health and socio-economic development. The purpose of this study is to explore the diagnostic value of the quantitative computed tomography (CT) method by using different threshold segmentation techniques to distinguish between patients with or without COVID-19 pneumonia.\nA total of 47 patients with suspected COVID-19 were retrospectively analyzed, including nine patients with positive real-time fluorescence reverse transcription polymerase chain reaction (RT-PCR) test (confirmed case group) and 38 patients with negative RT-PCR test (excluded case group). An improved 3D convolutional neural network (VB-Net) was used to automatically extract lung lesions. Eight different threshold segmentation methods were used to define the ground glass opacity (GGO) and consolidation. The receiver operating characteristic (ROC) curves were used to compare the performance of various parameters with different thresholds for diagnosing COVID-19 pneumonia.\nThe volume of GGO (VOGGO) and GGO percentage in the whole lung (GGOPITWL) were the most effective values for diagnosing COVID-19 at a threshold of -\u2009300 HU, with areas under the curve (AUCs) of 0.769 and 0.769, sensitivity of 66.67 and 66.67%, specificity of 94.74 and 86.84%. Compared with VOGGO or GGOPITWL at a threshold of -\u2009300 Hounsfield units (HU), the consolidation percentage in the whole lung (CPITWL) with thresholds at -\u2009400 HU, -\u2009350 HU, and -\u2009250 HU were statistically different. There were statistical differences in the infection volume and percentage of the whole lung, right lung, and lobes between the two groups. VOGGO, GGOPITWL, and volume of consolidation (VOC) were also statistically different at the threshold of -\u2009300 HU.\nQuantitative CT provides an image quantification method for the auxiliary diagnosis of COVID-19 and is expected to assist in confirming patients with COVID-19 pneumonia in suspected cases.", "journal": "BMC infectious diseases", "date": "2021-08-21", "authors": ["WeipingLu", "JianguoWei", "TingtingXu", "MiaoDing", "XiaoyanLi", "MengxueHe", "KaiChen", "XiaodanYang", "HuiyuanShe", "BingcangHuang"], "doi": "10.1186/s12879-021-06556-z\n10.1056/NEJMoa2002032\n10.1097/CM9.0000000000000819\n10.1148/radiol.2020200343\n10.1148/radiol.2020200642\n10.2214/AJR.20.23012\n10.1097/RTI.0000000000000524\n10.1148/radiol.2020200370\n10.1097/RLI.0000000000000674\n10.1148/radiol.2020200230\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06915-5\n10.1148/radiol.2020201491\n10.1148/radiol.2020200905\n10.1038/s41591-020-0931-3\n10.7150/thno.45985\n10.7150/thno.46465\n10.1016/j.jpha.2020.03.004\n10.1016/j.cell.2020.04.045\n10.1038/s41598-020-80261-w\n10.1007/s00330-014-3427-z\n10.1007/s00330-016-4317-3\n10.1001/jama.2020.1585\n10.1148/radiol.2020200463\n10.21037/qims-20-531"}
{"title": "Erratum: On the role of artificial intelligence in medical imaging of COVID-19.", "abstract": "[This corrects the article DOI: 10.1016/j.patter.2021.100269.].", "journal": "Patterns (New York, N.Y.)", "date": "2021-08-19", "authors": ["JannisBorn", "DavidBeymer", "DeeptaRajan", "AdamCoy", "Vandana VMukherjee", "MatteoManica", "PrasanthPrasanna", "DeddehBallah", "MichalGuindy", "DorithShaham", "Pallav LShah", "EmmanouilKarteris", "Jan LRobertus", "MariaGabrani", "MichalRosen-Zvi"], "doi": "10.1016/j.patter.2021.100330"}
{"title": "Prediction of COVID Criticality Score with Laboratory, Clinical and CT Images using Hybrid Regression Models.", "abstract": "Rapid and precise diagnosis of COVID-19 is very critical in hotspot regions. The main aim of this proposed work is to investigate the baseline, laboratory and CT features of COVID-19 affected patients of two groups (Early and Critical stages). The detection model for COVID-19 is built depending upon the manifestations that define the severity of the disease.\nThe CT scan images are fed into the various deep learning, machine learning and hybrid learning models to mine the necessary features and predict CT Score. The predicted CT score along with other clinical, laboratory and CT scan image features are then passed to train the various Regression models for predicting the COVID Criticality (CC) Score. These baseline, laboratory and CT features of COVID-19 are reduced using Statistical analysis and Univariate logistic regression analysis.\nWhen analysing the prediction of CT scores using images alone, AlexNet+Lasso yields better outcome with regression score of 0.9643 and RMSE of 0.0023 when compared with Decision tree (RMSE of 0.0034; Regression score of 0.9578) and GRU (RMSE of 0.1253; regression score of 0.9323). When analysing the prediction of CC scores using CT scores and other baseline, laboratory and CT features, VGG-16+Linear Regression yields better results with regression score of 0.9911 and RMSE of 0.0002 when compared with Linear SVR (RMSE of 0.0006; Regression score of 0.9911) and LSTM (RMSE of 0.0005; Regression score of 0.9877). The correlation analysis is performed to identify the significance of utilizing other features in prediction of CC Score. The correlation coefficient of CT scores with actual value is 0.93 and 0.92 for Early stage group and Critical stage group respectively. The correlation coefficient of CC scores with actual value is 0.96 for Early stage group and 0.95 for Critical stage group.The classification of COVID-19 patients are carried out with the help of predicted CC Scores.\nThis proposed work is carried out in the motive of helping radiologists in faster categorization of COVID patients as Early or Severe staged using CC Scores. The automated prediction of COVID Criticality Score using our diagnostic model can help radiologists and physicians save time for carrying out further treatment and procedures.", "journal": "Computer methods and programs in biomedicine", "date": "2021-08-18", "authors": ["VaralakshmiPerumal", "VasumathiNarayanan", "Sakthi Jaya SundarRajasekar"], "doi": "10.1016/j.cmpb.2021.106336\n10.1148/radiol.2020200642\n10.1007/s13246-020-00865-4\n10.1148/radiol.2020201237\n10.3760/cma.j.issn.1001-0939.2020.0005\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200230\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30183-5\n10.1016/j.cmpb.2020.105581\n10.1007/s00330-020-06748-2\n10.1148/radiol.2020200236\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020200905\n10.1002/jmv.25786\n10.1016/j.cmpb.2020.105532\n10.1148/ryct.2020200034\n10.1007/s00330-020-06713-z\n10.1007/s00330-020-06731-x\n10.1017/ice.2020.61\n10.1148/radiol.2020200274\n10.1001/jama.2020.1585\n10.1093/cid/ciaa225\n10.1148/ryct.2020200031\n10.1001/jama.2020.2648\n10.1148/radiol.2020200343\n10.4103/ijmr.IJMR66320\n10.1007/s00330-020-06801-0\n10.1371/journal.pone.0236621\n10.1056/NEJMoa2001017\n10.1148/radiol.2020200490"}
{"title": "Multidimensional Evaluation of All-Cause Mortality Risk and Survival Analysis for Hospitalized Patients with COVID-19.", "abstract": "", "journal": "International journal of medical sciences", "date": "2021-08-18", "authors": ["JingwenLi", "HuLuo", "GangDeng", "JinyingChang", "XiaomingQiu", "ChenLiu", "BoQin"], "doi": "10.7150/ijms.58889"}
{"title": "iCOVID: interpretable deep learning framework for early recovery-time prediction of COVID-19 patients.", "abstract": "Most prior studies focused on developing models for the severity or mortality prediction of COVID-19 patients. However, effective models for recovery-time prediction are still lacking. Here, we present a deep learning solution named iCOVID that can successfully predict the recovery-time of COVID-19 patients based on predefined treatment schemes and heterogeneous multimodal patient information collected within 48\u2009hours after admission. Meanwhile, an interpretable mechanism termed FSR is integrated into iCOVID to reveal the features greatly affecting the prediction of each patient. Data from a total of 3008 patients were collected from three hospitals in Wuhan, China, for large-scale verification. The experiments demonstrate that iCOVID can achieve a time-dependent concordance index of 74.9% (95% CI: 73.6-76.3%) and an average day error of 4.4 days (95% CI: 4.2-4.6 days). Our study reveals that treatment schemes, age, symptoms, comorbidities, and biomarkers are highly related to recovery-time predictions.", "journal": "NPJ digital medicine", "date": "2021-08-18", "authors": ["JunWang", "ChenLiu", "JingwenLi", "ChengYuan", "LichiZhang", "ChengJin", "JianweiXu", "YaqiWang", "YaofengWen", "HongbingLu", "BiaoLi", "ChangChen", "XiangdongLi", "DinggangShen", "DahongQian", "JianWang"], "doi": "10.1038/s41746-021-00496-3\n10.1007/s42979-020-00401-x\n10.1109/TMI.2020.2994908\n10.1109/TMI.2020.2996645\n10.1109/ACCESS.2021.3058537\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100505\n10.1038/s41746-021-00399-3\n10.1038/s41746-020-00369-1\n10.1007/s42979-020-00335-4\n10.1038/s41746-020-00372-6\n10.1007/s42979-020-00300-1\n10.1002/dmrr.3319\n10.1016/j.clinthera.2020.04.009\n10.1007/s00330-020-06978-4\n10.1007/s42979-020-00216-w\n10.1038/s41467-020-20657-4\n10.1016/j.jaci.2020.04.006\n10.1038/s41551-020-00633-5\n10.1093/cid/ciaa1012\n10.1001/jamainternmed.2020.3539\n10.1016/j.inffus.2019.12.012\n10.1073/pnas.2005615117\n10.1038/s42256-021-00307-0\n10.1080/01621459.1989.10478874\n10.1002/sim.2427\n10.1214/08-AOAS169\n10.1016/j.ajem.2020.10.013\n10.1515/cclm-2020-0198\n10.1186/s12874-020-01153-1\n10.1038/s41467-020-20816-7\n10.1038/s41467-020-18297-9\n10.1038/s41467-020-18786-x\n10.21037/atm-20-3026\n10.1038/nrclinonc.2017.141\n10.1080/01621459.1988.10478612\n10.1097/CM9.0000000000000819\n10.2214/AJR.20.22954\n10.2214/AJR.20.22976\n10.1183/13993003.00775-2020\n10.1001/jama.1982.03320430047030\n10.1214/ss/1032280214"}
{"title": "Robotics for neuroendovascular intervention: Background and primer.", "abstract": "The simultaneous growth of robotic-assisted surgery and telemedicine in recent years has only been accelerated by the recent coronavirus disease 2019 pandemic. Robotic assistance for neurovascular intervention has garnered significant interest due to opportunities for tele-stroke models of care for remote underserved areas. Lessons learned from medical robots in interventional cardiology and neurosurgery have contributed to incremental but vital advances in medical robotics despite important limitations. In this article, we discuss robot types and their clinical justification and ethics, as well as a general overview on available robots in thoracic/abdominal surgery, neurosurgery, and cardiac electrophysiology. We conclude with current clinical research in neuroendovascular intervention and a perspective on future directions.", "journal": "The neuroradiology journal", "date": "2021-08-17", "authors": ["Kazim HNarsinh", "RicardoPaez", "KerstinMueller", "M TravisCaton", "AmandaBaker", "Randall THigashida", "Van VHalbach", "Christopher FDowd", "Matthew RAmans", "Steven WHetts", "Alexander MNorbash", "Daniel LCooke"], "doi": "10.1177/19714009211034829"}
{"title": "Hybrid Deep-Learning and Machine-Learning Models for Predicting COVID-19.", "abstract": "The COVID-19 pandemic has had a significant impact on public life and health worldwide, putting the world's healthcare systems at risk. The first step in stopping this outbreak is to detect the infection in its early stages, which will relieve the risk, control the outbreak's spread, and restore full functionality to the world's healthcare systems. Currently, PCR is the most prevalent diagnosis tool for COVID-19. However, chest X-ray images may play an essential role in detecting this disease, as they are successful for many other viral pneumonia diseases. Unfortunately, there are common features between COVID-19 and other viral pneumonia, and hence manual differentiation between them seems to be a critical problem and needs the aid of artificial intelligence. This research employs deep- and transfer-learning techniques to develop accurate, general, and robust models for detecting COVID-19. The developed models utilize either convolutional neural networks or transfer-learning models or hybridize them with powerful machine-learning techniques to exploit their full potential. For experimentation, we applied the proposed models to two data sets: the COVID-19 Radiography Database from Kaggle and a local data set from Asir Hospital, Abha, Saudi Arabia. The proposed models achieved promising results in detecting COVID-19 cases and discriminating them from normal and other viral pneumonia with excellent accuracy. The hybrid models extracted features from the flatten layer or the first hidden layer of the neural network and then fed these features into a classification algorithm. This approach enhanced the results further to full accuracy for binary COVID-19 classification and 97.8% for multiclass classification.", "journal": "Computational intelligence and neuroscience", "date": "2021-08-17", "authors": ["Talal SQaid", "HusseinMazaar", "Mohammad Yahya HAl-Shamri", "Mohammed SAlqahtani", "Abeer ARaweh", "WafaaAlakwaa"], "doi": "10.1155/2021/9996737\n10.1016/j.patcog.2020.107613\n10.1016/j.bcp.2020.114184\n10.1186/s40537-021-00444-8\n10.1148/rg.2018170048\n10.1016/j.compbiomed.2020.103792\n10.1177/2472630320958376\n10.1148/rg.2020200097\n10.1007/s10489-020-01867-1\n10.1007/s00521-020-05410-8\n10.1371/journal.pone.0247839\n10.1109/42.476112\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.104181\n10.1016/j.chaos.2021.110713\n10.1016/j.chaos.2020.110495\n10.1016/j.imu.2020.100505\n10.3390/v12070769\n10.1016/j.media.2020.101794\n10.1007/s10489-020-01902-1\n10.1186/s12938-020-00831-x\n10.1038/s41598-020-76550-z\n10.1371/journal.pone.0242535\n10.1016/j.imu.2020.100360\n10.1016/j.scs.2020.102589\n10.1109/JPROC.2020.3004555\n10.1109/ACCESS.2018.2871027\n10.17632/3pxjb8knp7.3"}
{"title": "Pareto optimization of deep networks for COVID-19 diagnosis from chest X-rays.", "abstract": "The year 2020 was characterized by the COVID-19 pandemic that has caused, by the end of March 2021, more than 2.5 million deaths worldwide. Since the beginning, besides the laboratory test, used as the gold standard, many applications have been applying deep learning algorithms to chest X-ray images to recognize COVID-19 infected patients. In this context, we found out that convolutional neural networks perform well on a single dataset but struggle to generalize to other data sources. To overcome this limitation, we propose a late fusion approach where we combine the outputs of several state-of-the-art CNNs, introducing a novel method that allows us to construct an optimum ensemble determining which and how many base learners should be aggregated. This choice is driven by a two-objective function that maximizes, on a validation set, the accuracy and the diversity of the ensemble itself. A wide set of experiments on several publicly available datasets, accounting for more than 92,000 images, shows that the proposed approach provides average recognition rates up to 93.54% when tested on external datasets.", "journal": "Pattern recognition", "date": "2021-08-17", "authors": ["ValerioGuarrasi", "Natascha ClaudiaD'Amico", "RosaSicilia", "ErmannoCordelli", "PaoloSoda"], "doi": "10.1016/j.patcog.2021.108242"}
{"title": "An approach to the classification of COVID-19 based on CT scans using convolutional features and genetic algorithms.", "abstract": "COVID-19 is a respiratory disease that, as of July 15th, 2021, has infected more than 187 million people worldwide and is responsible for more than 4 million deaths. An accurate diagnosis of COVID-19 is essential for the treatment and control of the disease. The use of computed tomography (CT) has shown to be promising for evaluating patients suspected of COVID-19 infection. The analysis of a CT examination is complex, and requires attention from a specialist. This paper presents a methodology for detecting COVID-19 from CT images. We first propose a convolutional neural network architecture to extract features from CT images, and then optimize the hyperparameters of the network using a tree Parzen estimator to choose the best parameters. Following this, we apply a selection of features using a genetic algorithm. Finally, classification is performed using four classifiers with different approaches. The proposed methodology achieved an accuracy of 0.997, a kappa index of 0.995, an AUROC of 0.997, and an AUPRC of 0.997 on the SARS-CoV-2 CT-Scan dataset, and an accuracy of 0.987, a kappa index of 0.975, an AUROC of 0.989, and an AUPRC of 0.987 on the COVID-CT dataset, using our CNN after optimization of the hyperparameters, the selection of features and the multi-layer perceptron classifier. Compared with pretrained CNNs and related state-of-the-art works, the results achieved by the proposed methodology were superior. Our results show that the proposed method can assist specialists in screening and can aid in diagnosing patients with suspected COVID-19.", "journal": "Computers in biology and medicine", "date": "2021-08-14", "authors": ["Edson DCarvalho", "Romuere R VSilva", "Fl\u00e1vio H DAra\u00fajo", "Ricardo de A LRabelo", "Ant\u00f4nio Oseasde Carvalho Filho"], "doi": "10.1016/j.compbiomed.2021.104744\n10.1148/radiol.2020200463\n10.1016/j.compbiomed.2021.104425\n10.1016/j.compbiomed.2020.103792\n10.1016/j.mehy.2020.109761\n10.1148/radiol.2020200463\n10.2214/AJR.20.22954\n10.2214/AJR.20.23034\n10.1016/j.patcog.2021.107826\n10.1016/j.artmed.2020.101845\n10.1016/j.compeleceng.2018.07.028\n10.1016/j.compeleceng.2018.03.038\n10.1016/j.compbiomed.2021.104454\n10.1007/s11042-021-10783-6\n10.1109/JBHI.2020.3023246\n10.1109/JTEHM.2021.3077142\n10.1007/s11548-020-02286-w\n10.1016/j.patcog.2020.107608\n10.1080/07391102.2020.1788642\n10.1145/3065386\n10.1016/j.patcog.2021.108005\n10.1007/s00521-020-05437-x\n10.1109/CEC48606.2020.9185599\n10.1023/A:1008306431147\n10.1007/978-1-4842-4470-8_7\n10.1109/5254.671091\n10.1145/2939672.2939785\n10.1148/radiology.143.1.7063747\n10.1371/journal.pone.0092209"}
{"title": "Deep learning and lung ultrasound for Covid-19 pneumonia detection and severity classification.", "abstract": "The Covid-19 European outbreak in February 2020 has challenged the world's health systems, eliciting an urgent need for effective and highly reliable diagnostic instruments to help medical personnel. Deep learning (DL) has been demonstrated to be useful for diagnosis using both computed tomography (CT) scans and chest X-rays (CXR), whereby the former typically yields more accurate results. However, the pivoting function of a CT scan during the pandemic presents several drawbacks, including high cost and cross-contamination problems. Radiation-free lung ultrasound (LUS) imaging, which requires high expertise and is thus being underutilised, has demonstrated a strong correlation with CT scan results and a high reliability in pneumonia detection even in the early stages. In this study, we developed a system based on modern DL methodologies in close collaboration with Fondazione IRCCS Policlinico San Matteo's Emergency Department (ED) of Pavia. Using a reliable dataset comprising ultrasound clips originating from linear and convex probes in 2908 frames from 450 hospitalised patients, we conducted an investigation into detecting Covid-19 patterns and ranking them considering two severity scales. This study differs from other research projects by its novel approach involving four and seven classes. Patients admitted to the ED underwent 12 LUS examinations in different chest parts, each evaluated according to standardised severity scales. We adopted residual convolutional neural networks (CNNs), transfer learning, and data augmentation techniques. Hence, employing methodological hyperparameter tuning, we produced state-of-the-art results meeting F1 score levels, averaged over the number of classes considered, exceeding 98%, and thereby manifesting stable measurements over precision and recall.", "journal": "Computers in biology and medicine", "date": "2021-08-14", "authors": ["MarcoLa Salvia", "GianmarcoSecco", "EmanueleTorti", "GiordanaFlorimbi", "LucaGuido", "PaoloLago", "FrancescoSalinaro", "StefanoPerlini", "FrancescoLeporati"], "doi": "10.1016/j.compbiomed.2021.104742\n10.1002/jum.15285\n10.1002/jmv.25727\n10.1148/radiol.2020200432\n10.1016/j.annemergmed.2020.03.033\n10.1148/radiol.2020200642\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2994459\n10.1007/978-3-319-15371-1_8\n10.1109/cvpr.2009.5206848"}
{"title": "Deep-learning based detection of COVID-19 using lung ultrasound imagery.", "abstract": "The COVID-19 pandemic has exposed the vulnerability of healthcare services worldwide, especially in underdeveloped countries. There is a clear need to develop novel computer-assisted diagnosis tools to provide rapid and cost-effective screening in places where massive traditional testing is not feasible. Lung ultrasound is a portable, easy to disinfect, low cost and non-invasive tool that can be used to identify lung diseases. Computer-assisted analysis of lung ultrasound imagery is a relatively recent approach that has shown great potential for diagnosing pulmonary conditions, being a viable alternative for screening and diagnosing COVID-19.\nTo evaluate and compare the performance of deep-learning techniques for detecting COVID-19 infections from lung ultrasound imagery.\nWe adapted different pre-trained deep learning architectures, including VGG19, InceptionV3, Xception, and ResNet50. We used the publicly available POCUS dataset comprising 3326 lung ultrasound frames of healthy, COVID-19, and pneumonia patients for training and fine-tuning. We conducted two experiments considering three classes (COVID-19, pneumonia, and healthy) and two classes (COVID-19 versus pneumonia and COVID-19 versus non-COVID-19) of predictive models. The obtained results were also compared with the POCOVID-net model. For performance evaluation, we calculated per-class classification metrics (Precision, Recall, and F1-score) and overall metrics (Accuracy, Balanced Accuracy, and Area Under the Receiver Operating Characteristic Curve). Lastly, we performed a statistical analysis of performance results using ANOVA and Friedman tests followed by post-hoc analysis using the Wilcoxon signed-rank test with the Holm's step-down correction.\nInceptionV3 network achieved the best average accuracy (89.1%), balanced accuracy (89.3%), and area under the receiver operating curve (97.1%) for COVID-19 detection from bacterial pneumonia and healthy lung ultrasound data. The ANOVA and Friedman tests found statistically significant performance differences between models for accuracy, balanced accuracy and area under the receiver operating curve. Post-hoc analysis showed statistically significant differences between the performance obtained with the InceptionV3-based model and POCOVID-net, VGG19-, and ResNet50-based models. No statistically significant differences were found in the performance obtained with InceptionV3- and Xception-based models.\nDeep learning techniques for computer-assisted analysis of lung ultrasound imagery provide a promising avenue for COVID-19 screening and diagnosis. Particularly, we found that the InceptionV3 network provides the most promising predictive results from all AI-based techniques evaluated in this work. InceptionV3- and Xception-based models can be used to further develop a viable computer-assisted screening tool for COVID-19 based on ultrasound imagery.", "journal": "PloS one", "date": "2021-08-14", "authors": ["JuliaDiaz-Escobar", "Nelson EOrd\u00f3\u00f1ez-Guill\u00e9n", "SalvadorVillarreal-Reyes", "AlejandroGalaviz-Mosqueda", "VitalyKober", "Ra\u00falRivera-Rodriguez", "Jose ELozano Rizk"], "doi": "10.1371/journal.pone.0255886\n10.1016/S0140-6736(20)30183-5\n10.1128/CMR.00023-07\n10.1128/CMR.00102-14\n10.3390/sym12071146\n10.7717/peerj-cs.495\n10.1101/2020.02.20.20025536v1\n10.1007/s10044-020-00950-0\n10.2174/1573405616666200604163954\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1109/RBME.2020.2987975\n10.1097/CCM.0b013e3181b08cdb\n10.1007/s11739-010-0381-x\n10.1093/bjaceaccp/mkv012\n10.1186/s13054-016-1487-y\n10.1164/rccm.201802-0236CI\n10.1109/CONCAPAN.2016.7942375\n10.1109/EMBC.2016.7591632\n10.1371/journal.pone.0206410\n10.1007/978-3-030-01045-4_8\n10.1007/978-3-030-13469-3_84\n10.1007/978-3-030-32875-7_9\n10.1109/JBHI.2019.2936151\n10.1109/TMI.2020.2994459\n10.3390/app11020672\n10.1016/S2213-2600(20)30120-X\n10.1016/j.annemergmed.2020.03.033\n10.1016/S2213-2600(20)30166-1\n10.1148/radiol.2020200847\n10.1016/j.crad.2020.04.003\n10.1002/jum.15284\n10.7863/ultra.32.12.2185\n10.1117/12.2216499\n10.1109/TMI.2017.2715880\n10.1016/j.radi.2020.04.005"}
{"title": "COVID-19 Screening in Chest X-Ray Images Using Lung Region Priors.", "abstract": "Early screening of COVID-19 is essential for pandemic control, and thus to relieve stress on the health care system. Lung segmentation from chest X-ray (CXR) is a promising method for early diagnoses of pulmonary diseases. Recently, deep learning has achieved great success in supervised lung segmentation. However, how to effectively utilize the lung region in screening COVID-19 still remains a challenge due to domain shift and lack of manual pixel-level annotations. We hereby propose a multi-appearance COVID-19 screening framework by using lung region priors derived from CXR images. Firstly, we propose a multi-scale adversarial domain adaptation network (MS-AdaNet) to boost the cross-domain lung segmentation task as the prior knowledge to the classification network. Then, we construct a multi-appearance network (MA-Net), which is composed of three sub-networks to realize multi-appearance feature extraction and fusion using lung region priors. At last, we can obtain prediction results from normal, viral pneumonia, and COVID-19 using the proposed MA-Net. We extend the proposed MS-AdaNet for lung segmentation task on three different public CXR datasets. The results suggest that the MS-AdaNet outperforms contrastive methods in cross-domain lung segmentation. Moreover, experiments reveal that the proposed MA-Net achieves accuracy of 98.83 % and F1-score of 98.71 % on COVID-19 screening. The results indicate that the proposed MA-Net can obtain significant performance on COVID-19 screening.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-08-14", "authors": ["JianpengAn", "QingCai", "ZhiyongQu", "ZhongkeGao"], "doi": "10.1109/JBHI.2021.3104629\n10.1109/TPAMI.2019.2918284"}
{"title": "Integrated Clinical and CT Based Artificial Intelligence Nomogram for Predicting Severity and Need for Ventilator Support in COVID-19 Patients: A Multi-Site Study.", "abstract": "Almost 25% of COVID-19 patients end up in ICU needing critical mechanical ventilation support. There is currently no validated objective way to predict which patients will end up needing ventilator support, when the disease is mild and not progressed. N = 869 patients from two sites (D", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-08-14", "authors": ["AmoghHiremath", "KaustavBera", "LeiYuan", "PranjalVaidya", "MehdiAlilou", "JenniferFurin", "KeithArmitage", "RobertGilkeson", "MengyaoJi", "PingfuFu", "AmitGupta", "ChengLu", "AnantMadabhushi"], "doi": "10.1109/JBHI.2021.3103389\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30566-3\n10.1148/radiol.2020200330\n10.1038/s41591-020-0931-3\n10.1007/978-3-319-24574-4_28\n10.2307/2531595\n10.1007/s11263-019-01228-7\n10.1001/jama.2020.6825\n10.1101/2020.05.30.20118067\n10.1001/jamanetworkopen.2020.12606\n10.1056/NEJMoa2020283"}
{"title": "A bagging dynamic deep learning network for diagnosing COVID-19.", "abstract": "COVID-19 is a serious ongoing worldwide pandemic. Using X-ray chest radiography images for automatically diagnosing COVID-19 is an effective and convenient means of providing diagnostic assistance to clinicians in practice. This paper proposes a bagging dynamic deep learning network (B-DDLN) for diagnosing COVID-19 by intelligently recognizing its symptoms in X-ray chest radiography images. After a series of preprocessing steps for images, we pre-train convolution blocks as a feature extractor. For the extracted features, a bagging dynamic learning network classifier is trained based on neural dynamic learning algorithm and bagging algorithm. B-DDLN connects the feature extractor and bagging classifier in series. Experimental results verify that the proposed B-DDLN achieves 98.8889% testing accuracy, which\u00a0shows the best diagnosis performance among the existing state-of-the-art methods on the open image set. It also provides evidence for further detection and treatment.", "journal": "Scientific reports", "date": "2021-08-13", "authors": ["ZhijunZhang", "BozhaoChen", "JianshengSun", "YameiLuo"], "doi": "10.1038/s41598-021-95537-y\n10.1038/s41598-020-74539-2\n10.1038/s42256-020-0184-3\n10.1007/s10096-020-03901-z\n10.1128/aem.63.10.3741-3751.1997\n10.26599/BDMA.2020.9020012\n10.1038/s42256-020-0180-7\n10.1148/radiol.2020200642\n10.1038/s42256-021-00305-2\n10.1038/s41598-019-45256-2\n10.1109/JBHI.2020.3037127\n10.1109/RBME.2020.2987975\n10.1016/j.patcog.2020.107613\n10.7717/peerj-cs.564\n10.1109/TMI.2020.3040950\n10.1142/S0218001421510046\n10.1016/j.cmpb.2020.105608\n10.1038/nature14539\n10.1038/s41598-020-70479-z\n10.1109/LGRS.2018.2886534\n10.1007/s42600-021-00151-6\n10.1038/s41598-020-76550-z\n10.1016/j.irbm.2020.07.001\n10.1109/TITS.2016.2639320\n10.1109/TAC.2018.2810039\n10.1109/TCYB.2017.2760883\n10.1109/TAC.2019.2921681\n10.1109/TCYB.2018.2841970\n10.1109/TNNLS.2019.2944485\n10.1109/TCST.2018.2872471\n10.1109/TMECH.2018.2799724\n10.1109/TCYB.2019.2923642\n10.1109/TNNLS.2018.2885042\n10.1109/TSMC.2018.2866843\n10.1109/TFUZZ.2019.2914618\n10.1016/j.neucom.2021.01.083\n10.1109/TCSI.2012.2188944\n10.3390/electronics9091388\n10.1016/j.compbiomed.2020.103792\n10.1155/2021/8829829\n10.1007/s10489-020-02149-6\n10.1007/s12652-020-02669-6"}
{"title": "Systematic Review of Artificial Intelligence in Acute Respiratory Distress Syndrome for COVID-19 Lung Patients: A Biomedical Imaging Perspective.", "abstract": "SARS-CoV-2 has infected over \u223c165 million people worldwide causing Acute Respiratory Distress Syndrome (ARDS) and has killed \u223c3.4 million people. Artificial Intelligence (AI) has shown to benefit in the biomedical image such as X-ray/Computed Tomography in diagnosis of ARDS, but there are limited AI-based systematic reviews (aiSR). The purpose of this study is to understand the Risk-of-Bias (RoB) in a non-randomized AI trial for handling ARDS using novel AtheroPoint-AI-Bias (AP(ai)Bias). Our hypothesis for acceptance of a study to be in low RoB must have a mean score of 80% in a study. Using the PRISMA model, 42 best AI studies were analyzed to understand the RoB. Using the AP(ai)Bias paradigm, the top 19 studies were then chosen using the raw-cutoff of 1.9. This was obtained using the intersection of the cumulative plot of \"mean score vs. study\" and score distribution. Finally, these studies were benchmarked against ROBINS-I and PROBAST paradigm. Our observation showed that AP(ai)Bias, ROBINS-I, and PROBAST had only 32%, 16%, and 26% studies, respectively in low-moderate RoB (cutoff>2.5), however none of them met the RoB hypothesis. Further, the aiSR analysis recommends six primary and six secondary recommendations for the non-randomized AI for ARDS. The primary recommendations for improvement in AI-based ARDS design inclusive of (i) comorbidity, (ii) inter-and intra-observer variability studies, (iii) large data size, (iv) clinical validation, (v) granularity of COVID-19 risk, and (vi) cross-modality scientific validation. The AI is an important component for diagnosis of ARDS and the recommendations must be followed to lower the RoB.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-08-12", "authors": ["Jasjit SSuri", "SushantAgarwal", "SuneetGupta", "AnudeepPuvvula", "KlaudijaViskovic", "NehaSuri", "AzraAlizad", "AymanEl-Baz", "LucaSaba", "MostafaFatemi", "D SubbaramNaidu"], "doi": "10.1109/JBHI.2021.3103839\n10.1007/s10554-020-02089-9\n10.1016/j.compbiomed.2020.103960\n10.2196/13090\n10.2196/18700\n10.1016/j.jiph.2020.06.028\n10.1016/j.jacr.2020.03.006\n10.1136/bmj.m689\n10.1136/bmj.m1328\n10.1080/15424065.2019.1691963\n10.1007/s11517-019-01975-2\n10.1016/j.compbiomed.2020.104043\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(20)30566-3\n10.1038/s41569-020-0360-5\n10.1007/s12250-020-00207-4\n10.1002/path.1440\n10.1002/jmv.25709\n10.3389/fimmu.2016.00366\n10.1038/nri.2017.105\n10.21037/cdt-20-561\n10.1172/JCI60331\n10.1016/0002-9343(79)90066-4\n10.1513/AnnalsATS.201609-728PS\n10.1056/NEJMoa012835\n10.1016/j.chaos.2020.110071\n10.1016/j.cmpb.2020.105608\n10.1007/s13246-020-00888-x\n10.3348/kjr.2020.0536\n10.1016/j.eswa.2020.114054\n10.1016/j.cmpb.2020.105581\n10.1101/2020.09.15.20195453\n10.1007/s00330-020-07042-x\n10.1016/j.compbiomed.2020.103869\n10.1016/j.ejro.2020.100272\n10.1016/j.irbm.2020.07.001\n10.1007/s00330-020-07044-9\n10.1007/s10916-015-0214-6\n10.1016/j.chaos.2020.110245\n10.1016/j.compbiomed.2020.103792\n10.1007/s10096-020-03901-z\n10.1016/j.chemolab.2020.104054\n10.1183/13993003.00775-2020\n10.1016/j.ejrad.2020.109041\n10.21037/atm.2020.03.132\n10.1007/s11042-020-09894-3\n10.3389/fmed.2020.00427\n10.1371/journal.pone.0236621\n10.1148/ryai.2020200048\n10.1080/07391102.2020.1788642\n10.3892/etm.2020.8797\n10.7759/cureus.9448\n10.3389/fmed.2020.608525\n10.1038/s41598-020-76550-z\n10.1016/j.ejrad.2019.02.038\n10.1016/j.neurad.2017.09.007\n10.1016/j.ejrad.2009.05.039\n10.1007/s11517-005-0016-y\n10.1136/bmj.i4919\n10.7326/M18-1376\n10.1016/j.compbiomed.2020.103958\n10.1016/j.compbiomed.2021.104210\n10.1007/s10916-016-0504-7\n10.21037/cdt.2020.01.07\n10.1213/ANE.0000000000002741\n10.1007/s10554-020-02099-7\n10.1016/j.compbiomed.2020.103804\n10.1007/s00259-020-04821-y"}
{"title": "Self-Ensembling Co-Training Framework for Semi-Supervised COVID-19 CT Segmentation.", "abstract": "The coronavirus disease 2019 (COVID-19) has become a severe worldwide health emergency and is spreading at a rapid rate. Segmentation of COVID lesions from computed tomography (CT) scans is of great importance for supervising disease progression and further clinical treatment. As labeling COVID-19 CT scans is labor-intensive and time-consuming, it is essential to develop a segmentation method based on limited labeled data to conduct this task. In this paper, we propose a self-ensembled co-training framework, which is trained by limited labeled data and large-scale unlabeled data, to automatically extract COVID lesions from CT scans. Specifically, to enrich the diversity of unsupervised information, we build a co-training framework consisting of two collaborative models, in which the two models teach each other during training by using their respective predicted pseudo-labels of unlabeled data. Moreover, to alleviate the adverse impacts of noisy pseudo-labels for each model, we propose a self-ensembling strategy to perform consistency regularization for the up-to-date predictions of unlabeled data, in which the predictions of unlabeled data are gradually ensembled via moving average at the end of every training epoch. We evaluate our framework on a COVID-19 dataset containing 103 CT scans. Experimental results show that our proposed method achieves better performance in the case of only 4 labeled CT scans compared to the state-of-the-art semi-supervised segmentation networks.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-08-11", "authors": ["CaiziLi", "LiDong", "QiDou", "FanLin", "KebaoZhang", "ZuxinFeng", "WeixinSi", "XuesongDeng", "ZheDeng", "Pheng-AnnHeng"], "doi": "10.1109/JBHI.2021.3103646"}
{"title": "Explainable DCNN based chest X-ray image analysis and classification for COVID-19 pneumonia detection.", "abstract": "To speed up the discovery of COVID-19 disease mechanisms by X-ray images, this research developed a new diagnosis platform using a deep convolutional neural network (DCNN) that is able to assist radiologists with diagnosis by distinguishing COVID-19 pneumonia from non-COVID-19 pneumonia in patients based on chest X-ray classification and analysis. Such a tool can save time in interpreting chest X-rays and increase the accuracy and thereby enhance our medical capacity for the detection and diagnosis of COVID-19. The explainable method is also used in the DCNN to select instances of the X-ray dataset images to explain the behavior of training-learning models to achieve higher prediction accuracy. The average accuracy of our method is above 96%, which can replace manual reading and has the potential to be applied to large-scale rapid screening of COVID-9 for widely use cases.", "journal": "Scientific reports", "date": "2021-08-11", "authors": ["JieHou", "TerryGao"], "doi": "10.1038/s41598-021-95680-6\n10.1001/jama.2020.1585\n10.3390/v12050527\n10.1007/s11481-020-09924-9\n10.1016/j.jaim.2020.05.009\n10.1056/NEJMoa2015432\n10.1016/j.cell.2020.05.025\n10.1136/jitc-2020-000892\n10.4103/ijmr.IJMR_518_20\n10.1016/j.neuroimage\n10.1016/j.jormas.2019.06.002\n10.1016/j.image.2017.08.010\n10.1016/j.neucom.2017.12.032\n10.1007/s42979-020-00401-x\n10.1007/s42979-020-00335-4\n10.1007/s42979-020-00300-1\n10.1016/j.neucom.2017.12.049\n10.1016/j.eswa.2017.04.053\n10.1109/tnnls.2016.2628878\n10.1109/tetci.2017.2775237\n10.1109/ACCESS.2021.3058537\n10.1007/s42979-020-00383-w\n10.1007/s42979-020-00216-w\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100505\n10.1007/s10489-020-02149-6\n10.1142/S0218001421510046\n10.1007/s12652-020-02669-6\n10.1148/radiol.2020200527\n10.1088/1742-6596/1518/1/012041\n10.1056/NEJMoa2001191\n10.7150/jca.28769"}
{"title": "Validating deep learning inference during chest X-ray classification for COVID-19 screening.", "abstract": "The new coronavirus unleashed a worldwide pandemic in early 2020, and a fatality rate several times that of the flu. As the number of infections soared, and capabilities for testing lagged behind, chest X-ray (CXR) imaging became more relevant in the early diagnosis and treatment planning for patients with suspected or confirmed COVID-19 infection. In a few weeks, proposed new methods for lung screening using deep learning rapidly appeared, while quality assurance discussions lagged behind. This paper proposes a set of protocols to validate deep learning algorithms, including our ROI Hide-and-Seek protocol, which emphasizes or hides key regions of interest from CXR data. Our protocol allows assessing the classification performance for anomaly detection and its correlation to radiological signatures, an important issue overlooked in several deep learning approaches proposed so far. By running a set of systematic tests over CXR representations using public image datasets, we demonstrate the weaknesses of current techniques and offer perspectives on the advantages and limitations of automated radiography analysis when using heterogeneous data sources.", "journal": "Scientific reports", "date": "2021-08-11", "authors": ["RobbieSadre", "BaskaranSundaram", "SharmilaMajumdar", "DanielaUshizima"], "doi": "10.1038/s41598-021-95561-y\n10.1038/s41598-020-77316-3\n10.1038/s41581-020-0284-7\n10.1016/S0140-6736(20)30728-5\n10.1148/ryct.2020200152\n10.1148/radiol.2020200642\n10.1016/S2589-7500(20)30109-6\n10.1038/s41598-019-56847-4\n10.1038/s41598-019-56847-4\n10.1016/S0140-6736(20)30183-5\n10.1007/s13246-020-00865-4\n10.1038/s41598-020-76550-z\n10.1038/s41598-021-87994-2\n10.1016/S2589-7500(20)30199-0\n10.1109/TMI.2020.2993291\n10.1007/s00264-020-04609-7\n10.1016/j.chaos.2020.110495\n10.1109/ACCESS.2020.3003810\n10.1016/j.bspc.2020.102365\n10.1038/s41598-020-78060-4\n10.1038/s41598-019-56847-4\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103792\n10.1007/s10489-020-01829-7\n10.1038/s41592-018-0261-2\n10.1049/el.2019.1719\n10.1007/s11263-019-01228-7"}
{"title": "Federated Semi-Supervised Multi-Task Learning to Detect COVID-19 and Lungs Segmentation Marking Using Chest Radiography Images and Raspberry Pi Devices: An Internet of Medical Things Application.", "abstract": "Internet of Medical Things (IoMT) provides an excellent opportunity to investigate better automatic medical decision support tools with the effective integration of various medical equipment and associated data. This study explores two such medical decision-making tasks, namely COVID-19 detection and lung area segmentation detection, using chest radiography images. We also explore different cutting-edge machine learning techniques, such as federated learning, semi-supervised learning, transfer learning, and multi-task learning to explore the issue. To analyze the applicability of computationally less capable edge devices in the IoMT system, we report the results using Raspberry Pi devices as accuracy, precision, recall, Fscore for COVID-19 detection, and average dice score for lung segmentation detection tasks. We also publish the results obtained through server-centric simulation for comparison. The results show that Raspberry Pi-centric devices provide better performance in lung segmentation detection, and server-centric experiments provide better results in COVID-19 detection. We also discuss the IoMT application-centric settings, utilizing medical data and decision support systems, and posit that such a system could benefit all the stakeholders in the IoMT domain.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-08-11", "authors": ["Mahbub UlAlam", "RahimRahmani"], "doi": "10.3390/s21155025\n10.12720/jcm.12.4.240-247\n10.1007/s00779-018-1178-6\n10.4258/hir.2016.22.3.156\n10.1007/978-3-030-55833-8_2\n10.1109/JIOT.2018.2849014\n10.1109/JSYST.2018.2890121\n10.1109/JIOT.2019.2946359\n10.2991/aebmr.k.200730.068\n10.1109/TMM.2017.2729400\n10.1016/j.eswa.2018.09.056\n10.1093/jamia/ocy068\n10.1016/j.future.2016.08.011\n10.1007/s41666-020-00082-4\n10.1109/TNN.2009.2015974\n10.1186/s40537-016-0043-6\n10.14445/22312803/IJCTT-V8P105\n10.2200/S00497ED1V01Y201304HLT021\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30627-9\n10.3390/electronics6030051\n10.1109/MSP.2020.2975749\n10.1109/MIS.2020.2988604\n10.1016/j.media.2019.03.009\n10.1117/1.JMI.6.3.034002\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.2214/ajr.174.1.1740071\n10.1109/ISBI48211.2021.9434167\n10.1109/JPROC.2020.3004555\n10.1109/TPAMI.2018.2798607\n10.1007/s10462-018-09679-z\n10.1002/aris.1440370103\n10.1007/s11042-020-10073-7\n10.1007/s00287-015-0913-x\n10.5220/0010190200470057\n10.1016/j.jhin.2021.01.023\n10.1016/j.procs.2020.07.058\n10.5220/0008911400450055\n10.1016/j.inffus.2021.04.008\n10.1016/j.comcom.2020.05.029"}
{"title": "Medical-VLBERT: Medical Visual Language BERT for COVID-19 CT Report Generation With Alternate Learning.", "abstract": "Medical imaging technologies, including computed tomography (CT) or chest X-Ray (CXR), are largely employed to facilitate the diagnosis of the COVID-19. Since manual report writing is usually too time-consuming, a more intelligent auxiliary medical system that could generate medical reports automatically and immediately is urgently needed. In this article, we propose to use the medical visual language BERT (Medical-VLBERT) model to identify the abnormality on the COVID-19 scans and generate the medical report automatically based on the detected lesion regions. To produce more accurate medical reports and minimize the visual-and-linguistic differences, this model adopts an alternate learning strategy with two procedures that are knowledge pretraining and transferring. To be more precise, the knowledge pretraining procedure is to memorize the knowledge from medical texts, while the transferring procedure is to utilize the acquired knowledge for professional medical sentences generations through observations of medical images. In practice, for automatic medical report generation on the COVID-19 cases, we constructed a dataset of 368 medical findings in Chinese and 1104 chest CT scans from The First Affiliated Hospital of Jinan University, Guangzhou, China, and The Fifth Affiliated Hospital of Sun Yat-sen University, Zhuhai, China. Besides, to alleviate the insufficiency of the COVID-19 training samples, our model was first trained on the large-scale Chinese CX-CHR dataset and then transferred to the COVID-19 CT dataset for further fine-tuning. The experimental results showed that Medical-VLBERT achieved state-of-the-art performances on terminology prediction and report generation with the Chinese COVID-19 CT dataset and the CX-CHR dataset. The Chinese COVID-19 CT dataset is available at https://covid19ct.github.io/.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-08-10", "authors": ["GuangyiLiu", "YinghongLiao", "FuyuWang", "BinZhang", "LuZhang", "XiaodanLiang", "XiangWan", "ShaolinLi", "ZhenLi", "ShuixingZhang", "ShuguangCui"], "doi": "10.1109/TNNLS.2021.3099165"}
{"title": "AI-Empowered Computational Examination of Chest Imaging for COVID-19 Treatment: A Review.", "abstract": "Since the first case of coronavirus disease 2019 (COVID-19) was discovered in December 2019, COVID-19 swiftly spread over the world. By the end of March 2021, more than 136 million patients have been infected. Since the second and third waves of the COVID-19 outbreak are in full swing, investigating effective and timely solutions for patients' check-ups and treatment is important. Although the SARS-CoV-2 virus-specific reverse transcription polymerase chain reaction test is recommended for the diagnosis of COVID-19, the test results are prone to be false negative in the early course of COVID-19 infection. To enhance the screening efficiency and accessibility, chest images captured ", "journal": "Frontiers in artificial intelligence", "date": "2021-08-10", "authors": ["HanqiuDeng", "XingyuLi"], "doi": "10.3389/frai.2021.612914\n10.1109/TNNLS.2021.3082015\n10.1007/s10489-020-01829-7\n10.1016/j.patrec.2020.09.010\n10.1109/ACCESS.2021.3064927\n10.7150/ijms.46684\n10.1016/j.compbiomed.2020.104037\n10.1109/INMIC50486.2020.9318212\n10.1007/s40846-020-00529-4\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103795\n10.1016/j.asoc.2020.106912\n10.1080/07391102.2020.1767212\n10.1109/ICCVW.2019.00052\n10.1001/jama.2020.2565\n10.1016/j.cmpb.2020.105608\n10.1016/j.eswa.2020.113909\n10.1016/j.media.2020.101844\n10.1109/ACCESS.2020.3010287\n10.7759/cureus.9448\n10.1016/j.media.2020.101910\n10.1109/rbme.2020.2990959\n10.1007/s12559-020-09802-9\n10.2174/1874347102012010011\n10.1109/tmi.2020.2996645\n10.1007/s11548-020-02299-5\n10.1109/tmi.2020.2996256\n10.1016/j.tmaid.2020.101627\n10.1101/2020.06.08.20125963\n10.1101/2020.04.13.20063941\n10.1371/journal.pone.0243963\n10.1038/s41746-021-00399-3\n10.1038/s41467-020-18685-1\n10.1016/j.cmpb.2020.105581\n10.1101/2020.05.12.20099937\n10.1109/TAI.2020.3020521\n10.1038/s41746-020-00369-1\n10.1016/j.media.2021.101978\n10.1016/j.ejrad.2020.108961\n10.1101/2020.05.12.20098954\n10.1101/2020.05.10.20097063\n10.1109/JBHI.2020.3034296\n10.1016/j.media.2020.101794\n10.1101/2020.05.20.20100362\n10.1016/j.irbm.2020.07.001\n10.21203/rs.3.rs-26500/v1\n10.1016/j.compbiomed.2020.103792\n10.1109/access.2020.3009328\n10.1007/s10489-020-01900-3\n10.1101/2020.06.08.20121541\n10.3390/diagnostics10060358\n10.1038/s42256-021-00307-0\n10.1007/978-3-319-24574-4_28\n10.2214/ajr.20.23034\n10.1109/rbme.2020.2987975\n10.1016/j.imu.2020.100405\n10.1016/j.compbiomed.2021.104304\n10.1101/2020.04.24.20078584\n10.1109/tcbb.2021.3065361\n10.1080/14737159.2020.1757437\n10.1016/j.mehy.2020.109761\n10.1007/s10140-020-01817-x\n10.1109/ACCESS.2020.2994762\n10.1016/j.asoc.2020.106897\n10.1109/tmi.2020.3000314\n10.1038/s41598-020-76550-z\n10.1007/s00330-021-07715-1\n10.1183/13993003.00775-2020\n10.1148/radiol.2020201160\n10.1109/tip.2021.3058783\n10.1093/cid/ciaa460\n10.1016/j.imu.2020.100412\n10.1101/2020.05.09.20096560\n10.1109/tmi.2020.3040950\n10.1016/j.cell.2020.04.045\n10.1109/tmi.2020.3001810\n10.2214/ajr.20.22975"}
{"title": "Automated detection of pneumonia in lung ultrasound using deep video classification for COVID-19.", "abstract": "There is a crucial need for quick testing and diagnosis of patients during the COVID-19 pandemic. Lung ultrasound is an imaging modality that is cost-effective, widely accessible, and can be used to diagnose acute respiratory distress syndrome in patients with COVID-19. It can be used to find important characteristics in the images, including A-lines, B-lines, consolidation, and pleural effusion, which all inform the clinician in monitoring and diagnosing the disease. With the use of portable ultrasound transducers, lung ultrasound images can be easily acquired, however, the images are often of poor quality. They often require an expert clinician interpretation, which may be time-consuming and is highly subjective. We propose a method for fast and reliable interpretation of lung ultrasound images by use of deep learning, based on the Kinetics-I3D network. Our learned model can classify an entire lung ultrasound scan obtained at point-of-care, without requiring the use of preprocessing or a frame-by-frame analysis. We compare our video classifier against ground truth classification annotations provided by a set of expert radiologists and clinicians, which include A-lines, B-lines, consolidation, and pleural effusion. Our classification method achieves an accuracy of 90% and an average precision score of 95% with the use of 5-fold cross-validation. The results indicate the potential use of automated analysis of portable lung ultrasound images to assist clinicians in screening and diagnosing patients.", "journal": "Informatics in medicine unlocked", "date": "2021-08-10", "authors": ["SaleheErfanian Ebadi", "DeepaKrishnaswamy", "Seyed Ehsan SeyedBolouri", "DornooshZonoobi", "RussellGreiner", "NathanielMeuser-Herr", "Jacob LJaremko", "JeeveshKapur", "MichelleNoga", "KumaradevanPunithakumar"], "doi": "10.1016/j.imu.2021.100687"}
{"title": "A Novel Weighted Consensus Machine Learning Model for COVID-19 Infection Classification Using CT Scan Images.", "abstract": "As COVID-19 has spread rapidly, detection of the COVID-19 infection from radiology and radiography images is probably one of the quickest ways to diagnose the patients. Many researchers found the necessity to utilize chest X-ray and chest computed tomography imaging to diagnose COVID-19 infection. In this paper, our objective is to minimize the false negatives and false positives in the detection process. Reduction in the number of false negatives minimizes community spread of the COVID-19 pandemic. Reducing false positives help people avoid mental trauma and wasteful expenses. This paper proposes a novel weighted consensus model to minimize the number of false negatives and false positives without compromising accuracy. In the proposed novel weighted consensus model, the accuracy of individual classification models is normalized. While predicting, different models predict different classes, and the sum of the normalized accuracy for a particular class is then considered based on a predefined threshold value. We used traditional Machine Learning classification algorithms like Linear Regression, Support Vector Machine, ", "journal": "Arabian journal for science and engineering", "date": "2021-08-10", "authors": ["Rohit KumarBondugula", "Siba KUdgata", "Nitin SaiBommi"], "doi": "10.1007/s13369-021-05879-y\n10.1001/jama.2020.1585\n10.1002/ped4.12178\n10.1148/radiol.2020200642\n10.1093/cid/ciaa344\n10.1148/radiol.2020200432\n10.1016/j.mayocp.2020.04.004\n10.1148/radiol.2020200230\n10.1148/ryct.2020200026\n10.1148/radiol.2020200274\n10.1148/radiol.2020200280\n10.1007/s10916-020-01562-1\n10.1007/s00330-020-07044-9\n10.3390/diagnostics10060358\n10.1007/s00138-020-01119-9\n10.1109/ACCESS.2020.3040245"}
{"title": "Data augmentation approaches using cycle-consistent adversarial networks for improving COVID-19 screening in portable chest X-ray images.", "abstract": "The current COVID-19 pandemic, that has caused more than 100 million cases as well as more than two million deaths worldwide, demands the development of fast and accurate diagnostic methods despite the lack of available samples. This disease mainly affects the respiratory system of the patients and can lead to pneumonia and to severe cases of acute respiratory syndrome that result in the formation of several pathological structures in the lungs. These pathological structures can be explored taking advantage of chest X-ray imaging. As a recommendation for the health services, portable chest X-ray devices should be used instead of conventional fixed machinery, in order to prevent the spread of the pathogen. However, portable devices present several problems (specially those related with capture quality). Moreover, the subjectivity and the fatigue of the clinicians lead to a very difficult diagnostic process. To overcome that, computer-aided methodologies can be very useful even taking into account the lack of available samples that the COVID-19 affectation shows. In this work, we propose an improvement in the performance of COVID-19 screening, taking advantage of several cycle generative adversarial networks to generate useful and relevant synthetic images to solve the lack of COVID-19 samples, in the context of poor quality and low detail datasets obtained from portable devices. For validating this proposal for improved COVID-19 screening, several experiments were conducted. The results demonstrate that this data augmentation strategy improves the performance of a previous COVID-19 screening proposal, achieving an accuracy of 98.61% when distinguishing among NON-COVID-19 (", "journal": "Expert systems with applications", "date": "2021-08-10", "authors": ["Daniel IglesiasMor\u00eds", "Jos\u00e9 Joaquimde Moura Ramos", "Jorge NovoBuj\u00e1n", "Marcos OrtegaHortas"], "doi": "10.1016/j.eswa.2021.115681\n10.1007/s10489-020-01829-7\n10.1101/2020.05.01.20088211\n10.1109/ACCESS.2020.3033762\n10.1101/2020.08.13.20173997\n10.1016/j.neucom.2018.09.013\n10.1016/j.eswa.2020.114054\n10.1007/978-1-4842-2766-4_8\n10.1016/j.jacr.2020.02.008\n10.1101/2020.03.30.20047787\n10.3390/electronics9091388\n10.28919/cmbn/4765\n10.1016/j.compbiomed.2020.103792\n10.1109/ICCV.2017.74\n10.1155/2020/8889023\n10.1007/s00521-020-05636-6\n10.1016/j.eswa.2021.114677\n10.1109/ACCESS.2018.2885997\n10.1007/s10489-020-01867-1\n10.1109/ICCV.2017.244"}
{"title": "Wavelet and deep learning-based detection of SARS-nCoV from thoracic X-ray images for rapid and efficient testing.", "abstract": "This paper proposes a wavelet and artificial intelligence-enabled rapid and efficient testing procedure for patients with Severe Acute Respiratory Coronavirus Syndrome (SARS-nCoV) through a deep learning approach from thoracic X-ray images. Presently, the virus infection is diagnosed primarily by a process called the real-time Reverse Transcriptase-Polymerase Chain Reaction (rRT-PCR) based on its genetic prints. This whole procedure takes a substantial amount of time to identify and diagnose the patients infected by the virus. The proposed research uses a wavelet-based convolution neural network architectures to detect SARS-nCoV. CNN is pre-trained on the ImageNet and trained end-to-end using thoracic X-ray images. To execute Discrete Wavelet Transforms (DWT), the available mother wavelet functions from different families, namely Haar, Daubechies, Symlet, Biorthogonal, Coiflet, and Discrete Meyer, were considered. Two-level decomposition via DWT is adopted to extract prominent features peripheral and subpleural ground-glass opacities, often in the lower lobes explicitly from thoracic X-ray images to suppress noise effect, further enhancing the signal to noise ratio. The proposed wavelet-based deep learning models of both, two-class instances (COVID vs. Normal) and four-class instances (COVID-19 vs. PNA bacterial vs. PNA viral vs. Normal) were validated from publicly available databases using k-Fold Cross Validation (k-Fold CV) technique. In addition to these X-ray images, images of recent COVID-19 patients were further used to examine the model's practicality and real-time feasibility in combating the current pandemic situation. It was observed that the Symlet 7 approximation component with two-level manifested the highest test accuracy of 98.87%, followed by Biorthogonal 2.6 with an efficiency of 98.73%. While the test accuracy for Symlet 7 and Biorthogonal 2.6 is high, Haar and Daubechies with two levels have demonstrated excellent validation accuracy on unseen data. It was also observed that the precision, the recall rate, and the dice similarity coefficient for four-class instances were 98%, 98%, and 99%, respectively, using the proposed algorithm.", "journal": "Expert systems with applications", "date": "2021-08-10", "authors": ["Amar KumarVerma", "InturiVamsi", "PrernaSaurabh", "RadhikaSudha", "SabareeshG R", "RajkumarS"], "doi": "10.1016/j.eswa.2021.115650"}
{"title": "Fully automated unified prognosis of Covid-19 chest X-ray/CT scan images using Deep Covix-Net model.", "abstract": "SARS-COV2 (Covid-19) prevails in the form of multiple mutant variants causing pandemic situations around the world. Thus, medical diagnosis is not accurate. Although several clinical diagnostic methodologies have been introduced hitherto, chest X-ray and computed tomography (CT) imaging techniques complement the analytical methods (for instance, RT-PCR) to a certain extent. In this context, we demonstrate a novel framework by employing various image segmentation models to leverage the available image databases (9000 chest X-ray images and 6000 CT scan images). The proposed methodology is expected to assist in the prognosis of Covid-19-infected individuals through examination of chest X-rays and CT scans of images using the Deep Covix-Net model for identifying novel coronavirus-infected patients effectively and efficiently. The slice of the precision score is analysed in terms of performance metrics such as accuracy, the confusion matrix, and the receiver operating characteristic curve. The result leans on the database obtainable in the GitHub and Kaggle repository, conforming to their endorsed chest X-ray and CT images. The classification performances of various algorithms were examined for a test set with 1800 images. The proposed model achieved a 96.8% multiple-classification accuracy among Covid-19, normal, and pneumonia chest X-ray databases. Moreover, it attained a 97% accuracy among Covid-19 and normal CT scan images. Thus, the proposed mechanism achieves the rigorousness associated with the machine learning technique, providing rapid outcomes for both training and testing datasets.", "journal": "Computers in biology and medicine", "date": "2021-08-09", "authors": ["Dasari NagaVinod", "B RebeccaJeyavadhanam", "Adamu MurtalaZungeru", "S R SPrabaharan"], "doi": "10.1016/j.compbiomed.2021.104729\n10.1007/s12098-020-03263-6\n10.1016/S0140-6736(20)30154-9\n10.1038/s41586-020-2008-3\n10.1016/j.ssci.2020.104806\n10.1016/j.ajem.2020.04.003\n10.1016/S0140-6736(19)32498-5\n10.2214/AJR.20.23034\n10.1016/j.ejrad.2020.108961\n10.1016/j.idm.2020.03.002\n10.1007/s10916-020-1536-6\n10.1016/j.clineuro.2020.105866\n10.1016/j.diii.2020.03.014\n10.1148/radiol.2020200642\n10.1109/TMI.2020.2993291\n10.3390/SYM12040651\n10.1016/j.imu.2020.100360\n10.31224/osf.io/wx89s\n10.3892/etm.2020.8797"}
{"title": "CoLe-CNN+: Context learning - Convolutional neural network for COVID-19-Ground-Glass-Opacities detection and segmentation.", "abstract": "The most common tool for population-wide COVID-19 identification is the Reverse Transcription-Polymerase Chain Reaction test that detects the presence of the virus in the throat (or sputum) in swab samples. This test has a sensitivity between 59% and 71%. However, this test does not provide precise information regarding the extension of the pulmonary infection. Moreover, it has been proven that through the reading of a computed tomography (CT) scan, a clinician can provide a more complete perspective of the severity of the disease. Therefore, we propose a comprehensive system for fully-automated COVID-19 detection and lesion segmentation from CT scans, powered by deep learning strategies to support decision-making process for the diagnosis of COVID-19.\nIn the workflow proposed, the input CT image initially goes through lung delineation, then COVID-19 detection and finally lesion segmentation. The chosen neural network has a U-shaped architecture using a newly introduced Multiple Convolutional Layers structure, that produces a lung segmentation mask within a novel pipeline for direct COVID-19 detection and segmentation. In addition, we propose a customized loss function that guarantees an optimal balance on average between sensitivity and precision.\nLungs' segmentation results show a sensitivity near 99% and Dice-score of 97%. No false positives were observed in the detection network after 10 different runs with an average accuracy of 97.1%. The average accuracy for lesion segmentation was approximately 99%. Using UNet as a benchmark, we compared our results with several other techniques proposed in the literature, obtaining the largest improvement over the UNet outcomes.\nThe method proposed in this paper outperformed the state-of-the-art methods for COVID-19 lesion segmentation from CT images, and improved by 38.2% the results for F1-score of UNet. The high accuracy observed in this work opens up a wide range of possible applications of our algorithm in other fields related to medical image segmentation.", "journal": "Computers in biology and medicine", "date": "2021-08-08", "authors": ["GiuseppePezzano", "OliverD\u00edaz", "Vicent RibasRipoll", "PetiaRadeva"], "doi": "10.1016/j.compbiomed.2021.104689\n10.1148/radiol.2020200642\n10.1007/s00330-021-07715-1"}
{"title": "COVID-19 diagnosis and severity detection from CT-images using transfer learning and back propagation neural network.", "abstract": "COVID-19 diagnosis in symptomatic patients is an important factor for arranging the necessary lifesaving facilities like ICU care and ventilator support. For this purpose, we designed a computer-aided diagnosis and severity detection method by using transfer learning and a back propagation neural network.\nTo increase the learning capability, we used data augmentation. Most of the previously done works in this area concentrate on private datasets, but we used two publicly available datasets. The first section diagnose COVID-19 from the input CT image using the transfer learning of the pre-trained network ResNet-50. We used ResNet-50 and DenseNet-201 pre-trained networks for feature extraction and trained a back propagation neural network to classify it into High, Medium, and Low severity.\nThe proposed method for COVID-19 diagnosis gave an accuracy of 98.5% compared with the state-of-the-art methods. The experimental evaluation shows that combining the ResNet-50 and DenseNet-201 features gave more accurate results with the test data. The proposed system for COVID-19 severity detection gave better average classification accuracy of 97.84% compared with the state-of-the-art methods. This enables medical practitioners to identify the resources and treatment plans correctly.\nThis work is useful in the medical field as a first-line severity risk detection that is helpful for medical personnel to plan patient care and assess the need for ICU facilities and ventilator support. A computer-aided system that is helpful to make a care plan for the huge amount of patient inflow each day is sure to be an asset in these turbulent times.", "journal": "Journal of infection and public health", "date": "2021-08-08", "authors": ["AswathyA L", "Anand HareendranS", "Vinod ChandraS S"], "doi": "10.1016/j.jiph.2021.07.015"}
{"title": "Automatic COVID-19 Detection Using Exemplar Hybrid Deep Features with X-ray Images.", "abstract": "COVID-19 and pneumonia detection using medical images is a topic of immense interest in medical and healthcare research. Various advanced medical imaging and machine learning techniques have been presented to detect these respiratory disorders accurately. In this work, we have proposed a novel COVID-19 detection system using an exemplar and hybrid fused deep feature generator with X-ray images. The proposed Exemplar COVID-19FclNet9 comprises three basic steps: exemplar deep feature generation, iterative feature selection and classification. The novelty of this work is the feature extraction using three pre-trained convolutional neural networks (CNNs) in the presented feature extraction phase. The common aspects of these pre-trained CNNs are that they have three fully connected layers, and these networks are AlexNet, VGG16 and VGG19. The fully connected layer of these networks is used to generate deep features using an exemplar structure, and a nine-feature generation method is obtained. The loss values of these feature extractors are computed, and the best three extractors are selected. The features of the top three fully connected features are merged. An iterative selector is used to select the most informative features. The chosen features are classified using a support vector machine (SVM) classifier. The proposed COVID-19FclNet9 applied nine deep feature extraction methods by using three deep networks together. The most appropriate deep feature generation model selection and iterative feature selection have been employed to utilise their advantages together. By using these techniques, the image classification ability of the used three deep networks has been improved. The presented model is developed using four X-ray image corpora (DB1, DB2, DB3 and DB4) with two, three and four classes. The proposed Exemplar COVID-19FclNet9 achieved a classification accuracy of 97.60%, 89.96%, 98.84% and 99.64% using the SVM classifier with 10-fold cross-validation for four datasets, respectively. Our developed Exemplar COVID-19FclNet9 model has achieved high classification accuracy for all four databases and may be deployed for clinical application.", "journal": "International journal of environmental research and public health", "date": "2021-08-08", "authors": ["Prabal DattaBarua", "Nadia FareedaMuhammad Gowdh", "KartiniRahmat", "NorlisahRamli", "Wei LinNg", "Wai YeeChan", "MutluKuluozturk", "SengulDogan", "MehmetBaygin", "OrhanYaman", "TurkerTuncer", "TaoWen", "Kang HaoCheong", "U RajendraAcharya"], "doi": "10.3390/ijerph18158052\n10.1109/ACCESS.2020.3005510\n10.1109/ACCESS.2020.2994762\n10.1002/bies.202000063\n10.1002/advs.202002324\n10.1002/bies.202000178\n10.1002/jmv.26699\n10.1080/07391102.2020.1788642\n10.1001/jama.2020.3786\n10.1007/s10489-020-01829-7\n10.1177/1471082X17722607\n10.1016/j.jinf.2020.03.047\n10.1016/j.jrid.2020.04.003\n10.1016/j.cmpb.2020.105532\n10.1007/s10489-020-01900-3\n10.1049/iet-ipr.2017.0232\n10.1109/TCSVT.2019.2916167\n10.1109/TNNLS.2020.3026621\n10.1109/TII.2019.2910876\n10.1080/03772063.2018.1531730\n10.1007/s12652-020-01816-3\n10.1016/j.knosys.2019.104923\n10.1016/j.bspc.2020.101872\n10.1002/ima.22552\n10.1016/j.bspc.2021.102622\n10.1183/13993003.00775-2020\n10.1109/ACCESS.2020.3001973\n10.1145/3065386\n10.1109/ACCESS.2020.2992641\n10.1007/s10044-021-00984-y\n10.1016/j.inffus.2021.02.013\n10.3390/sym12040651\n10.1007/s12652-021-02967-7\n10.1016/j.chemolab.2020.104054\n10.1016/j.compbiomed.2021.104425\n10.1016/j.cell.2018.02.010\n10.1016/j.compbiomed.2020.103792\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2021.104319\n10.1007/s12652-020-02688-3\n10.1016/j.bspc.2021.102490\n10.1016/j.radi.2020.10.018\n10.1016/j.eswa.2021.114883\n10.1016/j.chaos.2020.110495\n10.1109/JBHI.2021.3074893\n10.1007/s10489-020-01943-6\n10.1007/s10489-020-02055-x"}
{"title": "Illness duration and symptom profile in symptomatic UK school-aged children tested for SARS-CoV-2.", "abstract": "In children, SARS-CoV-2 infection is usually asymptomatic or causes a mild illness of short duration. Persistent illness has been reported; however, its prevalence and characteristics are unclear. We aimed to determine illness duration and characteristics in symptomatic UK school-aged children tested for SARS-CoV-2 using data from the COVID Symptom Study, one of the largest UK citizen participatory epidemiological studies to date.\nIn this prospective cohort study, data from UK school-aged children (age 5-17 years) were reported by an adult proxy. Participants were voluntary, and used a mobile application (app) launched jointly by Zoe Limited and King's College London. Illness duration and symptom prevalence, duration, and burden were analysed for children testing positive for SARS-CoV-2 for whom illness duration could be determined, and were assessed overall and for younger (age 5-11 years) and older (age 12-17 years) groups. Children with longer than 1 week between symptomatic reports on the app were excluded from analysis. Data from symptomatic children testing negative for SARS-CoV-2, matched 1:1 for age, gender, and week of testing, were also assessed.\n258\u2009790 children aged 5-17 years were reported by an adult proxy between March 24, 2020, and Feb 22, 2021, of whom 75\u2009529 had valid test results for SARS-CoV-2. 1734 children (588 younger and 1146 older children) had a positive SARS-CoV-2 test result and calculable illness duration within the study timeframe (illness onset between Sept 1, 2020, and Jan 24, 2021). The most common symptoms were headache (1079 [62\u00b72%] of 1734 children), and fatigue (954 [55\u00b70%] of 1734 children). Median illness duration was 6 days (IQR 3-11) versus 3 days (2-7) in children testing negative, and was positively associated with age (Spearman's rank-order r\nAlthough COVID-19 in children is usually of short duration with low symptom burden, some children with COVID-19 experience prolonged illness duration. Reassuringly, symptom burden in these children did not increase with time, and most recovered by day 56. Some children who tested negative for SARS-CoV-2 also had persistent and burdensome illness. A holistic approach for all children with persistent illness during the pandemic is appropriate.\nZoe Limited, UK Government Department of Health and Social Care, Wellcome Trust, UK Engineering and Physical Sciences Research Council, UK Research and Innovation London Medical Imaging and Artificial Intelligence Centre for Value Based Healthcare, UK National Institute for Health Research, UK Medical Research Council, British Heart Foundation, and Alzheimer's Society.", "journal": "The Lancet. Child & adolescent health", "date": "2021-08-07", "authors": ["ErikaMolteni", "Carole HSudre", "Liane SCanas", "Sunil SBhopal", "Robert CHughes", "MichelaAntonelli", "BenjaminMurray", "KerstinKl\u00e4ser", "EricKerfoot", "LiyuanChen", "JieDeng", "ChristinaHu", "SomeshSelvachandran", "KennethRead", "JoanCapdevila Pujol", "AlexanderHammers", "Tim DSpector", "SebastienOurselin", "Claire JSteves", "MarcModat", "MichaelAbsoud", "Emma LDuncan"], "doi": "10.1016/S2352-4642(21)00198-X\n10.1542/peds.2020-042929\n10.1101/2021.03.16.21253719\n10.1101/2020.12.15.20248096"}
{"title": "Early Prediction of COVID-19 Ventilation Requirement and Mortality from Routinely Collected Baseline Chest Radiographs, Laboratory, and Clinical Data with Machine Learning.", "abstract": "Coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), emerged in Wuhan, China, in late 2019 and created a global pandemic that overwhelmed healthcare systems. COVID-19, as of July 3, 2021, yielded 182 million confirmed cases and 3.9 million deaths globally according to the World Health Organization. Several patients who were initially diagnosed with mild or moderate COVID-19 later deteriorated and were reclassified to severe disease type.\nThe aim is to create a predictive model for COVID-19 ventilatory support and mortality early on from baseline (at the time of diagnosis) and routinely collected data of each patient (CXR, CBC, demographics, and patient history).\nFour common machine learning algorithms, three data balancing techniques, and feature selection are used to build and validate predictive models for COVID-19 mechanical requirement and mortality. Baseline CXR, CBC, demographic, and clinical data were retrospectively collected from April 2, 2020, till June 18, 2020, for 5739 patients with confirmed PCR COVID-19 at King Abdulaziz Medical City in Riyadh. However, of those patients, only 1508 and 1513 have met the inclusion criteria for ventilatory support and mortalilty endpoints, respectively.\nIn an independent test set, ventilation requirement predictive model with top 20 features selected with reliefF algorithm from baseline radiological, laboratory, and clinical data using support vector machines and random undersampling technique attained an AUC of 0.87 and a balanced accuracy of 0.81. For mortality endpoint, the top model yielded an AUC of 0.83 and a balanced accuracy of 0.80 using all features with balanced random forest. This indicates that with only routinely collected data our models can predict the outcome with good performance. The predictive ability of combined data consistently outperformed each data set individually for intubation and mortality. For the ventilator support, chest X-ray severity annotations alone performed better than comorbidity, complete blood count, age, or gender with an AUC of 0.85 and balanced accuracy of 0.79. For mortality, comorbidity alone achieved an AUC of 0.80 and a balanced accuracy of 0.72, which is higher than models that use either chest radiograph, laboratory, or demographic features only.\nThe experimental results demonstrate the practicality of the proposed COVID-19 predictive tool for hospital resource planning and patients' prioritization in the current COVID-19 pandemic crisis.", "journal": "Journal of multidisciplinary healthcare", "date": "2021-08-07", "authors": ["Abdulrhman FahadAljouie", "AhmedAlmazroa", "YahyaBokhari", "MohammedAlawad", "EbrahimMahmoud", "EmanAlawad", "AliAlsehawi", "MamoonRashid", "LamyaAlomair", "ShahadAlmozaai", "BedoorAlbesher", "HassanAlomaish", "RayyanDaghistani", "Naif KhalafAlharbi", "ManalAlaamery", "MohammadBosaeed", "HeshamAlshaalan"], "doi": "10.2147/JMDH.S322431\n10.1056/NEJMoa2002032\n10.1001/jama.2020.6775\n10.1016/S2589-7500(20)30217-X\n10.1038/s41392-020-0148-4\n10.1093/ofid/ofaa102\n10.1016/S0140-6736(20)30566-3\n10.1093/cid/ciaa248\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020201160\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020201754\n10.21037/atm-20-2464\n10.3390/jcm9061668\n10.1016/j.amsu.2020.09.044\n10.1136/bmjspcare-2020-002602\n10.1136/bmj.m1328\n10.1183/13993003.01104-2020\n10.1016/j.chest.2020.04.003\n10.1007/s13246-020-00865-4\n10.3389/fmed.2020.00427\n10.1007/s10096-020-03901-z\n10.1023/A:1025667309714\n10.1109/ACCESS.2020.3014362\n10.1016/j.ins.2013.07.007\n10.1007/BF00994018\n10.1023/A:1010933404324\n10.2307/2529310\n10.21037/qims-20-642\n10.1016/j.jiph.2020.05.026\n10.1186/s13293-020-00330-7\n10.1001/jamanetworkopen.2020.22058\n10.1183/13993003.01112-2020\n10.3390/ijerph17165974\n10.1007/s11239-020-02324-z\n10.3389/fpubh.2021.626697\n10.1002/jmv.25819\n10.1007/s12559-020-09812-7\n10.1186/s12890-020-01286-5\n10.3348/kjr.2020.0132\n10.1186/s12938-020-00807-x\n10.1016/j.clinimag.2020.04.001\n10.1007/s11547-020-01232-9\n10.1101/2020.03.28.20045997\n10.1186/s12931-020-01455-4\n10.1093/cid/ciaa538\n10.1371/journal.pone.0240200"}
{"title": "Learning-to-augment strategy using noisy and denoised data: Improving generalizability of deep CNN for the detection of COVID-19 in X-ray images.", "abstract": "Chest X-ray images are used in deep convolutional neural networks for the detection of COVID-19, the greatest human challenge of the 21st century. Robustness to noise and improvement of generalization are the major challenges in designing these networks. In this paper, we introduce a strategy for data augmentation using the determination of the type and value of noise density to improve the robustness and generalization of deep CNNs for COVID-19 detection. Firstly, we present a learning-to-augment approach that generates new noisy variants of the original image data with optimized noise density. We apply a Bayesian optimization technique to control and choose the optimal noise type and its parameters. Secondly, we propose a novel data augmentation strategy, based on denoised X-ray images, that uses the distance between denoised and original pixels to generate new data. We develop an autoencoder model to create new data using denoised images corrupted by the Gaussian and impulse noise. A database of chest X-ray images, containing COVID-19 positive, healthy, and non-COVID pneumonia cases, is used to fine-tune the pre-trained networks (AlexNet, ShuffleNet, ResNet18, and GoogleNet). The proposed method performs better results compared to the state-of-the-art learning to augment strategies in terms of sensitivity (0.808), specificity (0.915), and F-Measure (0.737). The source code of the proposed method is available at https://github.com/mohamadmomeny/Learning-to-augment-strategy.", "journal": "Computers in biology and medicine", "date": "2021-08-06", "authors": ["MohammadMomeny", "Ali AsgharNeshat", "Mohammad ArafatHussain", "SolmazKia", "MahmoudMarhamati", "AhmadJahanbakhshi", "GhassanHamarneh"], "doi": "10.1016/j.compbiomed.2021.104704\n10.1016/j.chaos.2020.110495\n10.1016/j.media.2021.102125\n10.1016/j.eswa.2020.114054\n10.1016/j.bspc.2021.102920\n10.1016/j.compeleceng.2017.08.012\n10.1093/bioinformatics/btz259\n10.1016/j.neunet.2021.04.008\n10.1038/s41598-018-36047-2\n10.1016/j.rineng.2021.100225\n10.1109/ICIP40778.2020.9191116\n10.1016/j.jhydrol.2021.126510\n10.1016/j.aca.2004.10.086\n10.1016/j.ultrasmedbio.2019.09.018\n10.1109/SECON.2017.7925268\n10.1109/TMI.2004.832656\n10.1016/j.heliyon.2020.e03680\n10.1016/j.measurement.2019.107426\n10.1016/j.bbe.2018.08.005\n10.1016/j.dsp.2020.102835\n10.1016/j.image.2020.116061\n10.1016/j.sigpro.2019.01.002\n10.1016/j.bspc.2020.102123\n10.1016/j.chemolab.2020.104063\n10.1016/j.jvcir.2020.102851"}
{"title": "Automated Diagnosis of COVID-19 Using Deep Supervised Autoencoder With Multi-View Features From CT Images.", "abstract": "Accurate and rapid diagnosis of coronavirus disease 2019 (COVID-19) from chest CT scans is of great importance and urgency during the worldwide outbreak. However, radiologists have to distinguish COVID-19 pneumonia from other pneumonia in a large number of CT scans, which is tedious and inefficient. Thus, it is urgently and clinically needed to develop an efficient and accurate diagnostic tool to help radiologists to fulfill the difficult task. In this study, we proposed a deep supervised autoencoder (DSAE) framework to automatically identify COVID-19 using multi-view features extracted from CT images. To fully explore features characterizing CT images from different frequency domains, DSAE was proposed to learn the latent representation by multi-task learning. The proposal was designed to both encode valuable information from different frequency features and construct a compact class structure for separability. To achieve this, we designed a multi-task loss function, which consists of a supervised loss and a reconstruction loss. Our proposed method was evaluated on a newly collected dataset of 787 subjects including COVID-19 pneumonia patients, other pneumonia patients, and normal subjects without abnormal CT findings. Extensive experimental results demonstrated that our proposed method achieved encouraging diagnostic performance and may have potential clinical application for the diagnosis of COVID-19.", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "date": "2021-08-06", "authors": ["JianhongCheng", "WeiZhao", "JinLiu", "XingzhiXie", "ShangjieWu", "LiangliangLiu", "HailinYue", "JunjianLi", "JianxinWang", "JunLiu"], "doi": "10.1109/TCBB.2021.3102584"}
{"title": "Coronavirus disease analysis using chest X-ray images and a novel deep convolutional neural network.", "abstract": "The recent emergence of a highly infectious and contagious respiratory viral disease known as COVID-19 has vastly impacted human lives and overloaded the health care system. Therefore, it is crucial to develop a fast and accurate diagnostic system for the timely identification of COVID-19 infected patients and thus to help control its spread.\nThis work proposes a new deep CNN based technique for COVID-19 classification in X-ray images. In this regard, two novel custom CNN architectures, namely COVID-RENet-1 and COVID-RENet-2, are developed for COVID-19 specific pneumonia analysis. The proposed technique systematically employs Region and Edge-based operations along with convolution operations. The advantage of the proposed idea is validated by performing series of experimentation and comparing results with two baseline CNNs that exploited either a single type of pooling operation or strided convolution down the architecture. Additionally, the discrimination capacity of the proposed technique is assessed by benchmarking it against the state-of-the-art CNNs on radiologist's authenticated chest X-ray dataset. Implementation is available at https://github.com/PRLAB21/Coronavirus-Disease-Analysis-using-Chest-X-Ray-Images.\nThe proposed classification technique shows good generalization as compared to existing CNNs by achieving promising MCC (0.96), F-score (0.98) and Accuracy (98%). This suggests that the idea of synergistically using Region and Edge-based operations aid in better exploiting the region homogeneity, textural variations, and region boundary-related information in an image, which helps to capture the pneumonia specific pattern.\nThe encouraging results of the proposed classification technique on the test set with high sensitivity (0.98) and precision (0.98) suggest the effectiveness of the proposed technique. Thus, it suggests the potential use of the proposed technique in other X-ray imagery-based infectious disease analysis.", "journal": "Photodiagnosis and photodynamic therapy", "date": "2021-08-05", "authors": ["Saddam HussainKhan", "AnabiaSohail", "Muhammad MohsinZafar", "AsifullahKhan"], "doi": "10.1016/j.pdpdt.2021.102473\n10.1142/S0218339020500096\n10.7150/ijbs.45053\n10.1016/j.jare.2020.03.005\n10.1016/j.pdpdt.2020.102112\n10.1016/j.diii.2020.03.014\n10.1016/j.cpcardiol.2020.100618\n10.1016/j.pdpdt.2020.101804\n10.1016/S1473-3099(20)30190-0\n10.1002/jmv.25721\n10.1016/j.jare.2020.08.002\n10.1038/s41591-020-0931-3\n10.1097/RTI.0000000000000533\n10.1002/mp.14142\n10.1186/s12890-020-01286-5\n10.1148/radiol.2020200642\n10.1007/s11547-020-01200-3\n10.1016/j.pdpdt.2020.101923\n10.1016/j.pdpdt.2020.101885\n10.1038/s41598-021-85652-1\n10.1016/j.media.2020.101794\n10.1186/s40537-019-0197-0\n10.1007/s10462-020-09825-6\n10.1109/CIEC.2014.6959172\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.5244/C.30.87\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.308\n10.1016/j.neucom.2019.01.112\n10.1093/jmicro/dfz002"}
{"title": "Role of deep learning in early detection of COVID-19: Scoping review.", "abstract": "Since the onset of the COVID-19 pandemic, the world witnessed disruption on an unprecedented scale affecting our daily lives including but not limited to healthcare, business, education, and transportation. Deep Learning (DL) is a branch of Artificial intelligence (AI) applications, the recent growth of DL includes features that could be helpful in fighting the COVID-19 pandemic. Utilizing such features could support public health efforts.\nInvestigate the literature available in the use of DL technology to support dealing with the COVID-19 crisis. We summarize the literature that uses DL features to analyze datasets for the purpose of a quick COVID-19 detection.\nThis review follows PRISMA Extension for Scoping Reviews (PRISMA-ScR). We have scanned the most two commonly used databases (IEEE, ACM). Search terms were identified based on the target intervention (DL) and the target population (COVID-19). Two authors independently handled study selection and one author assigned for data extraction. A narrative approach is used to synthesize the extracted data.\nWe retrieved 53 studies and after passing through PRISMA excluding criteria, only 17 studies are considered in this review. All studies used deep learning for detection of COVID-19 cases in early stage based on different diagnostic modalities. Convolutional Neural Network (CNN) and Transfer Learning (TL) were the most commonly used techniques.\nThe included studies showed that DL techniques has significant impact on early detection of COVID-19 with high accuracy rate. However, most of the proposed methods are still in development and not tested in a clinical setting. Further investigation and collaboration are required from the research community and healthcare professionals in order to develop and standardize guidelines for use of DL in the healthcare domain.", "journal": "Computer methods and programs in biomedicine update", "date": "2021-08-05", "authors": ["MahmoodAlzubaidi", "Haider DhiaZubaydi", "Ali AbdulqaderBin-Salem", "Alaa AAbd-Alrazaq", "ArfanAhmed", "MowafaHouseh"], "doi": "10.1016/j.cmpbup.2021.100025\n10.1038/s41564-020-0713-1\n10.6314/JIMT.202004_31(2).01\n10.1016/S1473-3099(20)30235-8\n10.1038/s41423-020-0407-x\n10.1038/s41591-020-0824-5\n10.2196/13659\n10.2196/12100\n10.1152/physiolgenomics.00029.2020\n10.1613/JAIR.1.12162\n10.1007/978-3-030-32644-9_32\n10.3390/electronics8030292\n10.1145/3305366.3328026\n10.1007/s10916-020-01562-1\n10.1109/RBME.2020.2987975\n10.1016/j.dsx.2020.04.012\n10.2196/20756\n10.1109/ACCESS.2020.2992341\n10.1109/ACCESS.2020.3009328\n10.1109/TCYB.2020.2990162\n10.7326/M18-0850\n10.1186/s12874-018-0611-x\n10.14245/ns.1938396.198\n10.1109/ISCV49265.2020.9204282\n10.1145/3414274.3414496\n10.1109/ACCESS.2020.3007939\n10.1109/TMI.2020.2993291\n10.1109/ACCESS.2020.3003810\n10.1109/ACCESS.2020.2994762\n10.1145/3411408.3411416\n10.1109/ICIRCA48905.2020.9183278\n10.1109/ISCV49265.2020.9204099\n10.1109/ACCESS.2020.3025164\n10.1109/ISIEA49364.2020.9188133\n10.1109/TMI.2020.2995965\n10.1109/ACCESS.2020.3005510\n10.1109/TMI.2020.2996256\n10.1109/JBHI.2020.3018181\n10.1109/JBHI.2020.3023246\n10.1109/ACCESS.2020.3016780\n10.1109/TMI.2020.2994459\n10.1109/ICOSEC49089.2020.9215294\n10.1109/ACCESS.2020.3001973"}
{"title": "[New developments and perspectives in acromegaly].", "abstract": "Acromegaly is a rare but severe disorder which is usually due to an excessive secretion of growth hormone (GH) by a pituitary adenoma. Screening mainly relies on the measurement of insulin-like growth factor 1, and confirmatory diagnostics includes a GH suppression test. As delayed diagnosis results in increased morbidity and mortality, we here discuss recently published suggestions regarding the biochemical work-up of suspected cases and the follow-up of co-morbidities. Moreover, new analytical tools (such as automatic identification of typical facial changes using artificial intelligence) are presented, hopefully allowing for an earlier diagnosis in the future. So far, surgery is still regarded as therapy of first choice. In cases without postoperative remission, a new imaging approach (combining sellar magnetic resonance imaging and position emission tomography) may improve the results of repeated surgery. The pharmaceutical arsenal now includes the first orally available somatostatin analogue, and recent data on possible drug combinations and the outcome of radiotherapy are presented. Finally, special attention is paid to older and pregnant patients, as well as certain considerations during the COVID-19 pandemic (where appropriate diagnosis and management of acromegaly is particularly challenging).\n\u2002Bei Verdacht auf eine Akromegalie wird zun\u00e4chst das Hormon Insulin-like growth factor 1 (IGF-1) als wesentlicher Mediator des Wachstumshormons (GH) bestimmt. Ist es erh\u00f6ht, schlie\u00dft sich eine Best\u00e4tigungsdiagnostik mittels GH-Suppressionstest an. Neue Arbeiten empfehlen f\u00fcr diesen Test niedrigere GH-Grenzwerte als fr\u00fcher, zudem sollen potenzielle Einflussgr\u00f6\u00dfen (z.\u200aB. Body-Mass-Index) st\u00e4rker ber\u00fccksichtigt werden. Perspektivisch k\u00f6nnten Erkrankte mittels einer automatisierten Gesichtserkennung ggf. leichter identifiziert werden. KOMORBIDIT\u00e4TEN: \u2002Bei einem unkontrollierten GH-Exzess sind Lebensqualit\u00e4t und -erwartung zum Teil erheblich reduziert. Eine Akromegalie sowie deren typische Folgeerkrankungen (z.\u200aB. Schlafapnoe, Kardiomyopathie, Arthropathie) m\u00fcssen daher fr\u00fchzeitig erkannt werden. K\u00fcrzlich wurden neue Empfehlungen f\u00fcr ein standardisiertes diagnostisches Vorgehen publiziert.\n\u2002Die operative Adenomentfernung durch einen erfahrenen Hypophysenchirurgen ist Therapie der Wahl. Bei residueller Erkrankung kann perspektivisch eine Kombination aus volumetrischer Magnetresonanztomografie (MRT) und 11C-Methionin-Positronen-Emissions-Tomografie (PET) eine Folgeoperation erleichtern. F\u00fcr die typische Zweitlinientherapie mit Somatostatin-Analoga (SSA) ist nun erstmals auch ein oral einzusetzendes Pr\u00e4parat verf\u00fcgbar. Neue Daten belegen die Wirksamkeit und Sicherheit einer Hypophysenbestrahlung.\n\u2002Schwangere und \u00e4ltere Patienten bed\u00fcrfen besonderer Aufmerksamkeit. Gem\u00e4\u00df aktueller Daten wirkt sich die COVID-Pandemie auch bei einer Akromegalie nachteilig auf Diagnostik und Therapie aus.", "journal": "Deutsche medizinische Wochenschrift (1946)", "date": "2021-08-04", "authors": ["MarioDetomas", "MiriamReuter", "TimoDeutschbein"], "doi": "10.1055/a-1495-2715"}
{"title": "Artificial intelligence-driven assessment of radiological images for COVID-19.", "abstract": "Artificial Intelligence (AI) methods have significant potential for diagnosis and prognosis of COVID-19 infections. Rapid identification of COVID-19 and its severity in individual patients is expected to enable better control of the disease individually and at-large. There has been remarkable interest by the scientific community in using imaging biomarkers to improve detection and management of COVID-19. Exploratory tools such as AI-based models may help explain the complex biological mechanisms and provide better understanding of the underlying pathophysiological processes. The present review focuses on AI-based COVID-19 studies as applies to chest x-ray (CXR) and computed tomography (CT) imaging modalities, and the associated challenges. Explicit radiomics, deep learning methods, and hybrid methods that combine both deep learning and explicit radiomics have the potential to enhance the ability and usefulness of radiological images to assist clinicians in the current COVID-19 pandemic. The aims of this review are: first, to outline COVID-19 AI-analysis workflows, including acquisition of data, feature selection, segmentation methods, feature extraction, and multi-variate model development and validation as appropriate for AI-based COVID-19 studies. Secondly, existing limitations of AI-based COVID-19 analyses are discussed, highlighting potential improvements that can be made. Finally, the impact of AI and radiomics methods and the associated clinical outcomes are summarized. In this review, pipelines that include the key steps for AI-based COVID-19 signatures identification are elaborated. Sample size, non-standard imaging protocols, segmentation, availability of public COVID-19 databases, combination of imaging and clinical information and full clinical validation remain major limitations and challenges. We conclude that AI-based assessment of CXR and CT images has significant potential as a viable pathway for the diagnosis, follow-up and prognosis of COVID-19.", "journal": "Computers in biology and medicine", "date": "2021-08-04", "authors": ["YassineBouchareb", "PegahMoradi Khaniabadi", "FaizaAl Kindi", "HumoudAl Dhuhli", "IsaacShiri", "HabibZaidi", "ArmanRahmim"], "doi": "10.1016/j.compbiomed.2021.104665\n10.1186/s12879-020-05383-y\n10.1016/j.diii.2020.03.014\n10.1097/MNM.0000000000001269\n10.21203/rs.3.rs-23196/v1\n10.1016/j.rbmo.2020.06.001\n10.1016/j.pdpdt.2021.102287\n10.1038/s41467-020-18685-1\n10.1007/s00216-020-02889-x\n10.5152/dir.2020.20205\n10.1016/j.ejmp.2021.03.008\n10.1186/s41824-020-00086-8\n10.1148/radiol.2020202944\n10.1148/radiol.2021202553\n10.1016/j.compbiomed.2020.104135\n10.1002/mp.14368\n10.1007/s11547-020-01169-z\n10.1007/s11307-020-01550-4\n10.1007/s11307-019-01411-9\n10.1007/s12350-020-02109-0\n10.1007/s00259-020-04852-5\n10.1088/1361-6560/ab8535\n10.1007/s00259-020-05167-1\n10.1007/s00259-020-05013-4\n10.1007/s12350-020-02119-y\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2995508\n10.1148/radiol.2020201491\n10.1007/s00330-020-07044-9\n10.1148/radiol.2020202439\n10.1038/s41467-020-17971-2\n10.1038/s41598-021-83237-6\n10.1016/j.patter.2021.100269\n10.1016/j.scs.2020.102589\n10.1016/j.compbiomed.2021.104210\n10.3389/fgene.2021.636441\n10.1093/cid/ciab147\n10.1088/1361-6560/abe838\n10.1101/2020.02.27.20028027\n10.1038/s41598-021-84637-4\n10.3389/fimmu.2020.01581\n10.1101/2020.03.09.20032219\n10.9781/ijimai.2020.02.002\n10.1101/2020.03.18.20035816\n10.1101/2020.02.29.20029603\n10.1148/radiol.2018181352\n10.21203/rs.3.rs-34648/v1\n10.1101/2020.08.03.20167007\n10.7150/ijbs.53982\n10.1016/j.compbiomed.2021.104304\n10.1097/RTI.0000000000000533\n10.1007/s11547-020-01200-3\n10.1016/j.clinimag.2020.04.001\n10.1016/j.irbm.2020.07.001\n10.1101/2020.03.26.20044610\n10.1007/s40846-020-00529-4\n10.1038/s41598-020-76550-z\n10.1177/0846537120913033\n10.1007/s42247-021-00164-y\n10.1007/s00330-020-06801-0\n10.1007/s00330-020-07225-6\n10.1002/mp.14609\n10.1101/2020.02.23.20026930\n10.1148/radiol.2020200905\n10.1148/radiol.2020200823\n10.2214/AJR.20.22954\n10.1109/TBDATA.2021.3056564\n10.1007/s10278-021-00434-5\n10.1016/j.ejmp.2019.10.001\n10.1016/j.compbiomed.2021.104526\n10.1101/2020.03.12.20027185\n10.1101/2020.02.25.20021568\n10.1148/ryct.2020200044\n10.1016/j.jpha.2020.03.004\n10.1371/journal.pone.0235187\n10.1016/j.compbiomed.2020.104037\n10.1002/mp.14676\n10.1007/s11432-020-2849-3\n10.1016/j.compbiomed.2012.12.004\n10.1002/mp.14609\n10.1002/mp.14609\n10.1101/2021.04.08.21255163\n10.1148/radiol.2020191145\n10.20944/preprints202003.0284.v1%20\n10.1016/j.imu.2020.100412\n10.13140/RG%202\n10.1007/s10489-020-01829-7\n10.21203/rs.3.rs-132785/v2\n10.1097/RTI.0000000000000544\n10.1109/JBHI.2020.3036722\n10.1016/j.jksuci.2020.12.010\n10.1007/s11517-020-02299-2\n10.1007/s12539-020-00403-6\n10.1101/2020.05.09.20096560\n10.7717/peerj.10086\n10.1101/2020.04.28.20082966\n10.1007/s13246-020-00952-6\n10.2174/1573405617999210112195450\n10.1007/s00330-020-07032-z\n10.1016/j.acra.2020.09.004\n10.20944/preprints202003.0300.v1\n10.1186/s12967-020-02692-3\n10.1007/s10044-021-00984-y%20\n10.1007/s13246-020-00865-4\n10.21203/rs.3.rs-74208/v1\n10.1186/s12880-021-00564-w\n10.1007/s00330-020-07012-3\n10.7150/thno.46428\n10.3233/XST-200831\n10.1016/j.eswa.2020.113909\n10.1016/j.ejrad.2021.109552\n10.1007/s10278-021-00421-w\n10.1016/j.ejro.2020.100271\n10.30476/IJMS.2021.88036.1858\n10.1186/s13054-021-03564-y\n10.1371/journal.pone.0246582\n10.1117/1.JMI.8.S1.014502\n10.1007/s10489-021-02352-z\n10.1016/j.eng.2020.10.013\n10.1148/ryct.2020200075\n10.1148/radiol.2020204226\n10.1007/s00330-020-07453-w\n10.1016/j.chaos.2020.110170\n10.1148/ryct.2020200322\n10.1186/s13244-020-00887-2\n10.2214/AJR.18.20624\n10.1016/j.jaci.2013.06.006\n10.1148/ryai.2020200098\n10.1148/radiol.2020200432"}
{"title": "The incremental value of computed tomography of COVID-19 pneumonia in predicting ICU admission.", "abstract": "Triage is crucial for patient's management and estimation of the required intensive care unit (ICU) beds is fundamental for health systems during the COVID-19 pandemic. We assessed whether chest computed tomography (CT) of COVID-19 pneumonia has an incremental role in predicting patient's admission to ICU. We performed volumetric and texture analysis of the areas of the affected lung in CT of 115 outpatients with COVID-19 infection presenting to the emergency room with dyspnea and unresponsive hypoxyemia. Admission blood laboratory including lymphocyte count, serum lactate dehydrogenase, D-dimer and C-reactive protein and the ratio between the arterial partial pressure of oxygen and inspired oxygen were collected. By calculating the areas under the receiver-operating characteristic curves (AUC), we compared the performance of blood laboratory-arterial gas analyses features alone and combined with the CT features in two hybrid models (Hybrid radiological and Hybrid radiomics)for predicting ICU admission. Following a machine learning approach, 63 patients were allocated to the training and 52 to the validation set. Twenty-nine (25%) of patients were admitted to ICU. The Hybrid radiological model comprising the lung %consolidation performed significantly (p\u2009=\u20090.04) better in predicting ICU admission in the validation (AUC\u2009=\u20090.82; 95% confidence interval 0.73-0.97) set than the blood laboratory-arterial gas analyses features alone (AUC\u2009=\u20090.71; 95% confidence interval 0.56-0.86). A risk calculator for ICU admission was derived and is available at: https://github.com/cgplab/covidapp . The volume of the consolidated lung in CT of patients with COVID-19 pneumonia has a mild but significant incremental value in predicting ICU admission.", "journal": "Scientific reports", "date": "2021-08-04", "authors": ["MaurizioBartolucci", "MatteoBenelli", "MargheritaBetti", "SaraBicchi", "LucaFedeli", "FedericoGiannelli", "DonatellaAquilini", "AlessioBaldini", "GuglielmoConsales", "Massimo EdoardoDi Natale", "PamelaLotti", "LetiziaVannucchi", "MicheleTrezzi", "Lorenzo NicolaMazzoni", "SandroSantini", "RobertoCarpi", "DanielaMatarrese", "LucaBernardi", "MarioMascalchi", "NoneNone"], "doi": "10.1038/s41598-021-95114-3\n10.1016/S1473-3099(20)30120-1\n10.1001/jama.2020.2648\n10.1056/NEJMp2005492\n10.1136/bmj.m1328\n10.1080/23744235.2020.1784457\n10.1016/j.ijid.2020.11.003\n10.1016/j.rmed.2020.106203\n10.1097/CCE.0000000000000253\n10.1038/s41598-020-75651-z\n10.7554/eLife.60519\n10.1371/journal.pone.0240711\n10.1007/s00430-020-00696-w\n10.1016/j.resuscitation.2020.10.039\n10.1016/j.hrtlng.2020.08.024\n10.7717/peerj.10337\n10.1164/rccm.2020C1\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020202723\n10.1007/s00330-020-07270-1\n10.1148/radiol.2020201433\n10.3348/kjr.2020.0096\n10.3390/jcm9051514\n10.1016/j.acra.2020.09.004\n10.1016/j.media.2020.101844\n10.1016/j.chest.2020.11.026\n10.1148/rg.2017170056\n10.1093/bioinformatics/btp621\n10.3174/ajnr.A3685\n10.1164/rccm.200812-1966OC\n10.1016/j.mri.2012.05.001\n10.1158/0008-5472.CAN-17-0339\n10.1158/0008-5472.CAN-20-0332\n10.18637/jss.v033.i01\n10.1186/1471-2105-12-77\n10.1056/NEJMoa052052\n10.1016/j.ejrad.2019.108748\n10.1164/rccm.202004-1412OC\n10.1164/rccm.202003-0817LE\n10.1001/jama.2020.6825\n10.1007/s00134-020-06033-2\n10.1164/rccm.202006-2157CP\n10.1016/S2213-2600(20)30370-2\n10.1002/jmv.26677\n10.1371/journal.pone.0237297\n10.1056/NEJMoa2020283\n10.1016/j.ijid.2020.10.003"}
{"title": "COVID TV-Unet: Segmenting COVID-19 chest CT images using connectivity imposed Unet.", "abstract": "The novel corona-virus disease (COVID-19) pandemic has caused a major outbreak in more than 200 countries around the world, leading to a severe impact on the health and life of many people globally. By October 2020, more than 44 million people were infected, and more than 1,000,000 deaths were reported. Computed Tomography (CT) images can be used as an alternative to the time-consuming RT-PCR test, to detect COVID-19. In this work we propose a segmentation framework to detect chest regions in CT images, which are infected by COVID-19. An architecture similar to a Unet model was employed to detect ground glass regions on a voxel level. As the infected regions tend to form connected components (rather than randomly distributed voxels), a suitable regularization term based on 2D-anisotropic total-variation was developed and added to the loss function. The proposed model is therefore called \"TV-Unet\". Experimental results obtained on a relatively large-scale CT segmentation dataset of around 900 images, incorporating this new regularization term leads to a 2% gain on overall segmentation performance compared to the Unet trained from scratch. Our experimental analysis, ranging from visual evaluation of the predicted segmentation results to quantitative assessment of segmentation performance (precision, recall, Dice score, and mIoU) demonstrated great ability to identify COVID-19 associated regions of the lungs, achieving a mIoU rate of over 99%, and a Dice score of around 86%.", "journal": "Computer methods and programs in biomedicine update", "date": "2021-08-03", "authors": ["NargesSaeedizadeh", "ShervinMinaee", "RaheleKafieh", "ShakibYazdani", "MilanSonka"], "doi": "10.1016/j.cmpbup.2021.100007\n10.1148/radiol.2020200432\n10.1016/j.media.2020.101794\n10.1016/j.ejrad.2020.109009\n10.1016/j.jinf.2020.04.004"}
{"title": "Deep Transfer Learning-Based Framework for COVID-19 Diagnosis Using Chest CT Scans and Clinical Information.", "abstract": "The Coronavirus Disease 2019 (COVID-19) which first emerged in Wuhan, China in late December, 2019, has now spread to all the countries in the world. Conventional testing methods such as the antigen test, serology tests, and polymerase chain reaction tests are widely used. However, the test results can take anything from a few hours to a few days to reach the patient. Chest CT scan images have been used as alternatives for the detection of COVID-19 infection. Use of CT scan images alone might have limited capabilities, which calls attention to incorporating clinical features. In this paper, deep learning algorithms have been utilized to integrate the chest CT scan images obtained from patients with their clinical characteristics for fast and accurate diagnosis of COVID-19 patients. The framework uses an ANN to obtain the probability of the patient being infected with COVID-19 using their clinical information. Beyond a certain threshold, the chest CT scan of the patient is classified using a deep learning model which has been trained to classify the CT scan with 99% accuracy.", "journal": "SN computer science", "date": "2021-08-03", "authors": ["ShreyasMishra"], "doi": "10.1007/s42979-021-00785-4\n10.1016/j.ajic.2020.07.011\n10.1148/radiol.2020201754\n10.1001/jama.2020.1097\n10.3390/jcm9020330\n10.1007/s12553-020-00437-2\n10.1177/0846537120913033\n10.1186/s12711-020-00531-z\n10.1109/ACCESS.2019.2896880\n10.1016/j.patrec.2019.03.022\n10.1016/j.compmedimag.2019.101673\n10.1016/j.media.2020.101694\n10.1016/j.clinimag.2020.04.001\n10.1016/S0140-6736(20)30183-5\n10.1148/ryct.2020200034\n10.1016/j.eng.2020.04.010\n10.1007/s00330-020-07225-6\n10.1001/jama.2020.12603\n10.1038/s41591-020-0916-2\n10.1136/gutjnl-2020-320926\n10.1016/j.ijid.2020.04.003\n10.1038/s41591-020-0931-3\n10.1007/s13246-020-00865-4\n10.1016/j.media.2020.101794\n10.1109/ACCESS.2021.3058854\n10.1038/s41598-019-56847-4\n10.1109/TMI.2020.2995965\n10.1016/j.bspc.2021.102588"}
{"title": "Robust chest CT image segmentation of COVID-19 lung infection based on limited data.", "abstract": "The coronavirus disease 2019 (COVID-19) affects billions of lives around the world and has a significant impact on public healthcare. For quantitative assessment and disease monitoring medical imaging like computed tomography offers great potential as alternative to RT-PCR methods. For this reason, automated image segmentation is highly desired as clinical decision support. However, publicly available COVID-19 imaging data is limited which leads to overfitting of traditional approaches.\nTo address this problem, we propose an innovative automated segmentation pipeline for COVID-19 infected regions, which is able to handle small datasets by utilization as variant databases. Our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation. For further reduction of the overfitting risk, we implemented a standard 3D U-Net architecture instead of new or computational complex neural network architectures.\nThrough a k-fold cross-validation on 20 CT scans as training and validation of COVID-19, we were able to develop a highly accurate as well as robust segmentation model for lungs and COVID-19 infected regions without overfitting on limited data. We performed an in-detail analysis and discussion on the robustness of our pipeline through a sensitivity analysis based on the cross-validation and impact on model generalizability of applied preprocessing techniques. Our method achieved Dice similarity coefficients for COVID-19 infection between predicted and annotated segmentation from radiologists of 0.804 on validation and 0.661 on a separate testing set consisting of 100 patients.\nWe demonstrated that the proposed method outperforms related approaches, advances the state-of-the-art for COVID-19 segmentation and improves robust medical image analysis based on limited data.", "journal": "Informatics in medicine unlocked", "date": "2021-08-03", "authors": ["DominikM\u00fcller", "I\u00f1akiSoto-Rey", "FrankKramer"], "doi": "10.1016/j.imu.2021.100681\n10.1016/S1473-3099(20)30120-1\n10.1016/j.ijsu.2020.02.034\n10.1101/2020.04.16.20064709\n10.1016/j.tmaid.2020.101623\n10.2214/AJR.20.23034\n10.1148/radiol.2020201365\n10.1148/ryct.2020200034\n10.1148/radiol.2020200432\n10.1146/annurev-bioeng-071516-044442\n10.1109/CVPR.2016.308\n10.1109/CVPR.2016.90\n10.1101/2020.03.19.20039354\n10.1186/s12880-015-0068-x\n10.1038/s41467-020-18685-1\n10.1088/1361-6560/abbf9e\n10.1186/s12880-020-00529-5\n10.1109/tmi.2020.2996645\n10.5281/zenodo.3757476\n10.1186/s12880-020-00543-7\n10.7937/tcia.2020.gqry-nc81\n10.1038/s41467-020-17971-2\n10.1186/s40537-019-0197-0\n10.5281/zenodo.3632567\n10.1007/s10462-020-09854-1\n10.1007/978-3-319-67389-9_44"}
{"title": "AI for COVID-19 Detection from Radiographs: Incisive Analysis of State of the Art Techniques, Key Challenges and Future Directions.", "abstract": "In recent years, Artificial Intelligence has had an evident impact on the way research addresses challenges in different domains. It has proven to be a huge asset, especially in the medical field, allowing for time-efficient and reliable solutions. This research aims to spotlight the impact of deep learning and machine learning models in the detection of COVID-19 from medical images. This is achieved by conducting a review of the state-of-the-art approaches proposed by the recent works in this field.\nThe main focus of this study is the recent developments of classification and segmentation approaches to image-based COVID-19 detection. The study reviews 140 research papers published in different academic research databases. These papers have been screened and filtered based on specified criteria, to acquire insights prudent to image-based COVID-19 detection.\nThe methods discussed in this review include different types of imaging modality, predominantly X-rays and CT scans. These modalities are used for classification and segmentation tasks as well. This review seeks to categorize and discuss the different deep learning and machine learning architectures employed for these tasks, based on the imaging modality utilized. It also hints at other possible deep learning and machine learning architectures that can be proposed for better results towards COVID-19 detection. Along with that, a detailed overview of the emerging trends and breakthroughs in Artificial Intelligence-based COVID-19 detection has been discussed as well.\nThis work concludes by stipulating the technical and non-technical challenges faced by researchers and illustrates the advantages of image-based COVID-19 detection with Artificial Intelligence techniques.", "journal": "Ingenierie et recherche biomedicale : IRBM = Biomedical engineering and research", "date": "2021-08-03", "authors": ["RKarthik", "RMenaka", "MHariharan", "G SKathiresan"], "doi": "10.1016/j.irbm.2021.07.002\n10.1111/tbed.13648\n10.1016/j.matpr.2020.09.352\n10.1148/radiol.2020201365\n10.1097/RTI.0000000000000516\n10.1016/j.media.2020.101794\n10.5152/dir\n10.18297/jri/vol4/iss1/53\n10.1016/j.patrec.2020.09.010\n10.1016/j.chaos.2020.110122\n10.36548/jiip.2020.3.003\n10.1371/journal.pone.0252573\n10.1016/j.chaos.2020.110071\n10.1177/2472630320958376\n10.9781/ijimai.2020.04.003\n10.1080/07391102.2020.1788642\n10.1016/j.chaos.2020.110153\n10.1007/s10489-020-01826-w\n10.1016/j.cmpb.2020.105608\n10.1016/j.patcog.2020.107613\n10.21203/rs.3.rs-40148/v1"}
{"title": "Multi-omic profiling of plasma reveals molecular alterations in children with COVID-19.", "abstract": "", "journal": "Theranostics", "date": "2021-08-03", "authors": ["ChongWang", "XufangLi", "WanshanNing", "SitangGong", "FengxiaYang", "ChunxiaoFang", "YuGong", "DiWu", "MuhanHuang", "YujieGou", "ShanshanFu", "YujieRen", "RuyiYang", "YangQiu", "YuXue", "YiXu", "XiZhou"], "doi": "10.7150/thno.61832"}
{"title": "CT Image Analysis and Clinical Diagnosis of New Coronary Pneumonia Based on Improved Convolutional Neural Network.", "abstract": "In this paper, based on the improved convolutional neural network, in-depth analysis of the CT image of the new coronary pneumonia, using the U-Net series of deep neural networks to semantically segment the CT image of the new coronary pneumonia, to obtain the new coronary pneumonia area as the foreground and the remaining areas as the background of the binary image, provides a basis for subsequent image diagnosis. Secondly, the target-detection framework Faster RCNN extracts features from the CT image of the new coronary pneumonia tumor, obtains a higher-level abstract representation of the data, determines the lesion location of the new coronary pneumonia tumor, and gives its bounding box in the image. By generating an adversarial network to diagnose the lesion area of the CT image of the new coronary pneumonia tumor, obtaining a complete image of the new coronary pneumonia, achieving the effect of the CT image diagnosis of the new coronary pneumonia tumor, and three-dimensionally reconstructing the complete new coronary pneumonia model, filling the current the gap in this aspect, provide a basis to produce new coronary pneumonia prosthesis and improve the accuracy of diagnosis.", "journal": "Computational and mathematical methods in medicine", "date": "2021-08-03", "authors": ["WuDeng", "BoYang", "WeiLiu", "WeiweiSong", "YuanGao", "JiaXu"], "doi": "10.1155/2021/7259414\n10.1097/RTI.0000000000000385\n10.1016/j.zemedi.2018.11.002\n10.1148/radiol.2017162326\n10.2214/AJR.18.20490\n10.1007/s10278-017-0028-9\n10.1097/RTI.0000000000000388\n10.1097/RLI.0000000000000448\n10.1016/S0140-6736(18)31645-3\n10.1016/S2589-7500(19)30123-2\n10.1148/radiol.2019190613\n10.1038/s41746-019-0142-9\n10.1038/s41591-018-0300-7\n10.1038/s41551-018-0305-z\n10.1007/s00330-019-06130-x\n10.1136/svn-2017-000101\n10.1148/radiol.2018180887\n10.1007/s00330-019-06532-x\n10.1002/jmri.26534\n10.1007/s10278-019-00180-9\n10.1148/radiol.16142770\n10.1148/radiol.2020192224\n10.1148/radiol.2017161177\n10.1016/S1470-2045(19)30149-4"}
{"title": "Early detection of COVID-19 in the UK using self-reported symptoms: a large-scale, prospective, epidemiological surveillance study.", "abstract": "Self-reported symptoms during the COVID-19 pandemic have been used to train artificial intelligence models to identify possible infection foci. To date, these models have only considered the culmination or peak of symptoms, which is not suitable for the early detection of infection. We aimed to estimate the probability of an individual being infected with SARS-CoV-2 on the basis of early self-reported symptoms to enable timely self-isolation and urgent testing.\nIn this large-scale, prospective, epidemiological surveillance study, we used prospective, observational, longitudinal, self-reported data from participants in the UK on 19 symptoms over 3 days after symptoms onset and COVID-19 PCR test results extracted from the COVID-19 Symptom Study mobile phone app. We divided the study population into a training set (those who reported symptoms between April 29, 2020, and Oct 15, 2020) and a test set (those who reported symptoms between Oct 16, 2020, and Nov 30, 2020), and used three models to analyse the self-reported symptoms: the UK's National Health Service (NHS) algorithm, logistic regression, and the hierarchical Gaussian process model we designed to account for several important variables (eg, specific COVID-19 symptoms, comorbidities, and clinical information). Model performance to predict COVID-19 positivity was compared in terms of sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) in the test set. For the hierarchical Gaussian process model, we also evaluated the relevance of symptoms in the early detection of COVID-19 in population subgroups stratified according to occupation, sex, age, and body-mass index.\nThe training set comprised 182\u2009991 participants and the test set comprised 15\u2009049 participants. When trained on 3 days of self-reported symptoms, the hierarchical Gaussian process model had a higher prediction AUC (0\u00b780 [95% CI 0\u00b780-0\u00b781]) than did the logistic regression model (0\u00b774 [0\u00b774-0\u00b775]) and the NHS algorithm (0\u00b767 [0\u00b767-0\u00b767]). AUCs for all models increased with the number of days of self-reported symptoms, but were still high for the hierarchical Gaussian process model at day 1 (0\u00b773 [95% CI 0\u00b773-0\u00b774]) and day 2 (0\u00b779 [0\u00b778-0\u00b779]). At day 3, the hierarchical Gaussian process model also had a significantly higher sensitivity, but a non-statistically lower specificity, than did the two other models. The hierarchical Gaussian process model also identified different sets of relevant features to detect COVID-19 between younger and older subgroups, and between health-care workers and non-health-care workers. When used during different pandemic periods, the model was robust to changes in populations.\nEarly detection of SARS-CoV-2 infection is feasible with our model. Such early detection is crucial to contain the spread of COVID-19 and efficiently allocate medical resources.\nZOE, the UK Government Department of Health and Social Care, the Wellcome Trust, the UK Engineering and Physical Sciences Research Council, the UK National Institute for Health Research, the UK Medical Research Council, the British Heart Foundation, the Alzheimer's Society, the Chronic Disease Research Foundation, and the Massachusetts Consortium on Pathogen Readiness.", "journal": "The Lancet. Digital health", "date": "2021-08-03", "authors": ["Liane SCanas", "Carole HSudre", "JoanCapdevila Pujol", "LorenzoPolidori", "BenjaminMurray", "ErikaMolteni", "Mark SGraham", "KerstinKlaser", "MichelaAntonelli", "SarahBerry", "RichardDavies", "Long HNguyen", "David ADrew", "JonathanWolf", "Andrew TChan", "TimSpector", "Claire JSteves", "SebastienOurselin", "MarcModat"], "doi": "10.1016/S2589-7500(21)00131-X\n10.1057/s41301-020-00256-y"}
{"title": "Quantum algorithm for quicker clinical prognostic analysis: an application and experimental study using CT scan images of COVID-19 patients.", "abstract": "In medical diagnosis and clinical practice, diagnosing a disease early is crucial for accurate treatment, lessening the stress on the healthcare system. In medical imaging research, image processing techniques tend to be vital in analyzing and resolving diseases with a high degree of accuracy. This paper establishes a new image classification and segmentation method through simulation techniques, conducted over images of COVID-19 patients in India, introducing the use of Quantum Machine Learning (QML) in medical practice.\nThis study establishes a prototype model for classifying COVID-19, comparing it with non-COVID pneumonia signals in Computed tomography (CT) images. The simulation work evaluates the usage of quantum machine learning algorithms, while assessing the efficacy for deep learning models for image classification problems, and thereby establishes performance quality that is required for improved prediction rate when dealing with complex clinical image data exhibiting high biases.\nThe study considers a novel algorithmic implementation leveraging quantum neural network (QNN). The proposed model outperformed the conventional deep learning models for specific classification task. The performance was evident because of the efficiency of quantum simulation and faster convergence property solving for an optimization problem for network training particularly for large-scale biased image classification task. The model run-time observed on quantum optimized hardware was 52\u00a0min, while on K80 GPU hardware it was 1\u00a0h 30\u00a0min for similar sample size. The simulation shows that QNN outperforms DNN, CNN, 2D CNN by more than 2.92% in gain in accuracy measure with an average recall of around 97.7%.\nThe results suggest that quantum neural networks outperform in COVID-19 traits' classification task, comparing to deep learning w.r.t model efficacy and training time. However, a further study needs to be conducted to evaluate implementation scenarios by integrating the model within medical devices.", "journal": "BMC medical informatics and decision making", "date": "2021-08-01", "authors": ["KinshukSengupta", "Praveen RanjanSrivastava"], "doi": "10.1186/s12911-021-01588-6\n10.1093/bioinformatics/btx652\n10.1093/bioinformatics/btz259\n10.1093/bioinformatics/btaa107\n10.1007/s42979-020-00335-4\n10.1007/s42979-020-00401-x\n10.1016/j.procs.2020.03.189\n10.1016/j.media.2019.101561\n10.1007/s42484-019-00006-5\n10.3389/fenvs.2015.00080\n10.1016/j.revip.2019.100028\n10.1016/j.drudis.2018.01.039\n10.1016/j.imu.2020.100297\n10.1007/978-94-007-0080-2_10\n10.3389/fpsyg.2016.00011\n10.1103/revmodphys.86.153\n10.1371/journal.pone.0224365\n10.1016/s1473-3099(20)30086-4\n10.1148/radiol.2020200905\n10.1016/j.diii.2020.03.014\n10.5121/ijcsit.2012.4103\n10.1145/3366423.3380063\n10.1007/s11548-018-1797-4\n10.1016/j.imu.2019.100282\n10.1016/j.jnlest.2020.100027\n10.1016/j.bmcl.2018.08.032\n10.1021/acs.molpharmaceut.6b00248\n10.1021/ci049613b\n10.1109/ACCESS.2021.3058537"}
{"title": "Deep learning with robustness to missing data: A novel approach to the detection of COVID-19.", "abstract": "In the context of the current global pandemic and the limitations of the RT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising Fully Connected Network). Since medical facilities around the world differ enormously in what laboratory tests or chest imaging may be available, DFCN is designed to be robust to missing input data. An ablation study extensively evaluates the performance benefits of the DFCN as well as its robustness to missing inputs. Data from 1088 patients with confirmed RT-PCR results are obtained from two independent medical facilities. The data includes results from 27 laboratory tests and a chest x-ray scored by a deep learning model. Training and test datasets are taken from different medical facilities. Data is made publicly available. The performance of DFCN in predicting the RT-PCR result is compared with 3 related architectures as well as a Random Forest baseline. All models are trained with varying levels of masked input data to encourage robustness to missing inputs. Missing data is simulated at test time by masking inputs randomly. DFCN outperforms all other models with statistical significance using random subsets of input data with 2-27 available inputs. When all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any other model. Furthermore, with clinically meaningful subsets of parameters consisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than any other model, with values of 0.909 and 0.919.", "journal": "PloS one", "date": "2021-07-31", "authors": ["Erdi\u00c7all\u0131", "KeelinMurphy", "SteefKurstjens", "TijsSamson", "RobertHerpers", "HenkSmits", "MatthieuRutten", "Bramvan Ginneken"], "doi": "10.1371/journal.pone.0255301\n10.1515/cclm-2020-0285\n10.1515/cclm-2020-0198\n10.1002/ajh.25829\n10.1515/cclm-2020-0369\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020201160\n10.1097/RTI.0000000000000533\n10.1148/radiol.2020201102\n10.1007/s11547-020-01200-3\n10.1515/cclm-2020-0593\n10.1109/ACCESS.2021.3058537\n10.18632/aging.100968\n10.1093/eurheartj/ehw302\n10.1038/s41746-019-0192-z\n10.1007/s10462-018-9625-3\n10.1007/978-3-030-30493-5_34\n10.1200/JCO.2011.38.7589\n10.1109/21.155944\n10.1118/1.3213517\n10.1002/aic.690370209\n10.1023/A:1010933404324\n10.2307/2531595\n10.1001/jama.2020.3786"}
{"title": "Deep learning-based mixed-dimensional Gaussian mixture model for characterizing variability in cryo-EM.", "abstract": "Structural flexibility and/or dynamic interactions with other molecules is a critical aspect of protein function. Cryogenic electron microscopy (cryo-EM) provides direct visualization of individual macromolecules sampling different conformational and compositional states. While numerous methods are available for computational classification of discrete states, characterization of continuous conformational changes or large numbers of discrete state without human supervision remains challenging. Here we present e2gmm, a machine learning algorithm to determine a conformational landscape for proteins or complexes using a three-dimensional Gaussian mixture model mapped onto two-dimensional particle images in known orientations. Using a deep neural network architecture, e2gmm can automatically resolve the structural heterogeneity within the protein complex and map particles onto a small latent space describing conformational and compositional changes. This system presents a more intuitive and flexible representation than other manifold methods currently in use. We demonstrate this method on both simulated data and three biological systems to explore compositional and conformational changes at a range of scales. The software is distributed as part of EMAN2.", "journal": "Nature methods", "date": "2021-07-31", "authors": ["MuyuanChen", "Steven JLudtke"], "doi": "10.1038/s41592-021-01220-5\n10.1145/1390156.1390294"}
{"title": "Author Correction: A deep-learning pipeline for the diagnosis and discrimination of viral, non-viral and COVID-19 pneumonia from chest X-ray images.", "abstract": null, "journal": "Nature biomedical engineering", "date": "2021-07-31", "authors": ["GuangyuWang", "XiaohongLiu", "JunShen", "ChengdiWang", "ZhihuanLi", "LinsenYe", "XingwangWu", "TingChen", "KaiWang", "XuanZhang", "ZhongguoZhou", "JianYang", "YeSang", "RuiyunDeng", "WenhuaLiang", "TaoYu", "MingGao", "JinWang", "ZehongYang", "HuiminCai", "GuangmingLu", "LingyanZhang", "LeiYang", "WenqinXu", "WinstonWang", "AndreaOlvera", "IanZiyar", "CharlotteZhang", "OulanLi", "WeihuaLiao", "JunLiu", "WenChen", "WeiChen", "JichanShi", "LianghongZheng", "LongjiangZhang", "ZhihanYan", "XiaoguangZou", "GuipingLin", "GuiqunCao", "Laurance LLau", "LongMo", "YongLiang", "MichaelRoberts", "EvisSala", "Carola-BibianeSch\u00f6nlieb", "MansonFok", "Johnson Yiu-NamLau", "TaoXu", "JianxingHe", "KangZhang", "WeiminLi", "TianxinLin"], "doi": "10.1038/s41551-021-00787-w"}
{"title": "MSDS-UNet: A multi-scale deeply supervised 3D U-Net for automatic segmentation of lung tumor in CT.", "abstract": "Lung cancer is one of the most common and deadly malignant cancers. Accurate lung tumor segmentation from CT is therefore very important for correct diagnosis and treatment planning. The automated lung tumor segmentation is challenging due to the high variance in appearance and shape of the targeting tumors. To overcome the challenge, we present an effective 3D U-Net equipped with ResNet architecture and a two-pathway deep supervision mechanism to increase the network's capacity for learning richer representations of lung tumors from global and local perspectives. Extensive experiments on two real medical datasets: the lung CT dataset from Liaoning Cancer Hospital in China with 220 cases and the public dataset of TCIA with 422 cases. Our experiments demonstrate that our model achieves an average dice score (0.675), sensitivity (0.731) and F1-score (0.682) on the dataset from Liaoning Cancer Hospital, and an average dice score (0.691), sensitivity (0.746) and F1-score (0.724) on the TCIA dataset, respectively. The results demonstrate that the proposed 3D MSDS-UNet outperforms the state-of-the-art segmentation models for segmenting all scales of tumors, especially for small tumors. Moreover, we evaluated our proposed MSDS-UNet on another challenging volumetric medical image segmentation task: COVID-19 lung infection segmentation, which shows consistent improvement in the segmentation performance.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2021-07-30", "authors": ["JinzhuYang", "BoWu", "LantingLi", "PengCao", "OsmarZaiane"], "doi": "10.1016/j.compmedimag.2021.101957"}
{"title": "3D CNN classification model for accurate diagnosis of coronavirus disease 2019 using computed tomography images.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-07-30", "authors": ["YifanLi", "XuanPei", "YandongGuo"], "doi": "10.1117/1.JMI.8.S1.017502\n10.1056/NEJMp2000929\n10.1016/S0140-6736(20)30185-9\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30154-9\n10.1056/NEJMc2001272\n10.1056/NEJMc2001468\n10.1056/NEJMoa2001017\n10.1001/jama.2020.2648\n10.1136/bmj.m641\n10.1148/radiol.2020200490\n10.1109/TMI.2020.2995508\n10.1148/radiol.2020200230\n10.1016/S0140-6736(18)31645-3\n10.1038/s41591-018-0316-z\n10.1016/j.ophtha.2018.01.023\n10.1038/s41591-018-0320-3\n10.1038/s41551-018-0195-0\n10.1001/jama.2017.18152\n10.1038/s41591-018-0300-7\n10.1016/j.cell.2020.04.045\n10.1109/TMI.2020.2968472\n10.1109/CVPR.2018.00675\n10.1109/ICCVW.2017.373\n10.1109/CVPR.2016.90\n10.1214/aos/1176344552"}
{"title": "An ensemble learning approach to digital corona virus preliminary screening from cough sounds.", "abstract": "This work develops a robust classifier for a COVID-19 pre-screening model from crowdsourced cough sound data. The crowdsourced cough recordings contain a variable number of coughs, with some input sound files more informative than the others. Accurate detection of COVID-19 from the sound datasets requires overcoming two main challenges (i) the variable number of coughs in each recording and (ii) the low number of COVID-positive cases compared to healthy coughs in the data. We use two open datasets of crowdsourced cough recordings and segment each cough recording into non-overlapping coughs. The segmentation enriches the original data without oversampling by splitting the original cough sound files into non-overlapping segments. Splitting the sound files enables us to increase the samples of the minority class (COVID-19) without changing the feature distribution of the COVID-19 samples resulted from applying oversampling techniques. Each cough sound segment is transformed into six image representations for further analyses. We conduct extensive experiments with shallow machine learning, Convolutional Neural Network (CNN), and pre-trained CNN models. The results of our models were compared to other recently published papers that apply machine learning to cough sound data for COVID-19 detection. Our method demonstrated a high performance using an ensemble model on the testing dataset with area under receiver operating characteristics curve\u2009=\u20090.77, precision\u2009=\u20090.80, recall\u2009=\u20090.71, F1 measure\u2009=\u20090.75, and Kappa\u2009=\u20090.53. The results show an improvement in the prediction accuracy of our COVID-19 pre-screening model compared to the other models.", "journal": "Scientific reports", "date": "2021-07-30", "authors": ["Emad AMohammed", "MohammadKeyhani", "AmirSanati-Nezhad", "S HosseinHejazi", "Behrouz HFar"], "doi": "10.1038/s41598-021-95042-2\n10.3389/fmicb.2020.01570\n10.1089/tmj.2020.0114\n10.1111/1467-6451.00133\n10.1016/j.imu.2020.100378\n10.1109/ACCESS.2020.3018028\n10.1016/j.pupt.2006.11.009\n10.1186/s12931-019-1046-6\n10.1109/JBHI.2018.2872038\n10.1088/1361-6579/aab6d0\n10.1049/iet-spr.2016.0341\n10.3390/ijerph18031117\n10.1109/OJEMB.2020.3026928\n10.11613/BM.2012.031\n10.1016/j.specom.2013.02.007\n10.3390/brainsci11010052\n10.1016/j.asoc.2020.106592"}
{"title": "Automatic Detection of Covid-19 with Bidirectional LSTM Network Using Deep Features Extracted from Chest X-ray Images.", "abstract": "Coronavirus disease, which comes up in China at the end of 2019 and showed different symptoms in people infected, affected millions of people. Computer-aided expert systems are needed due to the inadequacy of the reverse transcription-polymerase chain reaction kit, which is widely used in the diagnosis of this disease. Undoubtedly, expert systems that provide effective solutions to many problems will be very useful in the detection of Covid-19 disease, especially when unskilled personnel and financial deficiencies in underdeveloped countries are taken into consideration. In the literature, there are numerous machine learning approaches built with different classifiers in the detection of this disease. This paper proposes an approach based on deep learning which detects Covid-19 and no-finding cases using chest X-ray images. Here, the classification performance of the Bi-LSTM network on the deep features was compared with the Deep Neural Network within the frame of the fivefold cross-validation technique. Accuracy, sensitivity, specificity and precision metrics were used to evaluate the classification performance of the trained models. Bi-LSTM network presented better performance compare to DNN with 97.6% value of high accuracy despite the few numbers of Covid-19 images in the dataset. In addition, it is understood that concatenated deep features more meaningful than deep features obtained with pre-trained networks by one by, as well. Consequently, it is thought that the proposed study based on the Bi-LSTM network and concatenated deep features will be noteworthy in the design of highly sensitive automated Covid-19 monitoring systems.", "journal": "Interdisciplinary sciences, computational life sciences", "date": "2021-07-28", "authors": ["KemalAkyol", "Baha\u015een"], "doi": "10.1007/s12539-021-00463-2\n10.1016/j.ijantimicag.2020.105924\n10.1016/j.eswa.2021.114883\n10.1016/j.mehy.2020.109689\n10.1016/j.bspc.2020.102365\n10.1148/radiol.2020200370\n10.1148/radiol.2020200432\n10.1109/TMI.2020.2995965\n10.1148/radiol.2020200527\n10.1016/j.ijid.2020.03.071\n10.1007/s13246-020-00865-4\n10.1109/RBME.2020.2990959\n10.1016/j.irbm.2021.01.004\n10.1109/TMI.2020.2993291\n10.1007/s12553-021-00520-2\n10.1109/RBME.2020.2987975\n10.1101/2020.04.08.20040907\n10.1177/2472630320958376\n10.1016/j.matpr.2020.09.526\n10.1016/j.cosrev.2019.100203\n10.1016/j.cmpb.2019.06.016\n10.1016/j.irbm.2020.12.002\n10.1016/j.matpr.2020.11.600\n10.1097/IAE.0b013e318286c952\n10.1016/j.eswa.2020.113501\n10.1016/j.matpr.2020.10.951\n10.1016/j.eswa.2020.114161\n10.1016/j.matpr.2020.11.555\n10.1016/j.mlwa.2020.100003\n10.1016/j.fcij.2017.12.001\n10.1016/j.cogsys.2019.09.007\n10.1016/j.compbiomed.2019.103345\n10.1016/j.bbe.2020.06.001\n10.1016/j.matpr.2020.10.063\n10.1016/j.matpr.2021.01.601\n10.1016/j.jalz.2019.02.007\n10.1016/j.knosys.2020.106688\n10.1016/j.jbi.2020.103514\n10.1016/j.compbiomed.2020.103764\n10.1016/j.eswa.2019.06.038\n10.1016/j.micpro.2021.104007\n10.1016/j.neucom.2018.11.111\n10.1016/j.jbi.2020.103411\n10.1016/j.gltp.2021.01.008\n10.1016/j.chemolab.2020.103996\n10.1016/j.neunet.2020.07.018\n10.1007/s10140-020-01886-y\n10.1016/j.irbm.2020.05.003\n10.1007/s00521-020-05437-x\n10.1007/s10044-021-00984-y\n10.1021/ci0342472\n10.1186/s40537-016-0043-6\n10.1109/TMI.2016.2528162\n10.1016/j.patrec.2020.04.018\n10.1145/3065386"}
{"title": "Self-supervised deep learning model for COVID-19 lung CT image segmentation highlighting putative causal relationship among age, underlying disease and COVID-19.", "abstract": "Coronavirus disease 2019 (COVID-19) is very contagious. Cases appear faster than the available Polymerase Chain Reaction test kits in many countries. Recently, lung computerized tomography (CT) has been used as an auxiliary COVID-19 testing approach. Automatic analysis of the lung CT images is needed to increase the diagnostic efficiency and release the human participant. Deep learning is successful in automatically solving computer vision problems. Thus, it can be introduced to the automatic and rapid COVID-19 CT diagnosis. Many advanced deep learning-based computer vison techniques were developed to increase the model performance but have not been introduced to medical image analysis.\nIn this study, we propose a self-supervised two-stage deep learning model to segment COVID-19 lesions (ground-glass opacity and consolidation) from chest CT images to support rapid COVID-19 diagnosis. The proposed deep learning model integrates several advanced computer vision techniques such as generative adversarial image inpainting, focal loss, and lookahead optimizer. Two real-life datasets were used to evaluate the model's performance compared to the previous related works. To explore the clinical and biological mechanism of the predicted lesion segments, we extract some engineered features from the predicted lung lesions. We evaluate their mediation effects on the relationship of age with COVID-19 severity, as well as the relationship of underlying diseases with COVID-19 severity using statistic mediation analysis.\nThe best overall F1 score is observed in the proposed self-supervised two-stage segmentation model (0.63) compared to the two related baseline models (0.55, 0.49). We also identified several CT image phenotypes that mediate the potential causal relationship between underlying diseases with COVID-19 severity as well as the potential causal relationship between age with COVID-19 severity.\nThis work contributes a promising COVID-19 lung CT image segmentation model and provides predicted lesion segments with potential clinical interpretability. The model could automatically segment the COVID-19 lesions from the raw CT images with higher accuracy than related works. The features of these lesions are associated with COVID-19 severity through mediating the known causal of the COVID-19 severity (age and underlying diseases).", "journal": "Journal of translational medicine", "date": "2021-07-28", "authors": ["Daryl L XFung", "QianLiu", "JudahZammit", "Carson Kai-SangLeung", "PingzhaoHu"], "doi": "10.1186/s12967-021-02992-2\n10.1016/S1473-3099(20)30120-1\n10.1038/s41562-020-0931-9\n10.1038/s41467-020-17971-2\n10.1038/s42256-020-0180-7\n10.1016/j.jcv.2020.104378\n10.1016/S0140-6736(20)30854-0\n10.1007/s00330-002-1485-0\n10.1016/j.ejrad.2009.08.013\n10.1146/annurev.psych.58.110405.085542\n10.1016/j.ecoinf.2020.101085\n10.3390/s21062215\n10.1101/2020.05.08.20094664\n10.1002/mp.14676\n10.1016/j.eswa.2021.114677\n10.1016/j.asoc.2020.106912\n10.1109/TMI.2020.2996645\n10.1109/TMI.2021.3066161\n10.1007/978-3-030-01252-6_6\n10.1137/16M1080173\n10.1038/s41551-020-00633-5\n10.1158/0008-5472.CAN-17-0339\n10.18637/jss.v048.i02\n10.3906/elk-1710-157\n10.1109/ACCESS.2019.2896920\n10.1109/ACCESS.2019.2908991\n10.1080/01621459.1981.10477750\n10.1080/0284186X.2017.1350285\n10.1186/s40644-019-0239-z\n10.1148/radiol.2020200642\n10.1177/0846537120913033\n10.1148/ryct.2020200152\n10.1148/radiol.2020200823\n10.1148/ryct.2020200092\n10.1148/radiol.2020200432\n10.1148/ryct.2020200110\n10.1148/radiol.2016160749\n10.1002/jmri.25572\n10.1016/j.ejrad.2009.02.006\n10.1016/j.crad.2018.09.016"}
{"title": "Simultaneous detection of SARS-CoV-2 and pandemic (H1N1) 2009 virus with real-time isothermal platform.", "abstract": "The recent ongoing outbreak of novel coronavirus SARS-CoV-2 (known as COVID-19) is a severe threat to human health worldwide. By press time, more than 3.3 million people have died from COVID-19, with many countries experiencing peaks in infections and hospitalizations. The main symptoms of infection with SARS-CoV-2 include fever, chills, coughing, shortness of breath or difficulty breathing, fatigue, muscle or body aches and pains. While the symptoms of the pandemic (H1N1) 2009 virus have many similarities to the signs and transmission routes of the novel coronavirus, e.g., fever, cough, sore throat, body aches, headache, chills and fatigue. And a few cases of serious illness, rapid progress, can appear viral pneumonia, combined with respiratory failure, multiple organ function damage, serious people can die. Therefore, there is an urgent need to develop a rapid and accurate field diagnostic method to effectively identify the two viruses and treat these early infections on time, thus helping to control the spread of the disease. Among molecular detection methods, RT-LAMP (real-time reverse transcription-loop-mediated isothermal amplification) has some advantages in pathogen detection due to its rapid, accurate and effective detection characteristics. Here, we combined the primers of the two viruses with the fluorescent probes on the RT-LAMP detection platform to detect the two viruses simultaneously. Firstly, RT-LAMP method was used respectively to detect the two viruses at different concentrations to determine the effectiveness and sensitivity of probe primers to the RNA samples. And then, the two virus samples were detected simultaneously in the same reaction tube to validate if testing for the two viruses together had an impact on the results compared to detecting alone. We verified the detection efficiency of three highly active BST variants during RT-LAMP assay. We expect that this assay can effectively and accurately distinguish COVID-19 from the pandemic (H1N1) 2009, so that these two diseases with similar symptoms can be appropriately differentiated and treated.", "journal": "Heliyon", "date": "2021-07-27", "authors": ["LinYu", "JingyaoWang", "XuelongLi", "LinglingMao", "YiSui", "WeihuaChen", "VicentPelechano", "XingGuo", "XiushanYin"], "doi": "10.1016/j.heliyon.2021.e07584"}
{"title": "Arbitrary Scale Super-Resolution for Medical Images.", "abstract": "Single image super-resolution (SISR) aims to obtain a high-resolution output from one low-resolution image. Currently, deep learning-based SISR approaches have been widely discussed in medical image processing, because of their potential to achieve high-quality, high spatial resolution images without the cost of additional scans. However, most existing methods are designed for scale-specific SR tasks and are unable to generalize over magnification scales. In this paper, we propose an approach for medical image arbitrary-scale super-resolution (MIASSR), in which we couple meta-learning with generative adversarial networks (GANs) to super-resolve medical images at any scale of magnification in [Formula: see text]. Compared to state-of-the-art SISR algorithms on single-modal magnetic resonance (MR) brain images (OASIS-brains) and multi-modal MR brain images (BraTS), MIASSR achieves comparable fidelity performance and the best perceptual quality with the smallest model size. We also employ transfer learning to enable MIASSR to tackle SR tasks of new medical modalities, such as cardiac MR images (ACDC) and chest computed tomography images (COVID-CT). The source code of our work is also public. Thus, MIASSR has the potential to become a new foundational pre-/post-processing step in clinical image analysis tasks such as reconstruction, image quality enhancement, and segmentation.", "journal": "International journal of neural systems", "date": "2021-07-27", "authors": ["JinZhu", "ChuanTan", "JunweiYang", "GuangYang", "PietroLio'"], "doi": "10.1142/S0129065721500374"}
{"title": "Weakly unsupervised conditional generative adversarial network for image-based prognostic prediction for COVID-19 patients based on chest CT.", "abstract": "Because of the rapid spread and wide range of the clinical manifestations of the coronavirus disease 2019 (COVID-19), fast and accurate estimation of the disease progression and mortality is vital for the management of the patients. Currently available image-based prognostic predictors for patients with COVID-19 are largely limited to semi-automated schemes with manually designed features and supervised learning, and the survival analysis is largely limited to logistic regression. We developed a weakly unsupervised conditional generative adversarial network, called pix2surv, which can be trained to estimate the time-to-event information for survival analysis directly from the chest computed tomography (CT) images of a patient. We show that the performance of pix2surv based on CT images significantly outperforms those of existing laboratory tests and image-based visual and quantitative predictors in estimating the disease progression and mortality of COVID-19 patients. Thus, pix2surv is a promising approach for performing image-based prognostic predictions.", "journal": "Medical image analysis", "date": "2021-07-26", "authors": ["TomokiUemura", "Janne JN\u00e4ppi", "ChinatsuWatari", "ToruHironaka", "TohruKamiya", "HiroyukiYoshida"], "doi": "10.1016/j.media.2021.102159\n10.1148/radiol.2020200463\n10.1145/3368555.3384465\n10.1109/tnnls.2020.3029631\n10.1148/radiol.2020201433\n10.1080/01621459.1997.10474007\n10.1007/s00330-020-07033-y\n10.1111/coin.12411\n10.1016/j.media.2020.101800\n10.1038/s41467-020-17971-2\n10.1002/%28SICI%291097-0258%2819960229%2915%3A4%3C361%3A%3AAID-SIM168%3E3.0.CO%3B2-4\n10.1002/0470011815.b2a11047\n10.1148/ryct.2020200075\n10.1093/cid/ciaa414\n10.1002/scj.20178\n10.1007/s00330-020-07013-2\n10.1007/978-3-319-46487-9_43\n10.1007/s00330-020-06817-6\n10.7150/thno.45985\n10.1097/RLI.0000000000000689\n10.1186/s41747-020-00167-0\n10.1038/s41591-020-0931-3\n10.7326/M14-0698\n10.1118/1.2868757\n10.1007/BF01068419\n10.1186/s40537-019-0197-0\n10.18637/jss.v039.i05\n10.1038/s41591-021-01292-y\n10.1117/12.2551369\n10.21037/atm-20-3554\n10.1002/sim.4780111409\n10.7150/thno.46428\n10.3389/fbioe.2020.00898\n10.1038/s42256-020-0180-7\n10.7150/thno.46465"}
{"title": "Machine learning algorithm improves accuracy of ortho-K lens fitting in vision shaping treatment.", "abstract": "To construct a machine learning (ML)-based model for estimating the alignment curve (AC) curvature in orthokeratology lens fitting for vision shaping treatment (VST), which can minimize the number of lens trials, improving efficiency while maintaining accuracy, with regards to its improvement over a previous calculation method.\nData were retrospectively collected from the clinical case files of 1271 myopic subjects (1271 right eyes). The AC curvatures calculated with a previously published algorithm were used as the target data sets. Four kinds of machine learning algorithms were implemented in the experimental analyses to predict the targeted AC curvatures: robust linear regression models, support vector machine (SVM) regression models with linear kernel functions, bagging decision trees, and Gaussian processes. The previously published calculation method and the novel machine learning method were then compared to assess the final parameters of ordered lenses.\nThe linear SVM and Gaussian process machine learning models achieved the best performance. The input variables included sex, age, horizontal visible iris diameter (HVID), spherical refraction (SER), cylindrical refraction, eccentricity value (e value), flat K (K1) and steep K (K2) readings, anterior chamber depth (ACD), and axial length (AL). The R-squared values for the output AC1K1, AC1K2 and AC2K1 values were 0.91, 0.84, and 0.73, respectively. The previous calculation method and machine learning methods displayed excellent consistency, and the proposed methods performed best based on flat K reading and e values.\nThe ML model can provide practitioners with an efficient method for estimating the AC curvatures of VST lenses and reducing the probability of cross-infection originating from trial lenses, which is especially useful during pandemics, such as that for COVID-19.", "journal": "Contact lens & anterior eye : the journal of the British Contact Lens Association", "date": "2021-07-25", "authors": ["YuzhuoFan", "ZekuanYu", "TaoTang", "XiaoLiu", "QiongXu", "ZisuPeng", "YanLi", "KaiWang", "JiaQu", "MingweiZhao"], "doi": "10.1016/j.clae.2021.101474"}
{"title": "Impartially Validated Multiple Deep-Chain Models to Detect COVID-19 in Chest X-ray Using Latent Space Radiomics.", "abstract": "The COVID-19 pandemic continues to spread globally at a rapid pace, and its rapid detection remains a challenge due to its rapid infectivity and limited testing availability. One of the simply available imaging modalities in clinical routine involves chest X-ray (CXR), which is often used for diagnostic purposes. Here, we proposed a computer-aided detection of COVID-19 in CXR imaging using deep and conventional radiomic features. First, we used a 2D U-Net model to segment the lung lobes. Then, we extracted deep latent space radiomics by applying deep convolutional autoencoder (ConvAE) with internal dense layers to extract low-dimensional deep radiomics. We used Johnson-Lindenstrauss (JL) lemma, Laplacian scoring (LS), and principal component analysis (PCA) to reduce dimensionality in conventional radiomics. The generated low-dimensional deep and conventional radiomics were integrated to classify COVID-19 from pneumonia and healthy patients. We used 704 CXR images for training the entire model (i.e., U-Net, ConvAE, and feature selection in conventional radiomics). Afterward, we independently validated the whole system using a study cohort of 1597 cases. We trained and tested a random forest model for detecting COVID-19 cases through multivariate binary-class and multiclass classification. The maximal (full multivariate) model using a combination of the two radiomic groups yields performance in classification cross-validated accuracy of 72.6% (69.4-74.4%) for multiclass and 89.6% (88.4-90.7%) for binary-class classification.", "journal": "Journal of clinical medicine", "date": "2021-07-25", "authors": ["BardiaYousefi", "SatoruKawakita", "AryaAmini", "HamedAkbari", "Shailesh MAdvani", "MoulayAkhloufi", "Xavier P VMaldague", "SamadAhadian"], "doi": "10.3390/jcm10143100\n10.1148/radiol.2020200241\n10.1148/radiol.2020200463\n10.1148/radiol.2020200343\n10.1007/s13246-020-00865-4\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.2214/AJR.20.23034\n10.1109/RBME.2020.2987975\n10.1101/2020.03.12.20027185\n10.1148/ryct.2020200082\n10.1148/ryct.2020200075\n10.1101/2020.02.29.20029603\n10.1148/radiol.2020200905\n10.1101/2020.02.25.20021568\n10.1016/j.asoc.2020.106897\n10.1148/radiol.2020201160\n10.1016/j.ijmedinf.2020.104284\n10.1109/TMI.2020.2993291\n10.3389/fmed.2020.00427\n10.33889/IJMEMS.2020.5.4.052\n10.1016/j.cmpb.2020.105581\n10.1371/journal.pone.0242535\n10.1016/j.imu.2020.100360\n10.1016/j.compbiomed.2020.103792\n10.1007/s13755-020-00119-3\n10.1016/j.media.2020.101794\n10.2307/1932409\n10.1148/ryct.2020200322\n10.1007/s00330-020-07032-z\n10.3390/bios10110164\n10.1109/TPAMI.2012.277\n10.1038/323533a0\n10.21037/tlcr.2017.01.04\n10.1007/s10278-014-9718-8\n10.1158/0008-5472.CAN-17-0339\n10.1090/conm/026/737400\n10.1109/TIM.2004.834070\n10.1109/TMI.2013.2284099\n10.1109/TMI.2013.2290491\n10.7759/cureus.9448\n10.1038/s41598-020-76550-z\n10.3390/ijerph17186933"}
{"title": "Predisposing Variations in Fear-Related Brain Networks Prospectively Predict Fearful Feelings during the 2019 Coronavirus (COVID-19) Pandemic.", "abstract": "The novel coronavirus (COVID-19) pandemic has led to a surge in mental distress and fear-related disorders, including posttraumatic stress disorder (PTSD). Fear-related disorders are characterized by dysregulations in fear and the associated neural pathways. In the present study, we examined whether individual variations in the fear neural connectome can predict fear-related symptoms during the COVID-19 pandemic. Using machine learning algorithms and back-propagation artificial neural network (BP-ANN) deep learning algorithms, we demonstrated that the intrinsic neural connectome before the COVID-19 pandemic could predict who would develop high fear-related symptoms at the peak of the COVID-19 pandemic in China (Accuracy rate\u2009=\u200975.00%, Sensitivity rate\u2009=\u200965.83%, Specificity rate\u2009=\u200984.17%). More importantly, prediction models could accurately predict the level of fear-related symptoms during the COVID-19 pandemic by using the prepandemic connectome state, in which the functional connectivity of lvmPFC (left ventromedial prefrontal cortex)-rdlPFC (right dorsolateral), rdACC (right dorsal anterior cingulate cortex)-left insula, lAMY (left amygdala)-lHip (left hippocampus) and lAMY-lsgACC (left subgenual cingulate cortex) was contributed to the robust prediction. The current study capitalized on prepandemic data of the neural connectome of fear to predict participants who would develop high fear-related symptoms in COVID-19 pandemic, suggesting that individual variations in the intrinsic organization of the fear circuits represent a neurofunctional marker that renders subjects vulnerable to experience high levels of fear during the COVID-19 pandemic.", "journal": "Cerebral cortex (New York, N.Y. : 1991)", "date": "2021-07-24", "authors": ["PanFeng", "ZhiyiChen", "BenjaminBecker", "XiqinLiu", "FengZhou", "QinghuaHe", "JiangQiu", "XuLei", "HongChen", "TingyongFeng"], "doi": "10.1093/cercor/bhab232"}
{"title": "Ant colony optimization with Cauchy and greedy Levy mutations for multilevel COVID 19 X-ray image segmentation.", "abstract": "This paper focuses on the study of multilevel COVID-19 X-ray image segmentation based on swarm intelligence optimization to improve the diagnostic level of COVID-19. We present a new ant colony optimization with the Cauchy mutation and the greedy Levy mutation, termed CLACO, for continuous domains. Specifically, the Cauchy mutation is applied to the end phase of ant foraging in CLACO to enhance its searchability and to boost its convergence rate. The greedy Levy mutation is applied to the optimal ant individuals to confer an improved ability to jump out of the local optimum. Furthermore, this paper develops a novel CLACO-based multilevel image segmentation method, termed CLACO-MIS. Using 2D Kapur's entropy as the CLACO fitness function based on 2D histograms consisting of non-local mean filtered images and grayscale images, CLACO-MIS was successfully applied to the segmentation of COVID-19 X-ray images. A comparison of CLACO with some relevant variants and other excellent peers on 30 benchmark functions from IEEE CEC2014 demonstrates the superior performance of CLACO in terms of search capability, and convergence speed as well as ability to jump out of the local optimum. Moreover, CLACO-MIS was shown to have a better segmentation effect and a stronger adaptability at different threshold levels than other methods in performing segmentation experiments of COVID-19 X-ray images. Therefore, CLACO-MIS has great potential to be used for improving the diagnostic level of COVID-19. This research will host a webservice for any question at https://aliasgharheidari.com.", "journal": "Computers in biology and medicine", "date": "2021-07-23", "authors": ["LeiLiu", "DongZhao", "FanhuaYu", "Ali AsgharHeidari", "ChengyeLi", "JinshengOuyang", "HuilingChen", "MajdiMafarja", "HamzaTurabieh", "JingyePan"], "doi": "10.1016/j.compbiomed.2021.104609\n10.1016/j.asoc.2019.105946\n10.1016/j.neucom.2020.1010.1038\n10.1109/TIM.2020.2983233\n10.1109/TIM.2019.2948414\n10.1109/TITS.2020.3025796\n10.1016/j.eswa.2021.114864\n10.1016/j.knosys.2020.106510\n10.1109/TAFFC.2020.3023821\n10.1109/TSC.2020.3016660\n10.1109/TSTE.2021.3075615\n10.1109/TCYB.2021.3071860\n10.1109/TAFFC.2019.2936198\n10.1631/FITEE.2000229"}
{"title": "Reply to the Letter to the Editor: Quantitative evaluation of COVID-19 pneumonia severity by CT pneumonia analysis algorithm using deep learning technology and blood test results.", "abstract": null, "journal": "Japanese journal of radiology", "date": "2021-07-22", "authors": ["TomohisaOkuma"], "doi": "10.1007/s11604-021-01179-5\n10.1007/s11604-021-01134-4\n10.1053/j.ajkd.2008.12.034"}
{"title": "Deep Learning Analysis in Prediction of COVID-19 Infection Status Using Chest CT Scan Features.", "abstract": "Background and aims Non-contrast chest computed tomography (CT) scanning is one of the important tools for evaluating of lung lesions. The aim of this study was to use a deep learning approach for predicting the outcome of patients with COVID-19 into two groups of critical and non-critical according to their CT features. Methods This was carried out as a retrospective study from March to April 2020 in Baqiyatallah Hospital, Tehran, Iran. From total of 1078 patients with COVID-19 pneumonia who underwent chest CT, 169 were critical cases and 909 were non-critical. Deep learning neural networks were used to classify samples into critical or non-critical ones according to the chest CT results. Results The best accuracy of prediction was seen by the presence of diffuse opacities and lesion distribution (both=0.91, 95% CI: 0.83-0.99). The largest sensitivity was achieved using lesion distribution (0.74, 95% CI: 0.55-0.93), and the largest specificity was for presence of diffuse opacities (0.95, 95% CI: 0.9-1). The total model showed an accuracy of 0.89 (95% CI: 0.79-0.99), and the corresponding sensitivity and specificity were 0.71 (95% CI: 0.51-0.91) and 0.93 (95% CI: 0.87-0.96), respectively. Conclusions The results showed that CT scan can accurately classify and predict critical and non-critical COVID-19 cases.", "journal": "Advances in experimental medicine and biology", "date": "2021-07-20", "authors": ["AsmaPourhoseingholi", "MohsenVahedi", "SamiraChaibakhsh", "Mohamad AminPourhoseingholi", "AmirVahedian-Azimi", "Paul CGuest", "FarshidRahimi-Bashar", "AmirhosseinSahebkar"], "doi": "10.1007/978-3-030-71697-4_11\n10.1016/j.ijantimicag.2020.105951\n10.1056/NEJMoa2001017\n10.1016/j.jhepr.2020.100113\n10.1016/j.ajg.2020.03.002\n10.1038/s41572-019-0069-0\n10.1007/s11604-020-00967-9\n10.1021/acsnano.0c02624\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200330\n10.1148/radiol.2020200343\n10.2214/AJR.20.22975\n10.1148/radiol.2020200230\n10.1016/j.clinimag.2020.04.001\n10.1016/j.jrid.2020.04.001\n10.1016/j.cmpb.2017.11.003\n10.1016/j.scitotenv.2017.11.291\n10.5582/bst.2017.01257\n10.4258/hir.2017.23.4.277\n10.1038/s41598-018-23075-1\n10.1007/s00330-020-06801-0\n10.1148/radiol.2462070712\n10.1186/s12879-019-4592-0\n10.1148/radiol.2363040958\n10.1148/radiol.2020200463\n10.1016/S0140-6736(20)30185-9\n10.1148/radiol.2020200432\n10.1186/s12967-020-02324-w\n10.1148/radiol.2020200905\n10.1016/j.ejrad.2020.108941\n10.1097/RLI.0000000000000672\n10.1097/CM9.0000000000000775\n10.1080/14737159.2020.1830760\n10.1007/s10900-020-00920-x"}
{"title": "Comparison of Clinical Characteristics Among COVID-19 and Non-COVID-19 Pediatric Pneumonias: A Multicenter Cross-Sectional Study.", "abstract": "The pandemic of Coronavirus Disease 2019 (COVID-19) brings new challenges for pediatricians, especially in the differentiation with non-COVID-19 pneumonia in the peak season of pneumonia. We aimed to compare the clinical characteristics of pediatric patients with COVID-19 and other respiratory pathogens infected pneumonias.\nWe conducted a multi-center, cross-sectional study of pediatric inpatients in China. Based on pathogenic test results, pediatric patients were divided into three groups, including COVID-19 pneumonia group, Non-COVID-19 viral (NCV) pneumonia group and Non-viral (NV) pneumonia group. Their clinical characteristics were compared by Kruskal-Wallis H test or chi-square test.\nA total of 636 pediatric pneumonia inpatients, among which 87 in COVID-19 group, 194 in NCV group, and 355 in NV group, were included in analysis. Compared with NCV and NV patients, COVID-19 patients were older (median age 6.33, IQR 2.00-12.00 years), and relatively fewer COVID-19 patients presented fever (63.2%), cough (60.9%), shortness of breath (1.1%), and abnormal pulmonary auscultation (18.4%). The results were verified by the comparison of COVID-19, respiratory syncytial virus (RSV) and influenza A (IFA) pneumonia patients. Approximately 42.5%, 44.8%, and 12.6% of the COVID-19 patients presented simply ground-glass opacity (GGO), simply consolidation, and the both changes on computed tomography (CT) scans, respectively; the proportions were similar as those in NCV and NV group (p>0.05). Only 47.1% of COVID-19 patients had both lungs pneumonia, which was significantly lower than that proportion of nearly 80% in the other two groups. COVID-19 patients presented lower proportions of increased white blood cell count (16.5%) and abnormal procalcitonin (PCT) (10.7%), and a higher proportion of decreased lymphocyte count (44.0%) compared with the other two groups.\nMajority clinical characteristics of pediatric COVID-19 pneumonia patients were milder than non-COVID-19 patients. However, lymphocytopenia remained a prominent feature of COVID-19 pediatric pneumonia.", "journal": "Frontiers in cellular and infection microbiology", "date": "2021-07-20", "authors": ["ZhongweiJia", "XiangyuYan", "LiweiGao", "ShenggangDing", "YanBai", "YuejieZheng", "YuxiaCui", "XianfengWang", "JingfengLi", "GenLu", "YiXu", "XiangyuZhang", "JunhuaLi", "NingChen", "YunxiaoShang", "MingfengHan", "JunLiu", "HourongZhou", "CenLi", "WanqiuLu", "JunLiu", "LinaWang", "QihongFan", "JiangWu", "HanlingShen", "RongJiao", "ChunxiChen", "XiaolingGao", "MaoqiangTian", "WeiLu", "YonghongYang", "Gary Wing-KinWong", "TianyouWang", "RunmingJin", "AdongShen", "BaopingXu", "KunlingShen"], "doi": "10.3389/fcimb.2021.663884\n10.1016/S1473-3099(20)30396-0\n10.15585/mmwr.mm6914e4\n10.1002/jmv.25810\n10.1183/13993003.00749-2020\n10.1371/journal.pone.0236312\n10.1056/NEJMoa2021680\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1186/s12967-020-02374-0\n10.1016/S0140-6736(16)31593-8\n10.1016/j.ijid.2020.06.026\n10.1111/apa.15270\n10.1056/NEJMc2005073\n10.1016/S0140-6736(20)30251-8\n10.1016/j.ultrasmedbio.2020.04.026\n10.1002/ppul.25255\n10.1542/peds.2014-2833\n10.1002/jmv.25807\n10.1007/s12519-020-00343-7\n10.1080/22221751.2020.1744483\n10.1002/ppul.24718\n10.1001/jamapediatrics.2020.2430\n10.1164/rccm.202001-0179LE\n10.1016/j.jinf.2020.04.030\n10.1056/NEJMoa2001017\n10.1097/INF.0000000000002660"}
{"title": "MIDCAN: A multiple input deep convolutional attention network for Covid-19 diagnosis based on chest CT and chest X-ray.", "abstract": "COVID-19 has caused 3.34m deaths till 13/May/2021. It is now still causing confirmed cases and ongoing deaths every day.\nThis study investigated whether fusing chest CT with chest X-ray can help improve the AI's diagnosis performance. Data harmonization is employed to make a homogeneous dataset. We create an end-to-end multiple-input deep convolutional attention network (MIDCAN) by using the convolutional block attention module (CBAM). One input of our model receives 3D chest CT image, and other input receives 2D X-ray image. Besides, multiple-way data augmentation is used to generate fake data on training set. Grad-CAM is used to give explainable heatmap.\nThe proposed MIDCAN achieves a sensitivity of 98.10\u00b11.88%, a specificity of 97.95\u00b12.26%, and an accuracy of 98.02\u00b11.35%.\nOur MIDCAN method provides better results than 8 state-of-the-art approaches. We demonstrate the using multiple modalities can achieve better results than individual modality. Also, we demonstrate that CBAM can help improve the diagnosis performance.", "journal": "Pattern recognition letters", "date": "2021-07-20", "authors": ["Yu-DongZhang", "ZhengZhang", "XinZhang", "Shui-HuaWang"], "doi": "10.1016/j.patrec.2021.06.021\n10.1038/s41416-020-01209-5\n10.1007/s12559-020-09776-8\n10.1007/s10044-021-00970-4\n10.1177/1740774520972687\n10.1002/mp.14701"}
{"title": "Computational pathology reveals unique spatial patterns of immune response in H&E images from COVID-19 autopsies: preliminary findings.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-07-17", "authors": ["Germ\u00e1nCorredor", "PaulaToro", "KaustavBera", "DylanRasmussen", "Vidya SankarViswanathan", "ChristinaBuzzy", "PingfuFu", "Lisa MBarton", "EdanaStroberg", "EricDuval", "HannahGilmore", "SanjayMukhopadhyay", "AnantMadabhushi"], "doi": "10.1117/1.JMI.8.S1.017501\n10.3389/fimmu.2020.01441\n10.1016/j.lfs.2020.117900\n10.1016/j.jaci.2009.12.980\n10.1016/S0952-7915(01)00308-9\n10.1309/AJCP35KOZSAVNQZW\n10.1016/S2213-2600(20)30243-5\n10.1093/ajcp/aqaa062\n10.1016/j.jtho.2020.02.010\n10.1038/s41379-020-0536-x\n10.7326/M20-2003\n10.7326/M20-2566\n10.1111/his.14134\n10.1111/his.14180\n10.1038/s41379-020-0603-3\n10.1016/j.chest.2020.03.032\n10.2214/AJR.20.23267\n10.2214/AJR.20.23214\n10.1016/S0140-6736(20)30566-3\n10.1016/j.jpha.2020.03.001\n10.1164/rccm.200909-1420OC\n10.3389/fimmu.2018.01521\n10.5858/134.2.235\n10.1038/modpathol.2011.125\n10.1016/j.celrep.2018.03.086\n10.10.1038/s41591-020-0900-x\n10.10.1158/1078-0432.CCR-18-2013\n10.1109/TBME.2009.2035305\n10.1097/PAS.0000000000000086\n10.1038/srep36231\n10.1109/ISBI.2009.5193250\n10.1371/journal.pone.0070221\n10.1038/modpathol.2017.98\n10.1038/s41598-017-13773-7\n10.1117/12.2293147\n10.1016/0196-6774(85)90017-3\n10.1016/j.prevetmed.2009.08.006\n10.1073/pnas.1213237110\n10.1038/s41467-019-13056-x\n10.1142/S0219720017500172\n10.1371/journal.pone.0056883\n10.1056/NEJMoa2002032\n10.1016/S1473-3099(20)30232-2\n10.1016/j.cell.2020.04.026\n10.1038/s41586-020-2355-0\n10.1016/j.clim.2020.108393\n10.1038/s41418-020-0530-3\n10.1016/j.ijid.2020.04.086\n10.1016/S2213-2600(20)30119-3\n10.1038/s41392-020-0148-4\n10.1093/jnci/dju435\n10.1111/his.14201\n10.1016/j.clinthera.2020.04.003\n10.1093/cid/ciaa410\n10.1007/s00428-020-02886-6\n10.2144/000113754"}
{"title": "DeepEMhancer: a deep learning solution for cryo-EM volume post-processing.", "abstract": "Cryo-EM maps are valuable sources of information for protein structure modeling. However, due to the loss of contrast at high frequencies, they generally need to be post-processed to improve their interpretability. Most popular approaches, based on global B-factor correction, suffer from limitations. For instance, they ignore the heterogeneity in the map local quality that reconstructions tend to exhibit. Aiming to overcome these problems, we present DeepEMhancer, a deep learning approach designed to perform automatic post-processing of cryo-EM maps. Trained on a dataset of pairs of experimental maps and maps sharpened using their respective atomic models, DeepEMhancer has learned how to post-process experimental maps performing masking-like and sharpening-like operations in a single step. DeepEMhancer was evaluated on a testing set of 20 different experimental maps, showing its ability to reduce noise levels and obtain more detailed versions of the experimental maps. Additionally, we illustrated the benefits of DeepEMhancer on the structure of the SARS-CoV-2 RNA polymerase.", "journal": "Communications biology", "date": "2021-07-17", "authors": ["RubenSanchez-Garcia", "JosueGomez-Blanco", "AnaCuervo", "Jose MariaCarazo", "Carlos Oscar SSorzano", "JavierVargas"], "doi": "10.1038/s42003-021-02399-1\n10.1016/j.jmb.2003.07.013\n10.7554/eLife.18722\n10.7554/eLife.42166\n10.1107/S2059798318004655\n10.1016/j.jsb.2020.107447\n10.7554/eLife.27131\n10.1038/s41467-021-21509-5\n10.1038/s42003-019-0437-z\n10.1016/j.jsb.2016.07.006\n10.1186/s12859-017-1757-y\n10.1038/s41592-019-0500-1\n10.1038/s41598-019-56847-4\n10.3390/molecules24061181\n10.1107/S2052252519011692\n10.1109/TMM.2019.2919431\n10.1093/nar/gkv1126\n10.1126/science.aao1140\n10.1016/j.cell.2017.12.005\n10.7554/eLife.46986\n10.1126/science.abb7498\n10.1007/s11263-019-01198-w\n10.1107/S0907444904019158\n10.1002/jcc.20084"}
{"title": "Integrated digital pathology at scale: A solution for clinical diagnostics and cancer research at a large academic medical center.", "abstract": "Broad adoption of digital pathology (DP) is still lacking, and examples for DP connecting diagnostic, research, and educational use cases are missing. We blueprint a holistic DP solution at a large academic medical center ubiquitously integrated into clinical workflows; researchapplications including molecular, genetic, and tissue databases; and educational processes.\nWe built a vendor-agnostic, integrated viewer for reviewing, annotating, sharing, and quality assurance of digital slides in a clinical or research context. It is the first homegrown viewer cleared by New York State provisional approval in 2020 for primary diagnosis and remote sign-out during the COVID-19 (coronavirus disease 2019) pandemic. We further introduce an interconnected Honest Broker for BioInformatics Technology (HoBBIT) to systematically compile and share large-scale DP research datasets including anonymized images, redacted pathology reports, and clinical data of patients with consent.\nThe solution has been operationally used over 3 years by 926 pathologists and researchers evaluating 288 903 digital slides. A total of 51% of these were reviewed within 1 month after scanning. Seamless integration of the viewer into 4 hospital systems clearly increases the adoption of DP. HoBBIT directly impacts the translation of knowledge in pathology into effective new health measures, including artificial intelligence-driven detection models for prostate cancer, basal cell carcinoma, and breast cancer metastases, developed and validated on thousands of cases.\nWe highlight major challenges and lessons learned when going digital to provide orientation for other pathologists. Building interconnected solutions will not only increase adoption of DP, but also facilitate next-generation computational pathology at scale for enhanced cancer research.", "journal": "Journal of the American Medical Informatics Association : JAMIA", "date": "2021-07-15", "authors": ["Peter JSch\u00fcffler", "LukeGeneslaw", "D Vijay KYarlagadda", "Matthew GHanna", "JenniferSamboy", "EvangelosStamelos", "ChadVanderbilt", "JohnPhilip", "Marc-HenriJean", "LorraineCorsale", "AllyneManzo", "Neeraj H GParamasivam", "John SZiegler", "JianjiongGao", "Juan CPerin", "Young SukKim", "Umeshkumar KBhanot", "Michael H ARoehrl", "OrlyArdon", "SarahChiang", "Dilip DGiri", "Carlie SSigel", "Lee KTan", "MelissaMurray", "ChristinaVirgo", "ChristineEngland", "YukakoYagi", "S JosephSirintrapun", "DavidKlimstra", "MeeraHameed", "Victor EReuter", "Thomas JFuchs"], "doi": "10.1093/jamia/ocab085"}
{"title": "Diagnosis of COVID-19 Using Machine Learning and Deep Learning: A Review.", "abstract": "This paper provides a systematic review of the application of Artificial Intelligence (AI) in the form of Machine Learning (ML) and Deep Learning (DL) techniques in fighting against the effects of novel coronavirus disease (COVID-19).\nThe objective is to perform a scoping review on AI for COVID-19 using preferred reporting items of systematic reviews and meta-analysis (PRISMA) guidelines. A literature search was performed for relevant studies published from 1 January 2020 till 27 March 2021. Out of 4050 research papers available in reputed publishers, a full-text review of 440 articles was done based on the keywords of AI, COVID-19, ML, forecasting, DL, X-ray, and Computed Tomography (CT). Finally, 52 articles were included in the result synthesis of this paper. As part of the review, different ML regression methods were reviewed first in predicting the number of confirmed and death cases. Secondly, a comprehensive survey was carried out on the use of ML in classifying COVID-19 patients. Thirdly, different datasets on medical imaging were compared in terms of the number of images, number of positive samples and number of classes in the datasets. The different stages of the diagnosis, including preprocessing, segmentation and feature extraction were also reviewed. Fourthly, the performance results of different research papers were compared to evaluate the effectiveness of DL methods on different datasets.\nResults show that residual neural network (ResNet-18) and densely connected convolutional network (DenseNet 169) exhibit excellent classification accuracy for X-ray images, while DenseNet-201 has the maximum accuracy in classifying CT scan images. This indicates that ML and DL are useful tools in assisting researchers and medical professionals in predicting, screening and detecting COVID-19.\nFinally, this review highlights the existing challenges, including regulations, noisy data, data privacy, and the lack of reliable large datasets, then provides future research directions in applying AI in managing COVID-19.", "journal": "Current medical imaging", "date": "2021-07-15", "authors": ["M Rubaiyat HossainMondal", "SubratoBharati", "PrajoyPodder"], "doi": "10.2174/1573405617666210713113439"}
{"title": "Visceral Adiposity and Severe COVID-19 Disease: Application of an Artificial Intelligence Algorithm to Improve Clinical Risk Prediction.", "abstract": "Obesity has been linked to severe clinical outcomes among people who are hospitalized with coronavirus disease 2019 (COVID-19). We tested the hypothesis that visceral adipose tissue (VAT) is associated with severe outcomes in patients hospitalized with COVID-19, independent of body mass index (BMI).\nWe analyzed data from the Massachusetts General Hospital COVID-19 Data Registry, which included patients admitted with polymerase chain reaction-confirmed severe acute respiratory syndrome coronavirus 2 infection from March 11 to May 4, 2020. We used a validated, fully automated artificial intelligence (AI) algorithm to quantify VAT from computed tomography (CT) scans during or before the hospital admission. VAT quantification took an average of 2 \u00b1 0.5 seconds per patient. We dichotomized VAT as high and low at a threshold of \u2265100 cm\nA total of 378 participants had CT imaging. Kaplan-Meier curves showed that participants with high VAT had a greater risk of the outcome compared with those with low VAT (\nHigh VAT is associated with a greater risk of severe disease or death in COVID-19 and can offer more precise information to risk-stratify individuals beyond BMI. AI offers a promising approach to routinely ascertain VAT and improve clinical risk prediction in COVID-19.", "journal": "Open forum infectious diseases", "date": "2021-07-15", "authors": ["AlexanderGoehler", "Tzu-Ming HarryHsu", "Jacqueline ASeiglie", "Mark JSiedner", "JanetLo", "VirginiaTriant", "JohnHsu", "AndreaFoulkes", "IngridBassett", "RaminKhorasani", "Deborah JWexler", "PeterSzolovits", "James BMeigs", "JenniferManne-Goehler"], "doi": "10.1093/ofid/ofab275\n10.2337/dc20-2676"}
{"title": "Artificial Intelligence in COVID-19 Imaging Mismatched to the Clinic.", "abstract": null, "journal": "JAMA", "date": "2021-07-14", "authors": ["JenniferAbbasi"], "doi": "10.1001/jama.2021.10888"}
{"title": "Deep learning for COVID-19 detection based on CT images.", "abstract": "COVID-19 has tremendously impacted patients and medical systems globally. Computed tomography images can effectively complement the reverse transcription-polymerase chain reaction testing. This study adopted a convolutional neural network for COVID-19 testing. We examined the performance of different pre-trained models on CT testing and identified that larger, out-of-field datasets boost the testing power of the models. This suggests that a priori knowledge of the models from out-of-field training is also applicable to CT images. The proposed transfer learning approach proves to be more successful than the current approaches described in literature. We believe that our approach has achieved the state-of-the-art performance in identification thus far. Based on experiments with randomly sampled training datasets, the results reveal a satisfactory performance by our model. We investigated the relevant visual characteristics of the CT images used by the model; these may assist clinical doctors in manual screening.", "journal": "Scientific reports", "date": "2021-07-14", "authors": ["WentaoZhao", "WeiJiang", "XinguoQiu"], "doi": "10.1038/s41598-021-93832-2\n10.1038/s41564-020-0695-z\n10.1038/s41586-020-2008-3\n10.1097/RLI.0000000000000670\n10.1038/s41586-020-2012-7\n10.1148/radiol.2020200490\n10.1148/radiol.2020200432\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1016/S1473-3099(20)30086-4\n10.1016/j.tmaid.2020.101623\n10.1016/j.jtho.2020.02.010\n10.1148/radiol.2020200823\n10.1038/s41591-020-0931-3\n10.1038/s41598-019-56847-4\n10.1007/s42979-020-00401-x\n10.1007/s42979-020-00335-4\n10.1007/s42979-020-00300-1\n10.1007/s42979-020-00383-w\n10.1016/j.imu.2020.100412\n10.1016/j.imu.2020.100505\n10.1007/s42979-020-00216-w\n10.1109/ACCESS.2021.3058537\n10.1109/TKDE.2009.191\n10.1016/j.cell.2020.04.045\n10.1038/s41551-020-00633-5\n10.1109/TMI.2016.2535302\n10.1371/journal.pone.0248414\n10.1038/s41467-020-17971-2\n10.1118/1.3528204\n10.1016/j.ejrad.2020.108941\n10.1007/s11263-015-0816-y\n10.1016/j.chaos.2020.109944\n10.1016/j.chaos.2020.110190"}
{"title": "Imaging in the COVID-19 era: Lessons learned during a pandemic.", "abstract": "The first year of the coronavirus disease 2019 (COVID-19) pandemic has been a year of unprecedented changes, scientific breakthroughs, and controversies. The radiology community has not been spared from the challenges imposed on global healthcare systems. Radiology has played a crucial part in tackling this pandemic, either by demonstrating the manifestations of the virus and guiding patient management, or by safely handling the patients and mitigating transmission within the hospital. Major modifications involving all aspects of daily radiology practice have occurred as a result of the pandemic, including workflow alterations, volume reductions, and strict infection control strategies. Despite the ongoing challenges, considerable knowledge has been gained that will guide future innovations. The aim of this review is to provide the latest evidence on the role of imaging in the diagnosis of the multifaceted manifestations of COVID-19, and to discuss the implications of the pandemic on radiology departments globally, including infection control strategies and delays in cancer screening. Lastly, the promising contribution of artificial intelligence in the COVID-19 pandemic is explored.", "journal": "World journal of radiology", "date": "2021-07-13", "authors": ["Georgios AntoniosSideris", "MelinaNikolakea", "Aikaterini-EleftheriaKaranikola", "SofiaKonstantinopoulou", "DimitriosGiannis", "LucyModahl"], "doi": "10.4329/wjr.v13.i6.192"}
{"title": "COVID-19 imaging: Diagnostic approaches, challenges, and evolving advances.", "abstract": "The role of radiology and the radiologist have evolved throughout the coronavirus disease-2019 (COVID-19) pandemic. Early on, chest computed tomography was used for screening and diagnosis of COVID-19; however, it is now indicated for high-risk patients, those with severe disease, or in areas where polymerase chain reaction testing is sparsely available. Chest radiography is now utilized mainly for monitoring disease progression in hospitalized patients showing signs of worsening clinical status. Additionally, many challenges at the operational level have been overcome within the field of radiology throughout the COVID-19 pandemic. The use of teleradiology and virtual care clinics greatly enhanced our ability to socially distance and both are likely to remain important mediums for diagnostic imaging delivery and patient care. Opportunities to better utilize of imaging for detection of extrapulmonary manifestations and complications of COVID-19 disease will continue to arise as a more detailed understanding of the pathophysiology of the virus continues to be uncovered and identification of predisposing risk factors for complication development continue to be better understood. Furthermore, unidentified advancements in areas such as standardized imaging reporting, point-of-care ultrasound, and artificial intelligence offer exciting discovery pathways that will inevitably lead to improved care for patients with COVID-19.", "journal": "World journal of radiology", "date": "2021-07-13", "authors": ["Dante LPezzutti", "VibhorWadhwa", "Mina SMakary"], "doi": "10.4329/wjr.v13.i6.171"}
{"title": "A stacked ensemble for the detection of COVID-19 with high recall and accuracy.", "abstract": "The main challenges for the automatic detection of the coronavirus disease (COVID-19) from computed tomography (CT) scans of an individual are: a lack of large datasets, ambiguity in the characteristics of COVID-19 and the detection techniques having low sensitivity (or recall). Hence, developing diagnostic techniques with high recall and automatic feature extraction using the available data are crucial for controlling the spread of COVID-19. This paper proposes a novel stacked ensemble capable of detecting COVID-19 from a patient's chest CT scans with high recall and accuracy. A systematic approach for designing a stacked ensemble from pre-trained computer vision models using transfer learning (TL) is presented. A novel diversity measure that results in the stacked ensemble with high recall and accuracy is proposed. The stacked ensemble proposed in this paper considers four pre-trained computer vision models: the visual geometry group (VGG)-19, residual network (ResNet)-101, densely connected convolutional network (DenseNet)-169 and wide residual network (WideResNet)-50-2. The proposed model was trained and evaluated with three different chest CT scans. As recall is more important than precision, the trade-offs between recall and precision were explored in relevance to COVID-19. The optimal recommended threshold values were found for each dataset.", "journal": "Computers in biology and medicine", "date": "2021-07-12", "authors": ["EbenezerJangam", "Chandra Sekhara RaoAnnavarapu"], "doi": "10.1016/j.compbiomed.2021.104608\n10.1016/j.inpa.2020.05.003"}
{"title": "Comparison of deep learning, radiomics and subjective assessment of chest CT findings in SARS-CoV-2 pneumonia.", "abstract": "Comparison of deep learning algorithm, radiomics and subjective assessment of chest CT for predicting outcome (death or recovery) and intensive care unit (ICU) admission in patients with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection.\nThe multicenter, ethical committee-approved, retrospective study included non-contrast-enhanced chest CT of 221 SARS-CoV-2 positive patients from Italy (n\u00a0=\u00a0196 patients; mean age 64\u00a0\u00b1\u00a016\u00a0years) and Denmark (n\u00a0=\u00a025; mean age 69\u00a0\u00b1\u00a013\u00a0years). A thoracic radiologist graded presence, type and extent of pulmonary opacities and severity of motion artifacts in each lung lobe on all chest CTs. Thin-section CT images were processed with CT Pneumonia Analysis Prototype (Siemens Healthineers) which yielded segmentation masks from a deep learning (DL) algorithm to derive features of lung abnormalities such as opacity scores, mean HU, as well as volume and percentage of all-attenuation and high-attenuation (opacities >-200 HU) opacities. Separately, whole lung radiomics were obtained for all CT exams. Analysis of variance and multiple logistic regression were performed for data analysis.\nModerate to severe respiratory motion artifacts affected nearly one-quarter of chest CTs in patients. Subjective severity assessment, DL-based features and radiomics predicted patient outcome (AUC 0.76 vs AUC 0.88 vs AUC 0.83) and need for ICU admission (AUC 0.77 vs AUC 0.0.80 vs 0.82). Excluding chest CT with motion artifacts, the performance of DL-based and radiomics features improve for predicting ICU admission.\nDL-based and radiomics features of pulmonary opacities from chest CT were superior to subjective assessment for differentiating patients with favorable and adverse outcomes.", "journal": "Clinical imaging", "date": "2021-07-11", "authors": ["ChiaraArru", "ShadiEbrahimian", "ZenoFalaschi", "Jacob ValentinHansen", "AlessioPasche", "Mads DamLyhne", "MathisZimmermann", "FelixDurlak", "MatthiasMitschke", "AlessandroCarriero", "Jens ErikNielsen-Kudsk", "Mannudeep KKalra", "LucaSaba"], "doi": "10.1016/j.clinimag.2021.06.036\n10.1007/s00330-020-07033-y\n10.1007/s00330-020-06969-5\n10.1007/s11547-020-01232-9\n10.1148/ryct.2020200075\n10.1186/s41747-020-00167-0\n10.21203/rs.3.rs-30481/v1\n10.1148/ryct.2020200047\n10.1038/s41598-019-48423-7\n10.1101/2020.05.14.20101972\n10.1007/s00330-020-07012-3\n10.1016/j.clinimag.2020.04.042"}
{"title": "Machine learning application for the prediction of SARS-CoV-2 infection using blood tests and chest radiograph.", "abstract": "Triaging and prioritising patients for RT-PCR test had been essential in the management of COVID-19 in resource-scarce countries. In this study, we applied machine learning (ML) to the task of detection of SARS-CoV-2 infection using basic laboratory markers. We performed the statistical analysis and trained an ML model on a retrospective cohort of 5148 patients from 24 hospitals in Hong Kong to classify COVID-19 and other aetiology of pneumonia. We validated the model on three temporal validation sets from different waves of infection in Hong Kong. For predicting SARS-CoV-2 infection, the ML model achieved high AUCs and specificity but low sensitivity in all three validation sets (AUC: 89.9-95.8%; Sensitivity: 55.5-77.8%; Specificity: 91.5-98.3%). When used in adjunction with radiologist interpretations of chest radiographs, the sensitivity was over 90% while keeping moderate specificity. Our study showed that machine learning model based on readily available laboratory markers could achieve high accuracy in predicting SARS-CoV-2 infection.", "journal": "Scientific reports", "date": "2021-07-11", "authors": ["RichardDu", "Efstratios DTsougenis", "Joshua W KHo", "Joyce K YChan", "Keith W HChiu", "Benjamin X HFang", "Ming YenNg", "Siu-TingLeung", "Christine S YLo", "Ho-Yuen FWong", "Hiu-Yin SLam", "Long-Fung JChiu", "Tiffany YSo", "Ka TakWong", "Yiu Chung IWong", "KevinYu", "Yiu-CheongYeung", "ThomasChik", "Joanna W KPang", "Abraham Ka-ChungWai", "Michael DKuo", "Tina P WLam", "Pek-LanKhong", "Ngai-TseungCheung", "VarutVardhanabhuti"], "doi": "10.1038/s41598-021-93719-2\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2002032\n10.1136/bmj.m1091\n10.1001/jama.2020.1585\n10.1016/j.amjmed.2005.06.018\n10.1016/s0140-6736(20)30183-5\n10.1007/s10916-020-01597-4\n10.1086/644496\n10.1136/gutjnl-2020-321726\n10.1016/s1473-3099(20)30086-4\n10.1038/s42256-020-0180-7\n10.1001/jamainternmed.2020.2033\n10.1016/j.intimp.2020.106705\n10.1186/s12967-021-02720-w\n10.1038/s41598-021-83967-7\n10.1186/s12879-021-05839-9\n10.1038/s41598-021-81844-x\n10.1002/ctm2.323\n10.1016/j.compbiomed.2021.104335\n10.1038/s41598-021-86735-9\n10.1136/bmj.g7594\n10.1186/s40537-020-00369-8\n10.1093/biomet/26.4.404\n10.1002/sim.2677\n10.2307/2531595\n10.1038/s41551-018-0304-0"}
{"title": "Deep learning applied to lung ultrasound videos for scoring COVID-19 patients: A multicenter study.", "abstract": "In the current pandemic, lung ultrasound (LUS) played a useful role in evaluating patients affected by COVID-19. However, LUS remains limited to the visual inspection of ultrasound data, thus negatively affecting the reliability and reproducibility of the findings. Moreover, many different imaging protocols have been proposed, most of which lacked proper clinical validation. To address these problems, we were the first to propose a standardized imaging protocol and scoring system. Next, we developed the first deep learning (DL) algorithms capable of evaluating LUS videos providing, for each video-frame, the score as well as semantic segmentation. Moreover, we have analyzed the impact of different imaging protocols and demonstrated the prognostic value of our approach. In this work, we report on the level of agreement between the DL and LUS experts, when evaluating LUS data. The results show a percentage of agreement between DL and LUS experts of 85.96% in the stratification between patients at high risk of clinical worsening and patients at low risk. These encouraging results demonstrate the potential of DL models for the automatic scoring of LUS data, when applied to high quality data acquired accordingly to a standardized imaging protocol.", "journal": "The Journal of the Acoustical Society of America", "date": "2021-07-10", "authors": ["FedericoMento", "TizianoPerrone", "AnnaFiengo", "AndreaSmargiassi", "RiccardoInchingolo", "GinoSoldati", "LibertarioDemi"], "doi": "10.1121/10.0004855"}
{"title": "Machine Learning for COVID-19 Diagnosis and Prognostication: Lessons for Amplifying the Signal While Reducing the Noise.", "abstract": null, "journal": "Radiology. Artificial intelligence", "date": "2021-07-10", "authors": ["DerekDriggs", "IanSelby", "MichaelRoberts", "EffrossyniGkrania-Klotsas", "James H FRudd", "GuangYang", "JudithBabar", "EvisSala", "Carola-BibianeSch\u00f6nlieb", "NoneNone"], "doi": "10.1148/ryai.2021210011"}
{"title": "Fuzzy rank-based fusion of CNN models using Gompertz function for screening COVID-19 CT-scans.", "abstract": "COVID-19 has crippled the world's healthcare systems, setting back the economy and taking the lives of several people. Although potential vaccines are being tested and supplied around the world, it will take a long time to reach every human being, more so with new variants of the virus emerging, enforcing a lockdown-like situation on parts of the world. Thus, there is a dire need for early and accurate detection of COVID-19 to prevent the spread of the disease, even more. The current gold-standard RT-PCR test is only 71% sensitive and is a laborious test to perform, leading to the incapability of conducting the population-wide screening. To this end, in this paper, we propose an automated COVID-19 detection system that uses CT-scan images of the lungs for classifying the same into COVID and Non-COVID cases. The proposed method applies an ensemble strategy that generates fuzzy ranks of the base classification models using the Gompertz function and fuses the decision scores of the base models adaptively to make the final predictions on the test cases. Three transfer learning-based convolutional neural network models are used, namely VGG-11, Wide ResNet-50-2, and Inception v3, to generate the decision scores to be fused by the proposed ensemble model. The framework has been evaluated on two publicly available chest CT scan datasets achieving state-of-the-art performance, justifying the reliability of the model. The relevant source codes related to the present work is available in: GitHub.", "journal": "Scientific reports", "date": "2021-07-10", "authors": ["RohitKundu", "HritamBasak", "Pawan KumarSingh", "AliAhmadian", "MassimilianoFerrara", "RamSarkar"], "doi": "10.1038/s41598-021-93658-y\n10.1001/jama.2020.2783\n10.1109/OJEMB.2020.2999214\n10.1016/j.chaos.2020.109944\n10.1016/j.procs.2018.05.057\n10.1016/j.knosys.2020.106270\n10.1016/j.asoc.2020.106580\n10.1177/0846537120913033\n10.1016/S0140-6736(20)30728-5\n10.1148/radiol.2020200230\n10.3348/kjr.2020.0146\n10.1016/j.chaos.2020.110059\n10.1016/j.bdr.2021.100233\n10.1080/07391102.2020.1788642\n10.1016/j.chaos.2020.110190\n10.3390/diagnostics11050895\n10.1037/h0042519\n10.12688/wellcomeopenres.16006.1\n10.1016/j.imu.2020.100427\n10.1162/089976698300017197\n10.1371/journal.pone.0178691\n10.3390/e11040807"}
{"title": "COVID-19 lesion detection and segmentation-A deep learning method.", "abstract": "In this paper, we utilized deep learning methods to screen the positive COVID-19 cases in chest CT. Our primary goal is to supply rapid and precise assistance for disease surveillance on the medical imaging aspect.\nBasing on deep learning, we combined semantic segmentation and object detection methods to study the lesion performance of COVID-19. We put forward a novel end-to-end model which takes advantage of the Spatio-temporal features. Furthermore, a segmentation model attached with a fully connected CRF was designed for a more effective ROI input.\nOur method showed a better performance across different metrics against the comparison models. Moreover, our strategy highlighted strong robustness for the processed augmented testing samples.\nThe comprehensive fusion of Spatio-temporal correlations can exploit more valuable features for locating target regions, and this mechanism is friendly to detect tiny lesions. Although it remains in discrete form, the feature extracting in temporal dimension improves the precision of final prediction.", "journal": "Methods (San Diego, Calif.)", "date": "2021-07-09", "authors": ["LiuJingxin", "ZhangMengchao", "LiuYuchen", "CuiJinglei", "ZhongYutong", "ZhangZhong", "ZuLihui"], "doi": "10.1016/j.ymeth.2021.07.001\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2001316\n10.1109/JBHI.2017.2725903\n10.3390/s20185097\n10.1148/radiol.2020200905\n10.1038/s41598-020-76282-0\n10.1007/s11432-020-2849-3\n10.1007/s11263-019-01198-w"}
{"title": "Automated Diagnosis of COVID-19 Using Deep Features and Parameter Free BAT Optimization.", "abstract": "", "journal": "IEEE journal of translational engineering in health and medicine", "date": "2021-07-09", "authors": ["TaranjitKaur", "Tapan KGandhi", "Bijaya KPanigrahi"], "doi": "10.1109/JTEHM.2021.3077142\n10.1007/s00521-020-05437-x\n10.1101/2020.04.24.20078584\n10.1109/TCBB.2020.3009859\n10.1101/2020.04.13.20063941"}
{"title": "Epidemiologic Features, Radiological Findings andClinical Outcomes of 19 Patients with COVID-19in a Single Center in Beijing, China.", "abstract": "ObjectiveTo describe the epidemiologic, clinical, laboratory, and radiological characteristics and prognoses of COVID-19 confirmed patients in a single center in Beijing, China. Methods The study retrospectively included 19 patients with nucleic acid-confirmed SARS-CoV-2 infection at our hospital from January 20 to March 5, 2020. The final follow-up date was March 14, 2020. The epidemiologic and clinical information was obtained through direct communication with the patients or their family members. Laboratory results retrieved from medical records and radiological images were analyzed both qualitatively by two senior chest radiologists as well as quantitatively via an artificial intelligence software. Results We identified 5 family clusters (13/19, 68.4%) from the study cohort. All cases had good clinical prognoses and were either mild (3/19) or moderate (16/19) clinical types. Fever (15/19, 78.9%) and dry cough (11/19, 57.9%) were common symptoms. Two patients received negative results for more than three consecutive viral nucleic acid tests. The longest interval between an initial CT abnormal finding and a confirmed diagnosis was 30 days. One patient's nucleic acid test turned positive on the follow-up examination after discharge. The presence of radiological abnormalities was non-specific for the diagnosis of COVID-19. Conclusions COVID-19 patients with mild or no clinical symptoms are common in Beijing, China. Radiological abnormalities are mostly non-specific and massive CT examinations for COVID-19 screening should be avoided. Analyses of the contact histories of diagnosed cases in combination with clinical, radiological and laboratory findings are crucial for the early detection of COVID-19. Close monitoring after discharge is also recommended.", "journal": "Chinese medical sciences journal = Chung-kuo i hsueh k'o hsueh tsa chih", "date": "2021-07-08", "authors": ["LanSong", "Zhen ChenZhu", "Rui JieZhao", "Peng ChangLi", "Du XueTian", "Tie KuanDu", "YanXu", "QiwenYang", "WeiCao", "WeiSong", "Zheng YuJin"], "doi": "10.24920/003821\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30154-9\n10.1007/s00330-020-06731-x\n10.1056/NEJ-Moa2001017\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1016/S0140-6736(20)30185-9\n10.1016/S1473-3099(20)30086-4\n10.1016/j.jinf.2020.02.018\n10.1007/s11427-020-1661-4\n10.1016/j.jmii.2020.02.012\n10.1016/S0140-6736(20)30211-7\n10.1002/ppul.24718\n10.1001/jama.2020.3204\n10.1007/978-3-030-32226-7_2\n10.1001/jama.2020.1623\n10.1148/radiol.2020200230\n10.1148/radiol.2020200370\n10.1148/radiol.2283030541\n10.2214/AJR.14.13671\n10.2214/AJR.20.22954\n10.1148/radiol.2020200642\n10.1056/NEJMC2001737\n10.7499/j.issn.1008-8830.2020.03.007\n10.1001/jama.2020.2783\n10.1056/NEJMC2001468\n10.1016/S0140-6736(20)30260-9"}
{"title": "Letter to the Editor: Quantitative evaluation of COVID-19 pneumonia severity by CT pneumonia analysis algorithm using deep learning technology and blood test results.", "abstract": null, "journal": "Japanese journal of radiology", "date": "2021-07-06", "authors": ["RujittikaMungmunpuntipantip", "VirojWiwanitkit"], "doi": "10.1007/s11604-021-01169-7\n10.1007/s11604-021-01134-4\n10.7754/Clin.Lab.2019.190352"}
{"title": "Artificial intelligence for prediction of COVID-19 progression using CT imaging and clinical data.", "abstract": "Early recognition of coronavirus disease 2019 (COVID-19) severity can guide patient management. However, it is challenging to predict when COVID-19 patients will progress to critical illness. This study aimed to develop an artificial intelligence system to predict future deterioration to critical illness in COVID-19 patients.\nAn artificial intelligence (AI) system in a time-to-event analysis framework was developed to integrate chest CT and clinical data for risk prediction of future deterioration to critical illness in patients with COVID-19.\nA multi-institutional international cohort of 1,051 patients with RT-PCR confirmed COVID-19 and chest CT was included in this study. Of them, 282 patients developed critical illness, which was defined as requiring ICU admission and/or mechanical ventilation and/or reaching death during their hospital stay. The AI system achieved a C-index of 0.80 for predicting individual COVID-19 patients' to critical illness. The AI system successfully stratified the patients into high-risk and low-risk groups with distinct progression risks (p < 0.0001).\nUsing CT imaging and clinical data, the AI system successfully predicted time to critical illness for individual patients and identified patients with high risk. AI has the potential to accurately triage patients and facilitate personalized treatment.\n\u2022 AI system can predict time to critical illness for patients with COVID-19 by using CT imaging and clinical data.", "journal": "European radiology", "date": "2021-07-06", "authors": ["RobinWang", "ZhichengJiao", "LiYang", "Ji WhaeChoi", "ZengXiong", "KaseyHalsey", "Thi My LinhTran", "IanPan", "Scott ACollins", "XueFeng", "JingWu", "KenChang", "Lin-BoShi", "ShuaiYang", "Qi-ZhiYu", "JieLiu", "Fei-XianFu", "Xiao-LongJiang", "Dong-CuiWang", "Li-PingZhu", "Xiao-PingYi", "Terrance THealey", "Qiu-HuaZeng", "TaoLiu", "Ping-FengHu", "Raymond YHuang", "Yi-HuiLi", "Ronnie ASebro", "Paul J LZhang", "JianxinWang", "Michael KAtalay", "Wei-HuaLiao", "YongFan", "Harrison XBai"], "doi": "10.1007/s00330-021-08049-8\n10.1001/jama.2020.6627\n10.1056/NEJMoa2007016\n10.1186/s13613-020-00650-2\n10.7861/clinmed.2020-0214\n10.1136/bmj.m1198\n10.1148/ryct.2020200047\n10.2214/AJR.20.22976\n10.1097/RLI.0000000000000672\n10.1164/rccm.202002-0445OC\n10.1016/j.cell.2020.04.045\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1111/j.0006-341X.2005.030814.x\n10.1148/radiol.2363040958\n10.1186/s13613-020-00661-z\n10.1016/S0140-6736(20)31022-9\n10.1148/ryct.2020200322"}
{"title": "Images denoising for COVID-19 chest X-ray based on multi-resolution parallel residual CNN.", "abstract": "Chest X-ray (CXR) is a medical imaging technology that is common and economical to use in clinical. Recently, coronavirus (COVID-19) has spread worldwide, and the second wave is rebounding strongly now with the coming winter that has a detrimental effect on the global economy and health. To make pre-diagnosis of COVID-19 as soon as possible, and reduce the work pressure of medical staff, making use of deep learning networks to detect positive CXR images of infected patients is a critical step. However, there are complex edge structures and rich texture details in the CXR images susceptible to noise that can interfere with the diagnosis of the machines and the doctors. Therefore, in this paper, we proposed a novel multi-resolution parallel residual CNN (named MPR-CNN) for CXR images denoising and special application for COVID-19 which can improve the image quality. The core of MPR-CNN consists of several essential modules. (a) Multi-resolution parallel convolution streams are utilized for extracting more reliable spatial and semantic information in multi-scale features. (b) Efficient channel and spatial attention can let the network focus more on texture details in CXR images with fewer parameters. (c) The adaptive multi-resolution feature fusion method based on attention is utilized to improve the expression of the network. On the whole, MPR-CNN can simultaneously retain spatial information in the shallow layers with high resolution and semantic information in the deep layers with low resolution. Comprehensive experiments demonstrate that our MPR-CNN can better retain the texture structure details in CXR images. Additionally, extensive experiments show that our MPR-CNN has a positive impact on CXR images classification and detection of COVID-19 cases from denoised CXR images.", "journal": "Machine vision and applications", "date": "2021-07-06", "authors": ["XiaobenJiang", "YuZhu", "BingbingZheng", "DaweiYang"], "doi": "10.1007/s00138-021-01224-3\n10.1109/ACCESS.2020.2994762\n10.1016/j.ijsu.2020.02.034\n10.1109/TMI.2020.2993291\n10.1038/nature14539\n10.1016/j.neunet.2014.09.003\n10.1109/TPAMI.2020.2975798\n10.1109/TMM.2020.2967645\n10.1145/3404374\n10.1109/TCSVT.2021.3067449\n10.1016/j.media.2017.07.005\n10.3389/fnins.2019.00422\n10.1109/TMI.2020.2995508\n10.1007/s12559-020-09776-8\n10.1016/j.eswa.2021.114848\n10.1109/TMI.2020.2996645\n10.1109/RBME.2020.2987975\n10.1049/iet-ipr.2019.0241\n10.1186/s12938-018-0496-2\n10.1016/j.nima.2017.12.050\n10.1109/TBME.2009.2028876\n10.1109/TBME.2012.2217493\n10.1049/ip-vis:20050975\n10.1016/j.neucom.2015.08.117\n10.1109/TIP.2007.901238\n10.1002/mp.13252\n10.1016/j.neunet.2019.12.024\n10.1016/j.heliyon.2017.e00393\n10.1109/TIP.2017.2662206\n10.1109/LSP.2017.2782270\n10.1109/TIP.2018.2839891\n10.1016/S0031-3203(01)00152-2\n10.1016/S0031-3203(02)00262-5\n10.1109/83.913594\n10.1109/TIP.2003.819861"}
{"title": "Deep convolution neural networks to differentiate between COVID-19 and other pulmonary abnormalities on chest radiographs: Evaluation using internal and external datasets.", "abstract": "We aimed to evaluate the performance of convolutional neural networks (CNNs) in the classification of coronavirus disease 2019 (COVID-19) disease using normal, pneumonia, and COVID-19 chest radiographs (CXRs). First, we collected 9194 CXRs from open datasets and 58 from the Korea University Anam Hospital (KUAH). The number of normal, pneumonia, and COVID-19 CXRs were 4580, 3884, and 730, respectively. The CXRs obtained from the open dataset were randomly assigned to the training, tuning, and test sets in a 70:10:20 ratio. For external validation, the KUAH (20 normal, 20 pneumonia, and 18 COVID-19) dataset, verified by radiologists using computed tomography, was used. Subsequently, transfer learning was conducted using DenseNet169, InceptionResNetV2, and Xception to identify COVID-19 using open datasets (internal) and the KUAH dataset (external) with histogram matching. Gradient-weighted class activation mapping was used for the visualization of abnormal patterns in CXRs. The average AUC and accuracy of the multiscale and mixed-COVID-19Net using three CNNs over five folds were (0.99\u2009\u00b1\u20090.01 and 92.94%\u2009\u00b1\u20090.45%), (0.99\u2009\u00b1\u20090.01 and 93.12%\u2009\u00b1\u20090.23%), and (0.99\u2009\u00b1\u20090.01 and 93.57%\u2009\u00b1\u20090.29%), respectively, using the open datasets (internal). Furthermore, these values were (0.75 and 74.14%), (0.72 and 68.97%), and (0.77 and 68.97%), respectively, for the best model among the fivefold cross-validation with the KUAH dataset (external) using domain adaptation. The various state-of-the-art models trained on open datasets show satisfactory performance for clinical interpretation. Furthermore, the domain adaptation for external datasets was found to be important for detecting COVID-19 as well as other diseases.", "journal": "International journal of imaging systems and technology", "date": "2021-07-06", "authors": ["YongwonCho", "Sung HoHwang", "Yu-WhanOh", "Byung-JooHam", "Min JuKim", "Beom JinPark"], "doi": "10.1002/ima.22595\n10.3390/jcm9041225\n10.1001/jama.2020.3786\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1016/S0140-6736(98)02239-9\n10.1007/s13246-020-00865-4\n10.2214/AJR.20.23034\n10.1109/JBHI.2016.2635663\n10.1038/s41598-019-51832-3\n10.1002/ima.22508\n10.1038/s41598-020-74626-429\n10.1007/s00330-020-06892-9\n10.1148/radiol.2020201874"}
{"title": "Imaging Cardiovascular Inflammation in the COVID-19 Era.", "abstract": "Cardiac complications are among the most frequent extrapulmonary manifestations of COVID-19 and are associated with high mortality rates. Moreover, positive SARS-CoV-2 patients with underlying cardiovascular disease are more likely to require intensive care and are at higher risk of death. The underlying mechanism for myocardial injury is multifaceted, in which the severe inflammatory response causes myocardial inflammation, coronary plaque destabilization, acute thrombotic events, and ischemia. Cardiac magnetic resonance (CMR) imaging is the non-invasive method of choice for identifying myocardial injury, and it is able to differentiate between underlying causes in various and often challenging clinical scenarios. Multimodal imaging protocols that incorporate CMR and computed tomography provide a complex evaluation for both respiratory and cardiovascular complications of SARS-CoV2 infection. This, in relation to biological evaluation of systemic inflammation, can guide appropriate therapeutic management in every stage of the disease. The use of artificial intelligence can further improve the diagnostic accuracy of these imaging techniques, thus enabling risk stratification and evaluation of prognosis. The present manuscript aims to review the current knowledge on the possible modalities for imaging COVID-related myocardial inflammation or post-COVID coronary inflammation and atherosclerosis.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-07-03", "authors": ["AndrasMester", "ImreBenedek", "NoraRat", "CosminTolescu", "Stefania AlexandraPolexa", "TheodoraBenedek"], "doi": "10.3390/diagnostics11061114\n10.1038/s41591-020-0968-3\n10.1001/jamacardio.2020.1105\n10.1093/eurheartj/ehaa388\n10.1007/s11739-020-02493-y\n10.1001/jamacardio.2020.1017\n10.1001/jamacardio.2020.0950\n10.1016/j.pcad.2020.03.001\n10.1016/j.mehy.2020.110125\n10.1161/CIRCULATIONAHA.120.047549\n10.1161/CIRCRESAHA.120.317055\n10.1007/s42399-020-00344-7\n10.1016/j.hlc.2018.06.1057\n10.1093/eurheartj/ehz425\n10.2174/1389201018666170630120805\n10.1161/JAHA.116.005077\n10.1371/journal.pmed.1000286\n10.1111/joim.12406\n10.1093/cvr/cvz009\n10.1016/j.atherosclerosis.2010.05.034\n10.1007/s10554-020-01926-1\n10.1161/ATVBAHA.120.312470\n10.1093/cvr/cvaa106\n10.1016/j.lfs.2020.117723\n10.1016/j.ijcha.2020.100557\n10.7150/thno.48076\n10.1002/jmv.25785\n10.1161/CIRCRESAHA.120.317015\n10.1111/j.1365-2362.2009.02153.x\n10.1016/j.cjca.2020.05.018\n10.1161/CIRCULATIONAHA.120.047049\n10.1007/s00395-020-0791-5\n10.3389/fimmu.2020.01446\n10.1136/heartjnl-2020-317056\n10.1016/j.autrev.2020.102537\n10.15252/emmm.202012421\n10.1186/s43168-021-00054-1\n10.1093/cid/ciaa449\n10.1001/jamacardio.2020.3551\n10.1101/2020.12.19.20248542\n10.1038/s41379-021-00790-1\n10.1007/s00414-020-02500-z\n10.1161/CIRCULATIONAHA.120.050097\n10.1093/eurheartj/ehaa231\n10.1016/S1473-3099(20)30434-5\n10.2478/jce-2020-0002\n10.1093/eurheartj/ehaa623\n10.1093/eurheartj/ehaa664\n10.1016/j.cpcardiol.2020.100763\n10.4081/monaldi.2021.1710\n10.12691/ajmcr-8-10-8\n10.1007/s42399-021-00743-4\n10.1016/S0140-6736(20)30566-3\n10.1111/jth.14888\n10.1055/s-0040-1710317\n10.1111/jth.14867\n10.1056/NEJMc2013656\n10.1056/NEJMc2007575\n10.1007/s11883-020-00867-3\n10.2478/jce-2019-0005\n10.1016/j.thromres.2020.04.013\n10.1007/s11239-020-02176-7\n10.3174/ajnr.A6752\n10.3174/ajnr.A6674\n10.1016/j.trsl.2020.04.007\n10.1016/j.thromres.2020.04.024\n10.7326/M20-2003\n10.1056/NEJMc2009020\n10.1161/CIRCULATIONAHA.120.047525\n10.1161/CIRCULATIONAHA.120.049294\n10.1016/j.jccase.2020.09.012\n10.1002/ccd.28992\n10.1016/j.carrev.2020.07.032\n10.1002/ccr3.3112\n10.1001/jamanetworkopen.2020.14780\n10.1007/s42399-020-00557-w\n10.1093/eurheartj/ehaa409\n10.1056/NEJMc2010418\n10.1093/eurheartj/ehaa314\n10.1016/j.ahj.2020.05.009\n10.1016/j.jacc.2020.04.011\n10.1093/ehjci/jeaa136\n10.1186/s12968-020-00628-w\n10.1093/ehjci/jeaa072\n10.1186/s12968-020-00656-6\n10.1186/s12968-016-0308-4\n10.1016/j.jcmg.2020.06.003\n10.1093/ehjci/jeaa169\n10.1161/CIRCIMAGING.120.010897\n10.1016/j.healun.2020.04.025\n10.1016/j.recesp.2020.06.032\n10.1001/jamacardio.2020.4916\n10.1161/CIRCULATIONAHA.120.049252\n10.1016/j.jcmg.2020.05.004\n10.1016/j.jcmg.2020.08.012\n10.1093/eurheartj/ehab075\n10.1001/jamacardio.2020.3557\n10.1186/s12968-021-00710-x\n10.1097/RTI.0000000000000574\n10.1126/scitranslmed.aal2658\n10.1016/j.atherosclerosis.2021.03.041\n10.1016/j.jcct.2021.03.003\n10.1007/s00330-020-07622-x\n10.1016/j.jcmg.2020.06.001\n10.1016/j.jcmg.2020.05.017\n10.1016/j.tcm.2020.10.001\n10.1016/j.jcmg.2020.04.012\n10.1016/j.jacc.2020.06.080\n10.21037/cdt.2019.09.03\n10.1038/s41598-020-72685-1\n10.1161/JAHA.119.012788\n10.2196/21476\n10.1016/j.dsx.2020.04.012\n10.1007/s00330-020-07042-x\n10.1016/j.compbiomed.2020.103792\n10.3346/jkms.2021.36.e46\n10.1016/j.compbiomed.2020.103960"}
{"title": "Deep Learning-Based Stroke Disease Prediction System Using Real-Time Bio Signals.", "abstract": "The emergence of an aging society is inevitable due to the continued increases in life expectancy and decreases in birth rate. These social changes require new smart healthcare services for use in daily life, and COVID-19 has also led to a contactless trend necessitating more non-face-to-face health services. Due to the improvements that have been achieved in healthcare technologies, an increasing number of studies have attempted to predict and analyze certain diseases in advance. Research on stroke diseases is actively underway, particularly with the aging population. Stroke, which is fatal to the elderly, is a disease that requires continuous medical observation and monitoring, as its recurrence rate and mortality rate are very high. Most studies examining stroke disease to date have used MRI or CT images for simple classification. This clinical approach (imaging) is expensive and time-consuming while requiring bulky equipment. Recently, there has been increasing interest in using non-invasive measurable EEGs to compensate for these shortcomings. However, the prediction algorithms and processing procedures are both time-consuming because the raw data needs to be separated before the specific attributes can be obtained. Therefore, in this paper, we propose a new methodology that allows for the immediate application of deep learning models on raw EEG data without using the frequency properties of EEG. This proposed deep learning-based stroke disease prediction model was developed and trained with data collected from real-time EEG sensors. We implemented and compared different deep-learning models (LSTM, Bidirectional LSTM, CNN-LSTM, and CNN-Bidirectional LSTM) that are specialized in time series data classification and prediction. The experimental results confirmed that the raw EEG data, when wielded by the CNN-bidirectional LSTM model, can predict stroke with 94.0% accuracy with low FPR (6.0%) and FNR (5.7%), thus showing high confidence in our system. These experimental results demonstrate the feasibility of non-invasive methods that can easily measure brain waves alone to predict and monitor stroke diseases in real time during daily life. These findings are expected to lead to significant improvements for early stroke detection with reduced cost and discomfort compared to other measuring techniques.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-07-03", "authors": ["Yoon-AChoi", "Se-JinPark", "Jong-ArmJun", "Cheol-SigPyo", "Kang-HeeCho", "Han-SungLee", "Jae-HakYu"], "doi": "10.3390/s21134269\n10.1016/S0140-6736(13)61953-4\n10.1016/j.jacc.2020.11.010\n10.1161/01.STR.22.2.155\n10.1161/STROKEAHA.120.030685\n10.1161/STROKEAHA.120.030642\n10.5124/jkma.2002.45.12.1432\n10.4218/etrij.2018-0118\n10.1016/j.cmpb.2018.04.012\n10.1016/j.jneumeth.2019.03.017\n10.3390/s18051383\n10.1016/j.neunet.2019.11.023\n10.1006/cbmr.1998.1475\n10.1186/s12911-019-0998-2\n10.1088/1742-6596/820/1/012005\n10.11591/eei.v9i5.2005\n10.1080/1086508X.2005.11079517\n10.1016/j.clinph.2015.07.014\n10.1007/978-3-319-49557-6_9\n10.1589/jpts.26.215\n10.1016/j.bspc.2020.102178\n10.1145/3065386\n10.1007/s00521-019-04096-x\n10.1016/j.comnet.2019.01.019\n10.5120/3371-4657\n10.1109/72.159058\n10.4249/scholarpedia.1883\n10.1080/01431160412331269698\n10.1109/5254.708428\n10.3390/app11041761\n10.3390/app10196791\n10.1007/s11042-020-10043-z\n10.1109/ACCESS.2020.3011185\n10.1109/45.329294\n10.1109/TNSRE.2020.2981659\n10.1109/72.165591\n10.1162/neco.1997.9.8.1735\n10.1109/78.650093"}
{"title": "Risk Stratification for ECMO Requirement in COVID-19 ICU Patients Using Quantitative Imaging Features in CT Scans on Admission.", "abstract": "(1) Background: Extracorporeal membrane oxygenation (ECMO) therapy in intensive care units (ICUs) remains the last treatment option for Coronavirus disease 2019 (COVID-19) patients with severely affected lungs but is highly resource demanding. Early risk stratification for the need of ECMO therapy upon admission to the hospital using artificial intelligence (AI)-based computed tomography (CT) assessment and clinical scores is beneficial for patient assessment and resource management; (2) Methods: Retrospective single-center study with 95 confirmed COVID-19 patients admitted to the participating ICUs. Patients requiring ECMO therapy (", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-07-03", "authors": ["EvaGresser", "JakobReich", "Bastian OSabel", "Wolfgang GKunz", "Matthias PFabritius", "JohannesR\u00fcbenthaler", "MichaelIngrisch", "DietmarWassilowsky", "MichaelIrlbeck", "JensRicke", "DanielPuhr-Westerheide"], "doi": "10.3390/diagnostics11061029\n10.15585/mmwr.mm7014e1\n10.1016/S2213-2600(20)30316-7\n10.1371/journal.pone.0235653\n10.1186/s13054-020-02957-9\n10.1093/cid/ciaa576\n10.1001/jama.2020.5394\n10.1016/j.chest.2020.10.014\n10.1111/anae.15201\n10.1001/jama.2011.1471\n10.1001/jama.2019.9302\n10.1016/S2213-2600(20)30079-5\n10.1016/j.jcrc.2020.03.011\n10.1136/bmjresp-2019-000420\n10.1056/NEJMoa1214103\n10.1097/MAT.0000000000001422\n10.1016/S2213-2600(20)30328-3\n10.1007/s00134-020-06284-z\n10.1186/s13054-021-03486-9\n10.1016/S0140-6736(20)32008-0\n10.1053/j.jvca.2021.01.027\n10.1007/s00134-020-06272-3\n10.1097/CCM.0000000000003444\n10.1007/s00134-020-06331-9\n10.14814/phy2.14715\n10.1148/radiol.2020201874\n10.1016/j.metabol.2020.154378\n10.1186/s13104-015-1678-7\n10.1016/S0140-6736(09)61069-2\n10.1016/j.chest.2020.04.003\n10.1038/s41467-020-18786-x\n10.1007/s00330-020-07013-2\n10.1259/bjro.20200016\n10.3390/diagnostics10121108\n10.1038/s41591-019-0447-x\n10.1016/S2589-7500(19)30159-1\n10.1016/j.ejrad.2019.108748\n10.1148/radiol.2020200905\n10.7150/thno.45985\n10.3390/membranes11030170"}
{"title": "Machine Learning to Predict In-Hospital Mortality in COVID-19 Patients Using Computed Tomography-Derived Pulmonary and Vascular Features.", "abstract": "Pulmonary parenchymal and vascular damage are frequently reported in COVID-19 patients and can be assessed with unenhanced chest computed tomography (CT), widely used as a triaging exam. Integrating clinical data, chest CT features, and CT-derived vascular metrics, we aimed to build a predictive model of in-hospital mortality using univariate analysis (Mann-Whitney ", "journal": "Journal of personalized medicine", "date": "2021-07-03", "authors": ["SimoneSchiaffino", "MarinaCodari", "AndreaCozzi", "DomenicoAlbano", "MarcoAl\u00ec", "RobertoArioli", "EmanueleAvola", "ClaudioBn\u00e0", "MaurizioCariati", "SerenaCarriero", "MassimoCressoni", "Pietro S CDanna", "GianmarcoDella Pepa", "GiovanniDi Leo", "FrancescoDolci", "ZenoFalaschi", "NicolaFlor", "Riccardo AFo\u00e0", "SalvatoreGitto", "GiovanniLeati", "VeronicaMagni", "Alexis EMalavazos", "GiovanniMauri", "CarmeloMessina", "LorenzoMonfardini", "AlessioPasch\u00e8", "FilippoPesapane", "Luca MSconfienza", "FrancescoSecchi", "EdoardoSegalini", "AngeloSpinazzola", "ValeriaTombini", "SilviaTresoldi", "AngeloVanzulli", "IlariaVicentin", "DomenicoZagaria", "DominikFleischmann", "FrancescoSardanelli"], "doi": "10.3390/jpm11060501\n10.1148/radiol.2020203173\n10.1148/radiol.2020201365\n10.1097/RTI.0000000000000516\n10.1111/jth.14768\n10.1016/j.jacc.2020.08.041\n10.1111/bjh.16749\n10.1093/eurheartj/ehaa500\n10.1161/CIRCULATIONAHA.120.048925\n10.1038/s41569-020-00469-1\n10.1038/s41577-020-0343-0\n10.1161/CIRCULATIONAHA.120.050354\n10.1016/S0140-6736(20)30937-5\n10.1056/NEJMoa2015432\n10.7326/M20-2566\n10.1016/S1473-3099(20)30434-5\n10.1148/radiol.2020201561\n10.1148/radiol.2020201544\n10.1259/bjr.20200407\n10.1097/MD.0000000000024002\n10.1053/j.jvca.2021.01.011\n10.1148/radiol.2020200230\n10.1148/radiol.2020200463\n10.21037/qims-20-708\n10.1016/j.acra.2020.07.019\n10.1007/s00330-020-07622-x\n10.21037/qims-20-546\n10.1007/s11604-020-01085-2\n10.1016/j.acra.2020.09.012\n10.1016/j.jcmg.2020.06.004\n10.1016/j.jcmg.2020.04.014\n10.1016/j.jcmg.2020.05.032\n10.1097/RTI.0000000000000530\n10.7326/M20-2003\n10.1177/2045894020924566\n10.1016/j.acra.2020.04.023\n10.1016/j.clinimag.2014.02.004\n10.1097/MD.0000000000000256\n10.1111/resp.13066\n10.1183/13993003.02168-2016\n10.1378/chest.13-1422\n10.1148/radiol.2020203108\n10.1016/j.jcct.2021.03.004\n10.1016/S1473-3099(20)30521-1\n10.1056/NEJMoa1203830\n10.1111/j.2517-6161.1996.tb02080.x\n10.1016/0895-4356(95)00048-8\n10.1016/S0895-4356(96)00236-3\n10.1148/ryct.2021200596\n10.3390/jpm11050343\n10.1038/s42256-021-00307-0\n10.1148/ryai.2021210011\n10.1016/j.media.2021.102046\n10.1016/j.ejrad.2020.109017\n10.1097/RLI.0000000000000674\n10.1148/radiol.2021204141\n10.1016/j.orcp.2020.12.002\n10.1001/jama.2020.4031"}
{"title": "Estimation of Respiratory Rate from Thermography Using Respiratory Likelihood Index.", "abstract": "Respiration is a key vital sign used to monitor human health status. Monitoring respiratory rate (RR) under non-contact is particularly important for providing appropriate pre-hospital care in emergencies. We propose an RR estimation system using thermal imaging cameras, which are increasingly being used in the medical field, such as recently during the COVID-19 pandemic. By measuring temperature changes during exhalation and inhalation, we aim to track the respiration of the subject in a supine or seated position in real-time without any physical contact. The proposed method automatically selects the respiration-related regions from the detected facial regions and estimates the respiration rate. Most existing methods rely on signals from nostrils and require close-up or high-resolution images, while our method only requires the facial region to be captured. Facial region is detected using YOLO v3, an object detection model based on deep learning. The detected facial region is divided into subregions. By calculating the respiratory likelihood of each segmented region using the newly proposed index, called the Respiratory Quality Index, the respiratory region is automatically selected and the RR is estimated. An evaluation of the proposed RR estimation method was conducted on seven subjects in their early twenties, with four 15 s measurements being taken. The results showed a mean absolute error of 0.66 bpm. The proposed method can be useful as an RR estimation method.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-07-03", "authors": ["YudaiTakahashi", "YiGu", "TakaakiNakada", "RyuzoAbe", "ToshiyaNakaguchi"], "doi": "10.3390/s21134406\n10.12968/bjon.2012.21.10.621\n10.1007/BF02600071\n10.1093/bja/aeh113\n10.1016/S0300-9572(02)00100-4\n10.1111/j.1365-2044.2005.04186.x\n10.1046/j.1365-2044.2003.03258.x\n10.1016/j.resuscitation.2006.08.020\n10.1016/j.resuscitation.2004.11.017\n10.1109/JSEN.2017.2695565\n10.1007/s10877-013-9486-x\n10.1016/j.infrared.2012.07.001\n10.1016/j.annemergmed.2004.06.016\n10.1067/mem.2002.122017\n10.1007/BF02348078\n10.1063/1.4845635\n10.3390/s21103456\n10.3390/app10020607\n10.1364/BOE.5.001075\n10.1186/1475-925X-10-93\n10.1364/BOE.6.004378\n10.1111/j.1469-8986.2010.01167.x\n10.1109/TPAMI.2016.2577031\n10.3390/s21041495"}
{"title": "Can Clinical Symptoms and Laboratory Results Predict CT Abnormality? Initial Findings Using Novel Machine Learning Techniques in Children With COVID-19 Infections.", "abstract": "The rapid spread of coronavirus 2019 disease (COVID-19) has manifested a global public health crisis, and chest CT has been proven to be a powerful tool for screening, triage, evaluation and prognosis in COVID-19 patients. However, CT is not only costly but also associated with an increased incidence of cancer, in particular for children. This study will question whether clinical symptoms and laboratory results can predict the CT outcomes for the pediatric patients with positive RT-PCR testing results in order to determine the necessity of CT for such a vulnerable group. Clinical data were collected from 244 consecutive pediatric patients (16 years of age and under) treated at Wuhan Children's Hospital with positive RT-PCR testing, and the chest CT were performed within 3 days of clinical data collection, from January 21 to March 8, 2020. This study was approved by the local ethics committee of Wuhan Children's Hospital. Advanced decision tree based machine learning models were developed for the prediction of CT outcomes. Results have shown that age, lymphocyte, neutrophils, ferritin and C-reactive protein are the most related clinical indicators for predicting CT outcomes for pediatric patients with positive RT-PCR testing. Our decision support system has managed to achieve an AUC of 0.84 with 0.82 accuracy and 0.84 sensitivity for predicting CT outcomes. Our model can effectively predict CT outcomes, and our findings have indicated that the use of CT should be reconsidered for pediatric patients, as it may not be indispensable.", "journal": "Frontiers in medicine", "date": "2021-07-02", "authors": ["HuijingMa", "QinghaoYe", "WeipingDing", "YinghuiJiang", "MinhaoWang", "ZhangmingNiu", "XiZhou", "YuanGao", "ChengjiaWang", "WadeMenpes-Smith", "Evandro FeiFang", "JianboShao", "JunXia", "GuangYang"], "doi": "10.3389/fmed.2021.699984\n10.1109/ACCESS.2020.3005510\n10.2214/AJR.20.22954\n10.1007/s00330-020-07022-1\n10.1177/0846537120913033\n10.1038/s42256-021-00307-0\n10.1007/s00330-020-06817-6\n10.1183/13993003.01241-2020\n10.1007/s00247-020-04726-w\n10.1148/ryai.2021210011\n10.1001/jamanetworkopen.2019.10584\n10.1016/S1473-3099(20)30198-5\n10.1016/S1473-3099(20)30287-5\n10.1056/NEJMc2005073\n10.1016/j.crad.2020.04.010\n10.1007/s00431-020-03684-7\n10.1007/s41999-020-00356-5\n10.1186/s12916-020-01596-9\n10.3389/fped.2020.00386\n10.1109/TPAMI.2018.2858826\n10.24963/ijcai.2017/239\n10.1016/j.cmi.2020.06.015\n10.21037/atm-20-3192\n10.3346/jkms.2020.35.e236\n10.1002/jmv.25871\n10.1016/j.medmal.2020.03.007\n10.1016/S2213-2600(20)30076-X\n10.1101/2020.04.11.20062349\n10.6061/clinics/2015(02)05\n10.1007/s42058-020-00031-5\n10.1016/j.inffus.2019.12.012\n10.1016/j.ejrad.2020.108961"}
{"title": "Artificial Neural Network-Based Deep Learning Model for COVID-19 Patient Detection Using X-Ray Chest Images.", "abstract": "The world is experiencing an unprecedented crisis due to the coronavirus disease (COVID-19) outbreak that has affected nearly 216 countries and territories across the globe. Since the pandemic outbreak, there is a growing interest in computational model-based diagnostic technologies to support the screening and diagnosis of COVID-19 cases using medical imaging such as chest X-ray (CXR) scans. It is discovered in initial studies that patients infected with COVID-19 show abnormalities in their CXR images that represent specific radiological patterns. Still, detection of these patterns is challenging and time-consuming even for skilled radiologists. In this study, we propose a novel convolutional neural network- (CNN-) based deep learning fusion framework using the transfer learning concept where parameters (weights) from different models are combined into a single model to extract features from images which are then fed to a custom classifier for prediction. We use gradient-weighted class activation mapping to visualize the infected areas of CXR images. Furthermore, we provide feature representation through visualization to gain a deeper understanding of the class separability of the studied models with respect to COVID-19 detection. Cross-validation studies are used to assess the performance of the proposed models using open-access datasets containing healthy and both COVID-19 and other pneumonia infected CXR images. Evaluation results show that the best performing fusion model can attain a classification accuracy of 95.49% with a high level of sensitivity and specificity.", "journal": "Journal of healthcare engineering", "date": "2021-07-02", "authors": ["MohammadShorfuzzaman", "MehediMasud", "HeshamAlhumyani", "DivyaAnand", "AmanSingh"], "doi": "10.1155/2021/5513679\n10.1038/s41564-020-0695-z\n10.1016/j.jare.2020.03.005\n10.1148/radiol.2020200527\n10.1145/3241056\n10.1016/j.future.2019.06.027\n10.1016/j.suscom.2019.07.003\n10.3390/app10020559\n10.1137/0330046\n10.1109/tmi.2020.2993291\n10.1016/j.compbiomed.2020.103792\n10.1007/s10489-020-01902-1\n10.3389/fmed.2020.00427\n10.1155/2020/8889023\n10.1109/access.2020.3025010\n10.3390/ai1030027\n10.1109/access.2020.2994762\n10.1016/j.cmpb.2020.105532\n10.1016/j.imu.2020.100412\n10.1007/s12652-020-02669-6\n10.1007/s10489-020-02149-6\n10.1007/s13246-020-00888-x\n10.32604/cmc.2020.011326\n10.1109/ACCESS.2021.3050852\n10.1007/s00500-020-05275-y\n10.1109/mnet.011.2000458\n10.1016/j.patcog.2020.107700\n10.1186/s12890-020-01286-5\n10.1007/s11263-019-01228-7\n10.1049/trit.2019.0034\n10.1016/j.healun.2020.03.012"}
{"title": "An Overview of Deep Learning Techniques on Chest X-Ray and CT Scan Identification of COVID-19.", "abstract": "Pneumonia is an infamous life-threatening lung bacterial or viral infection. The latest viral infection endangering the lives of many people worldwide is the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), which causes COVID-19. This paper is aimed at detecting and differentiating viral pneumonia and COVID-19 disease using digital X-ray images. The current practices include tedious conventional processes that solely rely on the radiologist or medical consultant's technical expertise that are limited, time-consuming, inefficient, and outdated. The implementation is easily prone to human errors of being misdiagnosed. The development of deep learning and technology improvement allows medical scientists and researchers to venture into various neural networks and algorithms to develop applications, tools, and instruments that can further support medical radiologists. This paper presents an overview of deep learning techniques made in the chest radiography on COVID-19 and pneumonia cases.", "journal": "Computational and mathematical methods in medicine", "date": "2021-07-02", "authors": ["Woan ChingSerena Low", "Joon HuangChuah", "Clarence Augustine T HTee", "ShaziaAnis", "Muhammad AliShoaib", "AmirFaisal", "AziraKhalil", "Khin WeeLai"], "doi": "10.1155/2021/5528144\n10.3390/app10093233\n10.3390/app10020559\n10.3390/diagnostics10060358\n10.1148/radiol.2020201365\n10.1148/radiol.2020200823\n10.1007/s10489-020-01829-7\n10.1016/j.cell.2018.02.010\n10.3390/sym12040651\n10.1016/j.compag.2019.05.019\n10.1038/s41576-019-0122-6\n10.1016/j.tibtech.2018.08.005\n10.1016/j.zemedi.2018.11.002\n10.1016/j.zemedi.2018.12.003\n10.1109/access.2019.2912200\n10.1145/3150226\n10.1097/QCO.0000000000000089\n10.1097/01.rti.0000213581.14225.f1\n10.1148/radiol.2020200230\n10.1016/S0140-6736(20)30154-9\n10.1148/radiol.2020200257\n10.2214/AJR.20.22969\n10.1109/access.2020.3010287\n10.1001/jama.2020.3786\n10.1007/s11356-020-10133-3\n10.1109/cvpr.2017.369\n10.1007/s00330-021-07715-1\n10.2139/ssrn.3557984\n10.1101/2020.03.20.20039834\n10.1101/2020.03.19.20039354\n10.1007/s10044-021-00984-y\n10.1038/s41598-020-76550-z\n10.1117/12.2588672\n10.1007/s13755-021-00146-8\n10.36227/techrxiv.12464402\n10.1183/13993003.00775-2020\n10.1109/TCBB.2021.3065361\n10.1007/s13246-020-00865-4\n10.1016/j.asoc.2020.106580\n10.1016/j.asoc.2020.106912\n10.1109/tmi.2020.2993291\n10.1080/07391102.2020.1788642\n10.1016/j.irbm.2020.07.001\n10.1016/j.compbiomed.2020.103792\n10.1016/j.asoc.2020.106859\n10.1145/3411408.3411416\n10.1016/j.patrec.2020.09.010\n10.1016/j.media.2020.101794\n10.1016/j.imu.2020.100412\n10.3390/sym12091530\n10.1016/j.ijmedinf.2020.104284\n10.1007/s11548-020-02286-w\n10.1101/2020.08.14.20170290\n10.32604/cmc.2021.012874\n10.32604/cmc.2021.012955\n10.1109/access.2020.2995597\n10.1007/s00500-020-05424-3\n10.1007/s10489-020-01888-w\n10.1016/j.compbiomed.2020.103795\n10.1016/j.cmpb.2020.105532\n10.1016/j.cmpb.2020.105608\n10.1109/tmi.2020.2996645\n10.1109/embc.2019.8857516\n10.1109/CVPR.2009.5206848\n10.1109/isbi.2018.8363762"}
{"title": "Artificial Intelligence and COVID-19: Deep Learning Approaches for Diagnosis and Treatment.", "abstract": "COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19's spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-07-01", "authors": ["Mohammad BehdadJamshidi", "AliLalbakhsh", "JakubTalla", "ZdenekPeroutka", "FarimahHadjilooei", "PedramLalbakhsh", "MortezaJamshidi", "Luigi LaSpada", "MirhamedMirmozafari", "MojganDehghani", "AsalSabet", "SaeedRoshani", "SobhanRoshani", "NimaBayat-Makou", "BahareMohamadzade", "ZahraMalek", "AlirezaJamshidi", "SarahKiani", "HamedHashemi-Dezaki", "WahabMohyuddin"], "doi": "10.1109/ACCESS.2020.3001973\n10.1001/jamacardio.2020.1017\n10.1001/jamacardio.2020.1286\n10.1016/j.jrid.2020.03.006\n10.1001/jamacardio.2020.1096\n10.1111/codi.15138"}
{"title": "Coronavirus disease (COVID-19) detection using X-ray images and enhanced DenseNet.", "abstract": "The 2019 novel coronavirus (COVID-19) originating from China, has spread rapidly among people living in other countries. According to the World Health Organization (WHO), by the end of January, more than 104 million people have been affected by COVID-19, including more than 2 million deaths. The number of COVID-19 test kits available in hospitals is reduced due to the increase in regular cases. Therefore, an automatic detection system should be introduced as a fast, alternative diagnostic to prevent COVID-19 from spreading among humans. For this purpose, three different BiT models: DenseNet, InceptionV3, and Inception-ResNetV4 have been proposed in this analysis for the diagnosis of patients infected with coronavirus pneumonia using X-ray radiographs in the chest. These three models give and examine Receiver Operating Characteristic (ROC) analyses and uncertainty matrices, using 5-fold cross-validation. We have performed the simulations which have visualized that the pre-trained DenseNet model has the best classification efficiency with 92% among two other models proposed (83.47% accuracy for inception V3 and 85.57% accuracy for Inception-ResNetV4).", "journal": "Applied soft computing", "date": "2021-07-01", "authors": ["SalehAlbahli", "NasirAyub", "MuhammadShiraz"], "doi": "10.1016/j.asoc.2021.107645\n10.1136/bmj.m641\n10.2196/23693\n10.1148/radiol.2020200642\n10.1007/s00134-020-05990-y"}
{"title": "COVID-19 Diagnosis Using an Enhanced Inception-ResNetV2 Deep Learning Model in CXR Images.", "abstract": "The COVID-19 pandemic has a significant negative effect on people's health, as well as on the world's economy. Polymerase chain reaction (PCR) is one of the main tests used to detect COVID-19 infection. However, it is expensive, time-consuming, and lacks sufficient accuracy. In recent years, convolutional neural networks have grabbed many researchers' attention in the machine learning field, due to its high diagnosis accuracy, especially the medical image recognition. Many architectures such as Inception, ResNet, DenseNet, and VGG16 have been proposed and gained an excellent performance at a low computational cost. Moreover, in a way to accelerate the training of these traditional architectures, residual connections are combined with inception architecture. Therefore, many hybrid architectures such as Inception-ResNetV2 are further introduced. This paper proposes an enhanced Inception-ResNetV2 deep learning model that can diagnose chest X-ray (CXR) scans with high accuracy. Besides, a Grad-CAM algorithm is used to enhance the visualization of the infected regions of the lungs in CXR images. Compared with state-of-the-art methods, our proposed paper proves superiority in terms of accuracy, recall, precision, and ", "journal": "Journal of healthcare engineering", "date": "2021-07-01", "authors": ["MadallahAlruwaili", "AbdulazizShehab", "SamehAbd El-Ghany"], "doi": "10.1155/2021/6658058\n10.1093/clinchem/hvaa029\n10.1002/jmv.25674\n10.1148/radiol.2020201160\n10.18632/aging.103298\n10.1016/j.imu.2020.100378\n10.1016/j.chaos.2020.110170\n10.1038/s41598-020-76550-z\n10.1016/j.chaos.2020.110122\n10.1016/j.eng.2020.04.010\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110245\n10.1007/s11263-015-0816-y\n10.1016/j.ejrad.2004.03.010\n10.1183/09031936.01.00213501\n10.1016/j.compbiomed.2020.103792\n10.1007/s13246-020-00865-4\n10.1148/radiol.2020200905"}
{"title": "Segmentation and quantification of COVID-19 infections in CT using pulmonary vessels extraction and deep learning.", "abstract": "At the end of 2019, the World Health Organization (WHO) reported pneumonia that started in Wuhan, China, as a global emergency problem. Researchers quickly advanced in research to try to understand this COVID-19 and sough solutions for the front-line professionals fighting this fatal disease. One of the tools to aid in the detection, diagnosis, treatment, and prevention of this disease is computed tomography (CT). CT images provide valuable information on how this new disease affects the lungs of patients. However, the analysis of these images is not trivial, especially when researchers are searching for quick solutions. Detecting and evaluating this disease can be tiring, time-consuming, and susceptible to errors. Thus, in this study, we aim to automatically segment infections caused by COVID19 and provide quantitative measures of these infections to specialists, thus serving as a support tool. We use a database of real clinical cases from Pedro Ernesto University Hospital of the State of Rio de Janeiro, Brazil. The method involves five steps: lung segmentation, segmentation and extraction of pulmonary vessels, infection segmentation, infection classification, and infection quantification. For the lung segmentation and infection segmentation tasks, we propose modifications to the traditional U-Net, including batch normalization, leaky ReLU, dropout, and residual block techniques, and name it as Residual U-Net. The proposed method yields an average Dice value of 77.1% and an average specificity of 99.76%. For quantification of infectious findings, the proposed method achieves results like that of specialists, and no measure presented a value of ", "journal": "Multimedia tools and applications", "date": "2021-07-01", "authors": ["Jo\u00e3o O BDiniz", "Darlan B PQuintanilha", "Antonino CSantos Neto", "Giovanni L Fda Silva", "Jonnison LFerreira", "Stelmo M BNetto", "Jos\u00e9 D LAra\u00fajo", "Luana BDa Cruz", "Thamila F BSilva", "Caio Mda S Martins", "Marcos MFerreira", "Venicius GRego", "Jos\u00e9 M CBoaro", "Carolina L SCipriano", "Arist\u00f3fanes CSilva", "Anselmo Cde Paiva", "Geraldo BrazJunior", "Jo\u00e3o D Sde Almeida", "Rodolfo ANunes", "RobertoMogami", "MGattass"], "doi": "10.1007/s11042-021-11153-y\n10.1109/TMI.2005.844167\n10.1016/j.acra.2004.06.005\n10.1016/S0140-6736(20)30154-9\n10.1038/s41591-018-0177-5\n10.2307/1932409\n10.1016/j.cmpb.2018.01.007\n10.1016/j.cmpb.2018.04.011\n10.1016/j.cmpb.2019.01.005\n10.1186/1475-925X-13-41\n10.1109/34.232073\n10.1111/j.1469-8137.1912.tb05611.x\n10.3390/app9030404\n10.7150/thno.45985\n10.1109/TMI.2014.2337057\n10.1049/iet-ipr.2016.0526\n10.1023/A:1024099825458\n10.1016/j.compmedimag.2008.04.005\n10.1016/S1361-8415(98)80009-1\n10.1371/journal.pone.0251591\n10.1016/j.cmpb.2019.06.005\n10.1186/s12880-015-0068-x\n10.1038/s41592-019-0686-2\n10.1038/s41598-019-56847-4\n10.1001/jama.2020.2648\n10.7150/thno.46465\n10.1016/S0002-8703(98)70131-0\n10.1056/NEJMoa2001017"}
{"title": "COVID-19\u00a0after 18\u00a0months: Where do we stand?", "abstract": null, "journal": "Diagnostic and interventional imaging", "date": "2021-06-30", "authors": ["GuillaumeChassagnon", "LucileRegard", "PhilippeSoyer", "Marie-PierreRevel"], "doi": "10.1016/j.diii.2021.06.003\n10.1016/j.diii.2021.05.006"}
{"title": "Covid-19 imaging: A narrative review.", "abstract": "The 2019 novel coronavirus disease (COVID-19) imaging data is dispersed in numerous publications. A cohesive literature review is to be assembled.\nTo summarize the existing literature on Covid-19 pneumonia imaging including precautionary measures for radiology departments, Chest CT's role in diagnosis and management, imaging findings of Covid-19 patients including children and pregnant women, artificial intelligence applications and practical recommendations.\nA systematic literature search of PubMed/med line electronic databases.\nThe radiology department's staff is on the front line of the novel coronavirus outbreak. Strict adherence to precautionary measures is the main defense against infection's spread. Although nucleic acid testing is Covid-19's pneumonia diagnosis gold standard; kits shortage and low sensitivity led to the implementation of the highly sensitive chest computed tomography amidst initial diagnostic tools. Initial Covid-19 CT features comprise bilateral, peripheral or posterior, multilobar ground-glass opacities, predominantly in the lower lobes. Consolidations superimposed on ground-glass opacifications are found in few cases, preponderantly in the elderly. In later disease stages, GGO transformation into multifocal consolidations, thickened interlobular and intralobular lines, crazy paving, traction bronchiectasis, pleural thickening, and subpleural bands are reported. Standardized CT reporting is recommended to guide radiologists. While lung ultrasound, pulmonary MRI, and PET CT are not Covid-19 pneumonia's first-line investigative diagnostic modalities, their characteristic findings and clinical value are outlined. Artificial intelligence's role in strengthening available imaging tools is discussed.\nThis review offers an exhaustive analysis of the current literature on imaging role and findings in COVID-19 pneumonia.", "journal": "Annals of medicine and surgery (2012)", "date": "2021-06-29", "authors": ["HanaeRamdani", "NazikAllali", "LatifaChat", "SihamEl Haddad"], "doi": "10.1016/j.amsu.2021.102489"}
{"title": "Dense GAN and multi-layer attention based lesion segmentation method for COVID-19 CT images.", "abstract": "As the COVID-19 virus spreads around the world, testing and screening of patients have become a headache for governments. With the accumulation of clinical diagnostic data, the imaging big data features of COVID-19 are gradually clear, and CT imaging diagnosis results become more important. To obtain clear lesion information from the CT images of patients' lungs is helpful for doctors to adopt effective medical methods, and at the same time, is helpful to screen the patients with real infection. Deep learning image segmentation is widely used in the field of medical image segmentation. However, there are some challenges in using deep learning to segment the lung lesions of COVID-19 patients. Since image segmentation requires the labeling of lesion information on a pixel by pixel basis, most professional radiologists need to screen and diagnose patients on the front line, and they do not have enough energy to label a large amount of image data. In this paper, an improved Dense GAN to expand data set is developed, and a multi-layer attention mechanism method, combined with U-Net's COVID-19 pulmonary CT image segmentation, is proposed. The experimental results showed that the segmentation method proposed in this paper improved the segmentation accuracy of COVID-19 pulmonary medical CT image by comparing with other image segmentation methods.", "journal": "Biomedical signal processing and control", "date": "2021-06-29", "authors": ["JuZhang", "LundunYu", "DechengChen", "WeidongPan", "ChaoShi", "YanNiu", "XinweiYao", "XiaobinXu", "YunCheng"], "doi": "10.1016/j.bspc.2021.102901\n10.29322/ijsrp.10.06.2020.p10284"}
{"title": "Medical imaging and computational image analysis in COVID-19 diagnosis: A review.", "abstract": "Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. The disease presents with symptoms such as shortness of breath, fever, dry cough, and chronic fatigue, amongst others. The disease may be asymptomatic in some patients in the early stages, which can lead to increased transmission of the disease to others. This study attempts to review papers on the role of imaging and medical image computing in COVID-19 diagnosis. For this purpose, PubMed, Scopus and Google Scholar were searched to find related studies until the middle of 2021. The contribution of this study is four-fold: 1) to use as a tutorial of the field for both clinicians and technologists, 2) to comprehensively review the characteristics of COVID-19 as presented in medical images, 3) to examine automated artificial intelligence-based approaches for COVID-19 diagnosis, 4) to express the research limitations in this field and the methods used to overcome them. Using machine learning-based methods can diagnose the disease with high accuracy from medical images and reduce time, cost and error of diagnostic procedure. It is recommended to collect bulk imaging data from patients in the shortest possible time to improve the performance of COVID-19 automated diagnostic methods.", "journal": "Computers in biology and medicine", "date": "2021-06-28", "authors": ["ShahabedinNabavi", "AzarEjmalian", "Mohsen EbrahimiMoghaddam", "Ahmad AliAbin", "Alejandro FFrangi", "MohammadMohammadi", "Hamidreza SalighehRad"], "doi": "10.1016/j.compbiomed.2021.104605\n10.1016/j.crad.2020.03.003\n10.1016/S1473-3099(20)30241-3\n10.2214/AJR.20.23202\n10.1016/S2213-2600(20)30132-6\n10.1007/s00330-020-06860-3\n10.1007/s42058-020-00031-5\n10.1080/22221751.2020.1750307\n10.1016/j.ajem.2020.04.016\n10.1177/0846537120916419\n10.1016/j.chest.2020.04.003\n10.1016/j.hrtlng.2020.03.007\n10.2174/1573405616999200320163751\n10.1007/s00330-020-06827-4\n10.1007/s00330-020-06801-0\n10.1002/jmv.25910\n10.1016/j.jrid.2020.04.001\n10.1016/j.diii.2020.03.014\n10.1007/s00330-020-06934-2\n10.1007/s00330-020-06975-7\n10.1007/s00330-020-06967-7\n10.1007/s00330-020-07018-x\n10.1016/j.jacr.2020.03.006\n10.2214/AJR.20.23034\n10.1016/j.jinf.2020.03.007\n10.1148/ryct.2020200047\n10.1186/s13054-020-02876-9\n10.1097/RLI.0000000000000672\n10.1016/j.ejrad.2020.108941\n10.1097/RLI.0000000000000689\n10.1016/j.ijid.2020.04.003\n10.1007/s11604-020-00956-y\n10.1097/RCT.0000000000001023\n10.1007/s00330-020-06731-x\n10.1097/RLI.0000000000000674\n10.1002/ijgo.13165\n10.1016/j.acra.2020.03.002\n10.1093/cid/ciaa271\n10.1016/j.crad.2020.03.004\n10.1007/s00330-020-06817-6\n10.1016/j.ejrad.2020.108956\n10.1016/j.jinf.2020.02.016\n10.1016/S1473-3099(20)30086-4\n10.1016/j.ijid.2020.03.040\n10.1007/s00247-020-04656-7\n10.1097/RTI.0000000000000513\n10.1016/j.clinimag.2020.04.001\n10.1148/ryct.2020200280\n10.1007/s11547-020-01272-1\n10.1183/13993003.04188-2020\n10.1007/s11547-020-01232-9\n10.1016/j.ejro.2020.100231\n10.1148/radiol.2020201160\n10.3348/kjr.2020.0132\n10.1055/a-1154-8795\n10.1016/j.advms.2020.06.005\n10.1007/s00134-020-05996-6\n10.4269/ajtmh.20-0280\n10.2214/AJR.20.23513\n10.1002/jum.15284\n10.1111/echo.14664\n10.1002/uog.22034\n10.1007/s00259-020-04767-1\n10.1148/radiol.2020200770\n10.1016/j.jtho.2020.03.022\n10.1007/s00259-020-04762-6\n10.1007/s00259-020-04820-z\n10.1097/RLU.0000000000003100\n10.1148/radiol.2020201187\n10.1016/j.thromres.2020.04.011\n10.3348/kjr.2020.0146\n10.1109/RBME.2020.2987975\n10.1007/s10489-020-01862-6\n10.1016/j.media.2017.07.005\n10.1007/s10044-021-00984-y\n10.1007/s00521-020-05437-x\n10.1007/s40846-020-00529-4\n10.1109/TMI.2020.2995965\n10.1109/access.2020.3005510\n10.1038/s41598-020-76550-z\n10.1109/TMI.2020.2993291\n10.1101/2020.04.13.20063479\n10.1007/s00330-020-07012-3\n10.1101/2020.05.09.20096560\n10.1371/journal.pone.0235187\n10.1007/s12559-021-09848-3\n10.1148/radiol.2020200642\n10.1007/s00330-020-07050-x\n10.1007/s00330-020-06916-4\n10.18632/aging.102999\n10.1148/radiol.2020201433\n10.1148/radiol.2020200823\n10.1007/s00330-020-06928-0\n10.1148/radiol.2020201237\n10.1007/s11604-020-00973-x\n10.1007/s00330-020-06829-2\n10.1186/s12967-020-02324-w\n10.3760/cma.j.cn112147-20200217-00106\n10.1007/s00330-020-06915-5\n10.1148/radiol.2020200463\n10.1007/s00330-020-06854-1\n10.7150/thno.45016\n10.2214/AJR.20.22961\n10.1007/s00330-020-06880-z\n10.1097/MD.0000000000019900\n10.1007/s00330-020-06879-6\n10.1148/radiol.2020200843\n10.1007/s00259-020-04735-9\n10.1007/s00330-020-06823-8\n10.1097/RLI.0000000000000670\n10.3785/j.issn.1008-9292.2020.03.05\n10.1007/s00330-020-06816-7\n10.2214/AJR.20.22975\n10.1007/s00330-020-06955-x\n10.1016/j.jinf.2020.04.004\n10.1007/s00330-020-06920-8\n10.2214/AJR.20.22954\n10.3785/j.issn.1008-9292.2020.02.03\n10.2214/AJR.20.23304\n10.1148/radiol.2020200432\n10.1016/j.jinf.2020.02.017\n10.1016/j.jinf.2020.03.020\n10.1183/13993003.00407-2020\n10.1016/j.ejrad.2020.108961\n10.1007/s00330-020-06978-4\n10.3760/cma.j.cn112137-20200203-00182\n10.2214/AJR.20.22959\n10.1007/s00330-020-07007-0\n10.1371/journal.pone.0230548\n10.2214/AJR.20.23240\n10.1007/s11604-020-00958-w\n10.1148/radiol.2020200230\n10.1148/radiol.2020200370\n10.3233/XST-200670\n10.36416/1806-3756/e20200121\n10.1016/j.jinf.2020.03.033\n10.1097/RTI.0000000000000508\n10.1148/radiol.2020200343\n10.1016/j.jtho.2020.04.007\n10.1017/ice.2020.171\n10.21037/atm.2020.02.91\n10.1016/j.jfma.2020.04.006\n10.1007/s00259-020-04795-x\n10.1016/j.jfma.2020.03.006\n10.1148/radiol.2020201183\n10.1055/a-1142-4094\n10.1007/s00330-020-06809-6\n10.1016/j.clinimag.2020.02.008\n10.1007/s00330-020-06748-2\n10.1007/s11604-020-00945-1\n10.1503/cmaj.200431\n10.1016/j.crad.2020.03.028\n10.1007/s10140-020-01775-4\n10.34172/aim.2020.11\n10.11622/smedj.2020066\n10.1016/j.acra.2020.03.028\n10.2214/AJR.20.23141\n10.1111/echo.14662\n10.2214/AJR.20.23064\n10.12182/20200360107\n10.2214/AJR.20.23035\n10.1016/j.acra.2020.03.003\n10.1016/j.jinf.2020.03.014\n10.1055/a-1138-8783\n10.3348/kjr.2020.0181\n10.1016/j.annonc.2020.03.001\n10.4274/balkanmedj.galenos.2020.2020.2.15\n10.3348/kjr.2020.0112\n10.1093/qjmed/hcaa038\n10.1148/radiol.2020200323\n10.1148/radiol.2020200269\n10.1148/radiol.2020200280\n10.1148/radiol.2020200241\n10.1016/j.radcr.2020.04.031\n10.1007/s11604-020-00967-9\n10.1016/j.tmaid.2020.101627\n10.1148/radiol.2020200236\n10.1007/s00330-020-06918-2\n10.1186/s12890-020-01286-5\n10.1007/s00330-020-07270-1\n10.1016/j.clinimag.2020.11.004\n10.1007/s13246-020-00865-4\n10.1007/s10489-020-01829-7\n10.1016/j.patrec.2020.09.010\n10.1109/access.2020.3010287\n10.1109/ACCESS.2020.3003810\n10.1007/s42600-021-00151-6\n10.3390/ijerph17186933\n10.1016/j.cmpb.2020.105581\n10.1007/s13246-020-00888-x\n10.1016/j.cmpb.2020.105532\n10.1148/radiol.2020201874\n10.1016/j.compbiomed.2020.103869\n10.1016/j.compbiomed.2021.104375\n10.1016/j.compbiomed.2020.104181\n10.1016/j.compbiomed.2021.104401\n10.1148/radiol.2020200905\n10.1148/ryct.2020200075\n10.1038/s41598-020-76282-0\n10.1109/TIP.2021.3058783\n10.1007/s00330-020-07044-9\n10.1016/j.compbiomed.2020.104037\n10.1117/12.2588672\n10.1016/j.compbiomed.2021.104348"}
{"title": "Transfer learning-based approach for detecting COVID-19 ailment in lung CT scan.", "abstract": "This research work aims to identify COVID-19 through deep learning models using lung CT-SCAN images. In order to enhance lung CT scan efficiency, a super-residual dense neural network was applied. The experimentation has been carried out using benchmark datasets like SARS-COV-2 CT-Scan and Covid-CT Scan. To mark COVID-19 as positive or negative for the improved CT scan, existing pre-trained models such as XceptionNet, MobileNet, InceptionV3, DenseNet, ResNet50, and VGG (Visual Geometry Group)16 have been used. Taking CT scans with super resolution using a residual dense neural network in the pre-processing step resulted in improving the accuracy, F1 score, precision, and recall of the proposed model. On the dataset Covid-CT Scan and SARS-COV-2 CT-Scan, the MobileNet model provided a precision of 94.12% and 100% respectively.", "journal": "Computers in biology and medicine", "date": "2021-06-22", "authors": ["VinayArora", "Eddie Yin-KweeNg", "Rohan SinghLeekha", "MedhaviDarshan", "ArshdeepSingh"], "doi": "10.1016/j.compbiomed.2021.104575\n10.1016/j.scitotenv.2020.138762\n10.1142/S2424862220500268\n10.1016/j.jrid.2020.05.001\n10.1016/j.chaos.2020.110190\n10.1007/s00330-020-07044-9\n10.3389/fbioe.2020.00898\n10.1155/2020/8843664\n10.2196/19569\n10.1007/s00259-021-05267-6\n10.4236/jbise.2020.137014\n10.1080/07391102.2020.1788642\n10.1007/s00521-020-05437-x\n10.1007/s10096-020-03901-z\n10.1148/radiol.2020200905\n10.1117/12.2576276\n10.1183/13993003.00775-2020\n10.1101/2020.04.13.20063941\n10.3233/shti200481\n10.3390/s21020455\n10.1016/j.imu.2020.100427\n10.7717/peerj-cs.306\n10.1007/978-3-030-00889-5_33\n10.1016/j.sysarc.2020.101830\n10.1016/j.asoc.2020.106885"}
{"title": "The application of artificial intelligence and data integration in COVID-19 studies: a scoping review.", "abstract": "To summarize how artificial intelligence (AI) is being applied in COVID-19 research and determine whether these AI applications integrated heterogenous data from different sources for modeling.\nWe searched 2 major COVID-19 literature databases, the National Institutes of Health's LitCovid and the World Health Organization's COVID-19 database on March 9, 2021. Following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guideline, 2 reviewers independently reviewed all the articles in 2 rounds of screening.\nIn the 794 studies included in the final qualitative analysis, we identified 7 key COVID-19 research areas in which AI was applied, including disease forecasting, medical imaging-based diagnosis and prognosis, early detection and prognosis (non-imaging), drug repurposing and early drug discovery, social media data analysis, genomic, transcriptomic, and proteomic data analysis, and other COVID-19 research topics. We also found that there was a lack of heterogenous data integration in these AI applications.\nRisk factors relevant to COVID-19 outcomes exist in heterogeneous data sources, including electronic health records, surveillance systems, sociodemographic datasets, and many more. However, most AI applications in COVID-19 research adopted a single-sourced approach that could omit important risk factors and thus lead to biased algorithms. Integrating heterogeneous data for modeling will help realize the full potential of AI algorithms, improve precision, and reduce bias.\nThere is a lack of data integration in the AI applications in COVID-19 research and a need for a multilevel AI framework that supports the analysis of heterogeneous data from different sources.", "journal": "Journal of the American Medical Informatics Association : JAMIA", "date": "2021-06-22", "authors": ["YiGuo", "YahanZhang", "TianchenLyu", "MattiaProsperi", "FeiWang", "HuaXu", "JiangBian"], "doi": "10.1093/jamia/ocab098"}
{"title": "Artificial Intelligence and Medical Internet of Things Framework for Diagnosis of Coronavirus Suspected Cases.", "abstract": "The world has been facing the COVID-19 pandemic since December 2019. Timely and efficient diagnosis of COVID-19 suspected patients plays a significant role in medical treatment. The deep transfer learning-based automated COVID-19 diagnosis on chest X-ray is required to counter the COVID-19 outbreak. This work proposes a real-time Internet of Things (IoT) framework for early diagnosis of suspected COVID-19 patients by using ensemble deep transfer learning. The proposed framework offers real-time communication and diagnosis of COVID-19 suspected cases. The proposed IoT framework ensembles four deep learning models such as InceptionResNetV2, ResNet152V2, VGG16, and DenseNet201. The medical sensors are utilized to obtain the chest X-ray modalities and diagnose the infection by using the deep ensemble model stored on the cloud server. The proposed deep ensemble model is compared with six well-known transfer learning models over the chest X-ray dataset. Comparative analysis revealed that the proposed model can help radiologists to efficiently and timely diagnose the COVID-19 suspected patients.", "journal": "Journal of healthcare engineering", "date": "2021-06-22", "authors": ["Ahmed IIskanderani", "Ibrahim MMehedi", "Abdulah JezaAljohani", "MohammadShorfuzzaman", "FarzanaAkther", "ThangamPalaniswamy", "Shaikh AbdulLatif", "AbdulLatif", "AftabAlam"], "doi": "10.1155/2021/3277988\n10.1109/mcom.2019.8647109\n10.1109/mcom.001.1900396\n10.1109/JIOT.2020.3034074\n10.1109/tmi.2020.2996256\n10.1109/jbhi.2020.3030853\n10.1109/access.2020.2994762\n10.1109/access.2020.3016780\n10.1109/access.2020.3010287\n10.1109/tmi.2020.2995508\n10.1109/access.2020.3025010\n10.1109/JIOT.2020.3013710\n10.1007/s12652-020-02669-6\n10.1142/s0218001421510046\n10.1007/s10489-020-02149-6\n10.1049/trit.2019.0028\n10.1049/trit.2019.0051\n10.1049/trit.2018.1006\n10.1109/tnnls.2019.2929059"}
{"title": "D2A U-Net: Automatic segmentation of COVID-19 CT slices based on dual attention and hybrid dilated convolution.", "abstract": "Coronavirus Disease 2019 (COVID-19) has become one of the most urgent public health events worldwide due to its high infectivity and mortality. Computed tomography (CT) is a significant screening tool for COVID-19 infection, and automatic segmentation of lung infection in COVID-19 CT images can assist diagnosis and health care of patients. However, accurate and automatic segmentation of COVID-19 lung infections is faced with a few challenges, including blurred edges of infection and relatively low sensitivity. To address the issues above, a novel dilated dual attention U-Net based on the dual attention strategy and hybrid dilated convolutions, namely D2A U-Net, is proposed for COVID-19 lesion segmentation in CT slices. In our D2A U-Net, the dual attention strategy composed of two attention modules is utilized to refine feature maps and reduce the semantic gap between different levels of feature maps. Moreover, the hybrid dilated convolutions are introduced to the model decoder to achieve larger receptive fields, which refines the decoding process. The proposed method is evaluated on an open-source dataset and achieves a Dice score of 0.7298 and recall score of 0.7071, which outperforms the popular cutting-edge methods in the semantic segmentation. The proposed network is expected to be a potential AI-based approach used for the diagnosis and prognosis of COVID-19 patients.", "journal": "Computers in biology and medicine", "date": "2021-06-20", "authors": ["XiangyuZhao", "PengZhang", "FanSong", "GuangdaFan", "YangyangSun", "YujiaWang", "ZheyuanTian", "LuqiZhang", "GuangleiZhang"], "doi": "10.1016/j.compbiomed.2021.104526"}
{"title": "RCoNet: Deformable Mutual Information Maximization and High-Order Uncertainty-Aware Learning for Robust COVID-19 Detection.", "abstract": "The novel 2019 Coronavirus (COVID-19) infection has spread worldwide and is currently a major healthcare challenge around the world. Chest computed tomography (CT) and X-ray images have been well recognized to be two effective techniques for clinical COVID-19 disease diagnoses. Due to faster imaging time and considerably lower cost than CT, detecting COVID-19 in chest X-ray (CXR) images is preferred for efficient diagnosis, assessment, and treatment. However, considering the similarity between COVID-19 and pneumonia, CXR samples with deep features distributed near category boundaries are easily misclassified by the hyperplanes learned from limited training data. Moreover, most existing approaches for COVID-19 detection focus on the accuracy of prediction and overlook uncertainty estimation, which is particularly important when dealing with noisy datasets. To alleviate these concerns, we propose a novel deep network named RCoNet ", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-06-19", "authors": ["ShunjieDong", "QianqianYang", "YuFu", "MeiTian", "ChengZhuo"], "doi": "10.1109/TNNLS.2021.3086570"}
{"title": "Automatic lung segmentation in COVID-19 patients: Impact on quantitative computed tomography analysis.", "abstract": "To assess the impact of lung segmentation accuracy in an automatic pipeline for quantitative analysis of CT images.\nFour different platforms for automatic lung segmentation based on convolutional neural network (CNN), region-growing technique and atlas-based algorithm were considered. The platforms were tested using CT images of 55 COVID-19 patients with severe lung impairment. Four radiologists assessed the segmentations using a 5-point qualitative score (QS). For each CT series, a manually revised reference segmentation (RS) was obtained. Histogram-based quantitative metrics (QM) were calculated from CT histogram using lung segmentationsfrom all platforms and RS. Dice index (DI) and differences of QMs (\u0394QMs) were calculated between RS and other segmentations.\nHighest QS and lower \u0394QMs values were associated to the CNN algorithm. However, only 45% CNN segmentations were judged to need no or only minimal corrections, and in only 17 cases (31%), automatic segmentations provided RS without manual corrections. Median values of the DI for the four algorithms ranged from 0.993 to 0.904. Significant differences for all QMs calculated between automatic segmentations and RS were found both when data were pooled together and stratified according to QS, indicating a relationship between qualitative and quantitative measurements. The most unstable QM was the histogram 90th percentile, with median \u0394QMs values ranging from 10HU and 158HU between different algorithms.\nNone of tested algorithms provided fully reliable segmentation. Segmentation accuracy impacts differently on different quantitative metrics, and each of them should be individually evaluated according to the purpose of subsequent analyses.", "journal": "Physica medica : PM : an international journal devoted to the applications of physics to medicine and biology : official journal of the Italian Association of Biomedical Physics (AIFB)", "date": "2021-06-18", "authors": ["LBerta", "FRizzetto", "CDe Mattia", "DLizio", "MFelisi", "P EColombo", "SCarrazza", "SGelmini", "LBianchi", "DArtioli", "FTravaglini", "AVanzulli", "ATorresin", "NoneNone"], "doi": "10.1016/j.ejmp.2021.06.001\n10.1111/joim.13089\n10.1093/cid/ciaa576\n10.1001/jama.2012.5669\n10.1007/s00134-020-06281-2\n10.1016/S2213-2600(20)30370-2\n10.1007/s00134-020-06033-2\n10.1148/radiol.2020200230\n10.1016/j.chest.2020.04.003\n10.1097/00005382-198607000-00005\n10.1148/radiol.2373041515\n10.1016/j.ejrad.2019.108748\n10.1007/s00330-020-07013-2\n10.1148/radiol.2020201433\n10.1186/s12931-017-0527-8\n10.1016/j.ejmp.2021.01.004\n10.1007/s00330-020-07012-3\n10.1016/j.jpha.2020.03.004\n10.1016/j.compbiomed.2020.103795\n10.1148/ryct.2020200075\n10.1148/radiol.2020202439\n10.21037/jtd.2017.08.17\n10.1385/NI:4:3:263\n10.1016/j.semradonc.2019.02.001\n10.1109/TMI.2020.2995108\n10.1016/j.ejmp.2019.12.001\n10.1186/s41747-020-00173-2\n10.1016/j.compmedimag.2014.10.008\n10.1148/rg.2015140232\n10.1002/mp.14424\n10.1186/s41747-020-00189-8\n10.1038/s41598-020-69534-6\n10.1080/0284186X.2018.1445283\n10.1371/journal.pone.0151498\n10.1186/1748-717X-6-110\n10.2307/1932409\n10.1186/s12880-015-0068-x\n10.1080/03610918.2018.1490428\n10.1080/00031305.2016.1141708\n10.1007/s10826-019-01536-z\n10.1007/s00330-020-06915-5\n10.1007/s00330-020-07033-y"}
{"title": "Online Interactive Platform for COVID-19 Literature Visual Analytics: Platform Development Study.", "abstract": "Papers on COVID-19 are being published at a high rate and concern many different topics. Innovative tools are needed to aid researchers to find patterns in this vast amount of literature to identify subsets of interest in an automated fashion.\nWe present a new online software resource with a friendly user interface that allows users to query and interact with visual representations of relationships between publications.\nWe publicly released an application called PLATIPUS (Publication Literature Analysis and Text Interaction Platform for User Studies) that allows researchers to interact with literature supplied by COVIDScholar via a visual analytics platform. This tool contains standard filtering capabilities based on authors, journals, high-level categories, and various research-specific details via natural language processing and dozens of customizable visualizations that dynamically update from a researcher's query.\nPLATIPUS is available online and currently links to over 100,000 publications and is still growing. This application has the potential to transform how COVID-19 researchers use public literature to enable their research.\nThe PLATIPUS application provides the end user with a variety of ways to search, filter, and visualize over 100,00 COVID-19 publications.", "journal": "Journal of medical Internet research", "date": "2021-06-18", "authors": ["AddyMoran", "ShawnHampton", "ScottDowson", "JohnDagdelen", "AmalieTrewartha", "GerbrandCeder", "KristinPersson", "EliseSaxon", "AndrewBarker", "LaurenCharles", "Bobbie-JoWebb-Robertson"], "doi": "10.2196/26995\n10.1038/s41594-020-0423-7\n10.1038/d41586-020-00694-1\n10.1093/nar/gkaa952\n10.1101/2020.12.21.423860\n10.3892/etm.2020.9324\n10.2337/dc20-1444\n10.1007/s40200-020-00665-3\n10.1111/1753-0407.13125\n10.1007/s00592-009-0109-4\n10.1007/s10067-021-05754-z\n10.1016/j.jpeds.2020.08.003\n10.1016/j.jpeds.2020.10.013\n10.1016/j.jpeds.2020.11.016\n10.1016/j.jemermed.2020.12.005"}
{"title": "Predicting Prolonged Hospitalization and Supplemental Oxygenation in Patients with COVID-19 Infection from Ambulatory Chest Radiographs using Deep Learning.", "abstract": "The clinical prognosis of outpatients with coronavirus disease 2019 (COVID-19) remains difficult to predict, with outcomes including asymptomatic, hospitalization, intubation, and death. Here we determined the prognostic value of an outpatient chest radiograph, together with an ensemble of deep learning algorithms predicting comorbidities and airspace disease to identify patients at a higher risk of hospitalization from COVID-19 infection.\nThis retrospective study included outpatients with COVID-19 confirmed by reverse transcription-polymerase chain reaction testing who received an ambulatory chest radiography between March 17, 2020 and October 24, 2020. In this study, full admission was defined as hospitalization within 14 days of the COVID-19 test for > 2 days with supplemental oxygen. Univariate analysis and machine learning algorithms were used to evaluate the relationship between the deep learning model predictions and hospitalization for > 2 days.\nThe study included 413 patients, 222 men (54%), with a median age of 51 years (interquartile range, 39-62 years). Fifty-one patients (12.3%) required full admission. A boosted decision tree model produced the best prediction. Variables included patient age, frontal chest radiograph predictions of morbid obesity, congestive heart failure and cardiac arrhythmias, and radiographic opacity, with an internally validated area under the curve (AUC) of 0.837 (95% CI: 0.791-0.883) on a test cohort.\nDeep learning analysis of single frontal chest radiographs was used to generate combined comorbidity and pneumonia scores that predict the need for supplemental oxygen and hospitalization for > 2 days in patients with COVID-19 infection with an AUC of 0.837 (95% confidence interval: 0.791-0.883). Comorbidity scoring may prove useful in other clinical scenarios.", "journal": "Academic radiology", "date": "2021-06-18", "authors": ["AyisPyrros", "Adam EugeneFlanders", "Jorge MarioRodr\u00edguez-Fern\u00e1ndez", "AndrewChen", "PatrickCole", "DanielWenzke", "EricHart", "SamuelHarford", "JeanneHorowitz", "PaulNikolaidis", "NadirMuzaffar", "VivekaBoddipalli", "JaiNebhrajani", "NasirSiddiqui", "MelindaWillis", "HoushangDarabi", "OluwasanmiKoyejo", "WilliamGalanter"], "doi": "10.1016/j.acra.2021.05.002\n10.1001/jama.2020.6775\n10.1016/j.ijid.2020.03.017\n10.1007/s10140-020-01808-y\n10.5334/ijic.2500\n10.1148/radiol.2020201754\n10.1148/radiol.2020202723\n10.1007/s10994-007-5040-8\n10.1038/s41597-019-0103-9\n10.1145/1390156.1390177\n10.1109/21.97458\n10.1145/2939672.2939785\n10.1136/bmj.m1328\n10.1371/journal.pone.0241825\n10.1148/radiol.2020200370\n10.1159/000495786\n10.5853/jos.2013.15.2.122\n10.1001/jamanetworkopen.2019.7416\n10.1093/ije/dyr041\n10.1017/S0950268820001727\n10.1016/j.amepre.2020.05.002"}
{"title": "Automated detection of Covid-19 disease using deep fused features from chest radiography images.", "abstract": "The health systems of many countries are desperate in the face of Covid-19, which has become a pandemic worldwide and caused the death of hundreds of thousands of people. In order to keep Covid-19, which has a very high propagation rate, under control, it is necessary to develop faster, low-cost and highly accurate methods, rather than a costly Polymerase Chain Reaction test that can yield results in a few hours. In this study, a deep learning-based approach that can detect Covid-19 quickly and with high accuracy on X-ray images, which are common in every hospital and can be obtained at low cost, was proposed. Deep features were extracted from X-Ray images in RGB, CIE Lab and RGB CIE color spaces using DenseNet121 and EfficientNet B0 pre-trained deep learning architectures and then obtained features were fed into a two-stage classifier approach. Each of the classifiers in the proposed approach performed binary classification. In the first stage, healthy and infected samples were separated, and in the second stage, infected samples were detected as Covid-19 or pneumonia. In the experiments, Bi-LSTM network and well-known ensemble approaches such as Gradient Boosting, Random Forest and Extreme Gradient Boosting were used as the classifier model and it was seen that the Bi-LSTM network had a superior performance than other classifiers with 92.489% accuracy.", "journal": "Biomedical signal processing and control", "date": "2021-06-17", "authors": ["EmineU\u00e7ar", "\u00dcmitAtila", "MuratU\u00e7ar", "KemalAkyol"], "doi": "10.1016/j.bspc.2021.102862\n10.1136/svn-2017-000101\n10.1016/J.APNU.2020.07.011\n10.1016/J.JAGP.2020.07.003\n10.1016/J.JPSYCHIRES.2020.05.026\n10.1016/J.IJID.2020.06.024\n10.1016/J.SCITOTENV.2020.140935\n10.1016/J.JIPH.2020.06.034\n10.1001/jama.2020.3786\n10.1148/radiol.2020200463\n10.1148/radiol.2020200432\n10.1038/nature14539\n10.1016/J.MEDIA.2015.08.001\n10.1080/21681163.2015.1124249\n10.1016/J.NEUROIMAGE.2016.01.024\n10.1109/TMI.2016.2548501\n10.3389/fnins.2014.00229\n10.1007/s00429-013-0687-3\n10.1016/J.CMPB.2020.105581\n10.1016/J.IRBM.2020.05.003\n10.1016/J.COMPBIOMED.2020.103792\n10.1016/J.CHAOS.2020.109944\n10.1088/1475-4878/33/3/301\n10.1162/neco.1997.9.8.1735\n10.1016/J.PATREC.2005.10.010\n10.1109/JAS.2020.1003387\n10.1016/j.jestch.2020.01.002"}
{"title": "Deep Learning in Classification of Covid-19 Coronavirus, Pneumonia and Healthy Lungs on CXR and CT Images.", "abstract": "In this paper, the transfer learning method has been implemented to chest X-ray (CXR) and computed tomography (CT) bio-images of diverse kinds of lungs maladies, including CORONAVIRUS 2019 (COVID-19). COVID-19 identification is a difficult assignment that constantly demands a careful analysis of a patient's clinical images, as COVID-19 is found to be very alike to pneumonic viral lung infection. In this paper, a transfer learning model to accelerate prediction processes and to assist medical professionals is proposed. Finally, the main purpose is to do an accurate classification between Covid-19, pneumonia and, healthy lungs using CXR and CT images.\nLearning transfer gives the possibility to find out about this new illness COVID-19, using the knowledge we have about the pneumonia virus. This demonstrates the apprehensiveness achieved from a new architecture trained to detect virus-related pneumonia that must be transferred for COVID-19 detection. Transfer learning presents a considerable dissimilarity in results when compared to the result of traditional groupings. It is not necessary to create a separate model for the classification of COVID-19. This simplifies complicated issues by adopting the available model for COVID-19 determination. Automated diagnosis of COVID-19 using Haralick texture features is focused on segmented lung images and problematic lung patches. Lung patches are necessary for the augmentation of COVID-19 image data.\nThe obtained outcomes are quite reliable for all distinctive processes as the proposed architecture can distinguish healthy lungs, pneumonia, COVID-19.\nThe results suggest that the implemented model is improved considering other existing models because the obtained classification accuracy is over the recently obtained results. It is a belief that the new architecture that is implemented in this study, delivers a petite step in building refined Coronavirus 2019 diagnosis architecture using CXR and CT bio-images.", "journal": "Journal of medical and biological engineering", "date": "2021-06-16", "authors": ["Mihaela-RuxandraLascu"], "doi": "10.1007/s40846-021-00630-2\n10.1016/j.ins.2020.09.041\n10.1183/13993003.00775-2020\n10.1109/TMI.2020.2993291\n10.1007/s10489-020-01831-z\n10.1109/TSMC.1973.4309314\n10.1101/2020.02.14.20023028\n10.1080/07391102.2020.1767212\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105581"}
{"title": "Res-CovNet: an internet of medical health things driven COVID-19 framework using transfer learning.", "abstract": "Major countries are globally facing difficult situations due to this pandemic disease, COVID-19. There are high chances of getting false positives and false negatives identifying the COVID-19 symptoms through existing medical practices such as PCR (polymerase chain reaction) and RT-PCR (reverse transcription-polymerase chain reaction). It might lead to a community spread of the disease. The alternative of these tests can be CT (Computer Tomography) imaging or X-rays of the lungs to identify the patient with COVID-19 symptoms more accurately. Furthermore, by using feasible and usable technology to automate the identification of COVID-19, the facilities can be improved. This notion became the basic framework, Res-CovNet, of the implemented methodology, a hybrid methodology to bring different platforms into a single platform. This basic framework is incorporated into IoMT based framework, a web-based service to identify and classify various forms of pneumonia or COVID-19 utilizing chest X-ray images. For the front end, the.NET framework along with C# language was utilized, MongoDB was utilized for the storage aspect, Res-CovNet was utilized for the processing aspect. Deep learning combined with the notion forms a comprehensive implementation of the framework, Res-CovNet, to classify the COVID-19 affected patients from pneumonia-affected patients as both lung imaging looks similar to the naked eye. The implemented framework, Res-CovNet, developed with the technique, transfer learning in which ResNet-50 used as a pre-trained model and then extended with classification layers. The work implemented using the data of X-ray images collected from the various trustable sources that include cases such as normal, bacterial pneumonia, viral pneumonia, and COVID-19, with the overall size of the data is about 5856. The accuracy of the model implemented is about 98.4% in identifying COVID-19 against the normal cases. The accuracy of the model is about 96.2% in the case of identifying COVID-19 against all other cases, as mentioned.", "journal": "Neural computing & applications", "date": "2021-06-16", "authors": ["Mangena VenuMadhavan", "AdityaKhamparia", "DeepakGupta", "SagarPande", "PrayagTiwari", "M ShamimHossain"], "doi": "10.1007/s00521-021-06171-8\n10.1109/JIOT.2020.3033129\n10.1109/MNET.011.2000458\n10.1109/MNET.011.2000353\n10.1007/s13246-020-00865-4\n10.1145/3421725\n10.1016/j.measurement.2019.05.076\n10.1109/TNSE.2020.3026637\n10.1148/radiol.2020200905\n10.1016/j.future.2018.04.027\n10.1002/cpe.4946\n10.1016/j.patrec.2018.07.026\n10.1109/JIOT.2017.2772959\n10.1109/ACCESS.2020.3037474\n10.1109/JIOT.2020.3013710\n10.1109/JIOT.2021.3051080\n10.1109/ACCESS.2019.2891390\n10.1109/MCOM.2018.1700571\n10.1109/ACCESS.2020.3011123\n10.1109/JSYST.2015.2470644\n10.1007/s11042-013-1590-x\n10.1109/JSAC.2020.3020654\n10.1145/3241056\n10.1109/TMM.2015.2393635\n10.1148/radiol.2020200343\n10.1109/ACCESS.2020.2995597\n10.1109/JIOT.2021.3050775\n10.1016/j.inffus.2021.02.013\n10.1016/j.patcog.2020.107700\n10.1007/s00521-020-05335-2\n10.1183/13993003.00775-2020\n10.1007/978-3-030-45099-1_18\n10.1109/5.726791"}
{"title": "NIA-Network: Towards improving lung CT infection detection for COVID-19 diagnosis.", "abstract": "During pandemics (e.g., COVID-19) physicians have to focus on diagnosing and treating patients, which often results in that only a limited amount of labeled CT images is available. Although recent semi-supervised learning algorithms may alleviate the problem of annotation scarcity, limited real-world CT images still cause those algorithms producing inaccurate detection results, especially in real-world COVID-19 cases. Existing models often cannot detect the small infected regions in COVID-19 CT images, such a challenge implicitly causes that many patients with minor symptoms are misdiagnosed and develop more severe symptoms, causing a higher mortality. In this paper, we propose a new method to address this challenge. Not only can we detect severe cases, but also detect minor symptoms using real-world COVID-19 CT images in which the source domain only includes limited labeled CT images but the target domain has a lot of unlabeled CT images. Specifically, we adopt Network-in-Network and Instance Normalization to build a new module (we term it NI module) and extract discriminative representations from CT images from both source and target domains. A domain classifier is utilized to implement infected region adaptation from source domain to target domain in an Adversarial Learning manner, and learns domain-invariant region proposal network (RPN) in the Faster R-CNN model. We call our model NIA-Network (Network-in-Network, Instance Normalization and Adversarial Learning), and conduct extensive experiments on two COVID-19 datasets to validate our approach. The experimental results show that our model can effectively detect infected regions with different sizes and achieve the highest diagnostic accuracy compared with existing SOTA methods.", "journal": "Artificial intelligence in medicine", "date": "2021-06-16", "authors": ["WeiLi", "JinlinChen", "PingChen", "LequanYu", "XiaohuiCui", "YiweiLi", "FangCheng", "WenOuyang"], "doi": "10.1016/j.artmed.2021.102082\n10.1109/CVPR.2009.5206848"}
{"title": "Deep supervised learning using self-adaptive auxiliary loss for COVID-19 diagnosis from imbalanced CT images.", "abstract": "The outbreak and rapid spread of coronavirus disease 2019 (COVID-19) has had a huge impact on the lives and safety of people around the world. Chest CT is considered an effective tool for the diagnosis and follow-up of COVID-19. For faster examination, automatic COVID-19 diagnostic techniques using deep learning on CT images have received increasing attention. However, the number and category of existing datasets for COVID-19 diagnosis that can be used for training are limited, and the number of initial COVID-19 samples is much smaller than the normal's, which leads to the problem of class imbalance. It makes the classification algorithms difficult to learn the discriminative boundaries since the data of some classes are rich while others are scarce. Therefore, training robust deep neural networks with imbalanced data is a fundamental challenging but important task in the diagnosis of COVID-19. In this paper, we create a challenging clinical dataset (named COVID19-Diag) with category diversity and propose a novel imbalanced data classification method using deep supervised learning with a self-adaptive auxiliary loss (DSN-SAAL) for COVID-19 diagnosis. The loss function considers both the effects of data overlap between CT slices and possible noisy labels in clinical datasets on a multi-scale, deep supervised network framework by integrating the effective number of samples and a weighting regularization item. The learning process jointly and automatically optimizes all parameters over the deep supervised network, making our model generally applicable to a wide range of datasets. Extensive experiments are conducted on COVID19-Diag and three public COVID-19 diagnosis datasets. The results show that our DSN-SAAL outperforms the state-of-the-art methods and is effective for the diagnosis of COVID-19 in varying degrees of data imbalance.", "journal": "Neurocomputing", "date": "2021-06-15", "authors": ["KaiHu", "YingjieHuang", "WeiHuang", "HuiTan", "ZhinengChen", "ZhengZhong", "XuanyaLi", "YuanZhang", "XiepingGao"], "doi": "10.1016/j.neucom.2021.06.012"}
{"title": "Deep convolutional neural networks for COVID-19 automatic diagnosis.", "abstract": "This article is mainly concerned with COVID-19 diagnosis from X-ray images. The number of cases infected with COVID-19 is increasing daily, and there is a limitation in the number of test kits needed in hospitals. Therefore, there is an imperative need to implement an efficient automatic diagnosis system to alleviate COVID-19 spreading among people. This article presents a discussion of the utilization of convolutional neural network (CNN) models with different learning strategies for automatic COVID-19 diagnosis. First, we consider the CNN-based transfer learning approach for automatic diagnosis of COVID-19 from X-ray images with different training and testing ratios. Different pre-trained deep learning models in addition to a transfer learning model are considered and compared for the task of COVID-19 detection from X-ray images. Confusion matrices of these studied models are presented and analyzed. Considering the performance results obtained, ResNet models (ResNet18, ResNet50, and ResNet101) provide the highest classification accuracy on the two considered datasets with different training and testing ratios, namely 80/20, 70/30, 60/40, and 50/50. The accuracies obtained using the first dataset with 70/30 training and testing ratio are 97.67%, 98.81%, and 100% for ResNet18, ResNet50, and ResNet101, respectively. For the second dataset, the reported accuracies are 99%, 99.12%, and 99.29% for ResNet18, ResNet50, and ResNet101, respectively. The second approach is the training of a proposed CNN model from scratch. The results confirm that training of the CNN from scratch can lead to the identification of the signs of COVID-19 disease.", "journal": "Microscopy research and technique", "date": "2021-06-15", "authors": ["Heba MEmara", "Mohamed RShoaib", "MohamedElwekeil", "WalidEl-Shafai", "Taha ETaha", "Adel SEl-Fishawy", "El-Sayed MEl-Rabaie", "Saleh AAlshebeili", "Moawad IDessouky", "Fathi EAbd El-Samie"], "doi": "10.1002/jemt.23713"}
{"title": "COVID-19 diagnosis on CT scan images using a generative adversarial network and concatenated feature pyramid network with an attention mechanism.", "abstract": "Coronavirus disease 2019 (COVID-19) has caused hundreds of thousands of infections and deaths. Efficient diagnostic methods could help curb its global spread. The purpose of this study was to develop and evaluate a method for accurately diagnosing COVID-19 based on computed tomography (CT) scans in real time.\nWe propose an architecture named \"concatenated feature pyramid network\" (\"Concat-FPN\") with an attention mechanism, by concatenating feature maps of multiple. The proposed architecture is then used to form two networks, which we call COVID-CT-GAN and COVID-CT-DenseNet, the former for data augmentation and the latter for data classification.\nThe proposed method is evaluated on 3 different numbers of magnitude of COVID-19 CT datasets. Compared with the method without GANs for data augmentation or the original network auxiliary classifier generative adversarial network, COVID-CT-GAN increases the accuracy by 2% to 3%, the recall by 2% to 4%, the precision by 1% to 3%, the F1-score by 1% to 3%, and the area under the curve by 1% to 4%. Compared with the original network DenseNet-201, COVID-CT-DenseNet increases the accuracy by 1% to 3%, the recall by 4% to 9%, the precision by 1%, the F1-score by 1% to 3%, and the area under the curve by 2%.\nThe experimental results show that our method improves the efficiency of diagnosing COVID-19 on CT images, and helps overcome the problem of limited training data when using deep learning methods to diagnose COVID-19.\nOur method can help clinicians build deep learning models using their private datasets to achieve automatic diagnosis of COVID-19 with a high precision.", "journal": "Medical physics", "date": "2021-06-13", "authors": ["ZongguiLi", "JunhuaZhang", "BoLi", "XiaoyingGu", "XudongLuo"], "doi": "10.1002/mp.15044\n10.1109/RBME.2020.2987975\n10.1016/j.crad.2020.03.003\n10.1101/2020.04.24.20078584\n10.1101/2020.04.13.2006394\n10.1109/CVPR.2017.243\n10.1101/2020.06.08.20125963\n10.1109/CVPR.2018.00685\n10.1109/TMI.2020.2995508\n10.1101/2020.02.14.20023028\n10.1101/2020.02.23.20026930\n10.1109/TMI.2020.2992546\n10.36227/techrxiv.12328061\n10.1109/TFUZZ.2020.2966163\n10.1109/tmi.2016.2535302\n10.1109/TMI.2016.2553401\n10.1007/s10278-018-0056-0\n10.3390/s19102361\n10.1109/TMI.2017.2759102\n10.1007/978-3-319-59050-9_12\n10.1109/JBHI.2018.2852639\n10.1117/12.2293971\n10.1109/ASYU48272.2019.8946442\n10.1109/CVPR.2016.314\n10.1109/CVPR.2016.98\n10.1007/978-3-319-46448-0_2\n10.1007/978-3-319-46493-0_22\n10.1109/CVPR.2017.106\n10.1109/34.730558\n10.1109/TPAMI.2019.2913372\n10.1109/CVPR.2017.683\n10.1007/978-3-030-01234-2_1"}
{"title": "COVID-FACT: A Fully-Automated Capsule Network-Based Framework for Identification of COVID-19 Cases from Chest CT Scans.", "abstract": "The newly discovered Coronavirus Disease 2019 (COVID-19) has been globally spreading and causing hundreds of thousands of deaths around the world as of its first emergence in late 2019. The rapid outbreak of this disease has overwhelmed health care infrastructures and arises the need to allocate medical equipment and resources more efficiently. The early diagnosis of this disease will lead to the rapid separation of COVID-19 and non-COVID cases, which will be helpful for health care authorities to optimize resource allocation plans and early prevention of the disease. In this regard, a growing number of studies are investigating the capability of deep learning for early diagnosis of COVID-19. Computed tomography (CT) scans have shown distinctive features and higher sensitivity compared to other diagnostic tests, in particular the current gold standard, i.e., the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Current deep learning-based algorithms are mainly developed based on Convolutional Neural Networks (CNNs) to identify COVID-19 pneumonia cases. CNNs, however, require extensive data augmentation and large datasets to identify detailed spatial relations between image instances. Furthermore, existing algorithms utilizing CT scans, either extend slice-level predictions to patient-level ones using a simple thresholding mechanism or rely on a sophisticated infection segmentation to identify the disease. In this paper, we propose a two-stage fully automated CT-based framework for identification of COVID-19 positive cases referred to as the \"COVID-FACT\". COVID-FACT utilizes Capsule Networks, as its main building blocks and is, therefore, capable of capturing spatial information. In particular, to make the proposed COVID-FACT independent from sophisticated segmentations of the area of infection, slices demonstrating infection are detected at the first stage and the second stage is responsible for classifying patients into COVID and non-COVID cases. COVID-FACT detects slices with infection, and identifies positive COVID-19 cases using an in-house CT scan dataset, containing COVID-19, community acquired pneumonia, and normal cases. Based on our experiments, COVID-FACT achieves an accuracy of ", "journal": "Frontiers in artificial intelligence", "date": "2021-06-12", "authors": ["ShahinHeidarian", "ParnianAfshar", "NastaranEnshaei", "FarnooshNaderkhani", "Moezedin JavadRafiee", "FaranakBabaki Fard", "KavehSamimi", "S FarokhAtashzar", "AnastasiaOikonomou", "Konstantinos NPlataniotis", "ArashMohammadi"], "doi": "10.3389/frai.2021.598932\n10.1016/j.patrec.2020.09.010\n10.1109/LSP.2020.3034858\n10.1109/ICIP.2018.8451379\n10.1038/s41598-020-64824-5\n10.1109/EMBC44109.2020.9175922\n10.1109/ICASSP.2019.8683759\n10.1109/ICIP.2019.8803615\n10.1148/radiol.2020200823\n10.1148/radiol.2020200230\n10.1109/CVPR.2009.5206848\n10.1148/radiol.2020200432\n10.1109/ICCVW.2017.373\n10.1109/ACCESS.2020.3005510\n10.1148/ryct.2020200110\n10.1145/3065386\n10.1148/radiol.2020200905\n10.1109/iccv.2017.324\n10.1016/j.compbiomed.2020.103869\n10.1148/ryct.2020200034\n10.1016/j.bspc.2021.102588\n10.1016/j.jacr.2013.05.032\n10.1007/978-3-319-24574-4_28\n10.1109/ICCV.2017.74\n10.20944/preprints202003.0300.v1\n10.1016/S1473-3099(20)30086-4\n10.1111/j.2517-6161.1974.tb00994.x\n10.1148/radiol.2020201160\n10.1007/s13244-018-0639-9\n10.21037/atm.2020.03.132\n10.1007/s42058-020-00034-2\n10.1016/j.cell.2020.04.045"}
{"title": "Autonomous Robotic Point-of-Care Ultrasound Imaging for Monitoring of COVID-19-Induced Pulmonary Diseases.", "abstract": "The COVID-19 pandemic has emerged as a serious global health crisis, with the predominant morbidity and mortality linked to pulmonary involvement. Point-of-Care ultrasound (POCUS) scanning, becoming one of the primary determinative methods for its diagnosis and staging, requires, however, close contact of healthcare workers with patients, therefore increasing the risk of infection. This work thus proposes an autonomous robotic solution that enables POCUS scanning of COVID-19 patients' lungs for diagnosis and staging. An algorithm was developed for approximating the optimal position of an ultrasound probe on a patient from prior CT scans to reach predefined lung infiltrates. In the absence of prior CT scans, a deep learning method was developed for predicting 3D landmark positions of a human ribcage given a torso surface model. The landmarks, combined with the surface model, are subsequently used for estimating optimal ultrasound probe position on the patient for imaging infiltrates. These algorithms, combined with a force-displacement profile collection methodology, enabled the system to successfully image all points of interest in a simulated experimental setup with an average accuracy of 20.6 \u00b1 14.7\u00a0mm using prior CT scans, and 19.8 \u00b1 16.9\u00a0mm using only ribcage landmark estimation. A study on a full torso ultrasound phantom showed that autonomously acquired ultrasound images were 100% interpretable when using force feedback with prior CT and 88% with landmark estimation, compared to 75 and 58% without force feedback, respectively. This demonstrates the preliminary feasibility of the system, and its potential for offering a solution to help mitigate the spread of COVID-19 in vulnerable environments.", "journal": "Frontiers in robotics and AI", "date": "2021-06-12", "authors": ["LidiaAl-Zogbi", "VivekSingh", "BrianTeixeira", "AvaniAhuja", "Pooyan SahbaeeBagherzadeh", "AnkurKapoor", "HamedSaeidi", "ThorstenFleiter", "AxelKrieger"], "doi": "10.3389/frobt.2021.645756\n10.1016/j.ultrasmedbio.2020.03.033\n10.1002/jum.15525\n10.1016/j.ultrasmedbio.2020.11.004\n10.1148/rg.2017160175\n10.7863/jum.2012.31.6.823\n10.1177/1941738111422691\n10.1002/rcs.1756\n10.1007/978-94-007-0017-8_2\n10.1006/cgip.1994.1042\n10.1121/1.391097\n10.1109/cvpr.2015.7299104\n10.1007/s00134-020-05996-6\n10.1007/s10514-013-9327-2\n10.1037/e584032011-011\n10.1038/s41598-019-48416-6\n10.1109/cvpr.2018.00944\n10.1109/70.34770\n10.1002/jum.15406\n10.37015/AUDT.2020.200023\n10.1186/s10033-020-00464-0\n10.1016/j.chest.2020.06.068"}
{"title": "Automatic Detection of Coronavirus Disease (COVID-19) in X-ray and CT Images: A Machine Learning Based Approach.", "abstract": "The newly identified Coronavirus pneumonia, subsequently termed COVID-19, is highly transmittable and pathogenic with no clinically approved antiviral drug or vaccine available for treatment. The most common symptoms of COVID-19 are dry cough, sore throat, and fever. Symptoms can progress to a severe form of pneumonia with critical complications, including septic shock, pulmonary edema, acute respiratory distress syndrome and multi-organ failure. While medical imaging is not currently recommended in Canada for primary diagnosis of COVID-19, computer-aided diagnosis systems could assist in the early detection of COVID-19 abnormalities and help to monitor the progression of the disease, potentially reduce mortality rates. In this study, we compare popular deep learning-based feature extraction frameworks for automatic COVID-19 classification. To obtain the most accurate feature, which is an essential component of learning, MobileNet, DenseNet, Xception, ResNet, InceptionV3, InceptionResNetV2, VGGNet, NASNet were chosen amongst a pool of deep convolutional neural networks. The extracted features were then fed into several machine learning classifiers to classify subjects as either a case of COVID-19 or a control. This approach avoided task-specific data pre-processing methods to support a better generalization ability for unseen data. The performance of the proposed method was validated on a publicly available COVID-19 dataset of chest X-ray and CT images. The DenseNet121 feature extractor with Bagging tree classifier achieved the best performance with 99% classification accuracy. The second-best learner was a hybrid of the a ResNet50 feature extractor trained by LightGBM with an accuracy of 98.", "journal": "Biocybernetics and biomedical engineering", "date": "2021-06-11", "authors": ["Sara HosseinzadehKassania", "Peyman HosseinzadehKassanib", "Michal JWesolowskic", "Kevin ASchneidera", "RalphDetersa"], "doi": "10.1016/j.bbe.2021.05.013\n10.1016/j.jare.2020.03.005\n10.1016/j.cub.2020.03.022\n10.1016/j.jaut.2020.102433\n10.1016/j.ijid.2020.03.031\n10.1145/2939672.2939785"}
{"title": "Progressive back-projection network for COVID-CT super-resolution.", "abstract": "Recently, the COVID-19 epidemic has become more and more serious around the world, how to improve the image resolution of COVID-CT is a very important task. The network based on progressive upsampling for COVID-CT super-resolution increases the reconstruction error. This paper proposes a progressive back-projection network (PBPN) for COVID-CT super-resolution to solve this problem.\nIn this paper, we propose a progressive back-projection network (PBPN) for COVID-CT super-resolution. PBPN is divided into two stages, and each stage consists of back-projection, deep feature extraction and upscaling. We design an up-projection and down-projection residual module to minimize the reconstruction error and construct a residual attention module to extract deep features. In each stage, firstly, PBPN performs back-projection to extract shallow features by two up-projection and down-projection residual modules; then, PBPN extracts deep features from the shallow features by two residual attention modules; finally, PBPN upsamples the deep features through sub-pixel convolution.\nThe proposed method achieves the improvements of about 0.14~0.47\u00a0dB/0.0012~0.0060 for\u00a0\u00d7\u00a02 scale factor, 0.02~0.08\u00a0dB/0.0024~0.0059 for\u00a0\u00d7\u00a03 scale factor, and 0.08~0.41\u00a0dB/ 0.0040~0.0147 for\u00a0\u00d7\u00a04 scale factor than state-of-the-art methods (Bicubic, SRCNN, FSRCNN, VDSR, LapSRN, DRCN and DSRN) in terms of PSNR/SSIM on benchmark datasets.\nThe proposed mehtod obtains better performance for COVID-CT super-resolution and reconstructs high-quality high-resolution COVID-CT images that contain more details and edges.", "journal": "Computer methods and programs in biomedicine", "date": "2021-06-10", "authors": ["ZhaoyangSong", "XiaoqiangZhao", "YongyongHui", "HongmeiJiang"], "doi": "10.1016/j.cmpb.2021.106193"}
{"title": "COVID-19 Detection from X-ray Images using Multi-Kernel-Size Spatial-Channel Attention Network.", "abstract": "Novel coronavirus 2019 (COVID-19) has spread rapidly around the world and is threatening the health and lives of people worldwide. Early detection of COVID-19 positive patients and timely isolation of the patients are essential to prevent its spread. Chest X-ray images of COVID-19 patients often show the characteristics of multifocality, bilateral hairy glass turbidity, patchy network turbidity, etc. It is crucial to design a method to automatically identify COVID-19 from chest X-ray images to help diagnosis and prognosis. Existing studies for the classification of COVID-19 rarely consider the role of attention mechanisms on the classification of chest X-ray images and fail to capture the cross-channel and cross-spatial interrelationships in multiple scopes. This paper proposes a multi-kernel-size spatial-channel attention method to detect COVID-19 from chest X-ray images. Our proposed method consists of three stages. The first stage is feature extraction. The second stage contains two parallel multi-kernel-size attention modules: multi-kernel-size spatial attention and multi-kernel-size channel attention. The two modules capture the cross-channel and cross-spatial interrelationships in multiple scopes using multiple 1D and 2D convolutional kernels of different sizes to obtain channel and spatial attention feature maps. The third stage is the classification module. We integrate the chest X-ray images from three public datasets: COVID-19 Chest X-ray Dataset Initiative, ActualMed COVID-19 Chest X-ray Dataset Initiative, and COVID-19 radiography database for evaluation. Experimental results demonstrate that the proposed method improves the performance of COVID-19 detection and achieves an accuracy of 98.2%.", "journal": "Pattern recognition", "date": "2021-06-10", "authors": ["YuqiFan", "JiahaoLiu", "RuixuanYao", "XiaohuiYuan"], "doi": "10.1016/j.patcog.2021.108055"}
{"title": "Early assessment of lung function in coronavirus patients using invariant markers from chest X-rays images.", "abstract": "The primary goal of this manuscript is to develop a computer assisted diagnostic (CAD) system to assess pulmonary function and risk of mortality in patients with coronavirus disease 2019 (COVID-19). The CAD system processes chest X-ray data and provides accurate, objective imaging markers to assist in the determination of patients with a higher risk of death and thus are more likely to require mechanical ventilation and/or more intensive clinical care.To obtain an accurate stochastic model that has the ability to detect the severity of lung infection, we develop a second-order Markov-Gibbs random field (MGRF) invariant under rigid transformation (translation or rotation of the image) as well as scale (i.e., pixel size). The parameters of the MGRF model are learned automatically, given a training set of X-ray images with affected lung regions labeled. An X-ray input to the system undergoes pre-processing to correct for non-uniformity of illumination and to delimit the boundary of the lung, using either a fully-automated segmentation routine or manual delineation provided by the radiologist, prior to the diagnosis. The steps of the proposed methodology are: (i) estimate the Gibbs energy at several different radii to describe the inhomogeneity in lung infection; (ii) compute the cumulative distribution function (CDF) as a new representation to describe the local inhomogeneity in the infected region of lung; and (iii) input the CDFs to a new neural network-based fusion system to determine whether the severity of lung infection is low or high. This approach is tested on 200 clinical X-rays from 200 COVID-19 positive patients, 100 of whom died and 100 who recovered using multiple training/testing processes including leave-one-subject-out (LOSO), tenfold, fourfold, and twofold cross-validation tests. The Gibbs energy for lung pathology was estimated at three concentric rings of increasing radii. The accuracy and Dice similarity coefficient (DSC) of the system steadily improved as the radius increased. The overall CAD system combined the estimated Gibbs energy information from all radii and achieved a sensitivity, specificity, accuracy, and DSC of 100%, 97% \u00b1 3%, 98% \u00b1 2%, and 98% \u00b1 2%, respectively, by twofold cross validation. Alternative classification algorithms, including support vector machine, random forest, naive Bayes classifier, K-nearest neighbors, and decision trees all produced inferior results compared to the proposed neural network used in this CAD system. The experiments demonstrate the feasibility of the proposed system as a novel tool to objectively assess disease severity and predict mortality in COVID-19 patients. The proposed tool can assist physicians to determine which patients might require more intensive clinical care, such a mechanical respiratory support.", "journal": "Scientific reports", "date": "2021-06-10", "authors": ["MohamedElsharkawy", "AhmedSharafeldeen", "FatmaTaher", "AhmedShalaby", "AhmedSoliman", "AliMahmoud", "MohammedGhazal", "AshrafKhalil", "Norah SalehAlghamdi", "Ahmed Abdel Khalek AbdelRazek", "EmanAlnaghy", "Moumen TEl-Melegy", "Harpal SinghSandhu", "Guruprasad AGiridharan", "AymanEl-Baz"], "doi": "10.1038/s41598-021-91305-0\n10.1016/j.jinf.2020.03.041\n10.1016/j.tmaid.2020.101623\n10.1093/qjmed/hcaa089\n10.1016/j.jinf.2020.03.004\n10.1016/j.tim.2016.03.003\n10.1148/radiol.2020200642\n10.1148/radiol.2020200343\n10.1001/jama.2020.2783\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020201160\n10.3348/kjr.2020.0132\n10.1097/RTI.0000000000000533\n10.7150/thno.45985\n10.5152/dir.2019.20294\n10.3348/kjr.2020.0146\n10.1097/RTI.0000000000000512\n10.1016/j.measurement.2019.107426\n10.3390/s18041019\n10.1016/j.eswa.2020.113514\n10.1016/j.eswa.2017.03.057\n10.1016/j.eswa.2020.113909\n10.1016/j.mehy.2020.109760\n10.1007/s00059-020-04923-1\n10.1056/NEJMoa2004500\n10.1001/jama.2020.4326\n10.1001/jama.2020.6775\n10.1001/jamainternmed.2020.0994\n10.1016/S0140-6736(20)30566-3"}
{"title": "Dual attention multiple instance learning with unsupervised complementary loss for COVID-19 screening.", "abstract": "Chest computed tomography (CT) based analysis and diagnosis of the Coronavirus Disease 2019 (COVID-19) plays a key role in combating the outbreak of the pandemic that has rapidly spread worldwide. To date, the disease has infected more than 18 million people with over 690k deaths reported. Reverse transcription polymerase chain reaction (RT-PCR) is the current gold standard for clinical diagnosis but may produce false positives; thus, chest CT based diagnosis is considered more viable. However, accurate screening is challenging due to the difficulty in annotation of infected areas, curation of large datasets, and the slight discrepancies between COVID-19 and other viral pneumonia. In this study, we propose an attention-based end-to-end weakly supervised framework for the rapid diagnosis of COVID-19 and bacterial pneumonia based on multiple instance learning (MIL). We further incorporate unsupervised contrastive learning for improved accuracy with attention applied both in spatial and latent contexts, herein we propose Dual Attention Contrastive based MIL (DA-CMIL). DA-CMIL takes as input several patient CT slices (considered as bag of instances) and outputs a single label. Attention based pooling is applied to implicitly select key slices in the latent space, whereas spatial attention learns slice spatial context for interpretable diagnosis. A contrastive loss is applied at the instance level to encode similarity of features from the same patient against representative pooled patient features. Empirical results show that our algorithm achieves an overall accuracy of 98.6% and an AUC of 98.4%. Moreover, ablation studies show the benefit of contrastive learning with MIL.", "journal": "Medical image analysis", "date": "2021-06-09", "authors": ["PhilipChikontwe", "MiguelLuna", "MyeongkyunKang", "Kyung SooHong", "June HongAhn", "Sang HyunPark"], "doi": "10.1016/j.media.2021.102105\n10.1148/radiol.2020200642\n10.1007/978-3-030-59722-1_50\n10.1109/TMI.2020.2996256\n10.1109/CVPR.2016.90\n10.3346/jkms.2021.36.e46\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2993291\n10.1109/TMI.2020.2995508\n10.1109/CVPR.2016.278\n10.1002/mp.14609\n10.1109/RBME.2020.2987975\n10.1088/1361-6560/abe838\n10.1109/TCBB.2021.3065361\n10.1016/j.asoc.2020.106897\n10.1007/s00330-021-07715-1\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2995108\n10.1007/978-3-030-32239-7_55\n10.1109/TPAMI.2016.2567393\n10.1016/j.cell.2020.04.045\n10.1007/978-3-319-46487-9_40\n10.1148/radiol.2020200490"}
{"title": "COVID-19 detection using federated machine learning.", "abstract": "The current COVID-19 pandemic threatens human life, health, and productivity. AI plays an essential role in COVID-19 case classification as we can apply machine learning models on COVID-19 case data to predict infectious cases and recovery rates using chest x-ray. Accessing patient's private data violates patient privacy and traditional machine learning model requires accessing or transferring whole data to train the model. In recent years, there has been increasing interest in federated machine learning, as it provides an effective solution for data privacy, centralized computation, and high computation power. In this paper, we studied the efficacy of federated learning versus traditional learning by developing two machine learning models (a federated learning model and a traditional machine learning model)using Keras and TensorFlow federated, we used a descriptive dataset and chest x-ray (CXR) images from COVID-19 patients. During the model training stage, we tried to identify which factors affect model prediction accuracy and loss like activation function, model optimizer, learning rate, number of rounds, and data Size, we kept recording and plotting the model loss and prediction accuracy per each training round, to identify which factors affect the model performance, and we found that softmax activation function and SGD optimizer give better prediction accuracy and loss, changing the number of rounds and learning rate has slightly effect on model prediction accuracy and prediction loss but increasing the data size did not have any effect on model prediction accuracy and prediction loss. finally, we build a comparison between the proposed models' loss, accuracy, and performance speed, the results demonstrate that the federated machine learning model has a better prediction accuracy and loss but higher performance time than the traditional machine learning model.", "journal": "PloS one", "date": "2021-06-09", "authors": ["MustafaAbdul Salam", "SanaaTaha", "MohamedRamadan"], "doi": "10.1371/journal.pone.0252573\n10.1093/ecco-jcc/jjaa058\n10.1016/j.chaos.2020.110203\n10.1145/3298981\n10.1109/MSP.2020.2975749\n10.1016/j.patrec.2020.09.010\n10.1007/s11831-020-09472-8\n10.3892/etm.2020.8797\n10.1016/j.bspc.2020.102149\n10.1016/j.chaos.2020.109944\n10.1007/s41870-020-00495-9\n10.1016/j.compag.2020.105802\n10.3390/a13100249\n10.1186/s13673-020-00245-7\n10.1016/j.scs.2020.102589\n10.1007/s12065-019-00327-1"}
{"title": "An AI-based auxiliary empirical antibiotic therapy model for children with bacterial pneumonia using low-dose chest CT images.", "abstract": "To construct an auxiliary empirical antibiotic therapy (EAT) multi-class classification model for children with bacterial pneumonia using radiomics features based on artificial intelligence and low-dose chest CT images.\nData were retrospectively collected from children with pathogen-confirmed bacterial pneumonia including Gram-positive bacterial pneumonia (122/389, 31%), Gram-negative bacterial pneumonia (159/389, 41%) and atypical bacterial pneumonia (108/389, 28%) from January 1 to June 30, 2019. Nine machine-learning models were separately evaluated based on radiomics features extracted from CT images; three optimal submodels were constructed and integrated to form a multi-class classification model.\nWe selected five features to develop three radiomics submodels: a Gram-positive model, a Gram-negative model and an atypical model. The comprehensive radiomics model using support vector machine method yielded an average area under the curve (AUC) of 0.75 [95% confidence interval (CI), 0.65-0.83] and accuracy (ACC) of 0.58 [sensitivity (SEN), 0.57; specificity (SPE), 0.78] in the training set, and an average AUC of 0.73 (95% CI 0.61-0.79) and ACC of 0.54 (SEN, 0.52; SPE, 0.75) in the test set.\nThis auxiliary EAT radiomics multi-class classification model was deserved to be researched in differential diagnosing bacterial pneumonias in children.", "journal": "Japanese journal of radiology", "date": "2021-06-09", "authors": ["MudanZhang", "SiweiYu", "XuntaoYin", "XianchunZeng", "XinfengLiu", "ZhiYanShen", "XiaoyongZhang", "ChencuiHuang", "RongpinWang"], "doi": "10.1007/s11604-021-01136-2\n10.1016/S1473-3099(18)30407-9\n10.1016/S1473-3099(18)30362-1\n10.1016/S1473-3099(13)70318-9\n10.1155/2019/1701276\n10.1136/archdischild-2016-310527\n10.1016/j.cmi.2015.12.002\n10.1080/20469047.2017.1409455\n10.3390/jcm9010248\n10.1001/jamanetworkopen.2019.1095\n10.1016/j.archger.2011.09.006\n10.1158/1078-0432.CCR-18-3190\n10.1007/s00247-017-3891-0\n10.1016/j.chest.2017.07.035\n10.1183/20734735.0319-2018\n10.1002/ppul.22644\n10.3892/mmr.2016.5765\n10.1016/j.resinv.2020.01.006\n10.1128/CMR.00107-18"}
{"title": "Deep Learning on Chest X-ray Images to Detect and Evaluate Pneumonia Cases at the Era of COVID-19.", "abstract": "Coronavirus disease 2019 (COVID-19) is an infectious disease with first symptoms similar to the flu. COVID-19 appeared first in China and very quickly spreads to the rest of the world, causing then the 2019-20 coronavirus pandemic. In many cases, this disease causes pneumonia. Since pulmonary infections can be observed through radiography images, this paper investigates deep learning methods for automatically analyzing query chest X-ray images with the hope to bring precision tools to health professionals towards screening the COVID-19 and diagnosing confirmed patients. In this context, training datasets, deep learning architectures and analysis strategies have been experimented from publicly open sets of chest X-ray images. Tailored deep learning models are proposed to detect pneumonia infection cases, notably viral cases. It is assumed that viral pneumonia cases detected during an epidemic COVID-19 context have a high probability to presume COVID-19 infections. Moreover, easy-to-apply health indicators are proposed for estimating infection status and predicting patient status from the detected pneumonia cases. Experimental results show possibilities of training deep learning models over publicly open sets of chest X-ray images towards screening viral pneumonia. Chest X-ray test images of COVID-19 infected patients are successfully diagnosed through detection models retained for their performances. The efficiency of proposed health indicators is highlighted through simulated scenarios of patients presenting infections and health problems by combining real and synthetic health data.", "journal": "Journal of medical systems", "date": "2021-06-09", "authors": ["KarimHammoudi", "HalimBenhabiles", "MahmoudMelkemi", "FadiDornaika", "IgnacioArganda-Carreras", "DominiqueCollard", "ArnaudScherpereel"], "doi": "10.1007/s10916-021-01745-4\n10.1007/s10916-020-1536-6.pdf\n10.1016/j.cell.2018.02.010\n10.1148/rg.246045031\n10.1016/S0140-6736(20)30566-3"}
{"title": "Rapid Artificial Intelligence Solutions in a Pandemic - The COVID-19-20 Lung CT Lesion Segmentation Challenge.", "abstract": "Artificial intelligence (AI) methods for the automatic detection and quantification of COVID-19 lesions in chest computed tomography (CT) might play an important role in the monitoring and management of the disease. We organized an international challenge and competition for the development and comparison of AI algorithms for this task, which we supported with public data and state-of-the-art benchmark methods. Board Certified Radiologists annotated 295 public images from two sources (A and B) for algorithms training (n=199, source A), validation (n=50, source A) and testing (n=23, source A; n=23, source B). There were 1,096 registered teams of which 225 and 98 completed the validation and testing phases, respectively. The challenge showed that AI models could be rapidly designed by diverse teams with the potential to measure disease or facilitate timely and patient-specific interventions. This paper provides an overview and the major outcomes of the COVID-19 Lung CT Lesion Segmentation Challenge - 2020.", "journal": "Research square", "date": "2021-06-09", "authors": ["Holger RRoth", "ZiyueXu", "Carlos TorDiez", "Ramon SanchezJacob", "JonathanZember", "JoseMolto", "WenqiLi", "ShengXu", "BarisTurkbey", "EvrimTurkbey", "DongYang", "AhmedHarouni", "NicolaRieke", "ShishuaiHu", "FabianIsensee", "ClaireTang", "QinjiYu", "JanS\u00f6lter", "TongZheng", "VitaliLiauchuk", "ZiqiZhou", "Jan HendrikMoltz", "BrunoOliveira", "YongXia", "Klaus HMaier-Hein", "QikaiLi", "AndreasHusch", "LuyangZhang", "VassiliKovalev", "LiKang", "AlessaHering", "Jo\u00e3o LVila\u00e7a", "MonaFlores", "DaguangXu", "BradfordWood", "Marius GeorgeLinguraru"], "doi": "10.21203/rs.3.rs-571332/v1\n10.1002/mp.14609\n10.1007/s00330-020-07271-0\n10.21203/rs.3.rs-126892/v1\n10.1002/ima.22527\n10.7937/tcia.2020.gqry-nc81\n10.7937/tcia.2020.py71-5978\n10.1109/cvpr.2018.00907\n10.1109/cvpr42600.2020.00418\n10.1109/3dv.2019.00035"}
{"title": "Assessment of protein-protein interfaces in cryo-EM derived assemblies.", "abstract": "Structures of macromolecular assemblies derived from cryo-EM maps often contain errors that become more abundant with decreasing resolution. Despite efforts in the cryo-EM community to develop metrics for map and atomistic model validation, thus far, no specific scoring metrics have been applied systematically to assess the interface between the assembly subunits. Here, we comprehensively assessed protein-protein interfaces in macromolecular assemblies derived by cryo-EM. To this end, we developed Protein Interface-score (PI-score), a density-independent machine learning-based metric, trained using the\u00a0features of\u00a0protein-protein interfaces in crystal structures. We evaluated 5873 interfaces in 1053 PDB-deposited cryo-EM models (including SARS-CoV-2 complexes), as well as the models submitted to CASP13 cryo-EM targets and the EM model challenge. We further inspected the interfaces associated with low-scores and found that some of those, especially in intermediate-to-low resolution (worse than 4\u2009\u00c5) structures, were not captured by density-based assessment scores. A combined score incorporating PI-score and fit-to-density score showed discriminatory power, allowing our method to provide a powerful complementary assessment tool for the ever-increasing number of complexes solved by cryo-EM.", "journal": "Nature communications", "date": "2021-06-09", "authors": ["SonyMalhotra", "Agnel PraveenJoseph", "JeyanThiyagalingam", "MayaTopf"], "doi": "10.1038/s41467-021-23692-x\n10.1016/j.tibs.2014.10.005\n10.1186/s12915-017-0417-z\n10.1107/S2059798317004181\n10.1016/j.sbi.2019.05.024\n10.1016/j.jsb.2017.05.007\n10.1016/j.ymeth.2016.03.007\n10.1107/S1600576715010092\n10.1038/s41592-020-0731-1\n10.1038/nmeth.3541\n10.1107/S0907444909042073\n10.1002/pro.3786\n10.1110/ps.03323604\n10.1371/journal.pone.0080255\n10.1002/1097-0134(20010101)42:1<108::AID-PROT110>3.0.CO;2-O\n10.1073/pnas.0505425102\n10.1016/j.jsb.2016.07.012\n10.1006/jmbi.1993.1648\n10.1002/bip.360340711\n10.1093/protein/gzl026\n10.1006/jmbi.1997.0987\n10.1016/S0022-2836(02)01223-8\n10.1002/1097-0134(20010501)43:2<89::AID-PROT1021>3.0.CO;2-H\n10.1006/jmbi.1998.2439\n10.1016/S0959-440X(00)00065-8\n10.1002/prot.10085\n10.1006/jmbi.1998.1843\n10.1093/emboj/cdg359\n10.1007/s10930-007-9108-x\n10.1093/protein/13.2.77\n10.1093/bib/bbx022\n10.1093/nar/28.1.235\n10.1093/bioinformatics/btq404\n10.1002/prot.10389\n10.1016/j.ijleo.2015.06.010\n10.1002/prot.25823\n10.1002/prot.25817\n10.1002/prot.25795\n10.1107/S0907444910007493\n10.1002/prot.22550\n10.1002/prot.24214\n10.1107/S2059798317007859\n10.1016/j.jsb.2016.04.010\n10.1107/S2059798316013541\n10.1371/journal.pbio.1001244\n10.1016/j.str.2015.10.013\n10.1371/journal.pone.0086738\n10.1093/nar/25.17.3389\n10.1093/bioinformatics/btq461\n10.1093/bioinformatics/18.suppl_1.S71\n10.1107/S0907444910045749\n10.1016/j.jmb.2007.05.022\n10.1002/jcc.20084"}
{"title": "COVID-19 pneumonia on chest X-rays: Performance of a deep learning-based computer-aided detection system.", "abstract": "Chest X-rays (CXRs) can help triage for Coronavirus disease (COVID-19) patients in resource-constrained environments, and a computer-aided detection system (CAD) that can identify pneumonia on CXR may help the triage of patients in those environment where expert radiologists are not available. However, the performance of existing CAD for identifying COVID-19 and associated pneumonia on CXRs has been scarcely investigated. In this study, CXRs of patients with and without COVID-19 confirmed by reverse transcriptase polymerase chain reaction (RT-PCR) were retrospectively collected from four and one institution, respectively, and a commercialized, regulatory-approved CAD that can identify various abnormalities including pneumonia was used to analyze each CXR. Performance of the CAD was evaluated using area under the receiver operating characteristic curves (AUCs), with reference standards of the RT-PCR results and the presence of findings of pneumonia on chest CTs obtained within 24 hours from the CXR. For comparison, 5 thoracic radiologists and 5 non-radiologist physicians independently interpreted the CXRs. Afterward, they re-interpreted the CXRs with corresponding CAD results. The performance of CAD (AUCs, 0.714 and 0.790 against RT-PCR and chest CT, respectively hereinafter) were similar with those of thoracic radiologists (AUCs, 0.701 and 0.784), and higher than those of non-radiologist physicians (AUCs, 0.584 and 0.650). Non-radiologist physicians showed significantly improved performance when assisted with the CAD (AUCs, 0.584 to 0.664 and 0.650 to 0.738). In addition, inter-reader agreement among physicians was also improved in the CAD-assisted interpretation (Fleiss' kappa coefficient, 0.209 to 0.322). In conclusion, radiologist-level performance of the CAD in identifying COVID-19 and associated pneumonia on CXR and enhanced performance of non-radiologist physicians with the CAD assistance suggest that the CAD can support physicians in interpreting CXRs and helping image-based triage of COVID-19 patients in resource-constrained environment.", "journal": "PloS one", "date": "2021-06-08", "authors": ["Eui JinHwang", "Ki BeomKim", "Jin YoungKim", "Jae-KwangLim", "Ju GangNam", "HyewonChoi", "HyungjinKim", "Soon HoYoon", "Jin MoGoo", "Chang MinPark"], "doi": "10.1371/journal.pone.0252440\n10.1056/NEJMoa2002032\n10.1056/NEJMc2013020\n10.1056/NEJMc2001468\n10.1016/S2214-109X(20)30074-7\n10.3348/kjr.2020.0132\n10.1148/radiol.2020200463\n10.1148/radiol.2020200230\n10.1148/radiol.2020201365\n10.1148/ryct.2020200214\n10.1056/NEJMsb2005114\n10.1056/NEJMp2006141\n10.1148/radiol.2020201160\n10.1148/ryct.2020200107\n10.1148/radiol.2020201214\n10.1001/jamanetworkopen.2019.1095\n10.1007/s00330-019-06532-x\n10.1148/radiol.2018180237\n10.1148/radiol.2019191293\n10.1148/radiol.2019191225\n10.1148/radiol.2018180921\n10.1148/radiol.2020201874\n10.1148/radiol.2020203511\n10.3348/kjr.2020.0536\n10.1148/ryct.2020200047\n10.1016/j.rmed.2020.105980\n10.1148/radiol.2020200702\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.2214/AJR.20.22976\n10.1148/radiol.2020201754\n10.1016/S1473-3099(20)30086-4\n10.1186/s41747-020-00203-z\n10.1016/j.media.2020.101794"}
{"title": "Learning from imbalanced COVID-19 chest X-ray (CXR) medical imaging data.", "abstract": "The trendy task of digital medical image analysis has been continually evolving. It has been an area of prominent and growing importance from both research and deployment perspectives. Nonetheless, it is necessary to realize that the use of algorithms, methodology, as well as the source of medical image data, must be strictly scrutinized. As the COVID-19 pandemic has been gripping much of the world recently, there has been much efforts gone into developing affordable testing for the masses, and it has been shown that the established and widely available chest X-rays (CXR) images may be used as a screening criteria for assistive diagnosis purpose. Thanks to the dedicated work by various individuals and organizations, publicly available CXR of COVID-19 subjects are available for analytic usage. We have also provided a publicly available CXR dataset on the Kaggle platform. As a case study, this paper presents a systematic approach to learn from a typically imbalanced set of CXR images, which consists of a limited number of publicly available COVID-19 images. Our results show that we are able to outperform the top finishers in a related Kaggle multi-class CXR challenge. The proposed methodology should be able to help guide medical personnel in obtaining a robust diagnosis model to discern COVID-19 from other conditions confidently.", "journal": "Methods (San Diego, Calif.)", "date": "2021-06-07", "authors": ["Jonathan HChan", "ChenqiLi"], "doi": "10.1016/j.ymeth.2021.06.002\n10.1016/j.knosys.2020.106631\n10.1109/ACCESS.2020.2994762\n10.1016/j.compbiomed.2020.103792\n10.1186/s12938-020-00831-x\n10.1016/j.mehy.2020.109761\n10.1007/s10489-020-01829-7\n10.1145/3429210.3429213\n10.1145/3429210.3429216"}
{"title": "DUDA-Net: a double U-shaped dilated attention network for automatic infection area segmentation in COVID-19 lung CT images.", "abstract": "The global health crisis caused by coronavirus disease 2019 (COVID-19) is a common threat facing all humankind. In the process of diagnosing COVID-19 and treating patients, automatic COVID-19 lesion segmentation from computed tomography images helps doctors and patients intuitively understand lung infection. To effectively quantify lung infections, a convolutional neural network for automatic lung infection segmentation based on deep learning is proposed.\nThis new type of COVID-19 lesion segmentation network is based on a U-Net backbone. First, a coarse segmentation network is constructed to extract the lung areas. Second, in the encoding and decoding process of the fine segmentation network, a new soft attention mechanism, namely the dilated convolutional attention (DCA) mechanism, is introduced to enable the network to focus on better quantitative information to strengthen the network's segmentation ability in the subtle areas of the lesions.\nThe experimental results show that the average Dice similarity coefficient (DSC), sensitivity (SEN), specificity (SPE) and area under the curve of DUDA-Net are 87.06%, 90.85%, 99.59% and 0.965, respectively. In addition, the introduction of a cascade U-shaped network scheme and DCA mechanism can improve the DSC by 24.46% and 14.33%, respectively.\nThe proposed DUDA-Net approach can automatically segment COVID-19 lesions with excellent performance, which indicates that the proposed method is of great clinical significance. In addition, the introduction of a coarse segmentation network and DCA mechanism can improve the COVID-19 segmentation performance.", "journal": "International journal of computer assisted radiology and surgery", "date": "2021-06-06", "authors": ["FengXie", "ZhengHuang", "ZhengjinShi", "TianyuWang", "GuoliSong", "BolunWang", "ZihongLiu"], "doi": "10.1007/s11548-021-02418-w\n10.1007/s11356-021-13249-2\n10.1016/j.chaos.2020.110170\n10.1016/j.mehy.2020.109761\n10.1164/rccm.200407-127OC\n10.1080/00207454.2021.1883602\n10.1155/2021/6653879\n10.1016/j.compbiomed.2020.104037\n10.1002/cem.1349\n10.17977/um017v2i12019p41-46\n10.1016/j.bspc.2020.101926"}
{"title": "Imaging of COVID-19: An update of current evidences.", "abstract": "Coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has been reported as a global emergency. As respiratory dysfunction is a major clinical presentation of COVID-19, chest computed tomography (CT) plays a central role in the diagnosis and management of patients with COVID-19. Recent advances in imaging approaches using artificial intelligence have been essential as a quantification and diagnostic tool to differentiate COVID-19 from other respiratory infectious diseases. Furthermore, cardiovascular involvement in patients with COVID-19 is not negligible and may result in rapid worsening of the disease and sudden death. Cardiac magnetic resonance imaging can accurately depict myocardial involvement in SARS-CoV-2 infection. This review summarizes the role of the radiology department in the management and the diagnosis of COVID-19, with a special emphasis on ultra-high-resolution CT findings, cardiovascular complications and the potential of artificial intelligence.", "journal": "Diagnostic and interventional imaging", "date": "2021-06-06", "authors": ["ShingoKato", "YoshinobuIshiwata", "RyoAoki", "TaeIwasawa", "EriHagiwara", "TakashiOgura", "DaisukeUtsunomiya"], "doi": "10.1016/j.diii.2021.05.006\n10.1007/s.11604.021.01118.4"}
{"title": "Patient-specific COVID-19 resource utilization prediction using fusion AI model.", "abstract": "The strain on healthcare resources brought forth by the recent COVID-19 pandemic has highlighted the need for efficient resource planning and allocation through the prediction of future consumption. Machine learning can predict resource utilization such as the need for hospitalization based on past medical data stored in electronic medical records (EMR). We conducted this study on 3194 patients (46% male with mean age 56.7 (\u00b116.8), 56% African American, 7% Hispanic) flagged as COVID-19 positive cases in 12 centers under Emory Healthcare network from February 2020 to September 2020, to assess whether a COVID-19 positive patient's need for hospitalization can be predicted at the time of RT-PCR test using the EMR data prior to the test. Five main modalities of EMR, i.e., demographics, medication, past medical procedures, comorbidities, and laboratory results, were used as features for predictive modeling, both individually and fused together using late, middle, and early fusion. Models were evaluated in terms of precision, recall, F1-score (within 95% confidence interval). The early fusion model is the most effective predictor with 84% overall F1-score [CI 82.1-86.1]. The predictive performance of the model drops by 6 % when using recent clinical data while omitting the long-term medical history. Feature importance analysis indicates that history of cardiovascular disease, emergency room visits in the past year prior to testing, and demographic factors are predictive of the disease trajectory. We conclude that fusion modeling using medical history and current treatment data can forecast the need for hospitalization for patients infected with COVID-19 at the time of the RT-PCR test.", "journal": "NPJ digital medicine", "date": "2021-06-05", "authors": ["AmaraTariq", "Leo AnthonyCeli", "Janice MNewsome", "SaptarshiPurkayastha", "Neal KumarBhatia", "HariTrivedi", "Judy WawiraGichoya", "ImonBanerjee"], "doi": "10.1038/s41746-021-00461-0\n10.1016/S0140-6736(20)30566-3\n10.1016/j.amepre.2020.05.002\n10.1101/2020.05.06.20092999v1\n10.3390/jcm9061668\n10.1093/infdis/jiaa663\n10.1148/radiol.2020202723\n10.1101/2020.04.22.20075416v3\n10.1038/s41746-019-0211-0\n10.1016/j.dsx.2020.03.016\n10.1016/j.kint.2020.03.005\n10.1089/thy.2020.0363\n10.7554/eLife.58227\n10.1101/2020.04.28.20075788v1\n10.1093/jamia/ocaa105\n10.1093/infdis/jiaa372\n10.1001/jamaoncol.2020.6178\n10.1161/CIRCULATIONAHA.114.014508\n10.1001/jamanetworkopen.2019.8719\n10.1016/j.media.2020.101844\n10.1038/s41551-020-00633-5"}
{"title": "Digital imaging, technologies and artificial intelligence applications during COVID-19 pandemic.", "abstract": "The advancement of technology remained an immersive interest for humankind throughout the past decades. Tech enterprises offered a stream of innovation to address the universal healthcare concerns. The novel coronavirus holds a substantial foothold of planet earth which is combatted by digital interventions across afflicted geographical boundaries and territories. This study aims to explore the trends of modern healthcare technologies and Artificial Intelligence (AI) during COVID-19 crisis, define the concepts and clinical role of AI in the mitigation of COVID-19, investigate and correlate the efficacy of AI-enabled technology in medical imaging during COVID-19 and determine advantages, drawbacks, and challenges of artificial intelligence during COVID-19 pandemic. The paper applied systematic review approach using a deliberated research protocol and Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow chart. Digital technologies can coordinate COVID-19 responses in a cascade fashion that extends from the clinical care facility to the exterior of the pending viral epicenter. With cases of healthcare robotics, aerial drones, and the internet of things as evidentiary examples. PCR tests and medical imaging are the frontier diagnostics of COVID-19. Computed tomography helped to correct the accuracy variation of PCR tests at a clinical sensitivity of 98 %. Artificial intelligence can enable autonomous COVID-19 responses using techniques like machine learning. Technology could be an endless system of innovation and opportunities when sourced effectively. Scientists can utilize technology to resolve global concerns challenging the history of tangible possibility. Digital interventions have enhanced the responses to COVID-19, magnified the role of medical imaging amid the COVID-19 crisis and have exposed healthcare professionals to the opportunity of contactless care.", "journal": "Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society", "date": "2021-06-04", "authors": ["MustafaAlhasan", "MohamedHasaneen"], "doi": "10.1016/j.compmedimag.2021.101933\n10.2196/19104\n10.3390/ijerph17155330\n10.1142/S0219622020500285\n10.1016/j.cmpb.2020.105617\n10.1016/j.jiph.2020.06.028\n10.1016/b978-0-12-824313-8.00006-1\n10.1007/s40846-020-00529-4\n10.22114/ajem.v4i2s.451\n10.1148/radiol.2020201491\n10.1007/s10916-020-01617-3\n10.1007/s11739-020-02512-y\n10.1038/s41591-020-1011-4\n10.1002/uog.22055\n10.1016/j.radi.2020.04.017\n10.1109/ACCESS.2020.3010287\n10.1213/ANE.0000000000004929\n10.1016/j.ijinfomgt.2020.102182\n10.1007/s11547-020-01232-9\n10.1186/s42234-020-00050-8\n10.1016/B978-0-12-802302-0.00002-9\n10.1016/j.ejrad.2020.109009\n10.1126/science.abc0473\n10.1002/jmrs.423\n10.1097/RCT.0000000000001073\n10.2196/19284\n10.4414/smw.2020.20304\n10.1007/s11604-020-00967-9\n10.1016/j.clinimag.2020.08.014\n10.1007/BF00209432\n10.1016/j.amjmed.2020.08.019\n10.1016/j.media.2020.101800\n10.1038/s41467-020-17971-2\n10.2196/19338\n10.3366/E135555020800009X\n10.2307/40033668\n10.1109/ACCESS.2020.3007939\n10.3348/kjr.2020.0536\n10.1016/j.ihj.2020.04.001\n10.1056/NEJMp2005835\n10.1093/shm/7.1.59\n10.15520/jmbas.v8i8.247\n10.2196/19569\n10.1016/j.dsx.2020.05.008\n10.1148/ryai.2020200053\n10.1016/j.chaos.2020.110059\n10.2214/AJR.20.22954\n10.1148/radiol.2020200905\n10.1007/s00330-020-07042-x\n10.1016/j.jclinepi.2009.06.006\n10.1093/jtm/taaa080\n10.1016/j.scitotenv.2020.138858\n10.1148/ryct.2020200210\n10.18502/jebhpme.v4i2.3438\n10.1016/j.dsx.2020.08.029\n10.1101/2020.04.12.20062661\n10.1016/j.ejrad.2020.109256\n10.1259/bjr.20200538\n10.1007/s00146-020-00978-0\n10.1007/s00330-020-07044-9\n10.21203/rs.3.rs-33150/v1\n10.1016/j.ejrad.2020.109236\n10.1016/j.compbiomed.2020.103792\n10.7507/1001-5515.202003045\n10.1093/jamia/ocaa078\n10.1148/ryai.2020190043\n10.1089/tmj.2020.0140\n10.1067/j.cpradiol.2020.06.009\n10.34172/hpp.2020.27\n10.1109/RBME.2020.2987975\n10.1007/s00259-020-04929-1\n10.1016/j.ultrasmedbio.2020.05.012\n10.1148/radiol.2020204226\n10.1016/j.chaos.2020.110338\n10.1007/s12195-020-00629-w\n10.3892/etm.2020.8797\n10.1007/s10994-020-05928-x\n10.1016/j.crad.2020.03.004\n10.1148/radiol.2020203511\n10.1016/S2589-7500(20)30142-4\n10.1016/j.eng.2020.04.010\n10.1186/s12938-020-00807-x\n10.1038/s41598-017-13481-2\n10.1148/radiol.2020202944\n10.1007/s00259-020-04953-1\n10.1371/journal.pone.0236621"}
{"title": "Covid-19 Imaging Tools: How Big Data is Big?", "abstract": "In this paper, considering year 2020 and Covid-19, we analyze medical imaging tools and their performance scores in accordance with the dataset size and their complexity. For this, we mainly consider AI-driven tools that employ two different types of image data, namely chest Computed Tomography (CT) and X-ray. We elaborate on their strengths and weaknesses by taking the following important factors into account: i) dataset size; ii) model fitting criteria (over-fitting and under-fitting); iii) transfer learning in the deep learning era; and iv) data augmentation. Medical imaging tools do not explicitly analyze model fitting. Also, using transfer learning, with fewer data, one could possibly build Covid-19 deep learning model but they are limited to education and training. We observe that, in both image modalities, neither the dataset size nor does data augmentation work well for Covid-19 screening purposes because a large dataset does not guarantee all possible Covid-19 manifestations and data augmentation does not create new Covid-19 cases.", "journal": "Journal of medical systems", "date": "2021-06-04", "authors": ["K CSantosh", "SourodipGhosh"], "doi": "10.1007/s10916-021-01747-2\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200642\n10.1109/RBME.2020.2987975\n10.1016/S2589-7500(20)30054-6\n10.1016/j.dsx.2020.04.012\n10.1038/s41591-020-0931-3\n10.1109/TMI.2020.2993291\n10.1109/TMI.2020.2996645\n10.1007/s10916-020-01562-1\n10.1007/s10916-020-01645-z\n10.1007/s10916-020-01668-6\n10.1007/s10916-020-01678-4\n10.1007/s10096-020-03901-z\n10.3390/e22050517\n10.1016/j.eng.2020.04.010\n10.1016/j.ejrad.2020.109041\n10.1016/j.compbiomed.2020.104037\n10.1016/j.compbiomed.2020.103795\n10.2196/19569\n10.3390/s21020455\n10.1007/s00330-020-07044-9\n10.1016/j.asoc.2020.106885\n10.1016/j.mehy.2020.109761\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103792\n10.3390/app10134640\n10.1016/j.eswa.2020.114054\n10.1016/j.chaos.2020.109944\n10.1016/j.asoc.2020.106580\n10.1016/j.chaos.2020.110122\n10.1016/j.cmpb.2020.105608\n10.1016/j.bbe.2020.08.008\n10.1016/j.cmpb.2020.105581\n10.1007/s13755-020-00116-6\n10.1016/j.asoc.2020.106691\n10.1007/s13246-020-00888-x"}
{"title": "Impact of fully automated assessment on interstudy reproducibility of biventricular volumes and function in cardiac magnetic resonance imaging.", "abstract": "Cardiovascular magnetic resonance (CMR) imaging provides reliable assessments of biventricular morphology and function. Since manual post-processing is time-consuming and prone to observer variability, efforts have been directed towards novel artificial intelligence-based fully automated analyses. Hence, we sought to investigate the impact of artificial intelligence-based fully automated assessments on the inter-study variability of biventricular volumes and function. Eighteen participants (11 with normal, 3 with heart failure and preserved and 4 with reduced ejection fraction (EF)) underwent serial CMR imaging at in median 63\u00a0days (range 49-87) interval. Short axis cine stacks were acquired for the evaluation of left ventricular (LV) mass, LV and right ventricular (RV) end-diastolic, end-systolic and stroke volumes as well as EF. Assessments were performed manually (QMass, Medis Medical Imaging Systems, Leiden, Netherlands) by an experienced (3\u00a0years) and inexperienced reader (no active reporting, 45\u00a0min of training with five cases from the SCMR consensus data) as well as fully automated (suiteHEART, Neosoft, Pewaukee, WI, USA) without any manual corrections. Inter-study reproducibility was overall excellent with respect to LV volumetric indices, best for the experienced observer (intraclass correlation coefficient (ICC)\u2009>\u20090.98, coefficient of variation (CoV,\u2009<\u20099.6%) closely followed by automated analyses (ICC\u2009>\u20090.93, CoV\u2009<\u200912.4%) and lowest for the inexperienced observer (ICC\u2009>\u20090.86, CoV\u2009<\u200918.8%). Inter-study reproducibility of RV volumes was excellent for the experienced observer (ICC\u2009>\u20090.88, CoV\u2009<\u200910.7%) but considerably lower for automated and inexperienced manual analyses (ICC\u2009>\u20090.69 and\u2009>\u20090.46, CoV\u2009<\u200922.8% and\u2009<\u200928.7% respectively). In this cohort, fully automated analyses allowed reliable serial investigations of LV volumes with comparable inter-study reproducibility to manual analyses performed by an experienced CMR observer. In contrast, RV automated quantification with current algorithms still relied on manual post-processing for reliability.", "journal": "Scientific reports", "date": "2021-06-04", "authors": ["S\u00f6ren JBackhaus", "AndreasSchuster", "TorbenLange", "ChristianStehning", "MarcusBilling", "JoachimLotz", "BurkertPieske", "GerdHasenfu\u00df", "SebastianKelle", "Johannes TKowallick"], "doi": "10.1038/s41598-021-90702-9\n10.1161/CIRCULATIONAHA.108.811547\n10.1016/j.jacc.2014.06.1194\n10.1056/NEJMoa013474\n10.1161/01.CIR.76.1.44\n10.1016/S0002-9149(02)02381-0\n10.1016/j.ahj.2003.10.005\n10.1016/j.media.2010.12.004\n10.1002/mrm.23127\n10.1016/j.jcmg.2014.04.015\n10.1002/jmri.21948\n10.1148/radiol.2401050471\n10.1093/ehjci/jev247\n10.1186/s12968-019-0532-9\n10.1161/JAHA.120.016612\n10.1186/1532-429X-15-91\n10.1186/1532-429X-15-35\n10.1186/s12968-020-00610-6\n10.1186/s12968-015-0170-9\n10.1186/1532-429X-14-43\n10.1016/S0140-6736(86)90837-8\n10.1186/s12968-016-0225-6\n10.1080/10976640701545073\n10.1186/s12968-019-0575-y\n10.1002/mrm.28437\n10.1016/j.jcmg.2017.11.034\n10.1016/j.jcmg.2017.10.024\n10.1161/CIRCIMAGING.115.004077\n10.1093/eurheartj/ehw128\n10.1093/eurheartj/ehq025\n10.1080/10976640500295516\n10.1136/heartjnl-2020-317184"}
{"title": "Proof of concept for real-time detection of SARS CoV-2 infection with an electronic nose.", "abstract": "Rapid diagnosis is key to curtailing the Covid-19 pandemic. One path to such rapid diagnosis may rely on identifying volatile organic compounds (VOCs) emitted by the infected body, or in other words, identifying the smell of the infection. Consistent with this rationale, dogs can use their nose to identify Covid-19 patients. Given the scale of the pandemic, however, animal deployment is a challenging solution. In contrast, electronic noses (eNoses) are machines aimed at mimicking animal olfaction, and these can be deployed at scale. To test the hypothesis that SARS CoV-2 infection is associated with a body-odor detectable by an eNose, we placed a generic eNose in-line at a drive-through testing station. We applied a deep learning classifier to the eNose measurements, and achieved real-time detection of SARS CoV-2 infection at a level significantly better than chance, for both symptomatic and non-symptomatic participants. This proof of concept with a generic eNose implies that an optimized eNose may allow effective real-time diagnosis, which would provide for extensive relief in the Covid-19 pandemic.", "journal": "PloS one", "date": "2021-06-03", "authors": ["KobiSnitz", "MichalAndelman-Gur", "LironPinchover", "ReutWeissgross", "AharonWeissbrod", "EvaMishor", "RoniZoller", "VeraLinetsky", "AbebeMedhanie", "SagitShushan", "EliJaffe", "NoamSobel"], "doi": "10.1371/journal.pone.0252121\n10.1002/cbic.201300695\n10.1002/anie.201500153\n10.1371/journal.pone.0192629\n10.1021/cr068121q\n10.1073/pnas.92.7.2652\n10.1088/1752-7155/10/3/036001\n10.1371/journal.pone.0217963\n10.1371/journal.pone.0188879\n10.1371/journal.pone.0135199\n10.1371/journal.pone.0151715\n10.1371/journal.pone.0132227\n10.1186/s12879-020-05281-3\n10.1088/1752-7163/aba105\n10.1088/1752-7155/7/1/016002\n10.1586/14737159.2015.1043895\n10.1016/j.rmed.2015.09.014\n10.1016/j.tube.2012.10.002\n10.1111/cea.12052\n10.1371/journal.pone.0115584\n10.1016/j.lungcan.2011.08.009\n10.1164/rccm.200409-1184OC\n10.1097/JNC.0000000000000146\n10.1021/acsnano.0c05657\n10.1007/s00464-020-08169-0\n10.1126/sciadv.abc5801\n10.1038/s41593-020-00758-5\n10.3390/s100201062\n10.1128/JCM.00512-20\n10.1371/journal.pone.0200605\n10.1148/radiol.2020200432"}
{"title": "COV-SNET: A deep learning model for X-ray-based COVID-19 classification.", "abstract": "The AI research community has recently been intensely focused on diagnosing COVID-19 by applying deep learning technology to the X-ray scans taken of COVID-19 patients. Differentiating COVID-19 from other pneumonia-inducing illnesses is a highly challenging task as it shares many of the same imaging characteristics as other pulmonary diseases. This is especially true given the small number of COVID-19 X-rays that are publicly available. Deep learning experts commonly use transfer learning to offset the small number of images typically available in medical imaging tasks. Our COV-SNET model is a deep neural network that was pretrained on over one hundred thousand X-ray images. In this paper, we designed two COV-SNET models with the purpose of diagnosing COVID-19. The experimental results demonstrate the robustness of our deep learning models, ultimately achieving sensitivities of 95% for our three-class and two-class models. We also discuss the strengths and weaknesses of such an approach, focusing mainly on the limitations of public X-ray datasets on current COVID-19 deep learning models. Finally, we conclude with possible future directions for this research.", "journal": "Informatics in medicine unlocked", "date": "2021-06-03", "authors": ["RobertHertel", "RachidBenlamri"], "doi": "10.1016/j.imu.2021.100620\n10.1016/j.patrec.2020.09.010\n10.1148/radiol.2020200642\n10.1007/s00500-020-05424-3\n10.1007/s13246-020-00865-4\n10.1136/bmj.m2426\n10.1007/s11547-020-01232-9\n10.1148/radiol.2020200432\n10.1016/j.imu.2020.100412\n10.1016/j.asoc.2020.106744\n10.1016/j.cell.2018.02.010\n10.7326/M20-1495\n10.1007/s00330-020-06823-8\n10.1016/j.jinf.2020.03.051\n10.1016/j.chaos.2020.110190\n10.1016/j.imu.2020.100360\n10.1109/ACCESS.2020.3003810\n10.1148/radiol.2020200274\n10.1109/ACCESS.2020.2994762\n10.1038/s41598-020-76550-z\n10.1109/CVPR.2017.369"}
{"title": "Automated AI-Driven CT Quantification of Lung Disease Predicts Adverse Outcomes in Patients Hospitalized for COVID-19 Pneumonia.", "abstract": "The purpose of our work was to assess the independent and incremental value of AI-derived quantitative determination of lung lesions extent on initial CT scan for the prediction of clinical deterioration or death in patients hospitalized with COVID-19 pneumonia. 323 consecutive patients (mean age 65 \u00b1 15 years, 192 men), with laboratory-confirmed COVID-19 and an abnormal chest CT scan, were admitted to the hospital between March and December 2020. The extent of consolidation and all lung opacities were quantified on an initial CT scan using a 3D automatic AI-based software. The outcome was known for all these patients. 85 (26.3%) patients died or experienced clinical deterioration, defined as intensive care unit admission. In multivariate regression based on clinical, biological and CT parameters, the extent of all opacities, and extent of consolidation were independent predictors of adverse outcomes, as were diabetes, heart disease, C-reactive protein, and neutrophils/lymphocytes ratio. The association of CT-derived measures with clinical and biological parameters significantly improved the risk prediction (", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-06-03", "authors": ["Marie LaureChabi", "Oph\u00e9lieDana", "TitouanKennel", "AlexiaGence-Breney", "H\u00e9l\u00e8neSalvator", "Marie ChristineBallester", "MarcVasse", "Anne LaureBrun", "Fran\u00e7oisMellot", "Philippe AGrenier"], "doi": "10.3390/diagnostics11050878\n10.1001/jama.2020.6775\n10.1136/bmj.m1985\n10.1001/jama.2020.7202\n10.1001/jamainternmed.2020.0994\n10.1016/S2213-2600(20)30161-2\n10.1001/jamainternmed.2020.2033\n10.1093/cid/ciaa414\n10.1136/bmj.m3339\n10.1136/bmj.m1328\n10.1016/S0140-6736(20)30566-3\n10.1148/radiol.2020200843\n10.1148/radiol.2020200463\n10.1148/radiol.2020200370\n10.1148/radiol.2020201433\n10.1371/journal.pone.0230548\n10.1097/RLI.0000000000000689\n10.1148/ryct.2020200130\n10.1148/ryct.2020200047\n10.1007/s00330-020-06817-6\n10.1016/j.jrid.2020.04.004\n10.1148/ryct.2020200441\n10.1038/s41467-020-20657-4\n10.1016/j.media.2020.101860\n10.1016/j.cell.2020.08.029\n10.1148/ryai.2020200048\n10.1038/s41379-020-0536-x\n10.1016/S1473-3099(20)30086-4\n10.1148/ryct.2020200389\n10.1007/s00134-020-05991-x\n10.1016/j.jaci.2020.04.006\n10.3389/fmicb.2019.02752\n10.1002/jmv.26750\n10.1186/s40001-020-00432-3\n10.3389/fmed.2020.625176"}
{"title": "Automated tracking of emergency department abdominal CT findings during the COVID-19 pandemic using natural language processing.", "abstract": "During the COVID-19 pandemic, emergency department (ED) volumes have fluctuated. We hypothesized that natural language processing (NLP) models could quantify changes in detection of acute abdominal pathology (acute appendicitis (AA), acute diverticulitis (AD), or bowel obstruction (BO)) on CT reports.\nThis retrospective study included 22,182 radiology reports from CT abdomen/pelvis studies performed at an urban ED between January 1, 2018 to August 14, 2020. Using a subset of 2448 manually annotated reports, we trained random forest NLP models to classify the presence of AA, AD, and BO in report impressions. Performance was assessed using 5-fold cross validation. The NLP classifiers were then applied to all reports.\nThe NLP classifiers for AA, AD, and BO demonstrated cross-validation classification accuracies between 0.97 and 0.99 and F1-scores between 0.86 and 0.91. When applied to all CT reports, the estimated numbers of AA, AD, and BO cases decreased 43-57% in April 2020 (first regional peak of COVID-19 cases) compared to 2018-2019. However, the number of abdominal pathologies detected rebounded in May-July 2020, with increases above historical averages for AD. The proportions of CT studies with these pathologies did not significantly increase during the pandemic period.\nDramatic decreases in numbers of acute abdominal pathologies detected by ED CT studies were observed early on during the COVID-19 pandemic, though these numbers rapidly rebounded. The proportions of CT cases with these pathologies did not increase, which suggests patients deferred care during the first pandemic peak. NLP can help automatically track findings in ED radiology reporting.", "journal": "The American journal of emergency medicine", "date": "2021-06-02", "authors": ["Matthew DLi", "Peter AWood", "Tarik KAlkasab", "Michael HLev", "JayashreeKalpathy-Cramer", "Marc DSucci"], "doi": "10.1016/j.ajem.2021.05.057\n10.1016/j.ajem.2020.10.081\n10.15585/mmwr.mm6923e1\n10.1016/j.jacr.2020.08.010\n10.1016/j.jacr.2020.05.004\n10.1016/j.jacr.2020.04.025\n10.1016/j.jacr.2020.07.028\n10.1016/j.acra.2020.08.008\n10.1016/j.jacr.2020.06.002\n10.1148/radiol.16142770\n10.3174/ajnr.A6961\n10.1007/s10140-020-01854-6\n10.1007/s10140-020-01865-3\n10.1016/j.jacr.2020.09.020\n10.1007/s10278-011-9426-6"}
{"title": "Early prediction of severity in coronavirus disease (COVID-19) using quantitative CT imaging.", "abstract": "To evaluate whether the extent of COVID-19 pneumonia on CT scans using quantitative CT imaging obtained early in the illness can predict its future severity.\nWe conducted a retrospective single-center study on confirmed COVID-19 patients between January 18, 2020 and March 5, 2020. A quantitative AI algorithm was used to evaluate each patient's CT scan to determine the proportion of the lungs with pneumonia (VR) and the rate of change (RAR) in VR from scan to scan. Patients were classified as being in the severe or non-severe group based on their final symptoms. Penalized B-splines regression modeling was used to examine the relationship between mean VR and days from onset of symptoms in the two groups, with 95% and 99% confidence intervals.\nMedian VR max was 18.6% (IQR 9.1-32.7%) in 21 patients in the severe group, significantly higher (P\u00a0<\u00a00.0001) than in the 53 patients in non-severe group (1.8% (IQR 0.4-5.7%)). RAR was increasing with a median RAR of 2.1% (IQR 0.4-5.5%) in severe and 0.4% (IQR 0.1-0.9%) in non-severe group, which was significantly different (P\u00a0<\u00a00.0001). Penalized B-spline analyses showed positive relationships between VR and days from onset of symptom. The 95% confidence limits of the predicted means for the two groups diverged 5\u00a0days after the onset of initial symptoms with a threshold of 11.9%.\nFive days after the initial onset of symptoms, CT could predict the patients who later developed severe symptoms with 95% confidence.", "journal": "Clinical imaging", "date": "2021-06-01", "authors": ["KunweiLi", "XueguoLiu", "RowenaYip", "David FYankelevitz", "Claudia IHenschke", "YayuanGeng", "YijieFang", "WenjuanLi", "CunxuePan", "XiaojunChen", "PeixinQin", "YinghuaZhong", "KunfengLiu", "ShaolinLi"], "doi": "10.1016/j.clinimag.2021.02.003\n10.1016/S0140-6736(20)30728-5\n10.3760/cma.j.issn.1005-1201.2020.0001\n10.1148/radiol.2020200230\n10.1148/ryct.2020200152\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1148/rg.2020200159\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200075"}
{"title": "The Promise of AI in Detection, Diagnosis, and Epidemiology for Combating COVID-19: Beyond the Hype.", "abstract": "COVID-19 has created enormous suffering, affecting lives, and causing deaths. The ease with which this type of coronavirus can spread has exposed weaknesses of many healthcare systems around the world. Since its emergence, many governments, research communities, commercial enterprises, and other institutions and stakeholders around the world have been fighting in various ways to curb the spread of the disease. Science and technology have helped in the implementation of policies of many governments that are directed toward mitigating the impacts of the pandemic and in diagnosing and providing care for the disease. Recent technological tools, artificial intelligence (AI) tools in particular, have also been explored to track the spread of the coronavirus, identify patients with high mortality risk and diagnose patients for the disease. In this paper, areas where AI techniques are being used in the detection, diagnosis and epidemiological predictions, forecasting and social control for combating COVID-19 are discussed, highlighting areas of successful applications and underscoring issues that need to be addressed to achieve significant progress in battling COVID-19 and future pandemics. Several AI systems have been developed for diagnosing COVID-19 using medical imaging modalities such as chest CT and X-ray images. These AI systems mainly differ in their choices of the algorithms for image segmentation, classification and disease diagnosis. Other AI-based systems have focused on predicting mortality rate, long-term patient hospitalization and patient outcomes for COVID-19. AI has huge potential in the battle against the COVID-19 pandemic but successful practical deployments of these AI-based tools have so far been limited due to challenges such as limited data accessibility, the need for external evaluation of AI models, the lack of awareness of AI experts of the regulatory landscape governing the deployment of AI tools in healthcare, the need for clinicians and other experts to work with AI experts in a multidisciplinary context and the need to address public concerns over data collection, privacy, and protection. Having a dedicated team with expertise in medical data collection, privacy, access and sharing, using federated learning whereby AI scientists hand over training algorithms to the healthcare institutions to train models locally, and taking full advantage of biomedical data stored in biobanks can alleviate some of problems posed by these challenges. Addressing these challenges will ultimately accelerate the translation of AI research into practical and useful solutions for combating pandemics.", "journal": "Frontiers in artificial intelligence", "date": "2021-06-01", "authors": ["MusaAbdulkareem", "Steffen EPetersen"], "doi": "10.3389/frai.2021.652669\n10.1016/j.scs.2020.102571\n10.1148/radiol.2020200642\n10.1371/journal.pone.0213653\n10.1016/j.jiph.2020.06.028\n10.1007/s00500-014-1484-5\n10.3390/jcm9030674\n10.1007/s10916-019-1338-x\n10.1007/s10916-018-1064-9\n10.1038/s41591-020-0820-9\n10.1038/280361a0\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103795\n10.1099/mgen.0.000397\n10.1038/s41398-018-0360-y\n10.1093/gerona/glaa183\n10.1038/s41591-020-1009-y\n10.2196/preprints.19526\n10.1007/s10916-020-01617-3\n10.1016/S0031-3203(02)00257-1\n10.1038/s42256-020-00254-2\n10.1162/neco.1989.1.3.295\n10.1016/j.artmed.2012.12.003\n10.1148/radiol.2020200463\n10.1177/2472630319890316\n10.3390/diagnostics10040231\n10.3390/ijerph17093176\n10.1023/A:1010933404324\n10.1016/j.ijmedinf.2018.01.007\n10.1145/3394486.3412865\n10.1111/1751-7915.13557\n10.1613/jair.1.12162\n10.12688/wellcomeopenres.15819.1\n10.1136/bmj.m2672\n10.1016/S0140-6736(20)30154-9\n10.1038/s41467-020-19393-6\n10.1109/TNN.2009.2015974\n10.1613/jair.953\n10.1101/2020.02.25.20021568\n10.1080/10659360500468468\n10.1038/d41586-020-00694-1\n10.1016/S0140-6736(20)30421-9\n10.1145/2939672.2939785\n10.1007/s40475-020-00201-6\n10.1093/ije/dyr120\n10.1093/ije/dyi174\n10.1038/s41562-020-0909-7\n10.3115/v1/W14-4012\n10.1148/radiol.2020200230\n10.1093/cid/ciaa1155\n10.1371/journal.pntd.0004549\n10.1016/j.chaos.2020.110057\n10.1007/BF00994018\n10.1109/TIT.1967.1053964\n10.1016/S1473-3099(20)30120-1\n10.1038/s42256-020-00252-4\n10.1148/radiol.2020200432\n10.9781/ijimai.2020.02.002\n10.1145/2810103.2813677\n10.1093/ije/dym091\n10.1038/s42256-020-0207-0\n10.1136/bmj.m1847\n10.47102/annals-acadmedsg.202057\n10.1016/j.dsx.2020.05.046\n10.1109/MCSE.2020.3019744\n10.1142/9789814583220_0020\n10.1109/TIT.1968.1054155\n10.1101/2020.03.30.20047787\n10.1016/j.dsx.2020.04.050\n10.1109/TKDE.2008.239\n10.1109/CVPR.2016.90\n10.1101/2020.02.08.20021162\n10.1145/3133956.3134012\n10.1162/neco.1997.9.8.1735\n10.1101/2020.05.23.112284\n10.1109/101.8118\n10.1002/9781118548387\n10.3389/frai.2020.00041\n10.1016/S0140-6736(20)30183-5\n10.1101/2020.03.23.20041608\n10.1016/j.imu.2020.100378\n10.31219/osf.io/cev6x\n10.1017/CBO9780511662935\n10.1007/s10676-020-09568-6\n10.1109/2.485891\n10.1109/ACCESS.2020.3001973\n10.1109/21.256541\n10.12688/f1000research.26707.2\n10.1001/jama.2020.6585\n10.32604/cmc.2020.010691\n10.1109/JSEN.2020.3004568\n10.1101/2020.10.06.20207209\n10.1101/2020.03.20.20039834\n10.3390/v12040372\n10.1016/j.jiph.2019.03.020\n10.1016/j.jacbts.2016.11.010\n10.1038/s42256-020-0186-1\n10.1016/S0140-6736(20)30553-5\n10.1109/TMI.2020.2992546\n10.1148/radiol.2020200241\n10.1109/TPAMI.2002.1017616\n10.5582/bst.2020.01482\n10.1016/j.chaos.2020.110056\n10.1101/2020.09.18.20197319\n10.1101/2020.03.22.20041079\n10.1038/s41564-020-0713-1\n10.1016/S1473-3099(20)30162-6\n10.1016/j.jacr.2020.02.008\n10.1007/s10462-007-9052-3\n10.2139/ssrn.3562458\n10.1016/S2468-2667(20)30157-2\n10.1145/3065386\n10.1016/j.dsx.2020.05.008\n10.1016/j.cose.2021.102248\n10.1016/j.chaos.2020.110059\n10.1007/3-540-48229-6_9\n10.1038/nature14539\n10.2196/medinform.7744\n10.1038/d41586-020-03518-4\n10.1148/radiol.2020200905\n10.1109/TCSS.2020.2980007\n10.1016/j.acra.2020.03.003\n10.1109/MSP.2020.2975749\n10.3390/ijms21082826\n10.1016/S0140-6736(20)30251-8\n10.1117/12.2588672\n10.1109/IRI49571.2020.00033\n10.1038/s41598-020-75912-x\n10.1186/s12968-019-0551-6\n10.1038/280455a0\n10.1007/BF00173264\n10.1146/annurev-virology-012420-022445\n10.1093/intqhc/mzaa117\n10.1007/s00146-020-00978-0\n10.1016/j.chaos.2020.109846\n10.36227/techrxiv.12743933\n10.1016/j.neucom.2010.11.024\n10.1016/j.compbiomed.2020.103792\n10.1148/radiol.2020200370\n10.3201/eid2610.201315\n10.21437/Interspeech.2017-1428\n10.1103/RevModPhys.87.925\n10.1108/DPM-05-2020-0164\n10.1016/j.mayocp.2020.11.024\n10.3389/fcvm.2019.00133\n10.1186/s12968-017-0327-9\n10.1016/S2213-2600(20)30161-2\n10.1101/2020.03.30.20047308\n10.1038/s41564-020-0768-z\n10.1101/2020.02.29.20029603\n10.1186/s40249-020-00650-1\n10.1038/s42256-020-00253-3\n10.1016/j.chaos.2020.110337\n10.1109/TPAMI.2016.2577031\n10.1038/s41598-019-52501-1\n10.1016/j.chaos.2020.109853\n10.1038/s41746-020-00323-1\n10.1186/s12968-019-0523-x\n10.1038/clpt.2008.89\n10.1161/CIRCRESAHA.118.314119\n10.1016/j.jaut.2020.102433\n10.2139/ssrn.3546547\n10.1007/s11263-015-0816-y\n10.1136/bmj.k4168\n10.1001/jamacardio.2018.1717\n10.4414/smw.2020.20225\n10.1016/j.dsx.2020.06.060\n10.3389/fdgth.2021.564906\n10.5858/arpa.2020-0901-SA\n10.1146/annurev-bioeng-071516-044442\n10.1109/RBME.2020.2987975\n10.1088/1361-6560/abe838\n10.2139/ssrn.3546089\n10.1136/amiajnl-2013-001935\n10.1001/jama.2018.17163\n10.4161/viru.24041\n10.1109/TCBB.2021.3065361\n10.1007/s10554-017-1225-9\n10.1016/j.compbiomed.2020.103960\n10.1109/CVPR.2016.308\n10.1088/1361-6560/abbf9e\n10.2807/1560-7917.ES.2020.25.5.200131e\n10.1038/s41591-020-0819-2\n10.1111/j.2517-6161.1996.tb02080.x\n10.1186/s40246-020-00288-y\n10.1021/acsnano.0c02624\n10.1016/j.jceh.2020.04.019\n10.1016/j.dsx.2020.04.012\n10.1016/j.cell.2020.02.058\n10.1128/JVI.00127-20\n10.1001/jama.2020.3151\n10.1126/sciadv.aaw3538\n10.1038/s41598-020-76550-z\n10.1101/2020.02.14.20023028\n10.1109/TMI.2020.2995965\n10.1109/CVPR.2017.369\n10.1002/jmv.25748\n10.1148/radiol.2020201160\n10.1038/sj.tpj.6500085\n10.1101/2020.04.02.20051136\n10.1016/S0140-6736(20)30260-9\n10.1136/bmj.m1328\n10.1038/s41597-020-0448-0\n10.1007/s41666-020-00082-4\n10.1016/j.eng.2020.04.010\n10.1038/s41591-020-0817-4\n10.1038/s42256-020-0180-7\n10.1101/2020.02.27.20028027\n10.1016/j.media.2019.101552\n10.1007/s00521-019-04325-3\n10.1001/jama.2020.4861\n10.1109/TMI.2020.3040950\n10.3390/jcm9020388\n10.1016/S0140-6736(20)30566-3\n10.1016/S2589-7500(20)30192-8\n10.31083/j.rcm.2020.03.120"}
{"title": "Radiologists versus Deep Convolutional Neural Networks: A Comparative Study for Diagnosing COVID-19.", "abstract": "The reverse transcriptase polymerase chain reaction (RT-PCR) is still the routinely used test for the diagnosis of SARS-CoV-2 (COVID-19). However, according to several reports, RT-PCR showed a low sensitivity and multiple tests may be required to rule out false negative results. Recently, chest computed tomography (CT) has been an efficient tool to diagnose COVID-19 as it is directly affecting the lungs. In this paper, we investigate the application of pre-trained models in diagnosing patients who are positive for COVID-19 and differentiating it from normal patients, who tested negative for coronavirus. The study aims to compare the generalization capabilities of deep learning models with two thoracic radiologists in diagnosing COVID-19 chest CT images. A dataset of 3000 images was obtained from the Near East Hospital, Cyprus, and used to train and to test the three employed pre-trained models. In a test set of 250 images used to evaluate the deep neural networks and the radiologists, it was found that deep networks (ResNet-18, ResNet-50, and DenseNet-201) can outperform the radiologists in terms of higher accuracy (97.8%), sensitivity (98.1%), specificity (97.3%), precision (98.4%), and F1-score (198.25%), in classifying COVID-19 images.", "journal": "Computational and mathematical methods in medicine", "date": "2021-06-01", "authors": ["AbdulkaderHelwan", "Mohammad Khaleel SallamMa'aitah", "HaniHamdan", "Dilber UzunOzsahin", "OzumTuncyurek"], "doi": "10.1155/2021/5527271\n10.1155/2020/9756518\n10.1148/rg.2020200159\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200823\n10.1148/radiol.2020200432\n10.1038/nature21056\n10.1016/S2589-7500(19)30123-2\n10.1038/s41591-020-0931-3\n10.1007/s11263-015-0816-y\n10.3233/JIFS-172261\n10.1145/3306241\n10.1109/TMI.2020.2995965\n10.1007/s10096-020-03901-z\n10.1007/s10489-020-01826-w\n10.1080/07391102.2020.1788642\n10.2196/19569\n10.1038/s41467-019-11786-6\n10.1166/jmihi.2019.2654"}
{"title": "Checklist for responsible deep learning modeling of medical images based\u00a0on COVID-19 detection studies.", "abstract": "The sudden outbreak and\u00a0uncontrolled spread of\u00a0COVID-19 disease is one of\u00a0the\u00a0most important global problems today. In\u00a0a\u00a0short period of\u00a0time, it has led to the\u00a0development of\u00a0many deep neural network models for\u00a0COVID-19 detection with modules for\u00a0explainability. In\u00a0this work, we carry out a\u00a0systematic analysis of\u00a0various aspects of\u00a0proposed models. Our analysis revealed numerous mistakes made at different stages of\u00a0data acquisition, model development, and\u00a0explanation construction. In\u00a0this work, we overview the\u00a0approaches proposed in\u00a0the\u00a0surveyed Machine Learning articles and\u00a0indicate typical errors emerging from the\u00a0lack of\u00a0deep understanding of\u00a0the\u00a0radiography domain. We present the\u00a0perspective of\u00a0both: experts in\u00a0the\u00a0field - radiologists and\u00a0deep learning engineers dealing with model explanations. The final result is a\u00a0proposed checklist with the minimum conditions to be met by\u00a0a\u00a0reliable COVID-19 diagnostic model.", "journal": "Pattern recognition", "date": "2021-06-01", "authors": ["WeronikaHryniewska", "Przemys\u0142awBombi\u0144ski", "PatrykSzatkowski", "PaulinaTomaszewska", "ArturPrzelaskowski", "Przemys\u0142awBiecek"], "doi": "10.1016/j.patcog.2021.108035\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/radiol.2020201160\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.2214/AJR.20.22954\n10.1148/ryct.2020200028\n10.1148/radiol.2020200432\n10.1148/radiol.2020200230\n10.1148/radiol.2020200823\n10.1148/radiol.2020201160\n10.1016/j.jacr.2020.03.011\n10.1016/j.ejrad.2020.108961\n10.1016/j.clinimag.2020.04.001\n10.1016/j.patcog.2018.05.014\n10.1016/j.patcog.2021.107856\n10.1016/j.patcog.2020.107413\n10.1016/j.patcog.2019.01.031\n10.1038/s41591-018-0268-3\n10.1016/j.patcog.2009.05.012\n10.1109/FG.2017.95\n10.1016/j.cmpb.2019.06.005\n10.1109/ISBI.2018.8363592\n10.1109/NSS/MIC42101.2019.9060056\n10.1371/journal.pmed.1000100\n10.1016/j.cmpb.2020.105608\n10.1109/TMI.2020.2996256\n10.1109/BIBM49941.2020.9313304\n10.1109/tmi.2020.2993291\n10.1109/tmi.2020.2995508\n10.1016/j.compbiomed.2020.103792\n10.1109/JBHI.2020.3037127\n10.3892/etm.2020.8797\n10.1016/j.mehy.2020.109761\n10.1038/s41598-020-76550-z\n10.1101/2020.07.08.20149161\n10.1101/2020.04.14.20065722\n10.20944/preprints202005.0151.v1\n10.1101/2020.05.24.20111922\n10.3348/kjr.2020.0132\n10.1007/978-3-030-23979-4_8\n10.1136/bmj.m1328\n10.1016/j.cobme.2018.12.005\n10.1016/j.jiph.2020.06.028\n10.18653/v1/2020.acl-main.408"}
{"title": "COVID-19 classification of X-ray images using deep neural networks.", "abstract": "In the midst of the coronavirus disease 2019 (COVID-19) outbreak, chest X-ray (CXR) imaging is playing an important role in diagnosis and monitoring of patients with COVID-19. We propose a deep learning model for detection of COVID-19 from CXRs, as well as a tool for retrieving similar patients according to the model's results on their CXRs. For training and evaluating our model, we collected CXRs from inpatients hospitalized in four different hospitals.\nIn this retrospective study, 1384 frontal CXRs, of COVID-19 confirmed patients imaged between March and August 2020, and 1024 matching CXRs of non-COVID patients imaged before the pandemic, were collected and used to build a deep learning classifier for detecting patients positive for COVID-19. The classifier consists of an ensemble of pre-trained deep neural networks (DNNS), specifically, ReNet34, ReNet50\u00b8 ReNet152, and vgg16, and is enhanced by data augmentation and lung segmentation. We further implemented a nearest-neighbors algorithm that uses DNN-based image embeddings to retrieve the images most similar to a given image.\nOur model achieved accuracy of 90.3%, (95% CI: 86.3-93.7%) specificity of 90% (95% CI: 84.3-94%), and sensitivity of 90.5% (95% CI: 85-94%) on a test dataset comprising 15% (350/2326) of the original images. The AUC of the ROC curve is 0.96 (95% CI: 0.93-0.97).\nWe provide deep learning models, trained and evaluated on CXRs that can assist medical efforts and reduce medical staff workload in handling COVID-19.\n\u2022 A machine learning model was able to detect chest X-ray (CXR) images of patients tested positive for COVID-19 with accuracy and detection rate above 90%. \u2022 A tool was created for finding existing CXR images with imaging characteristics most similar to a given CXR, according to the model's image embeddings.", "journal": "European radiology", "date": "2021-05-31", "authors": ["DaphnaKeidar", "DanielYaron", "ElishaGoldstein", "YairShachar", "AyeletBlass", "LeonidCharbinsky", "IsraelAharony", "LizaLifshitz", "DimitriLumelsky", "ZivNeeman", "MattiMizrachi", "MajdHajouj", "NethanelEizenbach", "EyalSela", "Chedva SWeiss", "PhilipLevin", "OferBenjaminov", "Gil NBachar", "ShlomitTamir", "YaelRapson", "DrorSuhami", "EliAtar", "Amiel ADror", "Naama RBogot", "AhuvaGrubstein", "NogahShabshin", "Yishai MElyada", "Yonina CEldar"], "doi": "10.1007/s00330-021-08050-1\n10.1148/ryct.2020200028\n10.1007/s00330-020-06731-x\n10.1148/ryct.2020200034\n10.1007/978-3-030-42750-4_6\n10.1109/MIS.2009.36"}
{"title": "Deep learning for predicting COVID-19 malignant progression.", "abstract": "As COVID-19 is highly infectious, many patients can simultaneously flood into hospitals for diagnosis and treatment, which has greatly challenged public medical systems. Treatment priority is often determined by the symptom severity based on first assessment. However, clinical observation suggests that some patients with mild symptoms may quickly deteriorate. Hence, it is crucial to identify patient early deterioration to optimize treatment strategy. To this end, we develop an early-warning system with deep learning techniques to predict COVID-19 malignant progression. Our method leverages CT scans and the clinical data of outpatients and achieves an AUC of 0.920 in the single-center study. We also propose a domain adaptation approach to improve the generalization of our model and achieve an average AUC of 0.874 in the multicenter study. Moreover, our model automatically identifies crucial indicators that contribute to the malignant progression, including Troponin, Brain natriuretic peptide, White cell count, Aspartate aminotransferase, Creatinine, and Hypersensitive C-reactive protein.", "journal": "Medical image analysis", "date": "2021-05-30", "authors": ["CongFang", "SongBai", "QianlanChen", "YuZhou", "LimingXia", "LixinQin", "ShiGong", "XudongXie", "ChunhuaZhou", "DandanTu", "ChangzhengZhang", "XiaowuLiu", "WeiweiChen", "XiangBai", "Philip H STorr"], "doi": "10.1016/j.media.2021.102096"}
{"title": "Toward understanding COVID-19 pneumonia: a deep-learning-based approach for severity analysis and monitoring the disease.", "abstract": "We report a new approach using artificial intelligence (AI) to study and classify the severity of COVID-19 using 1208 chest X-rays (CXRs) of 396 COVID-19 patients obtained through the course of the disease at Emory Healthcare affiliated hospitals (Atlanta, GA, USA). Using a two-stage transfer learning technique to train a convolutional neural network (CNN), we show that the algorithm is able to classify four classes of disease severity (normal, mild, moderate, and severe) with the average Area Under the Curve (AUC) of 0.93. In addition, we show that the outputs of different layers of the CNN under dominant filters provide valuable insight about the subtle patterns in the CXRs, which can improve the accuracy in the reading of CXRs by a radiologist. Finally, we show that our approach can be used for studying the disease progression in a single patient and its influencing factors. The results suggest that our technique can form the foundation of a more concrete clinical model to predict the evolution of COVID-19 severity and the efficacy of different treatments for each patient through using CXRs and clinical data in the early stages of the disease. This use of AI to assess the severity and possibly predicting the future stages of the disease early on, will be essential in dealing with the upcoming waves of COVID-19 and optimizing resource allocation and treatment.", "journal": "Scientific reports", "date": "2021-05-29", "authors": ["MohammadrezaZandehshahvar", "Marlyvan Assen", "HosseinMaleki", "YasharKiarashi", "Carlo NDe Cecco", "AliAdibi"], "doi": "10.1038/s41598-021-90411-3\n10.1148/radiol.11092149\n10.1148/radiol.2020200702\n10.1016/j.acra.2020.03.003\n10.1038/s41467-020-17971-2\n10.1038/s41746-020-00322-2\n10.1371/journal.pmed.1002686\n10.1038/s41598-019-56847-4\n10.1016/j.compbiomed.2020.103869\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.asoc.2020.106744\n10.1371/journal.pone.0236621\n10.1109/MSP.2017.2693418\n10.1109/TKDE.2009.191"}
{"title": "CT radiomic models to distinguish COVID-19 pneumonia from other interstitial pneumonias.", "abstract": "To classify COVID-19, COVID-19-like and non-COVID-19 interstitial pneumonia using lung CT radiomic features.\nCT data of 115 patients with respiratory symptoms suspected for COVID-19 disease were retrospectively analyzed. Based on the results of nasopharyngeal swab, patients were divided into two main groups, COVID-19 positive (C\u2009+) and COVID-19 negative (C-), respectively. C- patients, however, presented with interstitial lung involvement. A subgroup of C-, COVID-19-like (CL), were considered as highly suggestive of COVID pneumonia at CT. Radiomic features were extracted from the whole lungs. A dual machine learning (ML) model approach was used. The first one excluded CL patients from the training set, eventually included on the test set. The second model included the CL patients also in the training set.\nThe first model classified C\u2009+\u2009and C- pneumonias with AUC of 0.83. CL median response (0.80) was more similar to C\u2009+\u2009(0.92) compared to C- (0.17). Radiomic footprints of CL were similar to the C\u2009+\u2009ones (possibly false negative swab test). The second model, however, merging C\u2009+\u2009with CL patients in the training set, showed a slight decrease in classification performance (AUC\u2009=\u20090.81).\nWhole lung ML models based on radiomics can classify C\u2009+\u2009and C- interstitial pneumonia. This may help in the correct management of patients with clinical and radiological stigmata of COVID-19, however presenting with a negative swab test. CL pneumonia was similar to C\u2009+\u2009pneumonia, albeit with slightly different radiomic footprints.", "journal": "La Radiologia medica", "date": "2021-05-28", "authors": ["Nicol\u00f2Cardobi", "GiulioBenetti", "GiuseppeCardano", "CinziaArena", "ClaudioMicheletto", "CarloCavedon", "StefaniaMontemezzi"], "doi": "10.1007/s11547-021-01370-8\n10.1186/s40779-020-00240-0\n10.1001/jamacardio.2020.1096\n10.1016/j.ijid.2020.03.062\n10.3348/kjr.2020.0146\n10.1148/radiol.2020200432\n10.1148/radiol.2020200463\n10.1148/ryct.2020200034\n10.1016/j.ejca.2011.11.036\n10.1148/radiol.2015151169\n10.1016/j.ejrad.2019.06.019\n10.21873/anticanres.12803\n10.1038/s41598-018-38459-6\n10.1007/s00330-017-5236-7\n10.1016/j.crad.2018.08.014\n10.1186/s12880-019-0355-z\n10.1007/s10637-017-0524-2\n10.1101/2020.02.29.20029603\n10.1007/s11432-020-2849-3\n10.1148/radiol.2020201473\n10.1148/radiol.2020191145\n10.1101/2020.04.28.20082966\n10.7150/thno.46428\n10.1097/RTI.0000000000000544\n10.1186/s12938-020-00809-9\n10.1007/s00330-020-07032-z\n10.1038/s41586-020-2196-x\n10.3389/fmed.2020.558539\n10.26355/eurrev_202004_21035"}
{"title": "DIAG a Diagnostic Web Application Based on Lung CT Scan Images and Deep Learning.", "abstract": "Coronavirus disease is a pandemic that has infected millions of people around the world. Lung CT-scans are effective diagnostic tools, but radiologists can quickly become overwhelmed by the flow of infected patients. Therefore, automated image interpretation needs to be achieved. Deep learning (DL) can support critical medical tasks including diagnostics, and DL algorithms have successfully been applied to the classification and detection of many diseases. This work aims to use deep learning methods that can classify patients between Covid-19 positive and healthy patient. We collected 4 available datasets, and tested our convolutional neural networks (CNNs) on different distributions to investigate the generalizability of our models. In order to clearly explain the predictions, Grad-CAM and Fast-CAM visualization methods were used. Our approach reaches more than 92% accuracy on 2 different distributions. In addition, we propose a computer aided diagnosis web application for Covid-19 diagnosis. The results suggest that our proposed deep learning tool can be integrated to the Covid-19 detection process and be useful for a rapid patient management.", "journal": "Studies in health technology and informatics", "date": "2021-05-28", "authors": ["Amel ImeneHadj Bouzid", "SaidYahiaoui", "AnisLounis", "Sid-AhmedBerrani", "Hac\u00e8neBelbachir", "Qa\u00efsNa\u00efli", "Mohamed El HafedhAbdi", "KawtharBensalah", "DjamalBelazzougui"], "doi": "10.3233/SHTI210175"}
{"title": "COVID-19 Image Segmentation Based on Deep Learning and Ensemble Learning.", "abstract": "Medical imaging offers great potential for COVID-19 diagnosis and monitoring. Our work introduces an automated pipeline to segment areas of COVID-19 infection in CT scans using deep convolutional neural networks. Furthermore, we evaluate the performance impact of ensemble learning techniques (Bagging and Augmenting). Our models showed highly accurate segmentation results, in which Bagging achieved the highest dice similarity coefficient.", "journal": "Studies in health technology and informatics", "date": "2021-05-28", "authors": ["PhilipMeyer", "DominikM\u00fcller", "I\u00f1akiSoto-Rey", "FrankKramer"], "doi": "10.3233/SHTI210223"}
{"title": "4S-DT: Self-Supervised Super Sample Decomposition for Transfer Learning With Application to COVID-19 Detection.", "abstract": "Due to the high availability of large-scale annotated image datasets, knowledge transfer from pretrained models showed outstanding performance in medical image classification. However, building a robust image classification model for datasets with data irregularity or imbalanced classes can be a very challenging task, especially in the medical imaging domain. In this article, we propose a novel deep convolutional neural network, which we called self-supervised super sample decomposition for transfer learning (4S-DT) model. The 4S-DT encourages a coarse-to-fine transfer learning from large-scale image recognition tasks to a specific chest X-ray image classification task using a generic self-supervised sample decomposition approach. Our main contribution is a novel self-supervised learning mechanism guided by a super sample decomposition of unlabeled chest X-ray images. 4S-DT helps in improving the robustness of knowledge transformation via a downstream learning strategy with a class-decomposition (CD) layer to simplify the local structure of the data. The 4S-DT can deal with any irregularities in the image dataset by investigating its class boundaries using a downstream CD mechanism. We used 50000 unlabeled chest X-ray images to achieve our coarse-to-fine transfer learning with an application to COVID-19 detection, as an exemplar. The 4S-DT has achieved a high accuracy of 99.8% on the larger of the two datasets used in the experimental study and an accuracy of 97.54% on the smaller dataset, which was enriched by augmented images, out of which all real COVID-19 cases were detected.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-05-27", "authors": ["AsmaaAbbas", "Mohammed MAbdelsamea", "Mohamed MedhatGaber"], "doi": "10.1109/TNNLS.2021.3082015"}
{"title": "Artificial intelligence as a fundamental tool in management of infectious diseases and its current implementation in COVID-19 pandemic.", "abstract": "The world has never been prepared for global pandemics like the COVID-19, currently posing an immense threat to the public and consistent pressure on the global healthcare systems to navigate optimized tools, equipments, medicines, and techno-driven approaches to retard the infection spread. The synergized outcome of artificial intelligence paradigms and human-driven control measures elicit a significant impact on screening, analysis, prediction, and tracking the currently infected individuals, and likely the future patients, with precision and accuracy, generating regular international and national data on confirmed, recovered, and death cases, as the current status of 3,820,869 infected patients worldwide. Artificial intelligence is a frontline concept, with time-saving, cost-effective, and productive access to disease management, rendering positive results in physician assistance in high workload conditions, radiology imaging, computational tomography, and database formulations, to facilitate availability of information accessible to researchers all over the globe. The review tends to elaborate the role of industry 4.0 technology, fast diagnostic procedures, and convolutional neural networks, as artificial intelligence aspects, in potentiating the COVID-19 management criteria and differentiating infection in SARS-CoV-2 positive and negative groups. Therefore, the review successfully supplements the processes of vaccine development, disease management, diagnosis, patient records, transmission inhibition, social distancing, and future pandemic predictions, with artificial intelligence revolution and smart techno processes to ensure that the human race wins this battle with COVID-19 and many more combats in the future.", "journal": "Environmental science and pollution research international", "date": "2021-05-27", "authors": ["IshnoorKaur", "TapanBehl", "LotfiAleya", "HabiburRahman", "ArunKumar", "SandeepArora", "Israt JahanBulbul"], "doi": "10.1007/s11356-021-13823-8\n10.3390/ijerph13080757\n10.3390/v12030254\n10.1016/j.compbiomed.2020.103795\n10.21037/jtd.2017.03.157\n10.1038/nm.4381\n10.1016/j.crad.2004.07.008\n10.1371/journal.pcbi.1004185\n10.1371/journal.pntd.0004549\n10.1016/j.procs.2019.09.158\n10.7861/futurehosp.6-2-94\n10.1021/acs.jcim.6b00004\n10.1016/j.scitotenv.2020.138858\n10.1038/srep19218\n10.1016/S0933-3657(02)00053-2\n10.1016/j.ajic.2015.03.005\n10.1049/trit.2018.1008\n10.1002/jbio.201800101\n10.1016/j.compbiomed.2018.10.011\n10.1016/j.jcot.2018.09.015\n10.1016/j.ajem.2020.04.022\n10.1001/jama.2018.11100\n10.1016/j.dsx.2020.04.032\n10.1093/nar/gkw1004\n10.1136/svn-2017-000101\n10.1016/j.promfg.2020.01.189\n10.1186/1471-2105-15-276\n10.1038/s41746-019-0189-7\n10.1039/B603402K\n10.1016/j.cmi.2019.09.014\n10.4293/jsls.2018.00018\n10.1073/pnas.1006113108\n10.1016/j.artmed.2018.01.003\n10.1016/j.trpro.2017.05.209\n10.1016/j.dsx.2020.04.012\n10.21037/tcr.2018.05.02\n10.1371/journal.pntd.0005756\n10.1016/j.cmpb.2016.06.005\n10.4269/ajtmh.16-0648\n10.2196/19866\n10.1016/j.csbj.2018.02.001\n10.1371/journal.pone.0088075\n10.1158/1078-0432.ccr-04-2587"}
{"title": "Attention-RefNet: Interactive Attention Refinement Network for Infected Area Segmentation of COVID-19.", "abstract": "COVID-19 pneumonia is a disease that causes an existential health crisis in many people by directly affecting and damaging lung cells. The segmentation of infected areas from computed tomography (CT) images can be used to assist and provide useful information for COVID-19 diagnosis. Although several deep learning-based segmentation methods have been proposed for COVID-19 segmentation and have achieved state-of-the-art results, the segmentation accuracy is still not high enough (approximately 85%) due to the variations of COVID-19 infected areas (such as shape and size variations) and the similarities between COVID-19 and non-COVID-infected areas. To improve the segmentation accuracy of COVID-19 infected areas, we propose an interactive attention refinement network (Attention RefNet). The interactive attention refinement network can be connected with any segmentation network and trained with the segmentation network in an end-to-end fashion. We propose a skip connection attention module to improve the important features in both segmentation and refinement networks and a seed point module to enhance the important seeds (positions) for interactive refinement. The effectiveness of the proposed method was demonstrated on public datasets (COVID-19CTSeg and MICCAI) and our private multicenter dataset. The segmentation accuracy was improved to more than 90%. We also confirmed the generalizability of the proposed network on our multicenter dataset. The proposed method can still achieve high segmentation accuracy.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-05-26", "authors": ["TitinuntKitrungrotsakul", "QingqingChen", "HuitaoWu", "YutaroIwamoto", "HongjieHu", "WenchaoZhu", "ChaoChen", "FangyiXu", "YongZhou", "LanfenLin", "RuofengTong", "JingsongLi", "Yen-WeiChen"], "doi": "10.1109/JBHI.2021.3082527\n10.1186/s12931-020-01440-x\n10.1002/ima.22527\n10.1109/ICPR.2008.476133"}
{"title": "COVID-19 diagnosis by routine blood tests using machine learning.", "abstract": "Physicians taking care of patients with COVID-19 have described different changes in routine blood parameters. However, these changes hinder them from performing COVID-19 diagnoses. We constructed a machine learning model for COVID-19 diagnosis that was based and cross-validated on the routine blood tests of 5333 patients with various bacterial and viral infections, and 160 COVID-19-positive patients. We selected the operational ROC point at a sensitivity of 81.9% and a specificity of 97.9%. The cross-validated AUC was 0.97. The five most useful routine blood parameters for COVID-19 diagnosis according to the feature importance scoring of the XGBoost algorithm were: MCHC, eosinophil count, albumin, INR, and prothrombin activity percentage. t-SNE visualization showed that the blood parameters of the patients with a severe COVID-19 course are more like the parameters of a bacterial than a viral infection. The reported diagnostic accuracy is at least comparable and probably complementary to RT-PCR and chest CT studies. Patients with fever, cough, myalgia, and other symptoms can now have initial routine blood tests assessed by our diagnostic tool. All patients with a positive COVID-19 prediction would then undergo standard RT-PCR studies to confirm the diagnosis. We believe that our results represent a significant contribution to improvements in COVID-19 diagnosis.", "journal": "Scientific reports", "date": "2021-05-26", "authors": ["Matja\u017eKukar", "GregorGun\u010dar", "Toma\u017eVovko", "SimonPodnar", "Peter\u010cernel\u010d", "MiranBrvar", "MatejaZalaznik", "MatejaNotar", "Sa\u0161oMo\u0161kon", "MarkoNotar"], "doi": "10.1038/s41598-021-90265-9\n10.1056/NEJMoa2001017\n10.1038/s41564-020-0695-z\n10.3201/eid2607.200282\n10.1016/S0140-6736(20)30183-5\n10.1080/22221751.2020.1745095\n10.3348/kjr.2020.0146\n10.1148/radiol.2020200642\n10.1016/j.tmaid.2020.101623\n10.1038/s41598-017-18564-8\n10.1038/s41598-019-51147-3\n10.1373/49.1.1\n10.1038/s41467-019-13056-x\n10.1214/aos/1013203451\n10.1016/S0167-9473(01)00065-2\n10.1016/j.asoc.2018.12.024\n10.1186/1471-2105-14-106\n10.1214/ss/1009213286\n10.15252/emmm.202012560\n10.3389/fimmu.2020.01626"}
{"title": "CovFrameNet: An Enhanced Deep Learning Framework for COVID-19 Detection.", "abstract": "The novel coronavirus, also known as COVID-19, is a pandemic that has weighed heavily on the socio-economic affairs of the world. Research into the production of relevant vaccines is progressively being advanced with the development of the Pfizer and BioNTech, AstraZeneca, Moderna, Sputnik V, Janssen, Sinopharm, Valneva, Novavax and Sanofi Pasteur vaccines. There is, however, a need for a computational intelligence solution approach to mediate the process of facilitating quick detection of the disease. Different computational intelligence methods, which comprise natural language processing, knowledge engineering, and deep learning, have been proposed in the literature to tackle the spread of coronavirus disease. More so, the application of deep learning models have demonstrated an impressive performance compared to other methods. This paper aims to advance the application of deep learning and image pre-processing techniques to characterise and detect novel coronavirus infection. Furthermore, the study proposes a framework named CovFrameNet., which consist of a pipelined image pre-processing method and a deep learning model for feature extraction, classification, and performance measurement. The novelty of this study lies in the design of a CNN architecture that incorporates an enhanced image pre-processing mechanism. The National Institutes of Health (NIH) Chest X-Ray dataset and COVID-19 Radiography database were used to evaluate and validate the effectiveness of the proposed deep learning model. Results obtained revealed that the proposed model achieved an accuracy of 0.1, recall/precision of 0.85, F-measure of 0.9, and specificity of 1.0. Thus, the study's outcome showed that a CNN-based method with image pre-processing capability could be adopted for the pre-screening of suspected COVID-19 cases, and the confirmation of RT-PCR-based detected cases of COVID-19.", "journal": "IEEE access : practical innovations, open solutions", "date": "2021-05-25", "authors": ["Olaide NathanielOyelade", "Absalom El-ShamirEzugwu", "HarunaChiroma"], "doi": "10.1109/ACCESS.2021.3083516\n10.1136/bmj.m1328\n10.1007/s10096-020-03901-z\n10.1080/07391102.2020.1788642\n10.1007/s10489-020-02076-6\n10.1101/2020.05.18.20105577\n10.1016/j.imu.2020.100395\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01829-7\n10.1016/j.imu.2020.100405\n10.1016/j.chaos.2020.110338\n10.1152/physiolgenomics.00029.2020\n10.1016/j.patcog.2020.107700\n10.1007/s40009-020-01009-8\n10.1007/s10489-020-01904-z\n10.1007/s10489-020-02010-w\n10.1007/s13246-020-00934-8\n10.1148/radiol.2020200432\n10.1109/CVPR.2017.369\n10.1186/s40537-019-0197-0\n10.1109/RTEICT.2016.7808140\n10.1088/1742-6596/762/1/012035\n10.3390/sym10110648\n10.3389/fmed.2020.00427\n10.1016/j.chaos.2020.109944\n10.1186/s12890-020-01286-5\n10.22059/jitm.2020.79187\n10.32604/cmc.2021.014265\n10.1007/s00500-020-05424-3\n10.32604/cmc.2021.012955\n10.1109/JIOT.2021.3050775\n10.1109/ACCESS.2020.2995597\n10.32604/cmc.2021.012874"}
{"title": "2D medical image segmentation via learning multi-scale contextual dependencies.", "abstract": "Automatic medical image segmentation plays an important role as a diagnostic aid in the identification of diseases and their treatment in clinical settings. Recently proposed methods based on Convolutional Neural Networks (CNNs) have demonstrated their potential in image processing tasks, including some medical image analysis tasks. Those methods can learn various feature representations with numerous weight-shared convolutional kernels, however, the missed diagnosis rate of regions of interest (ROIs) is still high in medical image segmentation. Two crucial factors behind this shortcoming, which have been overlooked, are small ROIs from medical images and the limited context information from the existing network models. In order to reduce the missed diagnosis rate of ROIs from medical images, we propose a new segmentation framework which enhances the representative capability of small ROIs (particularly in deep layers) and explicitly learns global contextual dependencies in multi-scale feature spaces. In particular, the local features and their global dependencies from each feature space are adaptively aggregated based on both the spatial and the channel dimensions. Moreover, some visualization comparisons of the learned features from our framework further boost neural networks' interpretability. Experimental results show that, in comparison to some popular medical image segmentation and general image segmentation methods, our proposed framework achieves the state-of-the-art performance on the liver tumor segmentation task with 91.18% Sensitivity, the COVID-19 lung infection segmentation task with 75.73% Sensitivity and the retinal vessel detection task with 82.68% Sensitivity. Moreover, it is possible to integrate (parts of) the proposed framework into most of the recently proposed Fully CNN-based models, in order to improve their effectiveness in medical image segmentation tasks.", "journal": "Methods (San Diego, Calif.)", "date": "2021-05-25", "authors": ["ShuchaoPang", "AnanDu", "ZhenmeiYu", "Mehmet AOrgun"], "doi": "10.1016/j.ymeth.2021.05.015"}
{"title": "Artificial intelligence in clinical care amidst COVID-19 pandemic: A systematic review.", "abstract": "The worldwide health crisis caused by the SARS-Cov-2 virus has resulted in>3 million deaths so far. Improving early screening, diagnosis and prognosis of the disease are critical steps in assisting healthcare professionals to save lives during this pandemic. Since WHO declared the COVID-19 outbreak as a pandemic, several studies have been conducted using Artificial Intelligence techniques to optimize these steps on clinical settings in terms of quality, accuracy and most importantly time. The objective of this study is to conduct a systematic literature review on published and preprint reports of Artificial Intelligence models developed and validated for screening, diagnosis and prognosis of the coronavirus disease 2019. We included 101 studies, published from January 1st, 2020 to December 30th, 2020, that developed AI prediction models which can be applied in the clinical setting. We identified in total 14 models for screening, 38 diagnostic models for detecting COVID-19 and 50 prognostic models for predicting ICU need, ventilator need, mortality risk, severity assessment or hospital length stay. Moreover, 43 studies were based on medical imaging and 58 studies on the use of clinical parameters, laboratory results or demographic features. Several heterogeneous predictors derived from multimodal data were identified. Analysis of these multimodal data, captured from various sources, in terms of prominence for each category of the included studies, was performed. Finally, Risk of Bias (RoB) analysis was also conducted to examine the applicability of the included studies in the clinical setting and assist healthcare providers, guideline developers, and policymakers.", "journal": "Computational and structural biotechnology journal", "date": "2021-05-25", "authors": ["Eleni SAdamidi", "KonstantinosMitsis", "Konstantina SNikita"], "doi": "10.1016/j.csbj.2021.05.010\n10.1056/NEJMoa2001017\n10.1148/radiol.2020200905\n10.1101/2020.07.07.20148361\n10.1101/2020.06.07.20124594\n10.1186/s12880-020-00521-z\n10.26355/eurrev_202008_22510\n10.1371/journal.pmed.1000100\n10.1016/S0003-4975(01)02870-3\n10.3389/fpubh.2020.587937\n10.2196/21788\n10.7326/M18-1376\n10.3389/fpubh.2020.00357\n10.1016/j.imu.2020.100360\n10.12788/fp.0045\n10.1016/j.chaos.2020.110245\n10.1016/S0140-6736(20)30183-5\n10.7150/thno.45985\n10.1007/0-387-25465-X_9\n10.1007/s13218-020-00636-z\n10.2196/24207\n10.1038/s41598-020-69250-1\n10.1136/heartjnl-2011-301246\n10.1016/j.ejrad.2020.109041\n10.1016/S2589-7500(20)30199-0\n10.1101/2020.03.19.20039099\n10.1016/j.intimp.2020.106705\n10.2139/ssrn.3541119\n10.1038/s41591-020-0931-3\n10.1016/j.media.2020.101824\n10.3389/fbioe.2020.00898\n10.1109/TMI.4210.1109/TMI.2020.2994459\n10.1109/Access.628763910.1109/ACCESS.2020.3025164\n10.1148/radiol.2020201491\n10.1101/2020.02.14.20023028\n10.3390/jcm9061668\n10.2196/24225\n10.1371/journal.pone.0243262\n10.1016/j.compbiomed.2020.103949\n10.1016/j.acra.2020.09.004\n10.1038/s41598-020-71114-7\n10.1017/S0950268820001727\n10.2139/ssrn.3582866\n10.1038/s41467-020-17280-8\n10.1016/j.chaos.2020.110153\n10.1109/JBHI.622102010.1109/JBHI.2020.3018181\n10.1109/JBHI.622102010.1109/JBHI.2020.3019505\n10.1016/j.chest.2020.12.009\n10.1016/j.patter.2020.100074\n10.2196/24018\n10.1007/s11739-020-02475-0\n10.1016/j.amsu.2020.09.044"}
{"title": "Echocardiographic Correlates of In-Hospital Death in Patients with Acute COVID-19 Infection: The World Alliance Societies of Echocardiography (WASE-COVID) Study.", "abstract": "The novel severe acute respiratory syndrome coronavirus-2 virus, which has led to the global coronavirus disease-2019 (COVID-19) pandemic is known to adversely affect the cardiovascular system through multiple mechanisms. In this international, multicenter study conducted by the World Alliance Societies of Echocardiography, we aim to determine the clinical and echocardiographic phenotype of acute cardiac disease in COVID-19 patients, to explore phenotypic differences in different geographic regions across the world, and to identify parameters associated with in-hospital mortality.\nWe studied 870 patients with acute COVID-19 infection from 13 medical centers in four world regions (Asia, Europe, United States, Latin America) who had undergone transthoracic echocardiograms. Clinical and laboratory data were collected, including patient outcomes. Anonymized echocardiograms were analyzed with automated, machine learning-derived algorithms to calculate left ventricular (LV) volumes, ejection fraction, and LV longitudinal strain (LS). Right-sided echocardiographic parameters that were measured included right ventricular (RV) LS, RV free-wall strain (FWS), and RV basal diameter. Multivariate regression analysis was performed to identify clinical and echocardiographic parameters associated with in-hospital mortality.\nSignificant regional differences were noted in terms of patient comorbidities, severity of illness, clinical biomarkers, and LV and RV echocardiographic metrics. Overall in-hospital mortality was 21.6%. Parameters associated with mortality in a multivariate analysis were age (odds ratio [OR]\u00a0=\u00a01.12 [1.05, 1.22], P\u00a0=\u00a0.003), previous lung disease (OR\u00a0=\u00a07.32 [1.56, 42.2], P\u00a0=\u00a0.015), LVLS (OR\u00a0=\u00a01.18 [1.05, 1.36], P\u00a0=\u00a0.012), lactic dehydrogenase (OR\u00a0=\u00a06.17 [1.74, 28.7], P\u00a0=\u00a0.009), and RVFWS (OR\u00a0=\u00a01.14 [1.04, 1.26], P\u00a0=\u00a0.007).\nLeft ventricular dysfunction is noted in approximately 20% and RV dysfunction in approximately 30% of patients with acute COVID-19 illness and portend a poor prognosis. Age at presentation, previous lung disease, lactic dehydrogenase, LVLS, and RVFWS were independently associated with in-hospital mortality. Regional differences in cardiac phenotype highlight the significant differences in patient acuity as well as echocardiographic utilization in different parts of the world.", "journal": "Journal of the American Society of Echocardiography : official publication of the American Society of Echocardiography", "date": "2021-05-24", "authors": ["IlyaKaragodin", "CristianeCarvalho Singulane", "Gary MWoodward", "MingxingXie", "Edwin STucay", "Ana CTude Rodrigues", "Zuilma YVasquez-Ortiz", "AzinAlizadehasl", "Mark JMonaghan", "Bayardo AOrdonez Salazar", "LaurieSoulat-Dufour", "AtoosaMostafavi", "AntonellaMoreo", "RodolfoCitro", "AkhilNarang", "ChunWu", "TineDescamps", "KarimaAddetia", "Roberto MLang", "Federico MAsch", "NoneNone"], "doi": "10.1016/j.echo.2021.05.010\n10.1161/CIR.0000000000000940\n10.1161/CIRCULATIONAHA.120.052278"}
{"title": "Dynamic deformable attention network (DDANet) for COVID-19 lesions semantic segmentation.", "abstract": "Deep learning based medical image segmentation is an important step within diagnosis, which relies strongly on capturing sufficient spatial context without requiring too complex models that are hard to train with limited labelled data. Training data is in particular scarce for segmenting infection regions of CT images of COVID-19 patients. Attention models help gather contextual information within deep networks and benefit semantic segmentation tasks. The recent criss-cross-attention module aims to approximate global self-attention while remaining memory and time efficient by separating horizontal and vertical self-similarity computations. However, capturing attention from all non-local locations can adversely impact the accuracy of semantic segmentation networks. We propose a new Dynamic Deformable Attention Network (DDANet) that enables a more accurate contextual information computation in a similarly efficient way. Our novel technique is based on a deformable criss-cross attention block that learns both attention coefficients and attention offsets in a continuous way. A deep U-Net (Schlemper et al., 2019) segmentation network that employs this attention mechanism is able to capture attention from pertinent non-local locations and also improves the performance on semantic segmentation tasks compared to criss-cross attention within a U-Net on a challenging COVID-19 lesion segmentation task. Our validation experiments show that the performance gain of the recursively applied dynamic deformable attention blocks comes from their ability to capture dynamic and precise attention context. Our DDANet achieves Dice scores of 73.4% and 61.3% for Ground-glass opacity and consolidation lesions for COVID-19 segmentation and improves the accuracy by 4.9% points compared to a baseline U-Net and 24.4% points compared to current state of art methods (Fan et al., 2020).", "journal": "Journal of biomedical informatics", "date": "2021-05-23", "authors": ["Kumar TRajamani", "HannaSiebert", "Mattias PHeinrich"], "doi": "10.1016/j.jbi.2021.103816"}
{"title": "Health Care Professional Association Agency in Preparing for Artificial Intelligence: Protocol for a Multi-Case Study.", "abstract": "The emergence of artificial intelligence (AI) in health care has impacted health care systems, including employment, training, education, and professional regulation. It is incumbent on health professional associations to assist their membership in defining and preparing for AI-related change. Health professional associations, or the national groups convened to represent the interests of the members of a profession, play a unique role in establishing the sociocultural, normative, and regulative elements of health care professions.\nThe aim of this paper is to present a protocol for a proposed study of how, when faced with AI as a disruptive technology, health professional associations engage in sensemaking and legitimization of change to support their membership in preparing for future practice.\nAn exploratory multi-case study approach will be used. This study will be informed by the normalization process theory (NPT), which suggests behavioral constructs required for complex change, providing a novel lens through which to consider the agency of macrolevel actors in practice change. A total of 4 health professional associations will be studied, each representing an instrumental case and related fields selected for their early consideration of AI technologies. Data collection will consist of key informant interviews, observation of relevant meetings, and document review. Individual and collective sensemaking activities and action toward change will be identified using stakeholder network mapping. A hybrid inductive and deductive model will be used for a concurrent thematic analysis, mapping emergent themes against the NPT framework to assess fit and identify areas of discordance.\nAs of January 2021, we have conducted 17 interviews, with representation across the 4 health professional associations. Of these 17 interviews, 15 (88%) have been transcribed. Document review is underway and complete for one health professional association and nearly complete for another. Observation opportunities have been challenged by competing priorities during COVID-19 and may require revisiting. A linear cross-case analytic approach will be taken to present the data, highlighting both guidance for the implementation of AI and implications for the application of NPT at the macro level. The ability to inform consideration of AI will depend on the degree to which the engaged health professional associations have considered this topic at the time of the study and, hence, what priority it has been assigned within the health professional association and what actions have been taken to consider or prepare for it. The fact that this may differ between health professional associations and practice environments will require consideration throughout the analysis.\nUltimately, this protocol outlines a case study approach to understand how, when faced with AI as a disruptive technology, health professional associations engage in sensemaking and legitimization of change to support their membership in preparing for future practice.\nDERR1-10.2196/27340.", "journal": "JMIR research protocols", "date": "2021-05-20", "authors": ["CaitlinGillan", "BrianHodges", "DavidWiljer", "MarkDobrow"], "doi": "10.2196/27340\n10.1016/j.jaci.2018.02.025\n10.1038/nature21056\n10.1038/s41568-018-0016-5\n10.1080/09613218.2013.737096\n10.1177/0268580903018002005\n10.2307/3069285\n10.1001/jama.2016.17438\n10.12927/cjnl.2016.24563\n10.1016/j.artmed.2008.07.017\n10.1016/j.carj.2018.02.002\n10.1016/j.radonc.2018.05.030\n10.1136/bmjopen-2015-008592\n10.1186/s13012-018-0758-1\n10.1186/s13012-018-0758-1\n10.1177/001139292040001004\n10.1177/1077800405284363\n10.1016/j.ejmp.2016.05.035\n10.1002/cncr.21324\n10.1002/cncr.21324\n10.1007/s12194-014-0259-0\n10.1038/nrclinonc.2012.194\n10.1002/chp.141\n10.1016/j.semradonc.2007.07.001\n10.1017/s1460396918000468\n10.3109/13561820903550796\n10.1001/jamanetworkopen.2018.6937\n10.1001/jamanetworkopen.2018.6937\n10.1016/j.annemergmed.2015.06.024\n10.1016/j.ijmedinf.2019.05.011\n10.2307/259247\n10.3310/hsdr04010"}
{"title": "Joint segmentation and detection of COVID-19 via a sequential region generation network.", "abstract": "The fast pandemics of coronavirus disease (COVID-19) has led to a devastating influence on global public health. In order to treat the disease, medical imaging emerges as a useful tool for diagnosis. However, the computed tomography (CT) diagnosis of COVID-19 requires experts' extensive clinical experience. Therefore, it is essential to achieve rapid and accurate segmentation and detection of COVID-19. This paper proposes a simple yet efficient and general-purpose network, called Sequential Region Generation Network (SRGNet), to jointly detect and segment the lesion areas of COVID-19. SRGNet can make full use of the supervised segmentation information and then outputs multi-scale segmentation predictions. Through this, high-quality lesion-areas suggestions can be generated on the predicted segmentation maps, reducing the diagnosis cost. Simultaneously, the detection results conversely refine the segmentation map by a post-processing procedure, which significantly improves the segmentation accuracy. The superiorities of our SRGNet over the state-of-the-art methods are validated through extensive experiments on the built COVID-19 database.", "journal": "Pattern recognition", "date": "2021-05-19", "authors": ["JipengWu", "ShengchuanZhang", "XiLi", "JieChen", "HaiboXu", "JiawenZheng", "YueGao", "YonghongTian", "YongshengLiang", "RongrongJi"], "doi": "10.1016/j.patcog.2021.108006"}
{"title": "Adapting for the COVID-19 pandemic in Ecuador, a characterization of hospital strategies and patients.", "abstract": "The World Health Organization (WHO) declared coronavirus disease-2019 (COVID-19) a global pandemic on 11 March 2020. In Ecuador, the first case of COVID-19 was recorded on 29 February 2020. Despite efforts to control its spread, SARS-CoV-2 overran the Ecuadorian public health system, which became one of the most affected in Latin America on 24 April 2020. The Hospital General del Sur de Quito (HGSQ) had to transition from a general to a specific COVID-19 health center in a short period of time to fulfill the health demand from patients with respiratory afflictions. Here, we summarized the implementations applied in the HGSQ to become a COVID-19 exclusive hospital, including the rearrangement of hospital rooms and a triage strategy based on a severity score calculated through an artificial intelligence (AI)-assisted chest computed tomography (CT). Moreover, we present clinical, epidemiological, and laboratory data from 75 laboratory tested COVID-19 patients, which represent the first outbreak of Quito city. The majority of patients were male with a median age of 50 years. We found differences in laboratory parameters between intensive care unit (ICU) and non-ICU cases considering C-reactive protein, lactate dehydrogenase, and lymphocytes. Sensitivity and specificity of the AI-assisted chest CT were 21.4% and 66.7%, respectively, when considering a score >70%; regardless, this system became a cornerstone of hospital triage due to the lack of RT-PCR testing and timely results. If health workers act as vectors of SARS-CoV-2 at their domiciles, they can seed outbreaks that might put 1,879,047 people at risk of infection within 15 km around the hospital. Despite our limited sample size, the information presented can be used as a local example that might aid future responses in low and middle-income countries facing respiratory transmitted epidemics.", "journal": "PloS one", "date": "2021-05-18", "authors": ["DanielGarzon-Chavez", "DanielRomero-Alvarez", "MarcoBonifaz", "JuanGaviria", "DanielMero", "NarcisaGunsha", "AsirisPerez", "Mar\u00edaGarcia", "HugoEspejo", "FranklinEspinosa", "EdisonLig\u00f1a", "MauricioEspinel", "EmmanuelleQuentin", "EnriqueTeran", "FranciscoMora", "JorgeReyes"], "doi": "10.1371/journal.pone.0251295\n10.1056/NEJMoa2001017\n10.1016/S1473-3099(20)30120-1\n10.1016/S0140-6736(20)30851-5\n10.1016/S2214-109X(18)30245-6\n10.1016/S2213-2600(20)30071-0\n10.1016/S2213-2600(20)30114-4\n10.1056/nejmoa2002032\n10.1148/radiol.2020200843\n10.1001/jama.2020.1585\n10.1007/s00330-020-06748-2\n10.1016/S0140-6736(20)30728-5\n10.1136/bmj.m1117\n10.1371/journal.pone.0230548\n10.1101/2020.03.19.20039354\n10.1016/j.ajic.2015.01.013\n10.1186/s12879-017-2275-2\n10.1126/science.abb3221\n10.3201/eid2606.200495\n10.4103/0971-6203.58777\n10.1016/j.imu.2020.100297\n10.1101/2020.04.25.20074856\n10.1016/S2214-109X(20)30068-1\n10.1148/radiol.2020200642\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(20)30183-5\n10.4193/Rhin20.116\n10.1111/jdv.16387\n10.1016/S0140-6736(20)30566-3\n10.1016/j.ijantimicag.2020.105949\n10.1001/jamanetworkopen.2020.8857\n10.1016/j.medj.2020.06.001\n10.1016/S2665-9913(20)30390-8\n10.1056/NEJMoa2012410\n10.1056/NEJMoa2001282\n10.34141/LJCS2640133\n10.1016/S0140-6736(20)30644-9"}
{"title": "COVID-19-CT-CXR: A Freely Accessible and Weakly Labeled Chest X-Ray and CT Image Collection on COVID-19 From Biomedical Literature.", "abstract": "The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature, including those that report findings on radiographs. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. Because a large portion of figures in COVID-19 articles are not CXR or CT, we designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevant text. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved deep-learning (DL) performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza, another common infectious respiratory illness that may present similarly to COVID-19, and fine-tuned a baseline deep neural network to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We fine-tuned an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) From text-mined captions and figure descriptions, we compared 15 clinical symptoms and 20 clinical findings of COVID-19 versus those of influenza to demonstrate the disease differences in the scientific publications. Our database is unique, as the figures are retrieved along with relevant text with fine-grained descriptions, and it can be extended easily in the future. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.", "journal": "IEEE transactions on big data", "date": "2021-05-18", "authors": ["YifanPeng", "YuxingTang", "SungwonLee", "YingyingZhu", "Ronald MSummers", "ZhiyongLu"], "doi": "10.1109/tbdata.2020.3035935\n10.1109/RBME.2020.2987975\n10.1101/2020.04.13.20063941\n10.1101/2020.03.20.20039834\n10.1101/2020.02.14.20023028\n10.1016/j.eng.2020.04.010"}
{"title": "Light-weighted ensemble network with multilevel activation visualization for robust diagnosis of COVID19 pneumonia from large-scale chest radiographic database.", "abstract": "Currently, the coronavirus disease 2019 (COVID19) pandemic has killed more than one million people worldwide. In the present outbreak, radiological imaging modalities such as computed tomography (CT) and X-rays are being used to diagnose this disease, particularly in the early stage. However, the assessment of radiographic images includes a subjective evaluation that is time-consuming and requires substantial clinical skills. Nevertheless, the recent evolution in artificial intelligence (AI) has further strengthened the ability of computer-aided diagnosis tools and supported medical professionals in making effective diagnostic decisions. Therefore, in this study, the strength of various AI algorithms was analyzed to diagnose COVID19 infection from large-scale radiographic datasets. Based on this analysis, a light-weighted deep network is proposed, which is the first ensemble design (based on MobileNet, ShuffleNet, and FCNet) in medical domain (particularly for COVID19 diagnosis) that encompasses the reduced number of trainable parameters (a total of 3.16 million parameters) and outperforms the various existing models. Moreover, the addition of a multilevel activation visualization layer in the proposed network further visualizes the lesion patterns as multilevel class activation maps (ML-CAMs) along with the diagnostic result (either COVID19 positive or negative). Such additional output as ML-CAMs provides a visual insight of the computer decision and may assist radiologists in validating it, particularly in uncertain situations Additionally, a novel hierarchical training procedure was adopted to perform the training of the proposed network. It proceeds the network training by the adaptive number of epochs based on the validation dataset rather than using the fixed number of epochs. The quantitative results show the better performance of the proposed training method over the conventional end-to-end training procedure. A large collection of CT-scan and X-ray datasets (based on six publicly available datasets) was used to evaluate the performance of the proposed model and other baseline methods. The experimental results of the proposed network exhibit a promising performance in terms of diagnostic decision. An average F1 score (F1) of 94.60% and 95.94% and area under the curve (AUC) of 97.50% and 97.99% are achieved for the CT-scan and X-ray datasets, respectively. Finally, the detailed comparative analysis reveals that the proposed model outperforms the various state-of-the-art methods in terms of both quantitative and computational performance.", "journal": "Applied soft computing", "date": "2021-05-18", "authors": ["MuhammadOwais", "Hyo SikYoon", "TahirMahmood", "AdnanHaider", "HaseebSultan", "Kang RyoungPark"], "doi": "10.1016/j.asoc.2021.107490\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/ryct.2020200034\n10.1109/TMI.2020.2993291\n10.1007/s13246-020-00888-x\n10.1007/s10096-020-03901-z\n10.1016/j.cmpb.2020.105532\n10.1016/j.cmpb.2020.105581\n10.1080/07391102.2020.1767212\n10.1016/j.cmpb.2020.105608\n10.1109/TMI.2020.2996256\n10.1016/j.compbiomed.2020.103869\n10.1016/j.media.2020.101794\n10.3390/info11090419\n10.1007/s13246-020-00865-4\n10.1080/07391102.2020.1788642\n10.1016/j.compbiomed.2020.103795\n10.3892/etm.2020.8797\n10.1101/2020.04.24.20078998\n10.3390/electronics9091388\n10.1016/j.asoc.2020.106580\n10.1016/j.asoc.2020.106610\n10.1109/CVPR.2016.90\n10.1109/CVPR.2016.308\n10.1109/CVPR.2017.195\n10.1109/CVPR.2018.00474\n10.1109/CVPR.2018.00716\n10.1109/TNNLS.2017.2672978\n10.1016/j.asoc.2020.106859\n10.2196/21790\n10.1016/j.asoc.2020.106742\n10.1148/radiol.2020200370\n10.1148/radiol.2020201160\n10.1007/s10278-013-9622-7\n10.5121/ijdkp.2015.5201\n10.1016/j.sigpro.2011.12.005\n10.3390/s18030699\n10.5120/2968-3968\n10.1006/jcss.1997.1504\n10.1109/72.991427\n10.1023/A:1010933404324.pdf\n10.1109/TIT.1967.1053964"}
{"title": "COVID-19 Classification Based on Deep Convolution Neural Network Over a Wireless Network.", "abstract": "Corona Virus Disease 19 (COVID-19) firstly spread in China since December 2019. Then, it spread at a high rate around the world. Therefore, rapid diagnosis of COVID-19\u00a0has become a very hot research topic. One of the possible diagnostic tools is to use a deep convolution neural network (DCNN) to classify patient images. Chest X-ray is one of the most widely-used imaging techniques for classifying COVID-19\u00a0cases. This paper presents a proposed wireless communication and classification system for X-ray images to detect COVID-19\u00a0cases. Different modulation techniques are compared to select the most reliable one with less required bandwidth. The proposed DCNN architecture consists of deep feature extraction and classification layers. Firstly, the proposed DCNN hyper-parameters are adjusted in the training phase. Then, the tuned hyper-parameters are utilized in the testing phase. These hyper-parameters are the optimization algorithm, the learning rate, the\u00a0mini-batch size and the number of epochs. From simulation results, the proposed scheme outperforms other related pre-trained networks. The performance metrics are accuracy, loss, confusion matrix, sensitivity, precision, ", "journal": "Wireless personal communications", "date": "2021-05-18", "authors": ["Wafaa AShalaby", "WaleedSaad", "MonaShokair", "Fathi EAbd El-Samie", "Moawad IDessouky"], "doi": "10.1007/s11277-021-08523-y\n10.1007/s12098-020-03263-6\n10.1056/nejmoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1097/RTI.0000000000000404\n10.1186/s40537-014-0007-7\n10.1109/ACCESS.2014.2325029\n10.3390/e21020168\n10.3390/electronics8030292\n10.1016/j.imu.2020.100360\n10.1016/j.chaos.2020.109947\n10.1148/radiol.2020200905\n10.1007/s12652-021-02967-7"}
{"title": "Quantitative evaluation of COVID-19 pneumonia severity by CT pneumonia analysis algorithm using deep learning technology and blood test results.", "abstract": "To evaluate whether early chest computed tomography (CT) lesions quantified by an artificial intelligence (AI)-based commercial software and blood test values at the initial presentation can differentiate the severity of COVID-19 pneumonia.\nThis retrospective study included 100 SARS-CoV-2-positive patients with mild (n\u2009=\u200923), moderate (n\u2009=\u200937) or severe (n\u2009=\u200940) pneumonia classified according to the Japanese guidelines. Univariate Kruskal-Wallis and multivariate ordinal logistic analyses were used to examine whether CT parameters (opacity score, volume of opacity, % opacity, volume of high opacity, % high opacity and mean HU total on CT) as well as blood test parameters [procalcitonin, estimated glomerular filtration rate (eGFR), C-reactive protein, % lymphocyte, ferritin, aspartate aminotransferase, lactate dehydrogenase, alanine aminotransferase, creatine kinase, hemoglobin A1c, prothrombin time, activated partial prothrombin time (APTT), white blood cell count and creatinine] differed by disease severity.\nAll CT parameters and all blood test parameters except procalcitonin and APPT were significantly different among mild, moderate and severe groups. By multivariate analysis, mean HU total and eGFR were two independent factors associated with severity (p\u2009<\u20090.0001). Cutoff values for mean HU total and eGFR were, respectively,\u2009-\u2009801 HU and 77 ml/min/1.73 m\nThe mean HU total of the whole lung, determined by the AI algorithm, and eGFR reflect the severity of COVID-19 pneumonia.", "journal": "Japanese journal of radiology", "date": "2021-05-15", "authors": ["TomohisaOkuma", "ShinichiHamamoto", "TetsunoriMaebayashi", "AkishigeTaniguchi", "KyokoHirakawa", "ShuMatsushita", "KazukiMatsushita", "KatsukoMurata", "TakaoManabe", "YukioMiki"], "doi": "10.1007/s11604-021-01134-4\n10.1016/S0140-6736(20)30211-7\n10.1007/s11604-020-00958-w\n10.1007/s11604-020-01010-7\n10.1148/radiol.2020200642\n10.1371/journal.pone.0230548\n10.1007/s00330-020-06817-6\n10.21037/atm-20-3421\n10.1097/RLI.0000000000000674\n10.1007/s00330-020-07042-x\n10.1371/journal.pone.0236858\n10.1038/s41598-020-79097-1\n10.1186/s43055-020-00309-9\n10.1016/j.ejro.2020.100272\n10.7150/thno.45985\n10.1148/ryct.2020200389\n10.1001/jamainternmed.2020.2033\n10.1111/all.14496\n10.1016/S1473-3099(20)30086-4\n10.21037/jtd-20-1743\n10.1183/13993003.00547-2020\n10.1111/all.14657\n10.1016/j.jinf.2020.04.021\n10.1681/ASN.2020030276"}
{"title": "Digital holographic deep learning of red blood cells for field-portable, rapid COVID-19 screening.", "abstract": "Rapid screening of red blood cells for active infection of COVID-19 is presented using a compact and field-portable, 3D-printed shearing digital holographic microscope. Video holograms of thin blood smears are recorded, individual red blood cells are segmented for feature extraction, then a bi-directional long short-term memory network is used to classify between healthy and COVID positive red blood cells based on their spatiotemporal behavior. Individuals are then classified based on the simple majority of their cells' classifications. The proposed system may be beneficial for under-resourced healthcare systems. To the best of our knowledge, this is the first report of digital holographic microscopy for rapid screening of COVID-19.", "journal": "Optics letters", "date": "2021-05-15", "authors": ["TimothyO'Connor", "Jian-BingShen", "Bruce TLiang", "BahramJavidi"], "doi": "10.1364/OL.426152"}
{"title": "Quantitative assessment of lung involvement on chest CT at admission: Impact on hypoxia and outcome in COVID-19 patients.", "abstract": "The aim of this study was to quantify COVID-19 pneumonia features using CT performed at time of admission to emergency department in order to predict patients' hypoxia during the hospitalization and outcome.\nConsecutive chest CT performed in the emergency department between March 1st and April 7th 2020 for COVID-19 pneumonia were analyzed. The three features of pneumonia (GGO, semi-consolidation and consolidation) and the percentage of well-aerated lung were quantified using a HU threshold based software. ROC curves identified the optimal cut-off values of CT parameters to predict hypoxia worsening and hospital discharge. Multiple Cox proportional hazards regression was used to analyze the capability of CT quantitative features, demographic and clinical variables to predict the time to hospital discharge.\nSeventy-seven patients (median age 56-years-old, 51 men) with COVID-19 pneumonia at CT were enrolled. The quantitative features of COVID-19 pneumonia were not associated to age, sex and time-from-symptoms onset, whereas higher number of comorbidities was correlated to lower well-aerated parenchyma ratio (rho\u00a0=\u00a0-0.234, p\u00a0=\u00a00.04) and increased semi-consolidation ratio (rho\u00a0=\u00a0-0.303, p\u00a0=\u00a00.008). Well-aerated lung (\u226457%), semi-consolidation (\u226517%) and consolidation (\u22659%) predicted worst hypoxemia during hospitalization, with moderate areas under curves (AUC 0.76, 0.75, 0.77, respectively). Multiple Cox regression identified younger age (p\u00a0<\u00a00.01), female sex (p\u00a0<\u00a00.001), longer time-from-symptoms onset (p\u00a0=\u00a00.049), semi-consolidation \u226417% (p\u00a0<\u00a00.01) and consolidation \u226413% (p\u00a0=\u00a00.03) as independent predictors of shorter time to hospital discharge.\nQuantification of pneumonia features on admitting chest CT predicted hypoxia worsening during hospitalization and time to hospital discharge in COVID-19 patients.", "journal": "Clinical imaging", "date": "2021-05-14", "authors": ["AntonioEsposito", "AnnaPalmisano", "RobertaCao", "PaolaRancoita", "GiovanniLandoni", "DanieleGrippaldi", "EddaBoccia", "MicheleCosenza", "AntonioMessina", "SalvatoreLa Marca", "DiegoPalumbo", "CleliaDi Serio", "MarziaSpessot", "MorenoTresoldi", "PaoloScarpellini", "FabioCiceri", "AlbertoZangrillo", "FrancescoDe Cobelli"], "doi": "10.1016/j.clinimag.2021.04.033"}
{"title": "Joint Learning of 3D Lesion Segmentation and Classification for Explainable COVID-19 Diagnosis.", "abstract": "Given the outbreak of COVID-19 pandemic and the shortage of medical resource, extensive deep learning models have been proposed for automatic COVID-19 diagnosis, based on 3D computed tomography (CT) scans. However, the existing models independently process the 3D lesion segmentation and disease classification, ignoring the inherent correlation between these two tasks. In this paper, we propose a joint deep learning model of 3D lesion segmentation and classification for diagnosing COVID-19, called DeepSC-COVID, as the first attempt in this direction. Specifically, we establish a large-scale CT database containing 1,805 3D CT scans with fine-grained lesion annotations, and reveal 4 findings about lesion difference between COVID-19 and community acquired pneumonia (CAP). Inspired by our findings, DeepSC-COVID is designed with 3 subnets: a cross-task feature subnet for feature extraction, a 3D lesion subnet for lesion segmentation, and a classification subnet for disease diagnosis. Besides, the task-aware loss is proposed for learning the task interaction across the 3D lesion and classification subnets. Different from all existing models for COVID-19 diagnosis, our model is interpretable with fine-grained 3D lesion distribution. Finally, extensive experimental results show that the joint learning framework in our model significantly improves the performance of 3D lesion segmentation and disease classification in both efficiency and efficacy.", "journal": "IEEE transactions on medical imaging", "date": "2021-05-14", "authors": ["XiaofeiWang", "LaiJiang", "LiuLi", "MaiXu", "XinDeng", "LisongDai", "XiangyangXu", "TianyiLi", "YichenGuo", "ZulinWang", "Pier LuigiDragotti"], "doi": "10.1109/TMI.2021.3079709"}
{"title": "COVID19-CT-dataset: an open-access chest CT image repository of 1000+\u2009patients with confirmed COVID-19 diagnosis.", "abstract": "The ongoing Coronavirus disease 2019 (COVID-19) pandemic has drastically impacted the global health and economy. Computed tomography (CT) is the prime imaging modality for diagnosis of lung infections in COVID-19 patients. Data-driven and Artificial intelligence (AI)-powered solutions for automatic processing of CT images predominantly rely on large-scale, heterogeneous datasets. Owing to privacy and data availability issues, open-access and publicly available COVID-19 CT datasets are difficult to obtain, thus limiting the development of AI-enabled automatic diagnostic solutions. To tackle this problem, large CT image datasets encompassing diverse patterns of lung infections are in high demand.\nIn the present study, we provide an open-source repository containing 1000+\u2009CT images of COVID-19 lung infections established by a team of board-certified radiologists. CT images were acquired from two main general university hospitals in Mashhad, Iran from March 2020 until January 2021. COVID-19 infections were ratified with matching tests including Reverse transcription polymerase chain reaction (RT-PCR) and accompanying clinical symptoms. All data are 16-bit grayscale images composed of 512\u2009\u00d7\u2009512 pixels and are stored in DICOM standard. Patient privacy is preserved by removing all patient-specific information from image headers. Subsequently, all images corresponding to each patient are compressed and stored in RAR format.", "journal": "BMC research notes", "date": "2021-05-14", "authors": ["ShokouhShakouri", "Mohammad AminBakhshali", "ParvanehLayegh", "BehzadKiani", "FaridMasoumi", "SaeedehAtaei Nakhaei", "Sayyed MostafaMostafavi"], "doi": "10.1186/s13104-021-05592-x\n10.4081/gh.2020.953\n10.1109/RBME.2020.2987975\n10.1038/s41597-021-00900-3\n10.1148/radiol.2462070712\n10.7910/DVN/6ACUZJ"}
{"title": "Segmenting lung lesions of COVID-19 from CT images via pyramid pooling improved Unet.", "abstract": "Segmenting lesion regions of Coronavirus Disease 2019 (COVID-19) from computed tomography (CT) images is a challenge owing to COVID-19 lesions characterized by high variation, low contrast between infection lesions and around normal tissues, and blurred boundaries of infections. Moreover, a shortage of available CT dataset hinders deep learning techniques applying to tackling COVID-19. To address these issues, we propose a deep learning-based approach known as PPM-Unet to segmenting COVID-19 lesions from CT images. Our method improves an Unet by adopting pyramid pooling modules instead of the conventional skip connection and then enhances the representation of the neural network by aiding the global attention mechanism. We first pre-train PPM-Unet on COVID-19 dataset of pseudo labels containing1600 samples producing a coarse model. Then we fine-tune the coarse PPM-Unet on the standard COVID-19 dataset consisting of 100 pairs of samples to achieve a fine PPM-Unet. Qualitative and quantitative results demonstrate that our method can accurately segment COVID-19 infection regions from CT images, and achieve higher performance than other state-of-the-art segmentation models in this study. It offers a promising tool to lay a foundation for quantitatively detecting COVID-19 lesions.", "journal": "Biomedical physics & engineering express", "date": "2021-05-13", "authors": ["YinjinMa", "PengFeng", "PengHe", "YongRen", "XiaodongGuo", "XiaoliuYu", "BiaoWei"], "doi": "10.1088/2057-1976/ac008a"}
{"title": "The diagnostic accuracy of Artificial Intelligence-Assisted CT imaging in COVID-19 disease: A systematic review and meta-analysis.", "abstract": "Artificial intelligence (AI) systems have become critical in support of decision-making. This systematic review summarizes all the data currently available on the AI-assisted CT-Scan prediction accuracy for COVID-19. The ISI Web of Science, Cochrane Library, PubMed, Scopus, CINAHL, Science Direct, PROSPERO, and EMBASE were systematically searched. We used the revised Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) tool to assess all included studies' quality and potential bias. A hierarchical receiver-operating characteristic summary (HSROC) curve and a summary receiver operating characteristic (SROC) curve have been implemented. The area under the curve (AUC) was computed to determine the diagnostic accuracy. Finally, 36 studies (a total of 39,246 image data) were selected for inclusion into the final meta-analysis. The pooled sensitivity for AI was 0.90 (95% CI, 0.90-0.91), specificity was 0.91 (95% CI, 0.90-0.92) and the AUC was 0.96 (95% CI, 0.91-0.98). For deep learning (DL) method, the pooled sensitivity was 0.90 (95% CI, 0.90-0.91), specificity was 0.88 (95% CI, 0.87-0.88) and the AUC was 0.96 (95% CI, 0.93-0.97). In case of machine learning (ML), the pooled sensitivity was 0.90 (95% CI, 0.90-0.91), specificity was 0.95 (95% CI, 0.94-0.95) and the AUC was 0.97 (95% CI, 0.96-0.99). AI in COVID-19 patients is useful in identifying symptoms of lung involvement. More prospective real-time trials are required to confirm AI's role for high and quick COVID-19 diagnosis due to the possible selection bias and retrospective existence of currently available studies.", "journal": "Informatics in medicine unlocked", "date": "2021-05-13", "authors": ["MeisamMoezzi", "KiarashShirbandi", "Hassan KianiShahvandi", "BabakArjmand", "FakherRahim"], "doi": "10.1016/j.imu.2021.100591\n10.1007/s11548-020-02299-5\n10.1002/ima.22525"}
{"title": "Prospective Case-Control Study of Cardiovascular Abnormalities 6\u00a0Months\u00a0Following Mild COVID-19 in\u00a0Healthcare Workers.", "abstract": "The purpose of this study was to detect cardiovascular changes after mild severe acute respiratory syndrome-coronavirus-2 infection.\nConcern exists that mild coronavirus disease 2019 may cause myocardial and vascular disease.\nParticipants were recruited from COVIDsortium, a 3-hospital prospective study of 731 health care workers who underwent first-wave weekly symptom, polymerase chain reaction, and serology assessment over 4\u00a0months, with seroconversion in 21.5% (n\u00a0=\u00a0157). At 6\u00a0months post-infection, 74 seropositive and 75 age-, sex-, and ethnicity-matched seronegative control subjects were recruited for cardiovascular phenotyping (comprehensive phantom-calibrated cardiovascular magnetic resonance and blood biomarkers). Analysis was blinded, using objective artificial intelligence analytics where available.\nA total of 149 subjects (mean age 37 years, range 18 to 63 years, 58% women) were recruited. Seropositive infections had been mild with case definition, noncase definition, and asymptomatic disease in 45 (61%), 18 (24%), and 11 (15%), respectively, with 1 person hospitalized (for 2\u00a0days). Between seropositive and seronegative groups, there were no differences in cardiac structure (left ventricular volumes, mass, atrial area), function (ejection fraction, global longitudinal shortening, aortic distensibility), tissue characterization (T\nCardiovascular abnormalities are no more common in seropositive versus seronegative otherwise healthy, workforce representative individuals 6\u00a0months post-mild severe acute respiratory syndrome-coronavirus-2 infection.", "journal": "JACC. Cardiovascular imaging", "date": "2021-05-13", "authors": ["GeorgeJoy", "JessicaArtico", "HibbaKurdi", "AndreasSeraphim", "ClementLau", "George DThornton", "Marta FontesOliveira", "Robert DanielAdam", "NikooAziminia", "KatiaMenacho", "LizaChacko", "James TBrown", "Rishi KPatel", "HunainShiwani", "AnishBhuva", "Joao BAugusto", "MervynAndiapen", "AineMcKnight", "MahdadNoursadeghi", "IainPierce", "Timoth\u00e9eEvain", "GabriellaCaptur", "Rhodri HDavies", "John PGreenwood", "MariannaFontana", "PeterKellman", "Erik BSchelbert", "Thomas ATreibel", "CharlotteManisty", "James CMoon", "NoneNone"], "doi": "10.1016/j.jcmg.2021.04.011"}
{"title": "COVID-19 diagnosis from CT scans and chest X-ray images using low-cost Raspberry Pi.", "abstract": "The diagnosis of COVID-19 is of vital demand. Several studies have been conducted to decide whether the chest X-ray and computed tomography (CT) scans of patients indicate COVID-19. While these efforts resulted in successful classification systems, the design of a portable and cost-effective COVID-19 diagnosis system has not been addressed yet. The memory requirements of the current state-of-the-art COVID-19 diagnosis systems are not suitable for embedded systems due to the required large memory size of these systems (e.g., hundreds of megabytes). Thus, the current work is motivated to design a similar system with minimal memory requirements. In this paper, we propose a diagnosis system using a Raspberry Pi Linux embedded system. First, local features are extracted using local binary pattern (LBP) algorithm. Second, the global features are extracted from the chest X-ray or CT scans using multi-channel fractional-order Legendre-Fourier moments (MFrLFMs). Finally, the most significant features (local and global) are selected. The proposed system steps are integrated to fit the low computational and memory capacities of the embedded system. The proposed method has the smallest computational and memory resources,less than the state-of-the-art methods by two to three orders of magnitude, among existing state-of-the-art deep learning (DL)-based methods.", "journal": "PloS one", "date": "2021-05-12", "authors": ["Khalid MHosny", "Mohamed MDarwish", "KenliLi", "AhmadSalah"], "doi": "10.1371/journal.pone.0250688\n10.1016/j.clinimag.2020.04.001\n10.1056/NEJMoa2002032\n10.1007/s10115-020-01495-8\n10.1016/j.ijmedinf.2020.104284\n10.1016/j.ijmedinf.2020.104340\n10.1007/s13246-020-00865-4\n10.1016/j.asoc.2020.106504\n10.1016/j.patcog.2020.107324\n10.1109/TPAMI.2002.1017623\n10.1109/TAFFC.2017.2713359\n10.1016/j.patcog.2018.11.014\n10.1007/s10044-018-0740-1\n10.1007/s11554-016-0622-y\n10.1007/s11554-009-0135-z\n10.1016/j.procs.2015.02.025\n10.1007/s11227-016-1933-2\n10.1007/s13244-018-0639-9\n10.1016/j.compbiomed.2020.103792\n10.1371/journal.pone.0235187\n10.1007/s12559-020-09795-5\n10.1016/j.media.2020.101824\n10.1088/1361-6560/abbf9e\n10.1109/JBHI.2020.3019505\n10.1080/07391102.2020.1788642\n10.1109/TMI.2020.2996645\n10.1109/TIP.2021.3058783\n10.1007/s10140-020-01886-y\n10.1007/s00500-020-05275-y\n10.1101/2020.04.24.20078584"}
{"title": "COVID-Classifier: an automated machine learning model to assist in the diagnosis of COVID-19 infection in chest X-ray images.", "abstract": "Chest-X ray (CXR) radiography can be used as a first-line triage process for non-COVID-19 patients with pneumonia. However, the similarity between features of CXR images of COVID-19 and pneumonia caused by other infections makes the differential diagnosis by radiologists challenging. We hypothesized that machine learning-based classifiers can reliably distinguish the CXR images of COVID-19 patients from other forms of pneumonia. We used a dimensionality reduction method to generate a set of optimal features of CXR images to build an efficient machine learning classifier that can distinguish COVID-19 cases from non-COVID-19 cases with high accuracy and sensitivity. By using global features of the whole CXR images, we successfully implemented our classifier using a relatively small dataset of CXR images. We propose that our COVID-Classifier can be used in conjunction with other tests for optimal allocation of hospital resources by rapid triage of non-COVID-19 cases.", "journal": "Scientific reports", "date": "2021-05-12", "authors": ["AbolfazlZargari Khuzani", "MortezaHeidari", "S AliShariati"], "doi": "10.1038/s41598-021-88807-2\n10.1109/TMI.2020.2993291\n10.1109/TMI.2020.2994459\n10.1177/0846537120913033\n10.1007/s10439-018-2095-6\n10.1088/1361-6560/aaa1ca\n10.1109/TMI.2019.2946490\n10.1109/TMI.2014.2366792\n10.1088/1361-6560/aad3ab\n10.1038/s41746-019-0148-3\n10.1109/TIP.2019.2952079\n10.1016/j.cell.2020.04.045\n10.1186/s12931-018-0716-0\n10.1053/j.semvascsurg.2004.03.001\n10.1016/j.patcog.2006.05.011\n10.1016/j.jbi.2018.07.014\n10.2316/Journal.203.2014.3.203-0024\n10.1109/ACCESS.2019.2947701\n10.1109/TIP.2018.2808767\n10.1128/AEM.67.5.2129-2135.2001\n10.1007/s11269-017-1660-3\n10.1016/j.acra.2017.04.014\n10.1016/j.compbiomed.2019.103482\n10.6113/JPE.2011.11.2.228"}
{"title": "Classification of COVID-19 chest X-Ray and CT images using a type of dynamic CNN modification method.", "abstract": "Understanding and classifying Chest X-Ray (CXR) and computerised tomography (CT) images are of great significance for COVID-19 diagnosis. The existing research on the classification for COVID-19 cases faces the challenges of data imbalance, insufficient generalisability, the lack of comparative study, etc. To address these problems, this paper proposes a type of modified MobileNet to classify COVID-19 CXR images and a modified ResNet architecture for CT image classification. In particular, a modification method of convolutional neural networks (CNN) is designed to solve the gradient vanishing problem and improve the classification performance through dynamically combining features in different layers of a CNN. The modified MobileNet is applied to the classification of COVID-19, Tuberculosis, viral pneumonia (with the exception of COVID-19), bacterial pneumonia and normal controls using CXR images. Also, the proposed modified ResNet is used for the classification of COVID-19, non-COVID-19 infections and normal controls using CT images. The results show that the proposed methods achieve 99.6% test accuracy on the five-category CXR image dataset and 99.3% test accuracy on the CT image dataset. Six advanced CNN architectures and two specific COVID-19 detection models, i.e., COVID-Net and COVIDNet-CT are used in comparative studies. Two benchmark datasets and a CXR image dataset which combines eight different CXR image sources are employed to evaluate the performance of the above models. The results show that the proposed methods outperform the comparative models in classification accuracy, sensitivity, and precision, which demonstrate their potential in computer-aided diagnosis for healthcare applications.", "journal": "Computers in biology and medicine", "date": "2021-05-11", "authors": ["GuangyuJia", "Hak-KeungLam", "YujiaXu"], "doi": "10.1016/j.compbiomed.2021.104425"}
{"title": "On the role of artificial intelligence in medical imaging of COVID-19.", "abstract": "Although a plethora of research articles on AI methods on COVID-19 medical imaging are published, their clinical value remains unclear. We conducted the largest systematic review of the literature addressing the utility of AI in imaging for COVID-19 patient care. By keyword searches on PubMed and preprint servers throughout 2020, we identified 463 manuscripts and performed a systematic meta-analysis to assess their technical merit and clinical relevance. Our analysis evidences a significant disparity between clinical and AI communities, in the focus on both imaging modalities (AI experts neglected CT and ultrasound, favoring X-ray) and performed tasks (71.9% of AI papers centered on diagnosis). The vast majority of manuscripts were found to be deficient regarding potential use in clinical practice, but 2.7% (n = 12) publications were assigned a high maturity level and are summarized in greater detail. We provide an itemized discussion of the challenges in developing clinically relevant AI solutions with recommendations and remedies.", "journal": "Patterns (New York, N.Y.)", "date": "2021-05-11", "authors": ["JannisBorn", "DavidBeymer", "DeeptaRajan", "AdamCoy", "Vandana VMukherjee", "MatteoManica", "PrasanthPrasanna", "DeddehBallah", "MichalGuindy", "DorithShaham", "Pallav LShah", "EmmanouilKarteris", "Jan LRobertus", "MariaGabrani", "MichalRosen-Zvi"], "doi": "10.1016/j.patter.2021.100269\n10.1109/RBME.2020.2990959\n10.1016/j.chest.2020.04.003\n10.1016/j.ejro.2020.100231\n10.1109/RBME.2020.2987975\n10.1016/j.jiph.2020.06.028\n10.1097/RLI.0000000000000763\n10.1101/2020.04.24.20078584\n10.3390/app11020672\n10.1148/radiol.2021219004\n10.1128/JCM.00512-20\n10.1148/radiol.2020200905\n10.1007/s00330-020-07225-6\n10.1109/PIMRC.2017.8292361\n10.1093/intqhc/mzab010\n10.3174/ajnr.A2742\n10.1148/radiol.2020203173\n10.5811/westjem.2020.5.47743\n10.1183/23120541.00539-2020\n10.1016/j.ultrasmedbio.2020.07.003\n10.1016/S2213-2600(20)30120-X\n10.1136/bmj.m689\n10.1186/s12916-019-1426-2"}
{"title": "Clinical Factors and Quantitative CT Parameters Associated With ICU Admission in Patients of COVID-19 Pneumonia: A Multicenter Study.", "abstract": "The clinical spectrum of COVID-19 pneumonia is varied. Thus, it is important to identify risk factors at an early stage for predicting deterioration that require transferring the patients to ICU. A retrospective multicenter study was conducted on COVID-19 patients admitted to designated hospitals in China from Jan 17, 2020, to Feb 17, 2020. Clinical presentation, laboratory data, and quantitative CT parameters were also collected. The result showed that increasing risks of ICU admission were associated with age > 60 years (odds ratio [OR], 12.72; 95% confidence interval [CI], 2.42-24.61; ", "journal": "Frontiers in public health", "date": "2021-05-11", "authors": ["ChengxiYan", "YingChang", "HuanYu", "JingxuXu", "ChencuiHuang", "MingleiYang", "YiqiaoWang", "DiWang", "TianYu", "ShuqinWei", "ZhenyuLi", "FeifeiGong", "MingqingKou", "WenjingGou", "QiliZhao", "PenghuiSun", "XiuqinJia", "ZhaoyangFan", "JialiXu", "SijieLi", "QiYang"], "doi": "10.3389/fpubh.2021.648360\n10.1101/2020.02.06.20020974\n10.1038/s41586-020-2008-3\n10.1038/s41421-020-0147-1\n10.1186/s40779-020-00240-0\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200370\n10.1097/RTI.0000000000000524\n10.1001/jama.2020.5394\n10.1016/S2213-2600(20)30079-5\n10.1111/jebm.12418\n10.1148/radiol.2020200642\n10.1016/j.jinf.2020.03.005\n10.1097/RLI.0000000000000674\n10.1136/bmjopen-2020-044500\n10.7150/thno.46465\n10.1038/s41467-020-18786-x\n10.1016/j.ejrad.2019.108774\n10.1177/2333794X21991531\n10.1148/radiol.11092149\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30566-3\n10.1007/s11427-020-1661-4\n10.1016/j.antiviral.2016.11.006\n10.1007/BF01651146\n10.4081/cp.2020.1271\n10.2174/138161212799504731\n10.1016/S2468-1253(20)30084-4\n10.1148/radiol.2020200343\n10.1148/ryct.2020200110\n10.1097/RLI.0000000000000672\n10.3892/etm.2017.4449\n10.1016/j.compbiomed.2020.103792\n10.1148/radiol.2020200905\n10.1038/s41467-020-17280-8"}
{"title": "Computed Tomography Image Processing Analysis in COVID-19 Patient Follow-Up Assessment.", "abstract": "The rapid worldwide spread of the COVID-19 pandemic has infected patients around the world in a short space of time. Chest computed tomography (CT) images of patients who are infected with COVID-19 can offer early diagnosis and efficient forecast monitoring at a low cost. The diagnosis of COVID-19 on CT in an automated way can speed up many tasks and the application of medical treatments. This can help complement reverse transcription-polymerase chain reaction (RT-PCR) diagnosis. The aim of this work is to develop a system that automatically identifies ground-glass opacity (GGO) and pulmonary infiltrates (PIs) on CT images from patients with COVID-19. The purpose is to assess the disease progression during the patient's follow-up assessment and evaluation. We propose an efficient methodology that incorporates oversegmentation mean shift followed by superpixel-SLIC (simple linear iterative clustering) algorithm on CT images with COVID-19 for pulmonary parenchyma segmentation. To identify the pulmonary parenchyma, we described each superpixel cluster according to its position, grey intensity, second-order texture, and spatial-context-saliency features to classify by a tree random forest (TRF). Second, by applying the watershed segmentation to the mean-shift clusters, only pulmonary parenchyma segmentation-identified zones showed GGO and PI based on the description of each watershed cluster of its position, grey intensity, gradient entropy, second-order texture, Euclidean position to the border region of the PI zone, and global saliency features, after using TRF. Our classification results for pulmonary parenchyma identification on CT images with COVID-19 had a precision of over 92% and recall of over 92% on twofold cross validation. For GGO, the PI identification showed 96% precision and 96% recall on twofold cross validation.", "journal": "Journal of healthcare engineering", "date": "2021-05-11", "authors": ["SantiagoTello-Mijares", "LuisaWoo"], "doi": "10.1155/2021/8869372\n10.1016/j.chest.2020.04.003\n10.1109/RBME.2020.2987975\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/ryct.2020200034\n10.36227/techrxiv.12212516.v1\n10.1101/2020.02.14.20023028\n10.1101/2020.02.25.20021568\n10.1148/radiol.2020200905\n10.1007/s10489-020-01714-3\n10.1101/2020.04.13.20063479\n10.1101/2020.04.13.20063941\n10.1101/2020.02.23.20026930\n10.1109/TMI.2020.2996645\n10.1101/2020.04.16.20064709\n10.1109/TIT.1975.1055330\n10.1109/34.1000236\n10.1109/TPAMI.2012.120\n10.1109/PROC.1979.11328\n10.1155/2015/586928\n10.1155/2014/536308\n10.1109/CVPR.2012.6247743\n10.1145/957013.957094\n10.1111/j.1467-8659.2009.01645.x\n10.1109/ANZIIS.1994.396988\n10.1023/a:1010933404324\n10.1109/TPAMI.2014.2345401"}
{"title": "Discrepancies in the clinical and radiological profiles of COVID-19: A case-based discussion and review of literature.", "abstract": "The current gold standard for the diagnosis of coronavirus disease-19 (COVID-19) is a positive reverse transcriptase polymerase chain reaction (RT-PCR) test, on the background of clinical suspicion. However, RT-PCR has its limitations; this includes issues of low sensitivity, sampling errors and appropriate timing of specimen collection. As pulmonary involvement is the most common manifestation of severe COVID-19, early and appropriate lung imaging is important to aid diagnosis. However, gross discrepancies can occur between the clinical and imaging findings in patients with COVID-19, which can mislead clinicians in their decision making. Although chest X-ray (CXR) has a low sensitivity for the diagnosis of COVID-19 associated lung disease, especially in the earlier stages, a positive CXR increases the pre-test probability of COVID-19. CXR scoring systems have shown to be useful, such as the COVID-19 opacification rating score which helps to predict the need of tracheal intubation. Furthermore, artificial intelligence-based algorithms have also shown promise in differentiating COVID-19 pneumonia on CXR from other lung diseases. Although costlier than CXR, unenhanced computed tomographic (CT) chest scans have a higher sensitivity, but lesser specificity compared to RT-PCR for the diagnosis of COVID-19 pneumonia. A semi-quantitative CT scoring system has been shown to predict short-term mortality. The routine use of CT pulmonary angiography as a first-line imaging modality in patients with suspected COVID-19 is not justifiable due to the risk of contrast nephropathy. Scoring systems similar to those pioneered in CXR and CT can be used to effectively plan and manage hospital resources such as ventilators. Lung ultrasound is useful in the assessment of critically ill COVID-19 patients in the hands of an experienced operator. Moreover, it is a convenient tool to monitor disease progression, as it is cheap, non-invasive, easily accessible and easy to sterilise. Newer lung imaging modalities such as magnetic resonance imaging (MRI) for safe imaging among children, adolescents and pregnant women are rapidly evolving. Imaging modalities are also essential for evaluating the extra-pulmonary manifestations of COVID-19: these include cranial imaging with CT or MRI; cardiac imaging with ultrasonography (US), CT and MRI; and abdominal imaging with US or CT. This review critically analyses the utility of each imaging modality to empower clinicians to use them appropriately in the management of patients with COVID-19 infection.", "journal": "World journal of radiology", "date": "2021-05-11", "authors": ["HemantKumar", "Cornelius JamesFernandez", "SangeethaKolpattil", "MohamedMunavvar", "Joseph MPappachan"], "doi": "10.4329/wjr.v13.i4.75"}
{"title": "Prediction of COVID-19 with Computed Tomography Images using Hybrid Learning Techniques.", "abstract": "Reverse Transcription Polymerase Chain Reaction (RT-PCR) used for diagnosing COVID-19 has been found to give low detection rate during early stages of infection. Radiological analysis of CT images has given higher prediction rate when compared to RT-PCR technique. In this paper, hybrid learning models are used to classify COVID-19 CT images, Community-Acquired Pneumonia (CAP) CT images, and normal CT images with high specificity and sensitivity. The proposed system in this paper has been compared with various machine learning classifiers and other deep learning classifiers for better data analysis. The outcome of this study is also compared with other studies which were carried out recently on COVID-19 classification for further analysis. The proposed model has been found to outperform with an accuracy of 96.69%, sensitivity of 96%, and specificity of 98%.", "journal": "Disease markers", "date": "2021-05-11", "authors": ["VaralakshmiPerumal", "VasumathiNarayanan", "Sakthi Jaya SundarRajasekar"], "doi": "10.1155/2021/5522729\n10.1056/NEJMoa2001017\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200370\n10.1148/radiol.2020200343\n10.1148/radiol.2020200330\n10.1148/radiol.2020200432\n10.3348/kjr.2020.0157\n10.1148/radiol.2020200642\n10.1101/2020.02.14.20023028\n10.1007/s10140-020-01886-y\n10.1109/ACCESS.2020.3016780\n10.1007/s13246-020-00865-4\n10.20944/preprints202003.0300.v1"}
{"title": "Pneumonia Detection Using an Improved Algorithm Based on Faster R-CNN.", "abstract": "Pneumonia remains a threat to human health; the coronavirus disease 2019 (COVID-19) that began at the end of 2019 had a major impact on the world. It is still raging in many countries and has caused great losses to people's lives and property. In this paper, we present a method based on DeepConv-DilatedNet of identifying and localizing pneumonia in chest X-ray (CXR) images. Two-stage detector Faster R-CNN is adopted as the structure of a network. Feature Pyramid Network (FPN) is integrated into the residual neural network of a dilated bottleneck so that the deep features are expanded to preserve the deep feature and position information of the object. In the case of DeepConv-DilatedNet, the deconvolution network is used to restore high-level feature maps into its original size, and the target information is further retained. On the other hand, DeepConv-DilatedNet uses a popular fully convolution architecture with computation shared on the entire image. Then, Soft-NMS is used to screen boxes and ensure sample quality. Also, K-Means++ is used to generate anchor boxes to improve the localization accuracy. The algorithm obtained 39.23% Mean Average Precision (mAP) on the X-ray image dataset from the Radiological Society of North America (RSNA) and got 38.02% Mean Average Precision (mAP) on the ChestX-ray14 dataset, surpassing other detection algorithms. So, in this paper, an improved algorithm that can provide doctors with location information of pneumonia lesions is proposed.", "journal": "Computational and mathematical methods in medicine", "date": "2021-05-11", "authors": ["ShangjieYao", "YaowuChen", "XiangTian", "RongxinJiang"], "doi": "10.1155/2021/8854892\n10.1016/S0140-6736(16)31593-8"}
{"title": "Automatic prediction of COVID-\u200919 from chest images using modified ResNet50.", "abstract": "Recently coronavirus 2019 (COVID-2019), discovered in Wuhan city of China in December 2019 announced as world pandemic by the World Health Organization (WHO). It has catastrophic impacts on daily lives, public health, and the global economy. The detection of coronavirus (COVID-\u200919) is now a critical task for medical specialists. Laboratory methods for detecting the virus such as Polymerase Chain Reaction, antigens, and antibodies have pros and cons represented in time required to obtain results, accuracy, cost and suitability of the test to phase of infection. The need for accurate, fast, and cheap auxiliary diagnostic tools has become a necessity as there are no accurate automated toolkits available. Other medical investigations such as chest X-ray and Computerized Tomography scans are imaging techniques that play an important role in the diagnosis of COVID-\u200919 virus. Application of advanced artificial intelligence techniques for processing radiological imaging can be helpful for the accurate detection of this virus. However, Due to the small dataset available for COVID-\u200919, transfer learning from pre-trained convolution neural networks, CNNs can be used as a promising solution for diagnosis of coronavirus. Transfer learning becomes an effective mechanism by transferring knowledge from generic object recognition tasks to domain-specific tasks. Hence, the main contribution of this paper is to exploit the pre-trained deep learning CNN architectures as a cornerstone to enhance and build up an automated tool for detection and diagnosis of COVID-\u200919 in chest X-Ray and Computerized Tomography images. The main idea is to make use of their convolutional neural network structure and its learned weights on large datasets such as ImageNet. Moreover, a modification to ResNet50 is proposed to classify the patients as COVID infected or not. This modification includes adding three new layers, named, ", "journal": "Multimedia tools and applications", "date": "2021-05-11", "authors": ["MarwaElpeltagy", "HanySallam"], "doi": "10.1007/s11042-021-10783-6\n10.1016/S0140-6736(20)30211-7\n10.1016/j.compbiomed.2020.103792\n10.1007/s10096-019-03782-x\n10.1016/j.ejrad.2020.109041\n10.1148/ryct.2020200047"}
{"title": "Determination of COVID-19 pneumonia based on generalized convolutional neural network model from chest X-ray images.", "abstract": "X-ray units have become one of the most advantageous candidates for triaging the new Coronavirus disease COVID-19 infected patients thanks to its relatively low radiation dose, ease of access, practical, reduced prices, and quick imaging process. This research intended to develop a reliable convolutional-neural-network (CNN) model for the classification of COVID-19 from chest X-ray views. Moreover, it is aimed to prevent bias issues due to the database. Transfer learning-based CNN model was developed by using a sum of 1,218 chest X-ray images (CXIs) consisting of 368 COVID-19 pneumonia and 850 other pneumonia cases by pre-trained architectures, including DenseNet-201, ResNet-18, and SqueezeNet. The chest X-ray images were acquired from publicly available databases, and each individual image was carefully selected to prevent any bias problem. A stratified 5-fold cross-validation approach was utilized with a ratio of 90% for training and 10% for the testing (unseen folds), in which 20% of training data was used as a validation set to prevent overfitting problems. The binary classification performances of the proposed CNN models were evaluated by the testing data. The activation mapping approach was implemented to improve the causality and visuality of the radiograph. The outcomes demonstrated that the proposed CNN model built on DenseNet-201 architecture outperformed amongst the others with the highest accuracy, precision, recall, and F1-scores of 94.96%, 89.74%, 94.59%, and 92.11%, respectively. The results indicated that the reliable diagnosis of COVID-19 pneumonia from CXIs based on the CNN model opens the door to accelerate triage, save critical time, and prioritize resources besides assisting the radiologists.", "journal": "Expert systems with applications", "date": "2021-05-11", "authors": ["AdiAlhudhaif", "KemalPolat", "OnurKaraman"], "doi": "10.1016/j.eswa.2021.115141\n10.1148/radiol.2020200642\n10.1155/2020/8828855\n10.1016/S0140-6736(20)30211-7\n10.1007/s11554-021-01086-y\n10.1542/peds.2020-0702\n10.1109/JSAC.4910.1109/JSAC.2020.3020598\n10.1136/bmj.m1066\n10.1080/07391102.2020.1767212\n10.1016/j.scitotenv.2020.138858\n10.1164/rccm.2014P7\n10.1007/s10489-020-01902-1\n10.1148/radiol.2020200527\n10.1016/j.acra.2019.10.001\n10.1016/S1473-3099(20)30134\n10.1016/j.ijantimicag.2020.105951\n10.1016/j.media.2020.101794\n10.1016/j.bspc.2020.102365\n10.1109/JAS.2020.1003393\n10.1016/j.chaos.2020.110245\n10.1016/j.compbiomed.2020.103792\n10.1109/JIOT.2020.3038009\n10.3233/XST-200757\n10.1016/j.eswa.2014.07.013\n10.1109/Access.628763910.1109/ACCESS.2018.2817614\n10.1007/s12559-021-09848-3\n10.1007/s00264-020-04609-7\n10.1001/jama.2020.1585\n10.1038/s41598-020-76550-z\n10.1101/2020.02.14.20023028\n10.1148/radiol.2020200343\n10.1007/s11427-020-1637-5"}
{"title": "FBSED based automatic diagnosis of COVID-19 using X-ray and CT images.", "abstract": "This work introduces the Fourier-Bessel series expansion-based decomposition (FBSED) method, which is an implementation of the wavelet packet decomposition approach in the Fourier-Bessel series expansion domain. The proposed method has been used for the diagnosis of pneumonia caused by the 2019 novel coronavirus disease (COVID-19) using chest X-ray image (CXI) and chest computer tomography image (CCTI). The FBSED method is used to decompose CXI and CCTI into sub-band images (SBIs). The SBIs are then used to train various pre-trained convolutional neural network (CNN) models separately using a transfer learning approach. The combination of SBI and CNN is termed as one channel. Deep features from each channel are fused to get a feature vector. Different classifiers are used to classify pneumonia caused by COVID-19 from other viral and bacterial pneumonia and healthy subjects with the extracted feature vector. The different combinations of channels have also been analyzed to make the process computationally efficient. For CXI and CCTI databases, the best performance has been obtained with only one and four channels, respectively. The proposed model was evaluated using 5-fold and 10-fold cross-validation processes. The average accuracy for the CXI database was 100% for both 5-fold and 10-fold cross-validation processes, and for the CCTI database, it is 97.6% for the 5-fold cross-validation process. Therefore, the proposed method may be used by radiologists to rapidly diagnose patients with COVID-19.", "journal": "Computers in biology and medicine", "date": "2021-05-10", "authors": ["Pradeep KumarChaudhary", "Ram BilasPachori"], "doi": "10.1016/j.compbiomed.2021.104454\n10.20944/preprints202003.0300.v1\n10.1109/34.142909"}
{"title": "ai-corona: Radiologist-assistant deep learning framework for COVID-19 diagnosis in chest CT scans.", "abstract": "The development of medical assisting tools based on artificial intelligence advances is essential in the global fight against COVID-19 outbreak and the future of medical systems. In this study, we introduce ai-corona, a radiologist-assistant deep learning framework for COVID-19 infection diagnosis using chest CT scans. Our framework incorporates an EfficientNetB3-based feature extractor. We employed three datasets; the CC-CCII set, the MasihDaneshvari Hospital (MDH) cohort, and the MosMedData cohort. Overall, these datasets constitute 7184 scans from 5693 subjects and include the COVID-19, non-COVID abnormal (NCA), common pneumonia (CP), non-pneumonia, and Normal classes. We evaluate ai-corona on test sets from the CC-CCII set, MDH cohort, and the entirety of the MosMedData cohort, for which it gained AUC scores of 0.997, 0.989, and 0.954, respectively. Our results indicates ai-corona outperforms all the alternative models. Lastly, our framework's diagnosis capabilities were evaluated as assistant to several experts. Accordingly, We observed an increase in both speed and accuracy of expert diagnosis when incorporating ai-corona's assistance.", "journal": "PloS one", "date": "2021-05-08", "authors": ["MehdiYousefzadeh", "ParsaEsfahanian", "Seyed Mohammad SadeghMovahed", "SaeidGorgin", "DaraRahmati", "AtefehAbedini", "Seyed AlirezaNadji", "SaraHaseli", "MehrdadBakhshayesh Karam", "ArdaKiani", "MeisamHoseinyazdi", "JafarRoshandel", "RezaLashgari"], "doi": "10.1371/journal.pone.0250952\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1038/s41418-020-00720-9\n10.1001/jama.2020.3786\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1016/S2213-2600(20)30453-7\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06865-y\n10.1148/radiol.2020200463\n10.1016/S2589-7500(19)30123-2\n10.1038/s41591-019-0447-x\n10.1038/s41591-018-0177-5\n10.1016/j.jormas.2019.06.002\n10.1109/ACCESS.2020.3010287\n10.1371/journal.pmed.1002686\n10.1371/journal.pmed.1002699\n10.1016/j.cell.2020.04.045\n10.1038/s41467-020-18685-1\n10.1118/1.3528204\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1109/ACCESS.2018.2877890\n10.1016/j.ebiom.2020.102903"}
{"title": "Addressing Biodisaster X Threats With Artificial Intelligence and 6G Technologies: Literature Review and Critical Insights.", "abstract": "With advances in science and technology, biotechnology is becoming more accessible to people of all demographics. These advances inevitably hold the promise to improve personal and population well-being and welfare substantially. It is paradoxical that while greater access to biotechnology on a population level has many advantages, it may also increase the likelihood and frequency of biodisasters due to accidental or malicious use. Similar to \"Disease X\" (describing unknown naturally emerging pathogenic diseases with a pandemic potential), we term this unknown risk from biotechnologies \"Biodisaster X.\" To date, no studies have examined the potential role of information technologies in preventing and mitigating Biodisaster X.\nThis study aimed to explore (1) what Biodisaster X might entail and (2) solutions that use artificial intelligence (AI) and emerging 6G technologies to help monitor and manage Biodisaster X threats.\nA review of the literature on applying AI and 6G technologies for monitoring and managing biodisasters was conducted on PubMed, using articles published from database inception through to November 16, 2020.\nOur findings show that Biodisaster X has the potential to upend lives and livelihoods and destroy economies, essentially posing a looming risk for civilizations worldwide. To shed light on Biodisaster X threats, we detailed effective AI and 6G-enabled strategies, ranging from natural language processing to deep learning-based image analysis to address issues ranging from early Biodisaster X detection (eg, identification of suspicious behaviors), remote design and development of pharmaceuticals (eg, treatment development), and public health interventions (eg, reactive shelter-at-home mandate enforcement), as well as disaster recovery (eg, sentiment analysis of social media posts to shed light on the public's feelings and readiness for recovery building).\nBiodisaster X is a looming but avoidable catastrophe. Considering the potential human and economic consequences Biodisaster X could cause, actions that can effectively monitor and manage Biodisaster X threats must be taken promptly and proactively. Rather than solely depending on overstretched professional attention of health experts and government officials, it is perhaps more cost-effective and practical to deploy technology-based solutions to prevent and control Biodisaster X threats. This study discusses what Biodisaster X could entail and emphasizes the importance of monitoring and managing Biodisaster X threats by AI techniques and 6G technologies. Future studies could explore how the convergence of AI and 6G systems may further advance the preparedness for high-impact, less likely events beyond Biodisaster X.", "journal": "Journal of medical Internet research", "date": "2021-05-08", "authors": ["ZhaohuiSu", "DeanMcDonnell", "Barry LBentley", "JiguangHe", "FengShi", "AliCheshmehzangi", "JunaidAhmad", "PengJia"], "doi": "10.2196/26109\n10.1016/S1473-3099(13)70323-2\n10.2307/600071\n10.1007/s13280-016-0809-2\n10.1111/j.1523-1739.2006.00524.x\n10.1111/j.1539-6924.2007.00960.x\n10.1017/ice.2021.26\n10.2196/26111\n10.2196/26111\n10.1016/j.bbih.2020.100159\n10.1186/s12992-020-00654-4\n10.1186/s12992-020-00654-4\n10.1016/j.bbih.2021.100204\n10.1016/j.pdisas.2020.100091\n10.1016/j.pdisas.2020.100091\n10.1017/err.2020.34\n10.1111/1468-0009.12463\n10.1016/j.aucc.2020.07.006\n10.1038/d41586-020-01079-0\n10.1016/S2214-109X(20)30120-0\n10.1029/2020GH000303\n10.1016/S0140-6736(19)30803-7\n10.1071/MA20028\n10.1016/S1473-3099(20)30123-7\n10.1056/NEJMp058068\n10.1016/S0140-6736(12)61684-5\n10.1128/CMR.00033-15\n10.1016/S1473-3099(18)30298-6\n10.1126/science.283.5406.1279\n10.1017/S0963180113000753\n10.1016/s0140-6736(02)11797-1\n10.1136/bmj.1.4490.148\n10.1056/NEJM199005173222006\n10.1016/j.respol.2007.10.003\n10.1016/j.respol.2007.10.003\n10.1111/1469-0691.12699\n10.1177/000271622311000112\n10.1177/000271622311000112\n10.2307/1914185\n10.1016/j.avb.2020.101483\n10.1016/j.avb.2020.101483\n10.1016/j.jen.2014.03.001\n10.1001/jama.288.5.622\n10.1016/s0733-8627(02)00004-4\n10.1093/phr/116.S2.9\n10.1080/08998280.2004.11928002\n10.1002/0471686786.ebd0138\n10.1002/0471686786.ebd0138\n10.1001/jama.2020.4169\n10.1371/journal.pmed.1003144\n10.1371/journal.pmed.1003144\n10.1016/S2468-2667(20)30101-8\n10.1016/S2214-109X(20)30234-5\n10.1016/j.socscimed.2008.06.020\n10.1056/NEJMra1208802\n10.1136/bmjgh-2020-003746\n10.18006/2021.9(2).108.116\n10.1093/qjmed/hcaa343\n10.1016/S1473-3099(18)30359-1\n10.1086/590567\n10.1038/nrmicro1027\n10.7150/ijbs.45472\n10.1038/srep14830\n10.1038/srep14830\n10.1111/1556-4029.12312\n10.1111/1556-4029.12312\n10.1016/j.prevetmed.2014.11.004\n10.1002/cpmc.23\n10.1038/nature22975\n10.1371/journal.pone.0188453\n10.1371/journal.pone.0188453\n10.1089/hs.2017.0061\n10.1056/NEJMsb2021088\n10.1016/j.bbih.2020.100144\n10.1016/S2542-5196(18)30245-6\n10.1016/j.tim.2021.03.003\n10.1007/s11747-019-00696-0\n10.2196/17234\n10.1016/S2589-7500(20)30079-0\n10.1016/j.telpol.2020.101976\n10.1038/s41591-020-1123-x\n10.1080/14616688.2020.1762118\n10.1002/aisy.202000071\n10.1002/aisy.202000071\n10.1093/jtm/taaa080\n10.1007/s10916-020-01617-3\n10.3389/frai.2020.00065\n10.3389/frai.2020.00065\n10.1109/RBME.2020.2987975\n10.2196/25314\n10.1038/s41591-020-0921-5\n10.3390/s18051341\n10.3390/smartcities2030025\n10.1016/j.scs.2020.102364\n10.1016/j.scs.2020.102364\n10.1109/MNET.001.1900287\n10.1016/j.dcan.2020.05.003\n10.1016/j.dcan.2020.05.003\n10.14569/IJACSA.2020.0110281\n10.3390/sym12040676\n10.1038/s41928-019-0355-6\n10.1109/OJCOMS.2020.3010270\n10.1109/MCOM.2019.1900271\n10.1016/j.pt.2020.12.004\n10.1136/bmjgh-2020-002925\n10.1371/journal.pone.0239694\n10.1371/journal.pone.0239694\n10.1089/jwh.2020.8721\n10.1017/S1351324916000383\n10.1126/science.aaa8685\n10.4137/bii.s4706?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dpubmed\n10.4137/bii.s4706\n10.1109/BHI.2017.7897288\n10.1155/2016/8708434\n10.1155/2016/8708434\n10.2196/22635\n10.1016/j.scitotenv.2020.139298\n10.1109/ACCESS.2017.2756872\n10.1007/s11356-018-1438-z\n10.1016/j.scitotenv.2020.141145\n10.1016/j.jes.2015.01.007\n10.1136/bmj.m2599\n10.1016/j.scitotenv.2020.140980\n10.1093/infdis/jiaa175\n10.1631/FITEE.1700808\n10.1109/MCE.2016.2640698\n10.1155/2020/9756518\n10.1155/2020/9756518\n10.4103/ijcm.IJCM_366_20\n10.1016/j.measurement.2020.108288\n10.1109/ACCESS.2020.2977386"}
{"title": "Thoracic Point-of-Care Ultrasound: A SARS-CoV-2 Data Repository for Future Artificial Intelligence and Machine Learning.", "abstract": "Current experience suggests that artificial intelligence (AI) and machine learning (ML) may be useful in the management of hospitalized patients, including those with COVID-19. In light of the challenges faced with diagnostic and prognostic indicators in SARS-CoV-2 infection, our center has developed an international clinical protocol to collect standardized thoracic point of care ultrasound data in these patients for later AI/ML modeling. We surmise that in the future AI/ML may assist in the management of SARS-CoV-2 patients potentially leading to improved outcomes, and to that end, a corpus of curated ultrasound images and linked patient clinical metadata is an invaluable research resource.", "journal": "Surgical innovation", "date": "2021-05-08", "authors": ["Abdel-MoneimMohamed Ali", "EmranEl-Alali", "Adam SWeltz", "Scott TRehrig"], "doi": "10.1177/15533506211018671"}
{"title": "Lung Infection Segmentation for COVID-19 Pneumonia Based on a Cascade Convolutional Network from CT Images.", "abstract": "The COVID-19 pandemic is a global, national, and local public health concern which has caused a significant outbreak in all countries and regions for both males and females around the world. Automated detection of lung infections and their boundaries from medical images offers a great potential to augment the patient treatment healthcare strategies for tackling COVID-19 and its impacts. Detecting this disease from lung CT scan images is perhaps one of the fastest ways to diagnose patients. However, finding the presence of infected tissues and segment them from CT slices faces numerous challenges, including similar adjacent tissues, vague boundary, and erratic infections. To eliminate these obstacles, we propose a two-route convolutional neural network (CNN) by extracting global and local features for detecting and classifying COVID-19 infection from CT images. Each pixel from the image is classified into the normal and infected tissues. For improving the classification accuracy, we used two different strategies including fuzzy ", "journal": "BioMed research international", "date": "2021-05-07", "authors": ["RaminRanjbarzadeh", "SaeidJafarzadeh Ghoushchi", "MalikaBendechache", "AmirAmirabadi", "Mohd NizamAb Rahman", "SoroushBaseri Saadi", "AmirhosseinAghamohammadi", "MersedehKooshki Forooshani"], "doi": "10.1155/2021/5544742\n10.1080/07391102.2020.1788642\n10.1016/j.scitotenv.2020.138705\n10.1109/TMI.2020.2995965\n10.1007/s00500-019-04507-0\n10.1109/TMI.2020.3001810\n10.3390/math8081268\n10.1016/j.chest.2020.04.003\n10.1109/TMI.2020.3000314\n10.1016/j.compbiomed.2020.103795\n10.1109/TMI.2020.2996645\n10.1016/j.measurement.2019.107086\n10.1109/TMI.2020.2995508\n10.1016/j.media.2020.101794\n10.1371/journal.pone.0137016\n10.1109/TIFS.2019.2904844\n10.3390/sym12020310\n10.1080/23080477.2020.1799135\n10.1016/j.measurement.2019.107230\n10.1007/s11042-020-08699-8\n10.1016/j.measurement.2020.107989\n10.1016/j.chemolab.2020.104054\n10.1109/ACCESS.2018.2888856\n10.1016/j.eswa.2020.114549\n10.1016/j.measurement.2017.05.009\n10.1109/TIP.2016.2522378\n10.1016/j.patcog.2015.08.025\n10.1016/j.compeleceng.2017.04.019\n10.4103/jmss.JMSS_62_18\n10.1016/j.imu.2020.100412\n10.1504/IJLSM.2016.076473\n10.1016/j.matpr.2020.06.245\n10.1016/j.chaos.2020.110170\n10.1016/j.asoc.2020.106580\n10.1016/j.mehy.2020.109761\n10.1109/TIM.2017.2775345\n10.1162/tacl_a_00097\n10.1109/TIM.2018.2871353\n10.1016/j.neuroimage.2017.04.039\n10.1002/jnm.2682\n10.1016/j.eswa.2019.01.055\n10.1016/j.measurement.2019.01.060\n10.1007/s11135-019-00882-w\n10.1016/j.patcog.2015.04.019\n10.1016/j.eswa.2014.09.020\n10.1016/j.media.2016.05.004\n10.1109/TIP.2016.2547588\n10.1109/ACCESS.2020.3005510\n10.1016/j.compbiomed.2017.04.012\n10.1016/j.eswa.2020.113909\n10.1016/j.ejmp.2016.10.002\n10.1016/j.ijleo.2013.10.049\n10.1002/cpe.5293\n10.1016/j.eng.2020.04.010"}
{"title": "Prediction of Disease Progression of COVID-19 Based upon Machine Learning.", "abstract": "Since December 2019, COVID-19 has spread throughout the world. Clinical outcomes of COVID-19 patients vary among infected individuals. Therefore, it is vital to identify patients at high risk of disease progression.\nIn this retrospective, multicenter cohort study, COVID-19 patients from Huoshenshan Hospital and Taikang Tongji Hospital (Wuhan, China) were included. Clinical features showing significant differences between the severe and nonsevere groups were screened out by univariate analysis. Then, these features were used to generate classifier models to predict whether a COVID-19 case would be severe or nonsevere based on machine learning. Two test sets of data from the two hospitals were gathered to evaluate the predictive performance of the models.\nA total of 455 patients were included, and 21 features showing significant differences between the severe and nonsevere groups were selected for the training and validation set. The optimal subset, with eleven features in the \nThe predictive models were successfully established based on machine learning, and achieved satisfactory predictive performance of disease progression with optimal-feature subsets.", "journal": "International journal of general medicine", "date": "2021-05-07", "authors": ["FuminXu", "XiaoChen", "XinruYin", "QiuQiu", "JingjingXiao", "LiangQiao", "MiHe", "LiangTang", "XiaweiLi", "QiaoZhang", "YanlingLv", "ShiliXiao", "RongZhao", "YanGuo", "MingshengChen", "DongfengChen", "LiangzhiWen", "BinWang", "YongjianNian", "KaijunLiu"], "doi": "10.2147/IJGM.S294872\n10.1016/S0140-6736(20)30251-8\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1093/cid/ciaa322\n10.1016/S0140-6736(20)30566-3\n10.1183/13993003.00547-2020\n10.1016/j.jinf.2020.03.019\n10.1001/jamainternmed.2020.0994\n10.21037/apm.2020.03.26\n10.1056/NEJMoa2002032\n10.1093/cid/ciaa414\n10.1016/j.advms.2019.12.007\n10.1136/bmj.m1091\n10.1111/liv.14455\n10.1177/000313481708301123\n10.1016/j.ijcard.2010.05.061"}
{"title": "A novel augmented deep transfer learning for classification of COVID-19 and other thoracic diseases from X-rays.", "abstract": "Deep learning has provided numerous breakthroughs in natural imaging tasks. However, its successful application to medical images is severely handicapped with the limited amount of annotated training data. Transfer learning is commonly adopted for the medical imaging tasks. However, a large covariant shift between the source domain of natural images and target domain of medical images results in poor transfer learning. Moreover, scarcity of annotated data for the medical imaging tasks causes further problems for effective transfer learning. To address these problems, we develop an augmented ensemble transfer learning technique that leads to significant performance gain over the conventional transfer learning. Our technique uses an ensemble of deep learning models, where the architecture of each network is modified with extra layers to account for dimensionality change between the images of source and target data domains. Moreover, the model is hierarchically tuned to the target domain with augmented training data. Along with the network ensemble, we also utilize an ensemble of dictionaries that are based on features extracted from the augmented models. The dictionary ensemble provides an additional performance boost to our method. We first establish the effectiveness of our technique with the challenging ChestXray-14 radiography data set. Our experimental results show more than 50% reduction in the error rate with our method as compared to the baseline transfer learning technique. We then apply our technique to a recent COVID-19 data set for binary and multi-class classification tasks. Our technique achieves 99.49% accuracy for the binary classification, and 99.24% for multi-class classification.", "journal": "Neural computing & applications", "date": "2021-05-06", "authors": ["FouziaAltaf", "Syed M SIslam", "Naeem KhalidJanjua"], "doi": "10.1007/s00521-021-06044-0\n10.1016/j.patcog.2016.12.017\n10.1109/ACCESS.2019.2929365\n10.1016/j.cmpb.2019.105162\n10.3390/app10020559\n10.1109/JBHI.2016.2636929\n10.1007/s11263-009-0275-4\n10.3390/app9194130\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2017162326\n10.1038/nature14539\n10.1016/j.media.2017.07.005\n10.1016/j.compbiomed.2017.08.001\n10.1016/j.compbiomed.2018.05.018\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1007/s11263-015-0816-y\n10.1109/TMI.2016.2528162\n10.1016/j.cogsys.2018.12.007\n10.1016/j.compbiomed.2020.103805\n10.1016/j.chaos.2020.110122\n10.1109/MSP.2010.939537\n10.1016/j.mehy.2020.109761\n10.1109/TPAMI.2015.2502579"}
{"title": "Classification of COVID-19 Chest CT Images Based on Ensemble Deep Learning.", "abstract": "Novel coronavirus pneumonia (NCP) has become a global pandemic disease, and computed tomography-based (CT) image analysis and recognition are one of the important tools for clinical diagnosis. In order to assist medical personnel to achieve an efficient and fast diagnosis of patients with new coronavirus pneumonia, this paper proposes an assisted diagnosis algorithm based on ensemble deep learning. The method combines the Stacked Generalization ensemble learning with the VGG16 deep learning to form a cascade classifier, and the information constituting the cascade classifier comes from multiple subsets of the training set, each of which is used to collect deviant information about the generalization behavior of the data set, such that this deviant information fills the cascade classifier. The algorithm was experimentally validated for classifying patients with novel coronavirus pneumonia, patients with common pneumonia (CP), and normal controls, and the algorithm achieved a prediction accuracy of 93.57%, sensitivity of 94.21%, specificity of 93.93%, precision of 89.40%, and F1-score of 91.74% for the three categories. The results show that the method proposed in this paper has good classification performance and can significantly improve the performance of deep neural networks for multicategory prediction tasks.", "journal": "Journal of healthcare engineering", "date": "2021-05-04", "authors": ["XiaoshuoLi", "WenjunTan", "PanLiu", "QinghuaZhou", "JinzhuYang"], "doi": "10.1155/2021/5528441\n10.1016/j.cell.2020.04.045\n10.1002/jmv.25763\n10.1016/j.jinf.2020.03.033\n10.1007/s00330-020-06880-z\n10.1148/radiol.2462070712\n10.1109/tmi.2020.2995965\n10.1109/tmi.2020.2996256\n10.1109/access.2020.2994762\n10.1007/s10096-020-03901-z\n10.1109/TMI.2020.2993291\n10.1109/tmi.2020.2995508\n10.1088/1742-6596/1722/1/012072\n10.1007/s00138-020-01128-8\n10.1016/s0893-6080(05)80023-1"}
{"title": "Detection of COVID-19 from CT Lung Scans Using Transfer Learning.", "abstract": "This paper aims to investigate the use of transfer learning architectures in the detection of COVID-19 from CT lung scans. The study evaluates the performances of various transfer learning architectures, as well as the effects of the standard Histogram Equalization and Contrast Limited Adaptive Histogram Equalization. The findings of this study suggest that transfer learning-based frameworks are an alternative to the contemporary methods used to detect the presence of the virus in patients. The highest performing model, the VGG-19 implemented with the Contrast Limited Adaptive Histogram Equalization, on a SARS-CoV-2 dataset, achieved an accuracy and recall of 95.75% and 97.13%, respectively.", "journal": "Computational intelligence and neuroscience", "date": "2021-05-04", "authors": ["SahilLawton", "SerestinaViriri"], "doi": "10.1155/2021/5527923\n10.1186/s40537-016-0043-6\n10.1016/j.patcog.2019.01.006\n10.1080/07391102.2020.1788642\n10.1016/j.asoc.2020.106691\n10.1109/access.2020.3016780\n10.1016/j.ijmedinf.2018.06.003\n10.1118/1.2208736\n10.1101/2020.04.24.20078584\n10.1007/s00521-020-05437-x\n10.1371/journal.pone.0236621\n10.1109/access.2017.2776349\n10.1016/j.compbiomed.2020.103792"}
{"title": "Machine learning automatically detects COVID-19 using chest CTs in a large multicenter cohort.", "abstract": "To investigate machine learning classifiers and interpretable models using chest CT for detection of COVID-19 and differentiation from other pneumonias, interstitial lung disease (ILD) and normal CTs.\nOur retrospective multi-institutional study obtained 2446 chest CTs from 16 institutions (including 1161 COVID-19 patients). Training/validation/testing cohorts included 1011/50/100 COVID-19, 388/16/33 ILD, 189/16/33 other pneumonias, and 559/17/34 normal (no pathologies) CTs. A metric-based approach for the classification of COVID-19 used interpretable features, relying on logistic regression and random forests. A deep learning-based classifier differentiated COVID-19 via 3D features extracted directly from CT attenuation and probability distribution of airspace opacities.\nMost discriminative features of COVID-19 are the percentage of airspace opacity and peripheral and basal predominant opacities, concordant with the typical characterization of COVID-19 in the literature. Unsupervised hierarchical clustering compares feature distribution across COVID-19 and control cohorts. The metrics-based classifier achieved AUC = 0.83, sensitivity = 0.74, and specificity = 0.79 versus respectively 0.93, 0.90, and 0.83 for the DL-based classifier. Most of ambiguity comes from non-COVID-19 pneumonia with manifestations that overlap with COVID-19, as well as mild COVID-19 cases. Non-COVID-19 classification performance is 91% for ILD, 64% for other pneumonias, and 94% for no pathologies, which demonstrates the robustness of our method against different compositions of control groups.\nOur new method accurately discriminates COVID-19 from other types of pneumonia, ILD, and CTs with no pathologies, using quantitative imaging features derived from chest CT, while balancing interpretability of results and classification performance and, therefore, may be useful to facilitate diagnosis of COVID-19.\n\u2022 Unsupervised clustering reveals the key tomographic features including percent airspace opacity and peripheral and basal opacities most typical of COVID-19 relative to control groups. \u2022 COVID-19-positive CTs were compared with COVID-19-negative chest CTs (including a balanced distribution of non-COVID-19 pneumonia, ILD, and no pathologies). Classification accuracies for COVID-19, pneumonia, ILD, and CT scans with no pathologies are respectively 90%, 64%, 91%, and 94%. \u2022 Our deep learning (DL)-based classification method demonstrates an AUC of 0.93 (sensitivity 90%, specificity 83%). Machine learning methods applied to quantitative chest CT metrics can therefore improve diagnostic accuracy in suspected COVID-19, particularly in resource-constrained environments.", "journal": "European radiology", "date": "2021-05-03", "authors": ["Eduardo JMortani Barbosa", "BogdanGeorgescu", "ShikhaChaganti", "Gorka BastarrikaAleman", "Jordi BroncanoCabrero", "GuillaumeChabin", "ThomasFlohr", "PhilippeGrenier", "SasaGrbic", "NakulGupta", "Fran\u00e7oisMellot", "SavvasNicolaou", "ThomasRe", "PinaSanelli", "Alexander WSauter", "YoungjinYoo", "ValentinZiebandt", "DorinComaniciu"], "doi": "10.1007/s00330-021-07937-3"}
{"title": "CT-Based COVID-19 triage: Deep multitask learning improves joint identification and severity quantification.", "abstract": "The current COVID-19 pandemic overloads healthcare systems, including radiology departments. Though several deep learning approaches were developed to assist in CT analysis, nobody considered study triage directly as a computer science problem. We describe two basic setups: Identification of COVID-19 to prioritize studies of potentially infected patients to isolate them as early as possible; Severity quantification to highlight patients with severe COVID-19, thus direct them to a hospital or provide emergency medical care. We formalize these tasks as binary classification and estimation of affected lung percentage. Though similar problems were well-studied separately, we show that existing methods could provide reasonable quality only for one of these setups. We employ a multitask approach to consolidate both triage approaches and propose a convolutional neural network to leverage all available labels within a single model. In contrast with the related multitask approaches, we show the benefit from applying the classification layers to the most spatially detailed feature map at the upper part of U-Net instead of the less detailed latent representation at the bottom. We train our model on approximately 1500 publicly available CT studies and test it on the holdout dataset that consists of 123 chest CT studies of patients drawn from the same healthcare system, specifically 32 COVID-19 and 30 bacterial pneumonia cases, 30 cases with cancerous nodules, and 31 healthy controls. The proposed multitask model outperforms the other approaches and achieves ROC AUC scores of 0.87\u00b10.01 vs.\u00a0bacterial pneumonia, 0.93\u00b10.01 vs.\u00a0cancerous nodules, and 0.97\u00b10.01 vs.\u00a0healthy controls in Identification of COVID-19, and achieves 0.97\u00b10.01 Spearman Correlation in Severity quantification. We have released our code and shared the annotated lesions masks for 32 CT images of patients with COVID-19 from the test dataset.", "journal": "Medical image analysis", "date": "2021-05-02", "authors": ["MikhailGoncharov", "MaximPisov", "AlexeyShevtsov", "BorisShirokikh", "AnvarKurmukov", "IvanBlokhin", "ValeriaChernina", "AlexanderSolovev", "VictorGombolevskiy", "SergeyMorozov", "MikhailBelyaev"], "doi": "10.1016/j.media.2021.102054"}
{"title": "A comprehensive review of imaging findings in COVID-19 -\u00a0status in early 2021.", "abstract": "Medical imaging methods are assuming a greater role in the workup of patients with COVID-19, mainly in relation to the primary manifestation of pulmonary disease and the tissue distribution of the angiotensin-converting-enzyme 2 (ACE 2) receptor. However, the field is so new that no consensus view has emerged guiding clinical decisions to employ imaging procedures such as radiography, computer tomography (CT), positron emission tomography (PET), and magnetic resonance imaging, and in what measure the risk of exposure of staff to possible infection could be justified by the knowledge gained. The insensitivity of current RT-PCR methods for positive diagnosis is part of the rationale for resorting to imaging procedures. While CT is more sensitive than genetic testing in hospitalized patients, positive findings of ground glass opacities depend on the disease stage. There is sparse reporting on PET/CT with [", "journal": "European journal of nuclear medicine and molecular imaging", "date": "2021-05-02", "authors": ["AliAfshar-Oromieh", "HelmutProsch", "CorneliaSchaefer-Prokop", "Karl PeterBohn", "IanAlberts", "ClemensMingels", "MajdaThurnher", "PaulCumming", "KuangyuShi", "AlanPeters", "SilvanaGeleff", "XiaoliLan", "FengWang", "AdrianHuber", "ChristophGr\u00e4ni", "Johannes THeverhagen", "AxelRominger", "MatthiasFontanellaz", "HeikoSch\u00f6der", "AndreasChriste", "StavroulaMougiakakou", "LukasEbner"], "doi": "10.1007/s00259-021-05375-3"}
{"title": "Combining Initial Radiographs and Clinical Variables Improves Deep Learning Prognostication in Patients with COVID-19 from the Emergency Department.", "abstract": "To train a deep learning classification algorithm to predict chest radiograph severity scores and clinical outcomes in patients with coronavirus disease 2019 (COVID-19).\nIn this retrospective cohort study, patients aged 21-50 years who presented to the emergency department (ED) of a multicenter urban health system from March 10 to 26, 2020, with COVID-19 confirmation at real-time reverse-transcription polymerase chain reaction screening were identified. The initial chest radiographs, clinical variables, and outcomes, including admission, intubation, and survival, were collected within 30 days (\nThe model trained on the chest radiograph severity score produced the following areas under the receiver operating characteristic curves (AUCs): 0.80 (95% CI: 0.73, 0.88) for the chest radiograph severity score, 0.76 (95% CI: 0.68, 0.84) for admission, 0.66 (95% CI: 0.56, 0.75) for intubation, and 0.59 (95% CI: 0.49, 0.69) for death. The model trained on clinical variables produced an AUC of 0.64 (95% CI: 0.55, 0.73) for intubation and an AUC of 0.59 (95% CI: 0.50, 0.68) for death. Combining chest radiography and clinical variables increased the AUC of intubation and death to 0.88 (95% CI: 0.79, 0.96) and 0.82 (95% CI: 0.72, 0.91), respectively.\nThe combination of imaging and clinical information improves outcome predictions.", "journal": "Radiology. Artificial intelligence", "date": "2021-05-01", "authors": ["Young Joon FredKwon", "DanielleToussie", "MarkFinkelstein", "Mario ACedillo", "Samuel ZMaron", "SayanManna", "NicholasVoutsinas", "CoreyEber", "AdamJacobi", "AdamBernheim", "Yogesh SeanGupta", "Michael SChung", "Zahi AFayad", "Benjamin SGlicksberg", "Eric KOermann", "Anthony BCosta"], "doi": "10.1148/ryai.2020200098"}
{"title": "U-survival for prognostic prediction of disease progression and mortality of patients with COVID-19.", "abstract": "The rapid increase of patients with coronavirus disease 2019 (COVID-19) has introduced major challenges to healthcare services worldwide. Therefore, fast and accurate clinical assessment of COVID-19 progression and mortality is vital for the management of COVID-19 patients. We developed an automated image-based survival prediction model, called U-survival, which combines deep learning of chest CT images with the established survival analysis methodology of an elastic-net Cox survival model. In an evaluation of 383 COVID-19 positive patients from two hospitals, the prognostic bootstrap prediction performance of U-survival was significantly higher (P\u2009<\u20090.0001) than those of existing laboratory and image-based reference predictors both for COVID-19 progression (maximum concordance index: 91.6% [95% confidence interval 91.5, 91.7]) and for mortality (88.7% [88.6, 88.9]), and the separation between the Kaplan-Meier survival curves of patients stratified into low- and high-risk groups was largest for U-survival (P\u2009<\u20093\u2009\u00d7\u200910", "journal": "Scientific reports", "date": "2021-05-01", "authors": ["Janne JN\u00e4ppi", "TomokiUemura", "ChinatsuWatari", "ToruHironaka", "TohruKamiya", "HiroyukiYoshida"], "doi": "10.1038/s41598-021-88591-z\n10.1148/radiol.2020203173\n10.1038/s41591-020-0931-3\n10.1038/s41467-020-17971-2\n10.1148/radiol.2020201365\n10.1007/s00330-020-07033-y\n10.1186/s12931-020-01411-2\n10.1148/radiol.2020201433\n10.1148/radiol.2020200463\n10.7150/thno.45985\n10.7150/thno.46428\n10.1148/ryct.2020200075\n10.21037/atm-20-3554\n10.7150/thno.46465\n10.1016/j.cell.2020.04.045\n10.1007/s00330-020-07013-2\n10.1186/s41747-020-00167-0\n10.1148/ryct.2020200322\n10.3389/fbioe.2020.00898\n10.1148/ryai.2020200053\n10.1038/s42256-020-0180-7\n10.1097/RLI.0000000000000672\n10.1093/cid/ciaa414\n10.1002/scj.20178\n10.1007/s00330-020-06817-6\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1007/s12350-014-9908-2\n10.1186/s12859-020-3431-z\n10.1007/s00330-020-07034-x\n10.7326/M14-0698\n10.1093/aje/kwu140\n10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4\n10.1007/978-3-319-24574-4_28\n10.1109/RBME.2020.2987975\n10.1109/ACCESS.2017.2788044\n10.1038/nature14539\n10.18637/jss.v039.i05\n10.1080/01621459.1958.10501452\n10.1038/s42256-019-0019-2\n10.1007/s11548-019-02071-4"}
{"title": "COVID-CT-MD, COVID-19 computed tomography scan dataset applicable in machine learning and deep learning.", "abstract": "Novel Coronavirus (COVID-19) has drastically overwhelmed more than 200 countries affecting millions and claiming almost 2 million lives, since its emergence in late 2019. This highly contagious disease can easily spread, and if not controlled in a timely fashion, can rapidly incapacitate healthcare systems. The current standard diagnosis method, the Reverse Transcription Polymerase Chain Reaction (RT- PCR), is time consuming, and subject to low sensitivity. Chest Radiograph (CXR), the first imaging modality to be used, is readily available and gives immediate results. However, it has notoriously lower sensitivity than Computed Tomography (CT), which can be used efficiently to complement other diagnostic methods. This paper introduces a new COVID-19 CT scan dataset, referred to as COVID-CT-MD, consisting of not only COVID-19 cases, but also healthy and participants infected by Community Acquired Pneumonia (CAP). COVID-CT-MD dataset, which is accompanied with lobe-level, slice-level and patient-level labels, has the potential to facilitate the COVID-19 research, in particular COVID-CT-MD can assist in development of advanced Machine Learning (ML) and Deep Neural Network (DNN) based solutions.", "journal": "Scientific data", "date": "2021-05-01", "authors": ["ParnianAfshar", "ShahinHeidarian", "NastaranEnshaei", "FarnooshNaderkhani", "Moezedin JavadRafiee", "AnastasiaOikonomou", "Faranak BabakiFard", "KavehSamimi", "Konstantinos NPlataniotis", "ArashMohammadi"], "doi": "10.1038/s41597-021-00900-3\n10.1136/bmjopen-2020-042946\n10.21037/qims-20-564\n10.1137/S0036139901387186\n10.1016/j.jmir.2010.04.001\n10.4103/ijri.IJRI_34_19\n10.1007/s00330-020-07033-y\n10.1007/s42399-020-00341-w\n10.6084/m9.figshare.12991592\n10.1016/j.compbiomed.2020.103792\n10.1016/j.patrec.2020.09.010\n10.1109/TMI.2020.2996645\n10.5281/zenodo.3757476\n10.1183/13993003.01809-2020"}
{"title": "Radiation-Induced Pneumonitis in the Era of the COVID-19 Pandemic: Artificial Intelligence for Differential Diagnosis.", "abstract": "(1) Aim: To test the performance of a deep learning algorithm in discriminating radiation therapy-related pneumonitis (RP) from COVID-19 pneumonia. (2) Methods: In this retrospective study, we enrolled three groups of subjects: pneumonia-free (control group), COVID-19 pneumonia and RP patients. CT images were analyzed by mean of an artificial intelligence (AI) algorithm based on a novel deep convolutional neural network structure. The cut-off value of risk probability of COVID-19 was 30%; values higher than 30% were classified as COVID-19 High Risk, and values below 30% as COVID-19 Low Risk. The statistical analysis included the Mann-Whitney U test (significance threshold at ", "journal": "Cancers", "date": "2021-05-01", "authors": ["Francesco MariaGiordano", "EdyIppolito", "Carlo CosimoQuattrocchi", "CarloGreco", "Carlo AugustoMallio", "BiancaSanto", "PasqualeD'Alessio", "PierfilippoCrucitti", "MicheleFiore", "Bruno BeomonteZobel", "Rolando MariaD'Angelillo", "SaraRamella"], "doi": "10.3390/cancers13081960\n10.1016/S0140-6736(09)60737-6\n10.1093/jnci/djr325\n10.1200/JCO.2005.04.6110\n10.3389/fonc.2019.00877\n10.1016/j.ijrobp.2012.04.043\n10.1056/NEJMoa2002032\n10.1016/j.radonc.2020.04.009\n10.1007/s11547-020-01178-y\n10.1016/S1470-2045(20)30096-6\n10.1007/s11547-020-01232-9\n10.1007/s11547-020-01272-1\n10.1007/s11547-020-01200-3\n10.1007/s11547-020-01236-5\n10.1007/s11547-020-01202-1\n10.1007/s11547-020-01179-x\n10.1016/j.cell.2018.02.010\n10.1007/s11547-020-01195-x\n10.1007/s11547-020-01197-9\n10.1148/radiol.2020200905\n10.1148/ryct.2020200075\n10.1007/s11547-020-01291-y\n10.1016/S2589-7500(20)30199-0\n10.3390/cancers13040652\n10.21037/qims-20-782"}
{"title": "Pulmonary Hypertension in Association with Lung Disease: Quantitative CT and Artificial Intelligence to the Rescue? State-of-the-Art Review.", "abstract": "Accurate phenotyping of patients with pulmonary hypertension (PH) is an integral part of informing disease classification, treatment, and prognosis. The impact of lung disease on PH outcomes and response to treatment remains a challenging area with limited progress. Imaging with computed tomography (CT) plays an important role in patients with suspected PH when assessing for parenchymal lung disease, however, current assessments are limited by their semi-qualitative nature. Quantitative chest-CT (QCT) allows numerical quantification of lung parenchymal disease beyond subjective visual assessment. This has facilitated advances in radiological assessment and clinical correlation of a range of lung diseases including emphysema, interstitial lung disease, and coronavirus disease 2019 (COVID-19). Artificial Intelligence approaches have the potential to facilitate rapid quantitative assessments. Benefits of cross-sectional imaging include ease and speed of scan acquisition, repeatability and the potential for novel insights beyond visual assessment alone. Potential clinical benefits include improved phenotyping and prediction of treatment response and survival. Artificial intelligence approaches also have the potential to aid more focused study of pulmonary arterial hypertension (PAH) therapies by identifying more homogeneous subgroups of patients with lung disease. This state-of-the-art review summarizes recent QCT developments and potential applications in patients with PH with a focus on lung disease.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-05-01", "authors": ["KritDwivedi", "MichaelSharkey", "RobinCondliffe", "Johanna MUthoff", "SamerAlabed", "PeterMetherall", "HaipingLu", "Jim MWild", "Eric AHoffman", "Andrew JSwift", "David GKiely"], "doi": "10.3390/diagnostics11040679\n10.1093/eurheartj/ehv317\n10.1183/09031936.00079512\n10.1016/j.jacc.2013.10.036\n10.1183/16000617.0065-2019\n10.1183/13993003.01914-2018\n10.1164/rccm.201801-0095PP\n10.1164/rccm.200401-006OC\n10.1136/bmj.f2028\n10.21542/gcsp.2020.13\n10.1183/13993003.01899-2018\n10.1183/13993003.00041-2020\n10.1111/bph.15016\n10.1183/09031936.00078411\n10.1056/NEJMoa020204\n10.1016/j.jacc.2005.04.050\n10.3390/diagnostics11020311\n10.1378/chest.15-0300\n10.1016/j.healun.2018.09.011\n10.1016/j.jchf.2015.12.016\n10.1371/journal.pone.0078001\n10.1016/j.healun.2009.07.005\n10.1016/j.chest.2019.02.004\n10.1136/thx.30.5.548\n10.1164/rccm.201709-1815LE\n10.1183/13993003.00663-2018\n10.1136/thoraxjnl-2014-206088\n10.1007/s00330-020-06846-1\n10.1177/2045894019841990\n10.1038/s41568-018-0016-5\n10.1183/16000617.0181-2020\n10.1055/s-0029-1233315\n10.7326/0003-4819-158-9-201305070-00003\n10.1016/S2213-2600(19)30250-4\n10.1056/NEJMoa2008470\n10.1038/s41586-019-1799-6\n10.1613/jair.1.11222\n10.1126/science.aam6960\n10.1038/nature16961\n10.1016/S2589-7500(19)30123-2\n10.1016/S2589-7500(20)30267-3\n10.7759/cureus.10017\n10.1038/s41598-020-70629-3\n10.1038/d41586-020-03157-9\n10.1118/1.3528204\n10.1186/s13244-020-00887-2\n10.1007/s00259-019-04372-x\n10.1148/rg.2020190099\n10.1259/bjr.20170644\n10.1148/radiol.2020202439\n10.1164/rccm.201607-1385OC\n10.1148/radiol.12112516\n10.1148/radiology.211.3.r99jn05851\n10.1016/j.ejro.2020.100228\n10.1164/rccm.200606-833OC\n10.1097/RTI.0000000000000220\n10.1016/j.jacr.2017.07.007\n10.1016/j.acra.2015.05.007\n10.1053/j.semnuclmed.2011.06.004\n10.1016/j.ejrad.2018.06.020\n10.1016/j.acra.2007.04.005\n10.1148/radiol.2018180921\n10.1007/s00246-011-9963-2\n10.1016/j.media.2009.07.011\n10.1109/TMI.2018.2833385\n10.1186/s12890-019-0827-5\n10.1136/thoraxjnl-2014-206160\n10.1007/978-3-319-47157-0_22\n10.1007/978-3-319-66182-7_14\n10.1097/RCT.0b013e31820ccf18\n10.2214/AJR.16.16054\n10.12688/f1000research.18811.1\n10.1186/s13073-019-0685-z\n10.1016/S2213-2600(15)00544-5\n10.1161/CIRCULATIONAHA.105.601930\n10.1093/cvr/cvaa350\n10.1161/CIRCULATIONAHA.115.020696\n10.1183/13993003.01011-2016\n10.1183/09031936.00071812\n10.2147/TCRM.S181043\n10.1378/chest.12-2623\n10.1007/s00330-012-2427-0\n10.1136/thoraxjnl-2019-213865\n10.3109/15412550903499522\n10.1164/rccm.201506-1208PP\n10.1016/S2213-2600(17)30207-2\n10.1148/radiol.11110173\n10.1097/MCP.0000000000000252\n10.1097/01.rct.0000179595.09092.ee\n10.2217/bmm-2018-0491\n10.1148/ryct.2020190190\n10.1378/chest.113.5.1250\n10.1097/00005382-199910000-00007\n10.1016/S2213-2600(15)00096-X\n10.1164/rccm.201703-0451PP\n10.1631/jzus.B1700260\n10.1148/radiol.2015150709\n10.1016/j.acra.2014.09.001\n10.1158/0008-5472.CAN-17-0339\n10.1097/RLI.0000000000000180\n10.1038/s41598-018-28895-9\n10.1002/mp.12123\n10.1007/s00330-020-07174-0\n10.1038/s41598-019-57325-7\n10.1371/journal.pone.0102107\n10.1007/s10278-019-00204-4\n10.1148/radiol.2019191154\n10.1148/radiol.10091446\n10.1038/nm.2971\n10.5152/dir.2013.149\n10.1038/srep34921\n10.1016/j.clinimag.2016.11.005\n10.1109/MIS.2009.36\n10.1007/978-3-319-94878-2_6\n10.1038/sdata.2016.18\n10.1007/s10278-017-9976-3\n10.1016/j.clinimag.2020.04.025\n10.1136/bmj.m689\n10.1371/journal.pmed.1002683\n10.1016/j.carj.2019.06.002\n10.1016/S2589-7500(20)30219-3\n10.1016/S2589-7500(20)30218-1\n10.1148/ryai.2020190043\n10.1016/S2213-2600(18)30425-9"}
{"title": "Blockchain-Federated-Learning and Deep Learning Models for COVID-19 Detection Using CT Imaging.", "abstract": "With the increase of COVID-19 cases worldwide, an effective way is required to diagnose COVID-19 patients. The primary problem in diagnosing COVID-19 patients is the shortage and reliability of testing kits, due to the quick spread of the virus, medical practitioners are facing difficulty in identifying the positive cases. The second real-world problem is to share the data among the hospitals globally while keeping in view the privacy concerns of the organizations. Building a collaborative model and preserving privacy are the major concerns for training a global deep learning model. This paper proposes a framework that collects a small amount of data from different sources (various hospitals) and trains a global deep learning model using blockchain-based federated learning. Blockchain technology authenticates the data and federated learning trains the model globally while preserving the privacy of the organization. First, we propose a data normalization technique that deals with the heterogeneity of data as the data is gathered from different hospitals having different kinds of Computed Tomography (CT) scanners. Secondly, we use Capsule Network-based segmentation and classification to detect COVID-19 patients. Thirdly, we design a method that can collaboratively train a global model using blockchain technology with federated learning while preserving privacy. Additionally, we collected real-life COVID-19 patients' data open to the research community. The proposed framework can utilize up-to-date data which improves the recognition of CT images. Finally, we conducted comprehensive experiments to validate the proposed method. Our results demonstrate better performance for detecting COVID-19 patients.", "journal": "IEEE sensors journal", "date": "2021-04-30", "authors": ["RajeshKumar", "Abdullah AmanKhan", "JayKumar", "NoneZakria", "Noorbakhsh AmiriGolilarz", "SiminZhang", "YangTing", "ChengyuZheng", "WenyongWang"], "doi": "10.1109/JSEN.2021.3076767\n10.1109/JIOT.2020.3024180"}
{"title": "[Research progress in lung parenchyma segmentation based on computed tomography].", "abstract": "Lung diseases such as lung cancer and COVID-19 seriously endanger human health and life safety, so early screening and diagnosis are particularly important. computed tomography (CT) technology is one of the important ways to screen lung diseases, among which lung parenchyma segmentation based on CT images is the key step in screening lung diseases, and high-quality lung parenchyma segmentation can effectively improve the level of early diagnosis and treatment of lung diseases. Automatic, fast and accurate segmentation of lung parenchyma based on CT images can effectively compensate for the shortcomings of low efficiency and strong subjectivity of manual segmentation, and has become one of the research hotspots in this field. In this paper, the research progress in lung parenchyma segmentation is reviewed based on the related literatures published at domestic and abroad in recent years. The traditional machine learning methods and deep learning methods are compared and analyzed, and the research progress of improving the network structure of deep learning model is emphatically introduced. Some unsolved problems in lung parenchyma segmentation were discussed, and the development prospect was prospected, providing reference for researchers in related fields.\n\u80ba\u764c\u548c\u65b0\u51a0\u80ba\u708e\u7b49\u80ba\u90e8\u75be\u75c5\u4e25\u91cd\u5371\u5bb3\u7740\u4eba\u7c7b\u7684\u5065\u5eb7\u4e0e\u751f\u547d\u5b89\u5168\uff0c\u5176\u65e9\u671f\u7b5b\u67e5\u4e0e\u8bca\u65ad\u5c24\u4e3a\u91cd\u8981\u3002\u7535\u5b50\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\uff08CT\uff09\u6280\u672f\u662f\u80ba\u90e8\u75be\u75c5\u7b5b\u67e5\u7684\u91cd\u8981\u9014\u5f84\u4e4b\u4e00\u3002\u5176\u4e2d\uff0c\u57fa\u4e8e CT \u56fe\u50cf\u7684\u80ba\u5b9e\u8d28\u5206\u5272\u662f\u80ba\u90e8\u75be\u75c5\u7b5b\u67e5\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u9ad8\u8d28\u91cf\u7684\u80ba\u5b9e\u8d28\u5206\u5272\u80fd\u6709\u6548\u63d0\u9ad8\u80ba\u90e8\u75be\u75c5\u65e9\u671f\u8bca\u65ad\u548c\u6cbb\u7597\u6c34\u5e73\u3002\u57fa\u4e8e CT \u56fe\u50cf\u7684\u80ba\u5b9e\u8d28\u81ea\u52a8\u3001\u5feb\u901f\u3001\u51c6\u786e\u5206\u5272\u80fd\u6709\u6548\u5f25\u8865\u624b\u52a8\u5206\u5272\u6548\u7387\u4f4e\u3001\u4e3b\u89c2\u6027\u5f3a\u7b49\u4e0d\u8db3\uff0c\u5df2\u6210\u4e3a\u8be5\u9886\u57df\u7814\u7a76\u7684\u70ed\u70b9\u4e4b\u4e00\u3002\u672c\u6587\u7ed3\u5408\u8fd1\u5e74\u56fd\u5185\u5916\u53d1\u8868\u7684\u76f8\u5173\u6587\u732e\uff0c\u5bf9\u80ba\u5b9e\u8d28\u5206\u5272\u7684\u7814\u7a76\u8fdb\u5c55\u8fdb\u884c\u7efc\u8ff0\uff0c\u5bf9\u6bd4\u5206\u6790\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f51\u7edc\u7ed3\u6784\u7684\u6539\u8fdb\u7b49\u7814\u7a76\u8fdb\u5c55\u3002\u8ba8\u8bba\u4e86\u80ba\u5b9e\u8d28\u5206\u5272\u4e2d\u5f85\u89e3\u51b3\u7684\u4e00\u4e9b\u95ee\u9898\uff0c\u5bf9\u53d1\u5c55\u524d\u666f\u8fdb\u884c\u4e86\u5c55\u671b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u79d1\u7814\u5de5\u4f5c\u8005\u63d0\u4f9b\u53c2\u8003\u3002.", "journal": "Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi", "date": "2021-04-30", "authors": ["HanguangXiao", "ZhiqiangRan", "JinfengHuang", "HuijiaoRen", "ChangLiu", "BanglinZhang", "BolongZhang", "JunDang"], "doi": "10.7507/1001-5515.202008032"}
{"title": "A Deep Learning Radiomics Model to Identify Poor Outcome in COVID-19 Patients With Underlying Health Conditions: A Multicenter Study.", "abstract": "Coronavirus disease 2019 (COVID-19) has caused considerable morbidity and mortality, especially in patients with underlying health conditions. A precise prognostic tool to identify poor outcomes among such cases is desperately needed.\nTotal 400 COVID-19 patients with underlying health conditions were retrospectively recruited from 4 centers, including 54 dead cases (labeled as poor outcomes) and 346 patients discharged or hospitalized for at least 7 days since initial CT scan. Patients were allocated to a training set (n = 271), a test set (n = 68), and an external test set (n = 61). We proposed an initial CT-derived hybrid model by combining a 3D-ResNet10 based deep learning model and a quantitative 3D radiomics model to predict the probability of COVID-19 patients reaching poor outcome. The model performance was assessed by area under the receiver operating characteristic curve (AUC), survival analysis, and subgroup analysis.\nThe hybrid model achieved AUCs of 0.876 (95% confidence interval: 0.752-0.999) and 0.864 (0.766-0.962) in test and external test sets, outperforming other models. The survival analysis verified the hybrid model as a significant risk factor for mortality (hazard ratio, 2.049 [1.462-2.871], P < 0.001) that could well stratify patients into high-risk and low-risk of reaching poor outcomes (P < 0.001).\nThe hybrid model that combined deep learning and radiomics could accurately identify poor outcomes in COVID-19 patients with underlying health conditions from initial CT scans. The great risk stratification ability could help alert risk of death and allow for timely surveillance plans.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-04-28", "authors": ["SiwenWang", "DiDong", "LiangLi", "HailinLi", "YanBai", "YahuaHu", "YuanyiHuang", "XiangrongYu", "SibinLiu", "XiaomingQiu", "LigongLu", "MeiyunWang", "YunfeiZha", "JieTian"], "doi": "10.1109/JBHI.2021.3076086"}
{"title": "Functional Connectome Prediction of Anxiety Related to the COVID-19 Pandemic.", "abstract": "Increased anxiety in response to the COVID-19 pandemic has been widely noted. The purpose of this study was to test whether the prepandemic functional connectome predicted individual anxiety induced by the pandemic.\nAnxiety scores from healthy undergraduate students were collected during the severe and remission periods of the pandemic (first survey, February 22-28, 2020, N=589; second survey, April 24 to May 1, 2020, N=486). Brain imaging data and baseline (daily) anxiety ratings were acquired before the pandemic. The predictive performance of the functional connectome on individual anxiety was examined using machine learning and was validated in two external undergraduate student samples (N=149 and N=474). The clinical relevance of the findings was further explored by applying the connectome-based neuromarkers of pandemic-related anxiety to distinguish between individuals with specific mental disorders and matched healthy control subjects (generalized anxiety disorder, N=43; major depression, N=536; schizophrenia, N=72).\nAnxiety scores increased from the prepandemic baseline to the severe stage of the pandemic and remained high in the remission stage. The prepandemic functional connectome predicted pandemic-related anxiety and generalized to the external sample but showed poor performance for predicting daily anxiety. The connectome-based neuromarkers of pandemic-related anxiety further distinguished between participants with generalized anxiety and healthy control subjects but were not useful for diagnostic classification in major depression and schizophrenia.\nThese findings demonstrate the feasibility of using the functional connectome to predict individual anxiety induced by major stressful events (e.g., the current global health crisis), which advances our understanding of the neurobiological basis of anxiety susceptibility and may have implications for developing targeted psychological and clinical interventions that promote the reduction of stress and anxiety.", "journal": "The American journal of psychiatry", "date": "2021-04-27", "authors": ["LiHe", "DongtaoWei", "FanYang", "JieZhang", "WeiCheng", "JianfengFeng", "WenjingYang", "KaixiangZhuang", "QunlinChen", "ZhitingRen", "YuLi", "XiaoqinWang", "YuMao", "ZhiyiChen", "MeiLiao", "HuiruCui", "ChunboLi", "QinghuaHe", "XuLei", "TingyongFeng", "HongChen", "PengXie", "Edmund TRolls", "LinyanSu", "LingjiangLi", "JiangQiu"], "doi": "10.1176/appi.ajp.2020.20070979"}
{"title": "Lack of AI-based method for pneumocystis pneumonia classification in radiological diagnosis of SARS-CoV-2.", "abstract": null, "journal": "Clinical imaging", "date": "2021-04-26", "authors": ["NarjesBenameur", "RamziMahmoudi", "SorayaZaid", "YounesArous", "BadiiHmida", "AsmaMigaou", "Mohamed HediBedoui"], "doi": "10.1016/j.clinimag.2021.03.037\n10.1016/j.clinimag.2021.01.019\n10.1093/ofid/ofaa633\n10.1186/s41824-020-00088-6\n10.1007/s10489-020-01900-3\n10.1002/ima.22469\n10.1109/IIPHDW.2018.8388338"}
{"title": "DenseCapsNet: Detection of COVID-19 from X-ray images using a capsule neural network.", "abstract": "At present, the global pandemic as it relates to novel coronavirus pneumonia is still a very difficult situation. Due to the recent outbreak of novel coronavirus pneumonia, novel chest X-ray (CXR) images that can be used for deep learning analysis are very rare. To solve this problem, we propose a deep learning framework that integrates a convolutional neural network and a capsule network. DenseCapsNet, a new deep learning framework, is formed by the fusion of a dense convolutional network (DenseNet) and the capsule neural network (CapsNet), leveraging their respective advantages and reducing the dependence of convolutional neural networks on a large amount of data. Using 750 CXR images of lungs of healthy patients as well as those of patients with other pneumonia and novel coronavirus pneumonia, the method can obtain an accuracy of 90.7% and an F1 score of 90.9%, and the sensitivity for detecting COVID-19 can reach 96%. These results show that the deep fusion neural network DenseCapsNet has good performance in novel coronavirus pneumonia CXR radiography detection.", "journal": "Computers in biology and medicine", "date": "2021-04-24", "authors": ["HaoQuan", "XiaosongXu", "TingtingZheng", "ZhiLi", "MingfangZhao", "XiaoyuCui"], "doi": "10.1016/j.compbiomed.2021.104399\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3010287\n10.1016/j.compbiomed.2020.103869"}
{"title": "CARJ 2021: Year in Review.", "abstract": "The past year has been one of unprecedented challenge for the modern world and especially the medical profession. This review explores some of the most impactful topics published in the CARJ during the COVID-19 pandemic including physician wellbeing and burnout, patient safety, and technological innovations including dual energy CT, quantitative imaging and ultra-high frequency ultrasound. The impact of the COVID-19 pandemic on trainee education is discussed and evidence-based tips for providing value-added care are reviewed. Patient privacy considerations relevant to the development of artificial intelligence applications for medical imaging are explored. These publications in the CARJ demonstrate that although this year has brought adversity, it has also been a harbinger for new and exciting areas of focus in our field.", "journal": "Canadian Association of Radiologists journal = Journal l'Association canadienne des radiologistes", "date": "2021-04-24", "authors": ["Caitlin JWard", "Christian Bvan der Pol", "Michael NPatlas"], "doi": "10.1177/08465371211008428"}
{"title": "RANDGAN: Randomized generative adversarial network for detection of COVID-19 in chest X-ray.", "abstract": "COVID-19 spread across the globe at an immense rate and has left healthcare systems incapacitated to diagnose and test patients at the needed rate. Studies have shown promising results for detection of COVID-19 from viral bacterial pneumonia in chest X-rays. Automation of COVID-19 testing using medical images can speed up the testing process of patients where health care systems lack sufficient numbers of the reverse-transcription polymerase chain reaction tests. Supervised deep learning models such as convolutional neural networks need enough labeled data for all classes to correctly learn the task of detection. Gathering labeled data is a cumbersome task and requires time and resources which could further strain health care systems and radiologists at the early stages of a pandemic such as COVID-19. In this study, we propose a randomized generative adversarial network (RANDGAN) that detects images of an unknown class (COVID-19) from known and labelled classes (Normal and Viral Pneumonia) without the need for labels and training data from the unknown class of images (COVID-19). We used the largest publicly available COVID-19 chest X-ray dataset, COVIDx, which is comprised of Normal, Pneumonia, and COVID-19 images from multiple public databases. In this work, we use transfer learning to segment the lungs in the COVIDx dataset. Next, we show why segmentation of the region of interest (lungs) is vital to correctly learn the task of classification, specifically in datasets that contain images from different resources as it is the case for the COVIDx dataset. Finally, we show improved results in detection of COVID-19 cases using our generative model (RANDGAN) compared to conventional generative adversarial networks for anomaly detection in medical images, improving the area under the ROC curve from 0.71 to 0.77.", "journal": "Scientific reports", "date": "2021-04-23", "authors": ["SamanMotamed", "PatrikRogalla", "FarzadKhalvati"], "doi": "10.1038/s41598-021-87994-2\n10.1183/13993003.00524-2020\n10.1016/j.ejim.2012.04.016\n10.1148/ryai.2020200053\n10.7326/M20-1495\n10.2200/S00196ED1V01Y200906AIM006\n10.1148/ryct.2020200034"}
{"title": "Pandemic analysis of infection and death correlated with genomic open reading frame 10 mutation in severe acute respiratory syndrome coronavirus 2 victims.", "abstract": "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) continues the pandemic spread of the coronavirus disease 2019 (COVID-19), over 60 million people confirmed infected and at least 1.8 million dead. One of the most known features of this RNA virus is its easiness to be mutated. In late 2020, almost no region of this SARS-CoV-2 genome can be found completely conserved within the original Wuhan coronavirus. Any information of the SARS-CoV-2 variants emerged through as time being will be evaluated for diagnosis, treatment, and prevention of COVID-19.\nWe extracted more than two million data of SARS-CoV-2 infected patients from the open COVID-19 dashboard. The sequences of the 38-amino acid putative open reading frame 10 (Orf10) protein within infected patients were gathered output through from National Center for Biotechnology Information and the mutation rates in each position were analyzed and presented in each month of 2020. The mutation rates of A8 and V30 within Orf10 are displayed in selected counties: United States, India, German, and Japan.\nThe numbers of COVID-19 patients are correlated to the death numbers, but not with the death rates (stable and <3%). The amino acid positions locating at A8(F/G/L), I13, and V30(L) within the Orf10 sequence stay the highest mutation rate; N5, N25, and N36 rank at the lowest one. A8F expressed highly dominant in Japan (over 80%) and German (around 40%) coming to the end of 2020, but no significant finding in other countries.\nThe results demonstrate via mutation analysis of Orf10 can be further combined with advanced tools such as molecular simulation, artificial intelligence, and biosensors that can practically revealed for protein interactions and thus to imply the authentic Orf10 function of SARS-CoV-2 in the future.", "journal": "Journal of the Chinese Medical Association : JCMA", "date": "2021-04-23", "authors": ["De-MingYang", "Fan-ChiLin", "Pin-HsingTsai", "YuehChien", "Mong-LienWang", "Yi-PingYang", "Tai-JayChang"], "doi": "10.1097/JCMA.0000000000000542\n10.5281/zenodo.4028830\n10.1101/2020.09.06.284976\n10.1101/2020.10.26.355784"}
{"title": "COVID-19 Automatic Diagnosis With Radiographic Imaging: Explainable Attention Transfer Deep Neural Networks.", "abstract": "Researchers seek help from deep learning methods to alleviate the enormous burden of reading radiological images by clinicians during the COVID-19 pandemic. However, clinicians are often reluctant to trust deep models due to their black-box characteristics. To automatically differentiate COVID-19 and community-acquired pneumonia from healthy lungs in radiographic imaging, we propose an explainable attention-transfer classification model based on the knowledge distillation network structure. The attention transfer direction always goes from the teacher network to the student network. Firstly, the teacher network extracts global features and concentrates on the infection regions to generate attention maps. It uses a deformable attention module to strengthen the response of infection regions and to suppress noise in irrelevant regions with an expanded reception field. Secondly, an image fusion module combines attention knowledge transferred from teacher network to student network with the essential information in original input. While the teacher network focuses on global features, the student branch focuses on irregularly shaped lesion regions to learn discriminative features. Lastly, we conduct extensive experiments on public chest X-ray and CT datasets to demonstrate the explainability of the proposed architecture in diagnosing COVID-19.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-04-22", "authors": ["WenqiShi", "LiTong", "YuandaZhu", "May DWang"], "doi": "10.1109/JBHI.2021.3074893"}
{"title": "CT-based radiomics for predicting the rapid progression of coronavirus disease 2019 (COVID-19) pneumonia lesions.", "abstract": "To develop and validate a radiomic model to predict the rapid progression (defined as volume growth of pneumonia lesions > 50% within seven days) in patients with coronavirus disease 2019 (COVID-19).\nPatients with laboratory-confirmed COVID-19 who underwent longitudinal chest CT between January 01 and February 18, 2020 were included. A total of 1316 radiomic features were extracted from the lung parenchyma window for each CT. The least absolute shrinkage and selection operator (LASSO), Relief, Las Vegas Wrapper (LVW), L1-norm-Support Vector Machine (L1-norm-SVM), and recursive feature elimination (RFE) were applied to select the features that associated with rapid progression. Four machine learning classifiers were used for modeling, including Support Vector Machine (SVM), Random Forest (RF), Logistic Regression (LR), and Decision Tree (DT). Accordingly, 20 radiomic models were developed on the basis of 296 CT scans and validated in 74 CT scans. Model performance was determined by the receiver operating characteristic curve.\nA total of 107 patients (median age, 49.0 years, interquartile range, 35-54) were evaluated. The patients underwent a total of 370 chest CT scans with a median interval of 4 days (interquartile range, 3-5 days). The combination methods of L1-norm SVM and SVM with 17 radiomic features yielded the highest performance in predicting the likelihood of rapid progression of pneumonia lesions on next CT scan, with an AUC of 0.857 (95% CI: 0.766-0.947), sensitivity of 87.5%, and specificity of 70.7%.\nOur radiomic model based on longitudinal chest CT data could predict the rapid progression of pneumonia lesions, which may facilitate the CT follow-up intervals and reduce the radiation.\nRadiomic features extracted from the current chest CT have potential in predicting the likelihood of rapid progression of pneumonia lesions on the next chest CT, which would improve clinical decision-making regarding timely treatment.", "journal": "The British journal of radiology", "date": "2021-04-22", "authors": ["BinZhang", "Ma-Yi-di-LiNi-Jia-Ti", "RuikeYan", "NanAn", "LvChen", "ShuyiLiu", "LuyanChen", "QiuyingChen", "MinminLi", "ZhuozhiChen", "JingjingYou", "YuhaoDong", "ZhiyuanXiong", "ShuixingZhang"], "doi": "10.1259/bjr.20201007\n10.1001/jama.2020.12839\n10.1007/s00259-020-04795-x\n10.1148/radiol.2020203173\n10.1016/j.ejrad.2020.109202\n10.1007/s10140-020-01833-x\n10.4329/wjr.v8.i12.902\n10.1007/s10140-020-01838-6\n10.3238/arztebl.2020.0389\n10.12890/2020_001680\n10.1088/1361-6498/aba16a\n10.1186/s12880-020-00456-5\n10.1109/ACCESS.2019.2906350\n10.1023/A:1012487302797\n10.1109/LSP.2014.2337313\n10.1007/s00330-020-07032-z\n10.7150/thno.46428\n10.1097/RCT.0000000000001094\n10.1016/j.acra.2020.09.004\n10.1007/s00330-020-07012-3\n10.1088/1361-6560/abbf9e\n10.1097/RTI.0000000000000544\n10.21037/atm-20-3026\n10.1186/s12880-020-00521-z\n10.1038/nrclinonc.2017.141\n10.1016/j.canlet.2017.06.004\n10.1148/radiol.2015151169\n10.1093/annonc/mdx034"}
{"title": "Artificial intelligence (AI) for medical imaging to combat coronavirus disease (COVID-19): a detailed review with direction for future research.", "abstract": "Since early 2020, the whole world has been facing the deadly and highly contagious disease named coronavirus disease (COVID-19) and the World Health Organization declared the pandemic on 11 March 2020. Over 23 million positive cases of COVID-19 have been reported till late August 2020. Medical images such as chest X-rays and Computed Tomography scans are becoming one of the main leading clinical diagnosis tools in fighting against COVID-19, underpinned by Artificial Intelligence based techniques, resulting in rapid decision-making in saving lives. This article provides an extensive review of AI-based methods to assist medical practitioners with comprehensive knowledge of the efficient AI-based methods for efficient COVID-19 diagnosis. Nearly all the reported methods so far along with their pros and cons as well as recommendations for improvements are discussed, including image acquisition, segmentation, classification, and follow-up diagnosis phases developed between 2019 and 2020. AI and machine learning technologies have boosted the accuracy of Covid-19 diagnosis, and most of the widely used deep learning methods have been implemented and worked well with a small amount of data for COVID-19 diagnosis. This review presents a detailed mythological analysis for the evaluation of AI-based methods used in the process of detecting COVID-19 from medical images. However, due to the quick outbreak of Covid-19, there are not many ground-truth datasets available for the communities. It is necessary to combine clinical experts' observations and information from images to have a reliable and efficient COVID-19 diagnosis. This paper suggests that future research may focus on multi-modality based models as well as how to select the best model architecture where AI can introduce more intelligence to medical systems to capture the characteristics of diseases by learning from multi-modality data to obtain reliable results for COVID-19 diagnosis for timely treatment .", "journal": "Artificial intelligence review", "date": "2021-04-21", "authors": ["Toufique ASoomro", "LihongZheng", "Ahmed JAfifi", "AhmedAli", "MingYin", "JunbinGao"], "doi": "10.1007/s10462-021-09985-z\n10.1007/s13246-020-00865-4\n10.1007/s00330-018-5745-z\n10.1007/s11548-018-1895-3\n10.1038/s41598-019-56847-4\n10.1016/j.eswa.2019.113114\n10.1007/s11831-019-09344-w\n10.1016/j.measurement.2019.05.076\n10.1007/s11042-019-7327-8\n10.1145/3411760\n10.2214/AJR.06.0370\n10.1007/s10489-019-01511-7\n10.1145/2816795.2818013\n10.1016/j.jpha.2020.03.004\n10.1148/ryct.2020200044\n10.1038/s41598-019-56847-4\n10.1016/j.eng.2020.04.010"}
{"title": "Convolutional Sparse Support Estimator-Based COVID-19 Recognition From X-Ray Images.", "abstract": "Coronavirus disease (COVID-19) has been the main agenda of the whole world ever since it came into sight. X-ray imaging is a common and easily accessible tool that has great potential for COVID-19 diagnosis and prognosis. Deep learning techniques can generally provide state-of-the-art performance in many classification tasks when trained properly over large data sets. However, data scarcity can be a crucial obstacle when using them for COVID-19 detection. Alternative approaches such as representation-based classification [collaborative or sparse representation (SR)] might provide satisfactory performance with limited size data sets, but they generally fall short in performance or speed compared to the neural network (NN)-based methods. To address this deficiency, convolution support estimation network (CSEN) has recently been proposed as a bridge between representation-based and NN approaches by providing a noniterative real-time mapping from query sample to ideally SR coefficient support, which is critical information for class decision in representation-based techniques. The main premises of this study can be summarized as follows: 1) A benchmark X-ray data set, namely QaTa-Cov19, containing over 6200 X-ray images is created. The data set covering 462 X-ray images from COVID-19 patients along with three other classes; bacterial pneumonia, viral pneumonia, and normal. 2) The proposed CSEN-based classification scheme equipped with feature extraction from state-of-the-art deep NN solution for X-ray images, CheXNet, achieves over 98% sensitivity and over 95% specificity for COVID-19 recognition directly from raw X-ray images when the average performance of 5-fold cross validation over QaTa-Cov19 data set is calculated. 3) Having such an elegant COVID-19 assistive diagnosis performance, this study further provides evidence that COVID-19 induces a unique pattern in X-rays that can be discriminated with high accuracy.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-04-20", "authors": ["MehmetYamac", "MeteAhishali", "AysenDegerli", "SerkanKiranyaz", "Muhammad E HChowdhury", "MoncefGabbouj"], "doi": "10.1109/TNNLS.2021.3070467"}
{"title": "CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR.", "abstract": "To mitigate the spread of the current coronavirus disease 2019 (COVID-19) pandemic, it is crucial to have an effective screening of infected patients to be isolated and treated. Chest X-Ray (CXR) radiological imaging coupled with Artificial Intelligence (AI) applications, in particular Convolutional Neural Network (CNN), can speed the COVID-19 diagnostic process. In this paper, we optimize the data augmentation and the CNN hyperparameters for detecting COVID-19 from CXRs in terms of validation accuracy. This optimization increases the accuracy of the popular CNN architectures such as the Visual Geometry Group network (VGG-19) and the Residual Neural Network (ResNet-50), by 11.93% and 4.97%, respectively. We then proposed CovidXrayNet model that is based on EfficientNet-B0 and our optimization results. We evaluated CovidXrayNet on two datasets, including our generated balanced COVIDcxr dataset (960 CXRs) and the benchmark COVIDx dataset (15,496 CXRs). With only 30 epochs of training, CovidXrayNet achieves state-of-the-art accuracy of 95.82% on the COVIDx dataset in the three-class classification task (COVID-19, normal or pneumonia). The CovidXRayNet model, the COVIDcxr dataset, and several optimization experiments are publicly available at https://github.com/MaramMonshi/CovidXrayNet.", "journal": "Computers in biology and medicine", "date": "2021-04-19", "authors": ["Maram Mahmoud AMonshi", "JosiahPoon", "VeraChung", "Fahad MahmoudMonshi"], "doi": "10.1016/j.compbiomed.2021.104375"}
{"title": "COVID-19 pneumonia and the pulmonary vasculature: a marriage made in hell.", "abstract": "Automated analysis of medical images is not new [1\u20133]. Researchers in the respiratory sciences and, particularly, the field of interstitial lung diseases, have long enthused about the potential for computers to analyse medical images thereby revealing \u201csignals\u201d hitherto invisible to the human eye: an enthusiasm only enhanced by recent developments in machine learning and artificial intelligence [4\u20136]. By leveraging the central importance of computed tomography (CT) scanning for diagnosis, treatment decisions and prognostication, a key aim is to identify imaging biomarkers to more accurately phenotype disease and, in so doing, move a step closer to truly patient-centric medicine. Another goal is to apply novel imaging analyses to pathogenesis, disease \u201cbehaviour\u201d and prognostication in the hope that this might unlock new therapeutic approaches. Given the digital nature of the data and the potentially myriad imaging patterns, frequently compounded by patient, therapeutic and disease-based factors, lung imaging is ideally suited to more sophisticated analytic approaches.", "journal": "The European respiratory journal", "date": "2021-04-18", "authors": ["Peter MGeorge", "Sujal RDesai"], "doi": "10.1183/13993003.00811-2021\n10.1378/chest.94.4.782\n10.1148/radiol.2282020274\n10.1097/RTI.0b013e31825148c9\n10.1016/S2213-2600(18)30286-8\n10.1183/13993003.02341-2018\n10.1183/23120541.00290-2019\n10.1183/13993003.04133-2020\n10.1016/j.acra.2009.07.022\n10.1136/thoraxjnl-2013-204809\n10.1056/NEJMoa2015432\n10.1164/rccm.202004-1412OC\n10.1136/thoraxjnl-2020-215395\n10.1152/ajplung.00330.2019\n10.1056/NEJMoa1908681\n10.1016/S2213-2600(20)30355-6"}
{"title": "BS-Net: Learning COVID-19 pneumonia severity on a large chest X-ray dataset.", "abstract": "In this work we design an end-to-end deep learning architecture for predicting, on Chest X-rays images (CXR), a multi-regional score conveying the degree of lung compromise in COVID-19 patients. Such semi-quantitative scoring system, namely Brixia\u00a0score, is applied in serial monitoring of such patients, showing significant prognostic value, in one of the hospitals that experienced one of the highest pandemic peaks in Italy. To solve such a challenging visual task, we adopt a weakly supervised learning strategy structured to handle different tasks (segmentation, spatial alignment, and score estimation) trained with a \"from-the-part-to-the-whole\" procedure involving different datasets. In particular, we exploit a clinical dataset of almost 5,000 CXR annotated images collected in the same hospital. Our BS-Net demonstrates self-attentive behavior and a high degree of accuracy in all processing stages. Through inter-rater agreement tests and a gold standard comparison, we show that our solution outperforms single human annotators in rating accuracy and consistency, thus supporting the possibility of using this tool in contexts of computer-assisted monitoring. Highly resolved (super-pixel level) explainability maps are also generated, with an original technique, to visually help the understanding of the network activity on the lung areas. We also consider other scores proposed in literature and provide a comparison with a recently proposed non-specific approach. We eventually test the performance robustness of our model on an assorted public COVID-19 dataset, for which we also provide Brixia\u00a0score annotations, observing good direct generalization and fine-tuning capabilities that highlight the portability of BS-Net in other clinical settings. The CXR dataset along with the source code and the trained model are publicly released for research purposes.", "journal": "Medical image analysis", "date": "2021-04-17", "authors": ["AlbertoSignoroni", "MattiaSavardi", "SergioBenini", "NicolaAdami", "RiccardoLeonardi", "PaoloGibellini", "FilippoVaccher", "MarcoRavanelli", "AndreaBorghesi", "RobertoMaroldi", "DavideFarina"], "doi": "10.1016/j.media.2021.102046\n10.1148/radiol.2020202439\n10.1007/s00330-020-07504-2"}
{"title": "Automated detection of pneumonia cases using deep transfer learning with paediatric chest X-ray images.", "abstract": "Pneumonia is a lung infection and causes the inflammation of the small air sacs (Alveoli) in one or both lungs. Proper and faster diagnosis of pneumonia at an early stage is imperative for optimal patient care. Currently, chest X-ray is considered as the best imaging modality for diagnosing pneumonia. However, the interpretation of chest X-ray images is challenging. To this end, we aimed to use an automated convolutional neural network-based transfer-learning approach to detect pneumonia in paediatric chest radiographs.\nHerein, an automated convolutional neural network-based transfer-learning approach using four different pre-trained models (\nAll proposed models provide accuracy greater than 83.0% for binary classification. The pre-trained DenseNet121 model provides the highest classification performance of automated pneumonia classification with 86.8% accuracy, followed by Xception model with an accuracy of 86.0%. The sensitivity of the proposed models was greater than 91.0%. The Xception and DenseNet121 models achieve the highest classification performance with F1-score greater than 89.0%. The plotted area under curve of receiver operating characteristics of VGG19, Xception, ResNet50, and DenseNet121 models are 0.78, 0.81, 0.81, and 0.86, respectively.\nOur data showed that the proposed models achieve a high accuracy for binary classification. Transfer learning was used to accelerate training of the proposed models and resolve the problem associated with insufficient data. We hope that these proposed models can help radiologists for a quick diagnosis of pneumonia at radiology departments. Moreover, our proposed models may be useful to detect other chest-related diseases such as novel Coronavirus 2019.\nHerein, we used transfer learning as a machine learning approach to accelerate training of the proposed models and resolve the problem associated with insufficient data. Our proposed models achieved accuracy greater than 83.0% for binary classification.", "journal": "The British journal of radiology", "date": "2021-04-17", "authors": ["MohammadSalehi", "RezaMohammadi", "HamedGhaffari", "NahidSadighi", "RezaReiazi"], "doi": "10.1259/bjr.20201263\n10.3390/s20041068\n10.1016/S0140-6736(10)61459-6\n10.1016/j.compbiomed.2020.103898\n10.1016/j.cmpb.2019.06.023\n10.1016/j.crad.2018.12.015\n10.3390/s19122781\n10.1148/rg.2017160130\n10.1148/radiol.2018171820\n10.3348/kjr.2019.0312\n10.1155/2018/4168538\n10.1148/radiol.2017162326\n10.1109/TKDE.2009.191\n10.1186/s40537-016-0043-6\n10.1016/j.cell.2018.02.010\n10.1613/jair.953\n10.31661/jbpe.v0i0.2008-1153\n10.1007/978-981-15-6315-7_14\n10.2196/19104"}
{"title": "A deep-learning pipeline for the diagnosis and discrimination of viral, non-viral and COVID-19 pneumonia from chest X-ray images.", "abstract": "Common lung diseases are first diagnosed using chest X-rays. Here, we show that a fully automated deep-learning pipeline for the standardization of chest X-ray images, for the visualization of lesions and for disease diagnosis can identify viral pneumonia caused by coronavirus disease 2019 (COVID-19) and assess its severity, and can also discriminate between viral pneumonia caused by COVID-19 and other types of pneumonia. The deep-learning system was developed using a heterogeneous multicentre dataset of 145,202 images, and tested retrospectively and prospectively with thousands of additional images across four patient cohorts and multiple countries. The system generalized across settings, discriminating between viral pneumonia, other types of pneumonia and the absence of disease with areas under the receiver operating characteristic curve (AUCs) of 0.94-0.98; between severe and non-severe COVID-19 with an AUC of 0.87; and between COVID-19 pneumonia and other viral or non-viral pneumonia with AUCs of 0.87-0.97. In an independent set of 440 chest X-rays, the system performed comparably to senior radiologists and improved the performance of junior radiologists. Automated deep-learning systems for the assessment of pneumonia could facilitate early intervention and provide support for clinical decision-making.", "journal": "Nature biomedical engineering", "date": "2021-04-17", "authors": ["GuangyuWang", "XiaohongLiu", "JunShen", "ChengdiWang", "ZhihuanLi", "LinsenYe", "XingwangWu", "TingChen", "KaiWang", "XuanZhang", "ZhongguoZhou", "JianYang", "YeSang", "RuiyunDeng", "WenhuaLiang", "TaoYu", "MingGao", "JinWang", "ZehongYang", "HuiminCai", "GuangmingLu", "LingyanZhang", "LeiYang", "WenqinXu", "WinstonWang", "AndreaOlvera", "IanZiyar", "CharlotteZhang", "OulanLi", "WeihuaLiao", "JunLiu", "WenChen", "WeiChen", "JichanShi", "LianghongZheng", "LongjiangZhang", "ZhihanYan", "XiaoguangZou", "GuipingLin", "GuiqunCao", "Laurance LLau", "LongMo", "YongLiang", "MichaelRoberts", "EvisSala", "Carola-BibianeSch\u00f6nlieb", "MansonFok", "Johnson Yiu-NamLau", "TaoXu", "JianxingHe", "KangZhang", "WeiminLi", "TianxinLin"], "doi": "10.1038/s41551-021-00704-1"}
{"title": "GraphCovidNet: A graph neural network based model for detecting COVID-19 from CT scans and X-rays of chest.", "abstract": "COVID-19, a viral infection originated from Wuhan, China has spread across the world and it has currently affected over 115 million people. Although vaccination process has already started, reaching sufficient availability will take time. Considering the impact of this widespread disease, many research attempts have been made by the computer scientists to screen the COVID-19 from Chest X-Rays (CXRs) or Computed Tomography (CT) scans. To this end, we have proposed GraphCovidNet, a Graph Isomorphic Network (GIN) based model which is used to detect COVID-19 from CT-scans and CXRs of the affected patients. Our proposed model only accepts input data in the form of graph as we follow a GIN based architecture. Initially, pre-processing is performed to convert an image data into an undirected graph to consider only the edges instead of the whole image. Our proposed GraphCovidNet model is evaluated on four standard datasets: SARS-COV-2 Ct-Scan dataset, COVID-CT dataset, combination of covid-chestxray-dataset, Chest X-Ray Images (Pneumonia) dataset and CMSC-678-ML-Project dataset. The model shows an impressive accuracy of 99% for all the datasets and its prediction capability becomes 100% accurate for the binary classification problem of detecting COVID-19 scans. Source code of this work can be found at GitHub-link .", "journal": "Scientific reports", "date": "2021-04-17", "authors": ["PritamSaha", "DebadyutiMukherjee", "Pawan KumarSingh", "AliAhmadian", "MassimilianoFerrara", "RamSarkar"], "doi": "10.1038/s41598-021-87523-1\n10.1097/RLI.0000000000000670\n10.1016/S0140-6736(20)30728-5\n10.1007/s10489-020-01831-z\n10.1016/j.imu.2020.100427\n10.1007/s11356-020-10133-3\n10.1371/journal.pone.0235187\n10.1109/access.2020.3010287\n10.2214/ajr.174.1.1740071\n10.1016/j.media.2005.02.002\n10.1016/j.cell.2018.02.010\n10.1016/j.eswa.2020.113909\n10.1109/TMI.2013.2290491\n10.1109/TMI.2012.2206398\n10.1016/j.compag.2018.03.032\n10.3390/diagnostics11020315\n10.1016/j.measurement.2020.108288\n10.1109/TNN.2008.2005605"}
{"title": "Machine learning based on clinical characteristics and chest CT quantitative measurements for prediction of adverse clinical outcomes in hospitalized patients with COVID-19.", "abstract": "To develop and validate a machine learning model for the prediction of adverse outcomes in hospitalized patients with COVID-19.\nWe included 424 patients with non-severe COVID-19 on admission from January 17, 2020, to February 17, 2020, in the primary cohort of this retrospective multicenter study. The extent of lung involvement was quantified on chest CT images by a deep learning-based framework. The composite endpoint was the occurrence of severe or critical COVID-19 or death during hospitalization. The optimal machine learning classifier and feature subset were selected for model construction. The performance was further tested in an external validation cohort consisting of 98 patients.\nThere was no significant difference in the prevalence of adverse outcomes (8.7% vs. 8.2%, p = 0.858) between the primary and validation cohorts. The machine learning method extreme gradient boosting (XGBoost) and optimal feature subset including lactic dehydrogenase (LDH), presence of comorbidity, CT lesion ratio (lesion%), and hypersensitive cardiac troponin I (hs-cTnI) were selected for model construction. The XGBoost classifier based on the optimal feature subset performed well for the prediction of developing adverse outcomes in the primary and validation cohorts, with AUCs of 0.959 (95% confidence interval [CI]: 0.936-0.976) and 0.953 (95% CI: 0.891-0.986), respectively. Furthermore, the XGBoost classifier also showed clinical usefulness.\nWe presented a machine learning model that could be effectively used as a predictor of adverse outcomes in hospitalized patients with COVID-19, opening up the possibility for patient stratification and treatment allocation.\n\u2022 Developing an individually prognostic model for COVID-19 has the potential to allow efficient allocation of medical resources. \u2022 We proposed a deep learning-based framework for accurate lung involvement quantification on chest CT images. \u2022 Machine learning based on clinical and CT variables can facilitate the prediction of adverse outcomes of COVID-19.", "journal": "European radiology", "date": "2021-04-16", "authors": ["ZhichaoFeng", "HuiShen", "KaiGao", "JianpoSu", "ShanhuYao", "QinLiu", "ZhiminYan", "JunhongDuan", "DaliYi", "HuafeiZhao", "HuilingLi", "QizhiYu", "WenmingZhou", "XiaowenMao", "XinOuyang", "JiMei", "QiuhuaZeng", "LindyWilliams", "XiaoqianMa", "PengfeiRong", "DewenHu", "WeiWang"], "doi": "10.1007/s00330-021-07957-z\n10.1056/NEJMoa2002032\n10.1016/S2213-2600(20)30079-5\n10.1164/rccm.202002-0445OC\n10.1016/S0140-6736(20)31022-9\n10.1056/NEJMoa2007016\n10.1016/S0140-6736(20)31042-4\n10.14336/AD.2020.0630\n10.1001/jama.2019.20153\n10.1001/jamainternmed.2020.0994\n10.1136/bmj.m1328\n10.1093/cid/ciaa414\n10.1038/s41467-020-18786-x\n10.1016/S1473-3099(20)30086-4\n10.1016/j.cell.2020.04.045\n10.7326/M14-0697\n10.1016/j.media.2020.101836\n10.1016/j.ebiom.2019.05.023\n10.1016/j.jchf.2019.06.013\n10.1001/jamainternmed.2020.2033\n10.1371/journal.pone.0143486\n10.1111/bjh.14830\n10.1053/j.gastro.2020.03.065\n10.1002/hep.31301\n10.1038/s41569-020-0360-5\n10.7326/M18-0670\n10.1161/CIRCULATIONAHA.120.047008\n10.1183/13993003.00524-2020\n10.1016/j.chest.2020.04.010\n10.1056/NEJMsr2005760\n10.1042/CS20200363\n10.1007/s00330-020-07042-x\n10.1016/S0140-6736(20)30183-5"}
{"title": "An Insight of the First Community Infected COVID-19 Patient in Beijing by Imported Case: Role of Deep Learning-Assisted CT Diagnosis.", "abstract": "In the era of coronavirus disease 2019 (COVID-19) pandemic, imported COVID-19 cases pose great challenges to many countries. Chest CT examination is considered to be complementary to nucleic acid test for COVID-19 detection and diagnosis. We report the first community infected COVID-19 patient by an imported case in Beijing, which manifested as nodular lesions on chest CT imaging at the early stage. Deep Learning (DL)-based diagnostic systems quantitatively monitored the progress of pulmonary lesions in 6 days and timely made alert for suspected pneumonia, so that prompt medical isolation was taken. The patient was confirmed as COVID-19 case after nucleic acid test, for which the community transmission was prevented timely. The roles of DL-assisted diagnosis in helping radiologists screening suspected COVID cases were discussed.", "journal": "Chinese medical sciences journal = Chung-kuo i hsueh k'o hsueh tsa chih", "date": "2021-04-16", "authors": ["Da ShengLi", "Da WeiWang", "Na NaWang", "Hai WangXu", "HeHuang", "Jian PingDong", "ChenXia"], "doi": "10.24920/003788\n10.23750/abm.v91il.9397\n10.3348/kjr.2020.0146\n10.1186/s12967-020-02324-W\n10.3348/kjr.2020.0132\n10.1093/infdis/jiaa119\n10.1007/s11427-020-1661-4\n10.3348/kjr.2020.0195\n10.1148/ryai.2019180084\n10.1148/ryct.2020200075\n10.1016/j.ebiom.2019.05.040\n10.1016/S2589-7500(20)30199-0"}
{"title": "Deep Convolutional Neural Network-Based Computer-Aided Detection System for COVID-19 Using Multiple Lung Scans: Design and Implementation Study.", "abstract": "Owing to the COVID-19 pandemic and the imminent collapse of health care systems following the exhaustion of financial, hospital, and medicinal resources, the World Health Organization changed the alert level of the COVID-19 pandemic from high to very high. Meanwhile, more cost-effective and precise COVID-19 detection methods are being preferred worldwide.\nMachine vision-based COVID-19 detection methods, especially deep learning as a diagnostic method in the early stages of the pandemic, have been assigned great importance during the pandemic. This study aimed to design a highly efficient computer-aided detection (CAD) system for COVID-19 by using a neural search architecture network (NASNet)-based algorithm.\nNASNet, a state-of-the-art pretrained convolutional neural network for image feature extraction, was adopted to identify patients with COVID-19 in their early stages of the disease. A local data set, comprising 10,153 computed tomography scans of 190 patients with and 59 without COVID-19 was used.\nAfter fitting on the training data set, hyperparameter tuning, and topological alterations of the classifier block, the proposed NASNet-based model was evaluated on the test data set and yielded remarkable results. The proposed model's performance achieved a detection sensitivity, specificity, and accuracy of 0.999, 0.986, and 0.996, respectively.\nThe proposed model achieved acceptable results in the categorization of 2 data classes. Therefore, a CAD system was designed on the basis of this model for COVID-19 detection using multiple lung computed tomography scans. The system differentiated all COVID-19 cases from non-COVID-19 ones without any error in the application phase. Overall, the proposed deep learning-based CAD system can greatly help radiologists detect COVID-19 in its early stages. During the COVID-19 pandemic, the use of a CAD system as a screening tool would accelerate disease detection and prevent the loss of health care resources.", "journal": "Journal of medical Internet research", "date": "2021-04-14", "authors": ["MustafaGhaderzadeh", "FarkhondehAsadi", "RamezanJafari", "DavoodBashash", "HassanAbolghasemi", "MehradAria"], "doi": "10.2196/27468\n10.3390/jcm9020330\n10.1148/radiol.2020200230\n10.1007/s00330-020-06801-0\n10.1056/NEJMoa2001316\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-76550-z\n10.1016/S0140-6736(20)30183-5\n10.1148/ryct.2020200107\n10.1001/jama.2020.1585\n10.1148/radiol.2020200642\n10.1007/s13244-018-0639-9\n10.1007/s00330-021-07715-1\n10.1007/s00330-020-07044-9\n10.1016/j.cell.2020.04.045\n10.1183/13993003.00775-2020\n10.3390/e22050517\n10.1016/j.irbm.2020.05.003\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020200905\n10.1038/s41591-020-0931-3\n10.1007/s00259-020-04929-1\n10.1109/cvpr.2018.00907\n10.1016/j.media.2017.07.005\n10.17148/IARJSET.2015.2305\n10.1111/j.1467-9892.1994.tb00184.x\n10.1186/s40537-019-0197-0\n10.1080/07391102.2020.1788642\n10.1016/j.chaos.2020.109944\n10.1016/j.media.2020.101794\n10.1109/JBHI.2020.3023246\n10.1109/3dv.2016.79\n10.1007/s10489-020-02051-1\n10.1155/2021/6677314\n10.1155/2021/6677314"}
{"title": "Impact of Big Data Analytics on People's Health: Overview of Systematic Reviews and Recommendations for Future Studies.", "abstract": "Although the potential of big data analytics for health care is well recognized, evidence is lacking on its effects on public health.\nThe aim of this study was to assess the impact of the use of big data analytics on people's health based on the health indicators and core priorities in the World Health Organization (WHO) General Programme of Work 2019/2023 and the European Programme of Work (EPW), approved and adopted by its Member States, in addition to SARS-CoV-2-related studies. Furthermore, we sought to identify the most relevant challenges and opportunities of these tools with respect to people's health.\nSix databases (MEDLINE, Embase, Cochrane Database of Systematic Reviews via Cochrane Library, Web of Science, Scopus, and Epistemonikos) were searched from the inception date to September 21, 2020. Systematic reviews assessing the effects of big data analytics on health indicators were included. Two authors independently performed screening, selection, data extraction, and quality assessment using the AMSTAR-2 (A Measurement Tool to Assess Systematic Reviews 2) checklist.\nThe literature search initially yielded 185 records, 35 of which met the inclusion criteria, involving more than 5,000,000 patients. Most of the included studies used patient data collected from electronic health records, hospital information systems, private patient databases, and imaging datasets, and involved the use of big data analytics for noncommunicable diseases. \"Probability of dying from any of cardiovascular, cancer, diabetes or chronic renal disease\" and \"suicide mortality rate\" were the most commonly assessed health indicators and core priorities within the WHO General Programme of Work 2019/2023 and the EPW 2020/2025. Big data analytics have shown moderate to high accuracy for the diagnosis and prediction of complications of diabetes mellitus as well as for the diagnosis and classification of mental disorders; prediction of suicide attempts and behaviors; and the diagnosis, treatment, and prediction of important clinical outcomes of several chronic diseases. Confidence in the results was rated as \"critically low\" for 25 reviews, as \"low\" for 7 reviews, and as \"moderate\" for 3 reviews. The most frequently identified challenges were establishment of a well-designed and structured data source, and a secure, transparent, and standardized database for patient data.\nAlthough the overall quality of included studies was limited, big data analytics has shown moderate to high accuracy for the diagnosis of certain diseases, improvement in managing chronic diseases, and support for prompt and real-time analyses of large sets of varied input data to diagnose and predict disease outcomes.\nInternational Prospective Register of Systematic Reviews (PROSPERO) CRD42020214048; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=214048.", "journal": "Journal of medical Internet research", "date": "2021-04-14", "authors": ["Israel J\u00faniorBorges do Nascimento", "Milena SorianoMarcolino", "Hebatullah MohamedAbdulazeem", "IshankaWeerasekara", "NatashaAzzopardi-Muscat", "Marcos Andr\u00e9Gon\u00e7alves", "DavidNovillo-Ortiz"], "doi": "10.2196/27275\n10.1515/jib-2017-0030\n10.1109/BigData.2014.7004307\n10.7326/0003-4819-148-10-200805200-00010\n10.1371/journal.pmed.1000097\n10.1371/journal.pmed.1000097\n10.1136/bmj.d5928\n10.1016/s0140-6736(99)04149-5\n10.1007/978-1-62703-748-8_7\n10.1136/bmj.j4008\n10.1017/dmp.2018.73\n10.1016/j.neubiorev.2017.07.004\n10.1016/j.acmx.2017.03.002\n10.1017/S0033291719000151\n10.1016/j.oret.2018.10.014\n10.1002/phar.2146\n10.1186/s12874-020-0913-7\n10.1186/s12874-020-0913-7\n10.1016/j.ijinfomgt.2018.09.011\n10.3390/ijerph17165929\n10.1016/j.artmed.2019.07.007\n10.1016/j.csbj.2016.11.001\n10.1093/jamia/ocz009\n10.2196/medinform.5359\n10.3390/cancers12041032\n10.1016/j.jad.2018.11.073\n10.1007/s00134-019-05872-y\n10.1371/journal.pone.0221339\n10.1371/journal.pone.0221339\n10.1111/jop.12854\n10.3390/diagnostics9010029\n10.1007/s10928-020-09685-1\n10.3390/diagnostics9040207\n10.1016/j.jksuci.2020.06.013\n10.1016/j.jiph.2020.06.006\n10.1186/s12911-015-0224-9\n10.1186/s12911-015-0224-9\n10.5812/archcid.103232\n10.1136/bmjgast-2019-000371\n10.1007/s10916-020-01582-x\n10.1016/j.csbj.2016.12.005\n10.1016/j.socscimed.2019.112533\n10.4258/hir.2019.25.4.248\n10.5455/aim.2018.26.258-264\n10.1016/j.jbi.2018.10.008\n10.1371/journal.pone.0234722\n10.1371/journal.pone.0234722\n10.1136/neurintsurg-2019-015135\n10.1007/s10916-018-1018-2\n10.1016/S0140-6736(17)32154-2\n10.1016/j.jbusres.2016.08.001\n10.1186/s12913-019-4627-7\n10.1186/s12913-019-4627-7\n10.1016/S1470-2045(19)30149-4\n10.5811/westjem.2019.1.41244\n10.1136/bmj.j4071\n10.1016/j.ipm.2020.102481\n10.1161/CIRCULATIONAHA.115.001593\n10.1038/nrcardio.2016.42\n10.3389/fdigh.2018.00008"}
{"title": "Quantitative analysis based on chest CT classifies common and severe patients with coronavirus disease 2019 pneumonia in Wuhan, China.", "abstract": "This study aimed to compare quantifiable radiologic findings and their dynamic change throughout the clinical course of common and severe coronavirus disease 2019 (COVID-19), and to provide valuable evidence for radiologic classification of the two types of this disease.\n112 patients with laboratory-confirmed COVID-19 were retrospectively analyzed. Volumetric percentage of infection and density of the lung were measured by a computer-aided software. Clinical parameters were recorded to reflect disease progression. Baseline data and dynamic change were compared between two groups and a decision-tree algorithm was developed to determine the cut-off value for classification.\n93 patients were finally included and were divided into common group (\nVolumetric percentage between severe and common patients was significantly different. Because serial CT scans are systemically performed in patients with COVID-19 pneumonia, this quantitative analysis can simultaneously provide valuable information for physicians to evaluate their clinical course and classify common and severe patients accurately.", "journal": "Chinese journal of academic radiology", "date": "2021-04-14", "authors": ["ChongtuYang", "GuijuanCao", "FenLiu", "JiachengLiu", "SongjiangHuang", "BinXiong"], "doi": "10.1007/s42058-021-00061-7\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30251-8\n10.1007/s11427-020-1637-5\n10.1016/s2213-2600(20)30079-5\n10.1056/NEJMoa2002032\n10.1016/j.ejrad.2019.108774\n10.1186/s12931-019-1189-5\n10.1148/radiol.2020200463:200463\n10.1148/radiol.2020200642:200642\n10.1097/rli.0000000000000672\n10.1148/radiol.2020200370:200370\n10.1016/s1473-3099(20)30086-4\n10.1007/s00330-020-07013-2\n10.1016/j.jtcvs.2018.07.040"}
{"title": "Fusion of convolution neural network, support vector machine and Sobel filter for accurate detection of COVID-19 patients using X-ray images.", "abstract": "The coronavirus (COVID-19) is currently the most common contagious disease which is prevalent all over the world. The main challenge of this disease is the primary diagnosis to prevent secondary infections and its spread from one person to another. Therefore, it is essential to use an automatic diagnosis system along with clinical procedures for the rapid diagnosis of COVID-19 to prevent its spread. Artificial intelligence techniques using computed tomography (CT) images of the lungs and chest radiography have the potential to obtain high diagnostic performance for Covid-19 diagnosis. In this study, a fusion of convolutional neural network (CNN), support vector machine (SVM), and Sobel filter is proposed to detect COVID-19 using X-ray images. A new X-ray image dataset was collected and subjected to high pass filter using a Sobel filter to obtain the edges of the images. Then these images are fed to CNN deep learning model followed by SVM classifier with ten-fold cross validation strategy. This method is designed so that it can learn with not many data. Our results show that the proposed CNN-SVM with Sobel filter (CNN-SVM\u202f+\u202fSobel) achieved the highest classification accuracy, sensitivity and specificity of 99.02%, 100% and 95.23%, respectively in automated detection of COVID-19. It showed that using Sobel filter can improve the performance of CNN. Unlike most of the other researches, this method does not use a pre-trained network. We have also validated our developed model using ", "journal": "Biomedical signal processing and control", "date": "2021-04-14", "authors": ["DanialSharifrazi", "RoohallahAlizadehsani", "MohamadRoshanzamir", "Javad HassannatajJoloudari", "AfshinShoeibi", "MahboobehJafari", "SadiqHussain", "Zahra AlizadehSani", "FereshtehHasanzadeh", "FahimeKhozeimeh", "AbbasKhosravi", "SaeidNahavandi", "MaryamPanahiazar", "AssefZare", "Sheikh Mohammed SharifulIslam", "U RajendraAcharya"], "doi": "10.1016/j.bspc.2021.102622\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1056/NEJMoa2001316\n10.1056/NEJMoa2001191\n10.1148/radiol.2020200432\n10.1007/s42600-020-00091-7\n10.1007/s00330-019-06163-2\n10.1016/j.eswa.2020.113788\n10.1038/s41597-019-0206-3\n10.1109/TMI.2016.2535865\n10.1109/TMI.2020.2993291\n10.1080/07391102.2020.1767212\n10.1016/j.compbiomed.2020.103795\n10.1109/ACCESS.2019.2952946\n10.17632/2fxz4px6d8.4"}
{"title": "A multi-center study of COVID-19 patient prognosis using deep learning-based CT image analysis and electronic health records.", "abstract": "As of August 30th, there were in total 25.1 million confirmed cases and 845 thousand deaths caused by coronavirus disease of 2019 (COVID-19) worldwide. With overwhelming demands on medical resources, patient stratification based on their risks is essential. In this multi-center study, we built prognosis models to predict severity outcomes, combining patients' electronic health records (EHR), which included vital signs and laboratory data, with deep learning- and CT-based severity prediction.\nWe first developed a CT segmentation network using datasets from multiple institutions worldwide. Two biomarkers were extracted from the CT images: total opacity ratio (TOR) and consolidation ratio (CR). After obtaining TOR and CR, further prognosis analysis was conducted on datasets from INSTITUTE-1, INSTITUTE-2 and INSTITUTE-3. For each data cohort, generalized linear model (GLM) was applied for prognosis prediction.\nFor the deep learning model, the correlation coefficient of the network prediction and manual segmentation was 0.755, 0.919, and 0.824 for the three cohorts, respectively. The AUC (95 % CI) of the final prognosis models was 0.85(0.77,0.92), 0.93(0.87,0.98), and 0.86(0.75,0.94) for INSTITUTE-1, INSTITUTE-2 and INSTITUTE-3 cohorts, respectively. Either TOR or CR exist in all three final prognosis models. Age, white blood cell (WBC), and platelet (PLT) were chosen predictors in two cohorts. Oxygen saturation (SpO2) was a chosen predictor in one cohort.\nThe developed deep learning method can segment lung infection regions. Prognosis results indicated that age, SpO2, CT biomarkers, PLT, and WBC were the most important prognostic predictors of COVID-19 in our prognosis model.", "journal": "European journal of radiology", "date": "2021-04-14", "authors": ["KuangGong", "DufanWu", "Chiara DanielaArru", "FatemehHomayounieh", "NirNeumark", "JiahuiGuan", "VarunBuch", "KyungsangKim", "Bernardo CanedoBizzo", "HuiRen", "Won YoungTak", "Soo YoungPark", "Yu RimLee", "Min KyuKang", "Jung GilPark", "AlessandroCarriero", "LucaSaba", "MahsaMasjedi", "HamidrezaTalari", "RosaBabaei", "Hadi KarimiMobin", "ShadiEbrahimian", "NingGuo", "Subba RDigumarthy", "IttaiDayan", "Mannudeep KKalra", "QuanzhengLi"], "doi": "10.1016/j.ejrad.2021.109583\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200463\n10.2214/AJR.20.22976\n10.1148/radiol.2020200370\n10.1371/journal.pone.0093885\n10.1371/journal.pone.0230548\n10.1148/radiol.2020200905\n10.1007/s10489-020-01714-3\n10.1109/TMI.2020.2994908\n10.1148/ryct.2020200075\n10.1109/TMI.2020.2992546\n10.1148/ryct.2020200082\n10.1007/s00330-020-07087-y\n10.1016/j.compbiomed.2020.104037\n10.1101/2020.02.29.20029603\n10.2139/ssrn.3557984\n10.1016/j.compbiomed.2020.103949\n10.1016/S2214-109X(20)30068-1\n10.1038/s41591-020-0895-3\n10.1109/CVPR.2017.243\n10.1109/3DV.2016.79\n10.1109/TMI.2020.2996645\n10.1109/TMI.2020.3000314\n10.1109/JBHI.2020.3030224\n10.1088/1361-6560/ab440d\n10.1097/00003246-200006000-00031\n10.1016/j.cca.2020.03.022\n10.1016/S0140-6736(20)30566-3\n10.1186/s13054-020-2833-7\n10.1038/s41392-020-0148-4\n10.1038/s42256-020-0180-7\n10.2139/ssrn.3551365\n10.2139/ssrn.3543603\n10.2139/ssrn.3562456\n10.1101/2020.02.27.20028027\n10.1016/j.cell.2020.04.045"}
{"title": "Lung ultrasonography for long-term follow-up of COVID-19 survivors compared to chest CT scan.", "abstract": "While lung ultrasonography (LUS) has utility for the evaluation of the acute phase of COVID-19 related lung disease, its role in long-term follow-up of this condition has not been well described. The objective of this study is to compare LUS and chest computed tomography (CT) results in COVID-19 survivors with the intent of defining the utility of LUS for long-term follow-up of COVID-19 respiratory disease.\nProspective observational study that enrolled consecutive survivors of COVID-19 with acute hypoxemic respiratory failure (HARF) admitted to the Respiratory Intensive Care Unit. Three months following hospital discharge, patients underwent LUS, chest CT, body plethysmography and laboratory testing, the comparison of which forms the basis of this report.\n38 patients were enrolled, with a total of 190 lobes analysed: men 27/38 (71.1%), mean age 60.6\u00a0y (SD 10.4). LUS findings and pulmonary function tests outcomes were compared between patients with and without ILD, showing a statistically significant difference in terms of LUS score (p: 0.0002), FEV1 (p: 0.0039) and FVC (p: 0.012). ROC curve both in lobe by lobe and in patient's overall analysis revealed an outstanding ILD discrimination ability of LUS (AUC: 0.94 and 0.95 respectively) with a substantial Cohen's coefficient (K: 0.74 and 0.69).\nLUS has an outstanding discrimination ability compared to CT in identifying an ILD of at least mild grade in the post COVID-19 follow-up. LUS should be considered as the first-line tool in follow-up programs, while chest CT could be performed based on LUS findings.", "journal": "Respiratory medicine", "date": "2021-04-12", "authors": ["GuidoGiovannetti", "LucreziaDe Michele", "MicheleDe Ceglie", "PaolaPierucci", "AlessandraMirabile", "MarcoVita", "Vincenzo OstilioPalmieri", "Giovanna ElisianaCarpagnano", "ArnaldoScardapane", "CarloD'Agostino"], "doi": "10.1016/j.rmed.2021.106384\n10.1148/radiol.2020200843\n10.1148/radiol.2020200370\n10.1007/s00134-020-06096-1\n10.5152/eurjrheum.2020.2063\n10.1111/anae.15082\n10.1007/s40477-020-00458-7\n10.1371/journal.pone.0236312\n10.1159/000509223\n10.1002/jum.15284\n10.1093/eurheartj/ehv317\n10.1016/j.ejrnm.2017.01.005\n10.1148/radiol.2020200370\n10.1016/j.apmr.2016.07.011\n10.1007/s11547-020-01236-5\n10.1186/1476-7120-12-25"}
{"title": "Get Protected! Recommendations for Staff in IR.", "abstract": "Evaluation and registration of patient and staff doses are mandatory under the current European legislation, and the occupational dose limits recommended by the ICRP have been adopted by most of the countries in the world.\nRelevant documents and guidelines published by international organisations and interventional radiology societies are referred. Any potential reduction of patient and staff doses should be compatible with the clinical outcomes of the procedures.\nThe review summarises the most common protective measures and the needed quality control for them, the criteria to select the appropriate protection devices, and how to avoid unnecessary occupational radiation exposures. Moreover, the current and future advancements in personnel radiation protection using medical simulation with virtual and augmented reality, robotics, and artificial intelligence (AI) are commented. A section on the personnel radiation protection in the era of COVID-19 is introduced, showing the expanding role of the interventional radiology during the pandemic.\nThe review is completed with a summary of the main factors to be considered in the selection of the appropriate radiation protection tools and practical advices to improve the protection of the staff.", "journal": "Cardiovascular and interventional radiology", "date": "2021-04-11", "authors": ["GabrielBartal", "EliseoVano", "GracianoPaulo"], "doi": "10.1007/s00270-021-02828-y\n10.1007/s00270-009-9756-7\n10.1007/s00270-020-02517-2\n10.1016/j.jvir.2014.11.026\n10.1177/0146645317750356\n10.1007/s00270-013-0685-0\n10.4103/ijri.IJRI_374_17\n10.1118/1.1573207\n10.1016/j.jacc.2014.11.056\n10.1016/j.ejvs.2013.12.008\n10.1007/s00270-014-0997-8\n10.1053/j.tvir.2017.12.002\n10.4244/EIJV11I1A9\n10.1136/neurintsurg-2015-011703\n10.1007/s00270-020-02476-8\n10.1016/j.jvir.2020.07.019\n10.5152/dir.2020.20494\n10.1002/ccd.24694\n10.1161/CIRCINTERVENTIONS.118.006823\n10.1136/heart.89.10.1123\n10.1088/1361-6498/aad429\n10.1088/0952-4746/36/1/133\n10.1093/rpd/ncv049\n10.1016/j.ejvs.2015.10.011\n10.1016/j.jvir.2016.10.017\n10.2214/AJR.15.15986\n10.1016/j.jacc.2012.12.045"}
{"title": "Multilevel Deep-Aggregated Boosted Network to Recognize COVID-19 Infection from Large-Scale Heterogeneous Radiographic Data.", "abstract": "In the present epidemic of the coronavirus disease 2019 (COVID-19), radiological imaging modalities, such as X-ray and computed tomography (CT), have been identified as effective diagnostic tools. However, the subjective assessment of radiographic examination is a time-consuming task and demands expert radiologists. Recent advancements in artificial intelligence have enhanced the diagnostic power of computer-aided diagnosis (CAD) tools and assisted medical specialists in making efficient diagnostic decisions. In this work, we propose an optimal multilevel deep-aggregated boosted network to recognize COVID-19 infection from heterogeneous radiographic data, including X-ray and CT images. Our method leverages multilevel deep-aggregated features and multistage training via a mutually beneficial approach to maximize the overall CAD performance. To improve the interpretation of CAD predictions, these multilevel deep features are visualized as additional outputs that can assist radiologists in validating the CAD results. A total of six publicly available datasets were fused to build a single large-scale heterogeneous radiographic collection that was used to analyze the performance of the proposed technique and other baseline methods. To preserve generality of our method, we selected different patient data for training, validation, and testing, and consequently, the data of same patient were not included in training, validation, and testing subsets. In addition, fivefold cross-validation was performed in all the experiments for a fair evaluation. Our method exhibits promising performance values of 95.38%, 95.57%, 92.53%, 98.14%, 93.16%, and 98.55% in terms of average accuracy, F-measure, specificity, sensitivity, precision, and area under the curve, respectively and outperforms various state-of-the-art methods.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-04-10", "authors": ["MuhammadOwais", "Young WonLee", "TahirMahmood", "AdnanHaider", "HaseebSultan", "Kang RyoungPark"], "doi": "10.1109/JBHI.2021.3072076\n10.1109/JBHI.2020.3045274\n10.1109/JBHI.2020.3042069"}
{"title": "An automated and fast system to identify COVID-19 from X-ray radiograph of the chest using image processing and machine learning.", "abstract": "A type of coronavirus disease called COVID-19 is spreading all over the globe. Researchers and scientists are endeavoring to find new and effective methods to diagnose and treat this disease. This article presents an automated and fast system that identifies COVID-19 from X-ray radiographs of the chest using image processing and machine learning algorithms. Initially, the system extracts the feature descriptors from the radiographs of both healthy and COVID-19 affected patients using the speeded up robust features algorithm. Then, visual vocabulary is built by reducing the number of feature descriptors via quantization of feature space using the K-means clustering algorithm. The visual vocabulary train the support vector machine (SVM) classifier. During testing, an X-ray radiograph's visual vocabulary is sent to the trained SVM classifier to detect the absence or presence of COVID-19. The study used the dataset of 340 X-ray radiographs, 170 images of each Healthy and Positive COVID-19 class. During simulations, the dataset split into training and testing parts at various ratios. After training, the system does not require any human intervention and can process thousands of images with high precision in a few minutes. The performance of the system is measured using standard parameters of accuracy and confusion matrix. We compared the performance of the proposed SVM-based classier with the deep-learning-based convolutional neural networks (CNN). The SVM yields better results than CNN and achieves a maximum accuracy of up to 94.12%.", "journal": "International journal of imaging systems and technology", "date": "2021-04-07", "authors": ["Murtaza AliKhan"], "doi": "10.1002/ima.22564\n10.1007/s13369-020-04447-0\n10.1155/2020/8828855\n10.1148/radiol.2020200905\n10.1016/j.imu.2020.100378\n10.1007/11744023_32\n10.1017/CBO9780511801389\n10.1038/s41598-020-74539-2"}
{"title": "Convolutional capsule network for COVID-19 detection using radiography images.", "abstract": "Novel corona virus COVID-19 has spread rapidly all over the world. Due to increasing COVID-19 cases, there is a dearth of testing kits. Therefore, there is a severe need for an automatic recognition system as a solution to reduce the spreading of the COVID-19 virus. This work offers a decision support system based on the X-ray image to diagnose the presence of the COVID-19 virus. A deep learning-based computer-aided decision support system will be capable to differentiate between COVID-19 and pneumonia. Recently, convolutional neural network (CNN) is designed for the diagnosis of COVID-19 patients through ", "journal": "International journal of imaging systems and technology", "date": "2021-04-07", "authors": ["ShamikTiwari", "AnuragJain"], "doi": "10.1002/ima.22566\n10.34740/KAGGLE/DSV/1019469"}
{"title": "Future IoT tools for COVID-19 contact tracing and prediction: A review of the state-of-the-science.", "abstract": "In 2020 the world is facing unprecedented challenges due to COVID-19. To address these challenges, many digital tools are being explored and developed to contain the spread of the disease. With the lack of availability of vaccines, there is an urgent need to avert resurgence of infections by putting some measures, such as contact tracing, in place. While digital tools, such as phone applications are advantageous, they also pose challenges and have limitations (eg, wireless coverage could be an issue in some cases). On the other hand, wearable devices, when coupled with the Internet of Things (IoT), are expected to influence lifestyle and healthcare directly, and they may be useful for health monitoring during the global pandemic and beyond. In this work, we conduct a literature review of contact tracing methods and applications. Based on the literature review, we found limitations in gathering health data, such as insufficient network coverage. To address these shortcomings, we propose a novel intelligent tool that will be useful for contact tracing and prediction of COVID-19 clusters. The solution comprises a phone application combined with a wearable device, infused with unique intelligent IoT features (complex data analysis and intelligent data visualization) embedded within the system to aid in COVID-19 analysis. Contact tracing applications must establish data collection and data interpretation. Intelligent data interpretation can assist epidemiological scientists in anticipating clusters, and can enable them to take necessary action in improving public health management. Our proposed tool could also be used to curb disease incidence in future global health crises.", "journal": "International journal of imaging systems and technology", "date": "2021-04-07", "authors": ["VicneshJahmunah", "Vidya KSudarshan", "Shu LihOh", "RajGururajan", "RashmiGururajan", "XujuanZhou", "XiaohuiTao", "OliverFaust", "Edward JCiaccio", "Kwan HoongNg", "U RajendraAcharya"], "doi": "10.1002/ima.22552\n10.1186/s40537-019-0268-2\n10.7326/M20-1033\n10.1016/j.dcan.2017.10.002\n10.1016/j.compbiomed.2017.09.017\n10.1016/j.cmpb.2018.04.012\n10.1162/neco.1997.9.8.1735"}
{"title": "Automatic detection and localization of COVID-19 pneumonia using axial computed tomography images and deep convolutional neural networks.", "abstract": "COVID-19 was first reported as an unknown group of pneumonia in Wuhan City, Hubei province of China in late December of 2019. The rapid increase in the number of cases diagnosed with COVID-19 and the lack of experienced radiologists can cause diagnostic errors in the interpretation of the images along with the exceptional workload occurring in this process. Therefore, the urgent development of automated diagnostic systems that can scan radiological images quickly and accurately is important in combating the pandemic. With this motivation, a deep convolutional neural network (CNN)-based model that can automatically detect patterns related to lesions caused by COVID-19 from chest computed tomography (CT) images is proposed in this study. In this context, the image ground-truth regarding the COVID-19 lesions scanned by the radiologist was evaluated as the main criteria of the segmentation process. A total of 16\u2009040 CT image segments were obtained by applying segmentation to the raw 102 CT images. Then, 10\u2009420 CT image segments related to healthy lung regions were labeled as COVID-negative, and 5620 CT image segments, in which the findings related to the lesions were detected in various forms, were labeled as COVID-positive. With the proposed CNN architecture, 93.26% diagnostic accuracy performance was achieved. The sensitivity and specificity performance metrics for the proposed automatic diagnosis model were 93.27% and 93.24%, respectively. Additionally, it has been shown that by scanning the small regions of the lungs, COVID-19 pneumonia can be localized automatically with high resolution and the lesion densities can be successfully evaluated quantitatively.", "journal": "International journal of imaging systems and technology", "date": "2021-04-07", "authors": ["HasanPolat", "Mehmet Sira\u00e7\u00d6zerdem", "FaysalEkici", "VeysiAkpolat"], "doi": "10.1002/ima.22558\n10.1016/j.compbiomed.2020.103805\n10.1016/j.cmpb.2020.105532\n10.1016/j.amjsurg.2020.04.018\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200463\n10.2214/AJR.20.22976\n10.2214/AJR.20.23034\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020200370\n10.1016/j.cmpb.2020.105320\n10.1016/j.acra.2019.11.007\n10.1016/j.compbiomed.2019.103345\n10.1016/j.imu.2019.100173\n10.1016/j.crad.2020.01.010\n10.1016/j.neucom.2018.12.086\n10.1016/j.compbiomed.2020.103795\n10.1101/2020.04.16.20064709\n10.1016/j.mehy.2020.109761\n10.1109/TMI.2020.2995965\n10.1097/RTI.0000000000000387\n10.1016/j.ins.2018.01.051\n10.1016/j.compag.2019.104932\n10.1016/j.compmedimag.2017.06.001\n10.1016/j.compbiomed.2017.09.017\n10.1016/j.csite.2020.100625\n10.1109/ISCAS.2010.5537907\n10.1162/NECO_a_00990\n10.1007/978-3-642-15825-4_10\n10.1016/j.ecoinf.2018.10.002\n10.1016/j.neunet.2018.07.011\n10.1101/2020.02.23.20026930\n10.20944/preprints202003.0300.v1\n10.1016/j.compbiomed.2020.103792\n10.1007/s10489-020-01714-3"}
{"title": "A novel perceptual two layer image fusion using deep learning for imbalanced COVID-19 dataset.", "abstract": "COVID-19 is a new strain of viruses that causes life stoppage worldwide. At this time, the new coronavirus COVID-19 is spreading rapidly across the world and poses a threat to people's health. Experimental medical tests and analysis have shown that the infection of lungs occurs in almost all COVID-19 patients. Although Computed Tomography of the chest is a useful imaging method for diagnosing diseases related to the lung, chest X-ray (CXR) is more widely available, mainly due to its lower price and results. Deep learning (DL), one of the significant popular artificial intelligence techniques, is an effective way to help doctors analyze how a large number of CXR images is crucial to performance.\nIn this article, we propose a novel perceptual two-layer image fusion using DL to obtain more informative CXR images for a COVID-19 dataset. To assess the proposed algorithm performance, the dataset used for this work includes 87 CXR images acquired from 25 cases, all of which were confirmed with COVID-19. The dataset preprocessing is needed to facilitate the role of convolutional neural networks (CNN). Thus, hybrid decomposition and fusion of Nonsubsampled Contourlet Transform (NSCT) and CNN_VGG19 as feature extractor was used.\nOur experimental results show that imbalanced COVID-19 datasets can be reliably generated by the algorithm established here. Compared to the COVID-19 dataset used, the fuzed images have more features and characteristics. In evaluation performance measures, six metrics are applied, such as Q\nA novel image fusion algorithm using DL for an imbalanced COVID-19 dataset is the crucial contribution of this work. Extensive results of the experiment display that the proposed algorithm NSCT + CNN_VGG19 outperforms competitive image fusion algorithms.", "journal": "PeerJ. Computer science", "date": "2021-04-06", "authors": ["Omar MElzeki", "MohamedAbd Elfattah", "HanaaSalem", "Aboul EllaHassanien", "MahmoudShams"], "doi": "10.7717/peerj-cs.364\n10.1007/s10489-020-01829-7\n10.1016/j.inffus.2019.02.003\n10.3390/diagnostics10010027\n10.1007/s11042-018-6229-5\n10.1007/s40964-019-00108-3\n10.1109/JSEN.2015.2465935\n10.1109/TMM.2013.2244870\n10.1016/j.imavis.2007.12.002\n10.1007/s13755-020-00119-3\n10.1007/s11517-012-0943-3\n10.1016/j.neucom.2015.07.160\n10.1016/j.knosys.2016.09.008\n10.1016/j.eij.2015.09.002\n10.1016/j.scitotenv.2020.138858\n10.1007/s10278-015-9806-4\n10.4236/cs.2016.78139\n10.1007/s00138-020-01060-x\n10.1007/s00521-018-3441-1\n10.1049/el:20081754\n10.1109/ACCESS.2020.2974242\n10.3390/electronics9010190\n10.1155/2020/8279342\n10.1016/j.asoc.2008.05.001\n10.1007/s10462-020-09825-6\n10.1364/OL.33.000738\n10.1145/3065386\n10.3390/s18041019\n10.1016/j.inffus.2016.05.004\n10.1109/TIP.2013.2244222\n10.1016/j.artmed.2019.101744\n10.1016/j.inffus.2019.07.009\n10.1016/j.inffus.2016.12.001\n10.1016/j.inffus.2017.10.007\n10.1016/j.inffus.2014.09.004\n10.1007/s11045-015-0343-6\n10.1016/j.compmedimag.2019.05.005\n10.1016/j.jneumeth.2019.108520\n10.1016/j.inffus.2019.12.001\n10.1016/j.chaos.2020.110190\n10.1016/j.cmpb.2020.105532\n10.34172/aim.2020.02\n10.1038/s41598-019-56847-4\n10.1109/RBME.2020.2987975\n10.1049/iet-cvi.2015.0251\n10.1016/j.ins.2017.12.043\n10.1177/1460458218824711\n10.1016/j.inffus.2020.10.004\n10.1016/j.ijleo.2019.163497\n10.1016/j.infrared.2015.01.002\n10.1049/el:20000267\n10.1016/j.inffus.2006.09.001\n10.1109/TIM.2018.2838778\n10.1016/j.ins.2017.09.010\n10.1109/ACCESS.2019.2898111"}
{"title": "FUSI-CAD: Coronavirus (COVID-19) diagnosis based on the fusion of CNNs and handcrafted features.", "abstract": "The precise and rapid diagnosis of coronavirus (COVID-19) at the very primary stage helps doctors to manage patients in high workload conditions. In addition, it prevents the spread of this pandemic virus. Computer-aided diagnosis (CAD) based on artificial intelligence (AI) techniques can be used to distinguish between COVID-19 and non-COVID-19 from the computed tomography (CT) imaging. Furthermore, the CAD systems are capable of delivering an accurate faster COVID-19 diagnosis, which consequently saves time for the disease control and provides an efficient diagnosis compared to laboratory tests. In this study, a novel CAD system called FUSI-CAD based on AI techniques is proposed. Almost all the methods in the literature are based on individual convolutional neural networks (CNN). Consequently, the FUSI-CAD system is based on the fusion of multiple different CNN architectures with three handcrafted features including statistical features and textural analysis features such as discrete wavelet transform (DWT), and the grey level co-occurrence matrix (GLCM) which were not previously utilized in coronavirus diagnosis. The SARS-CoV-2 CT-scan dataset is used to test the performance of the proposed FUSI-CAD. The results show that the proposed system could accurately differentiate between COVID-19 and non-COVID-19 images, as the accuracy achieved is 99%. Additionally, the system proved to be reliable as well. This is because the sensitivity, specificity, and precision attained to 99%. In addition, the diagnostics odds ratio (DOR) is \u2265 100. Furthermore, the results are compared with recent related studies based on the same dataset. The comparison verifies the competence of the proposed FUSI-CAD over the other related CAD systems. Thus, the novel FUSI-CAD system can be employed in real diagnostic scenarios for achieving accurate testing for COVID-19 and avoiding human misdiagnosis that might exist due to human fatigue. It can also reduce the time and exertion made by the radiologists during the examination process.", "journal": "PeerJ. Computer science", "date": "2021-04-06", "authors": ["Dina ARagab", "OmneyaAttallah"], "doi": "10.7717/peerj-cs.306\n10.1148/radiol.2020200642\n10.3390/e20050344\n10.1101/2020.04.16.20064709\n10.1049/iet-cvi.2014.0193\n10.3390/diagnostics10050292\n10.1177/0954411917731592\n10.1186/s12911-017-0508-3\n10.3390/brainsci9090231\n10.3390/diagnostics10010027\n10.1007/s10489-020-01714-3\n10.1109/TFUZZ.91\n10.1016/j.crad.2004.07.008\n10.1101/2020.02.25.20021568\n10.1148/radiol.2020200230\n10.1098/rsos.140216\n10.1016/j.cmpb.2019.05.027\n10.1109/RBME.4664312\n10.1016/j.knosys.2017.10.028\n10.1049/iet-bmt.2017.0210\n10.1109/TSMC.1973.4309314\n10.3390/e22050517\n10.1101/2020.04.13.20063941\n10.1007/s11604-020-00956-y\n10.1101/2020.03.20.20039834\n10.1101/2020.03.19.20039354\n10.1371/journal.pone.0129024\n10.3174/ajnr.A2061\n10.1155/2013/104684\n10.1016/j.ejca.2011.11.036\n10.1148/radiol.2020200236\n10.1148/radiol.2020200905\n10.1016/S0734-189X(87)80186-X\n10.1016/j.patcog.2017.05.025\n10.3390/s18030699\n10.1001/jama.2020.0757\n10.1007/BF03178082\n10.7717/peerj.6201\n10.1148/radiol.2020200269\n10.1007/s10096-020-03901-z\n10.1101/2020.04.24.20078584\n10.1148/radiol.2020200274\n10.1101/2020.02.23.20026930\n10.1155/2017/9571262\n10.1016/j.neucom.2018.04.082\n10.1016/j.patcog.2019.03.009\n10.1109/TEVC.2009.2014613\n10.1109/JBHI.2017.2775662\n10.1101/2020.03.12.20027185"}
{"title": "A multi-task pipeline with specialized streams for classification and segmentation of infection manifestations in COVID-19 scans.", "abstract": "We are concerned with the challenge of coronavirus disease (COVID-19) detection in chest X-ray and Computed Tomography (CT) scans, and the classification and segmentation of related infection manifestations. Even though it is arguably not an established diagnostic tool, using machine learning-based analysis of COVID-19 medical scans has shown the potential to provide a preliminary digital second opinion. This can help in managing the current pandemic, and thus has been attracting significant research attention. In this research, we propose a multi-task pipeline that takes advantage of the growing advances in deep neural network models. In the first stage, we fine-tuned an Inception-v3 deep model for COVID-19 recognition using multi-modal learning, that is, using X-ray and CT scans. In addition to outperforming other deep models on the same task in the recent literature, with an attained accuracy of 99.4%, we also present comparative analysis for multi-modal learning against learning from X-ray scans alone. The second and the third stages of the proposed pipeline complement one another in dealing with different types of infection manifestations. The former features a convolutional neural network architecture for recognizing three types of manifestations, while the latter transfers learning from another knowledge domain, namely, pulmonary nodule segmentation in CT scans, to produce binary masks for segmenting the regions corresponding to these manifestations. Our proposed pipeline also features specialized streams in which multiple deep models are trained separately to segment specific types of infection manifestations, and we show the significant impact that this framework has on various performance metrics. We evaluate the proposed models on widely adopted datasets, and we demonstrate an increase of approximately 2.5% and 4.5% for dice coefficient and mean intersection-over-union (mIoU), respectively, while achieving 60% reduction in computational time, compared to the recent literature.", "journal": "PeerJ. Computer science", "date": "2021-04-06", "authors": ["ShimaaEl-Bana", "AhmadAl-Kabbany", "MahaSharkas"], "doi": "10.7717/peerj-cs.303\n10.1148/radiol.2020200642\n10.1101/2020.04.16.20064709\n10.1016/S0140-6736(20)30154-9\n10.2214/AJR.20.23012\n10.1093/clinchem/hvaa029\n10.3390/diagnostics10030131\n10.1101/2020.04.22.20074948\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30728-5\n10.1016/j.jacr.2020.03.011\n10.1007/s00330-020-06817-6\n10.1056/NEJMoa2001316\n10.1016/S2589-7500(19)30123-2\n10.3390/sym12040651\n10.3390/rs10071119\n10.3390/s19173722\n10.1038/s41746-018-0076-7\n10.3390/app9050940\n10.33889/IJMEMS.2020.5.4.052\n10.1001/jama.2020.3864\n10.2196/10010\n10.1101/2020.02.23.20026930\n10.1080/14737159.2020.1757437\n10.1001/jama.2020.1585\n10.1016/j.molcel.2018.03.004\n10.1016/S0140-6736(20)30260-9\n10.1007/s00330-020-06801-0\n10.1109/TMI.2020.2995965\n10.1109/TMI.2019.2959609\n10.1148/radiol.2020200490"}
{"title": "Helping Roles of Artificial Intelligence (AI) in the Screening and Evaluation of COVID-19 Based on the CT Images.", "abstract": "The aim of this study was to explore the role of the AI system which was designed and developed based on the characteristics of COVID-19 CT images in the screening and evaluation of COVID-19.\nThe research team adopted an improved U-shaped neural network to segment lungs and pneumonia lesions in CT images through multilayer convolution iterations. Then the appropriate 159 cases were selected to establish and train the model, and Dice loss function and Adam optimizer were used for network training with the initial learning rate of 0.001. Finally, 39 cases (29 positive and 10 negative) were selected for the comparative test. Experimental group: an attending physician a and an associate chief physician a read the CT images to diagnose COVID-19 with the help of the AI system. Control group: an attending physician b and an associate chief physician b did the diagnosis only by their experience, without the help of the AI system. The time spent by each doctor in the diagnosis and their diagnostic results were recorded. Paired \nThere was statistical significance in the time spent in the diagnosis of different groups (\nThe AI system developed by us, which was created due to COVID-19, had certain clinical practicability and was worth popularizing.", "journal": "Journal of inflammation research", "date": "2021-04-06", "authors": ["HuiXie", "QingLi", "Ping-FengHu", "Sen-HuaZhu", "Jian-FangZhang", "Hong-DaZhou", "Hai-BoZhou"], "doi": "10.2147/JIR.S301866\n10.1038/d41586-020-00153-x\n10.1016/j.ijid.2020.01.050\n10.1038/s41598-020-74164-z\n10.1101/2020.02.25.20021568\n10.1016/j.chaos.2020.110120\n10.1007/s00330-020-07087-y\n10.1148/radiol.2020200343\n10.13929/j.issn.1003-3289.2020.03.001\n10.1016/j.procs.2017.11.221\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020200274\n10.13437/j.cnki.jcr.20200206.002\n10.1016/S0140-6736(20)30183-5\n10.12116/j.issn.1004-5619.2020.01.00\n10.1148/radiol.2020200230\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200370"}
{"title": "Radiographic findings in COVID-19: Comparison between AI and radiologist.", "abstract": "As the burden of COVID-19 enhances, the need of a fast and reliable screening method is imperative. Chest radiographs plays a pivotal role in rapidly triaging the patients. Unfortunately, in low-resource settings, there is a scarcity of trained radiologists.\nThis study evaluates and compares the performance of an artificial intelligence (AI) system with a radiologist in detecting chest radiograph findings due to COVID-19.\nThe test set consisted of 457 CXR images of patients with suspected COVID-19 pneumonia over a period of three months. The radiographs were evaluated by a radiologist with experience of more than 13 years and by the AI system (NeuraCovid, a web application that pairs with the AI model COVID-NET). Performance of AI system and the radiologist were compared by calculating the sensitivity, specificity and generating a receiver operating characteristic curve. RT-PCR test results were used as the gold standard.\nThe radiologist obtained a sensitivity and specificity of 44.1% and 92.5%, respectively, whereas the AI had a sensitivity and specificity of 41.6% and 60%, respectively. The area under curve for correctly classifying CXR images as COVID-19 pneumonia was 0.48 for the AI system and 0.68 for the radiologist. The radiologist's prediction was found to be superior to that of the AI with a \nThe specificity and sensitivity of detecting lung involvement in COVID-19, by the radiologist, was found to be superior to that by the AI system.", "journal": "The Indian journal of radiology & imaging", "date": "2021-04-06", "authors": ["ArshSukhija", "MangalMahajan", "Priscilla CJoshi", "JohnDsouza", "Nagesh D NSeth", "Karamchand HPatil"], "doi": "10.4103/ijri.IJRI_777_20"}
{"title": "Comparing a deep learning model's diagnostic performance to that of radiologists to detect Covid -19 features on chest radiographs.", "abstract": "Whether the sensitivity of Deep Learning (DL) models to screen chest radiographs (CXR) for CoVID-19 can approximate that of radiologists, so that they can be adopted and used if real-time review of CXRs by radiologists is not possible, has not been explored before.\nTo evaluate the diagnostic performance of a doctor-trained DL model (Svita_DL8) to screen for COVID-19 on CXR, and to compare the performance of the DL model with that of expert radiologists.\nWe used a pre-trained convolutional neural network to develop a publicly available online DL model to evaluate CXR examinations saved in .jpeg or .png format. The initial model was subsequently curated and trained by an internist and a radiologist using 1062 chest radiographs to classify a submitted CXR as either normal, COVID-19, or a non-COVID-19 abnormal. For validation, we collected a separate set of 430 CXR examinations from numerous publicly available datasets from 10 different countries, case presentations, and two hospital repositories. These examinations were assessed for COVID-19 by the DL model and by two independent radiologists. Diagnostic performance was compared between the model and the radiologists and the correlation coefficient calculated.\nFor detecting COVID-19 on CXR, our DL model demonstrated sensitivity of 91.5%, specificity of 55.3%, PPV 60.9%, NPV 77.9%, accuracy 70.1%, and AUC 0.73 (95% CI: 0.86, 0.95). There was a significant correlation (r = 0.617, \nThe DL model demonstrated high sensitivity for detecting COVID-19 on CXR.\nThe doctor trained DL tool Svita_DL8 can be used in resource-constrained settings to quickly triage patients with suspected COVID-19 for further in-depth review and testing.", "journal": "The Indian journal of radiology & imaging", "date": "2021-04-06", "authors": ["SabithaKrishnamoorthy", "SudhakarRamakrishnan", "Lanson BrijeshColaco", "AkshayDias", "Indu KGopi", "Gautham A GGowda", "K CAishwarya", "VeenaRamanan", "ManjuChandran"], "doi": "10.4103/ijri.IJRI_914_20"}
{"title": "Financial impact of COVID-19 on radiology practice in India.", "abstract": "The COVID-19 pandemic will have serious financial effects on the healthcare sector business. There will be significant short-term and long-term effects of this on Radiology services throughout the country. Various social distancing measures undertaken by the government will bring larger economic hurdles with them. An attempt to achieve COVID-19 preparedness by hospitals has led to a significant decline in patient footfall and in turn imaging volumes. Despite relief measures provided by the government like providing a moratorium on EMIs of all outstanding loans for a specified period and allocating funds toward reinforcing healthcare infrastructure, the effects of this pandemic will leave the radiology business in a crippled state, in the foreseeable future. Radiology practices have seen a significant impact on business to the extent of almost 60%-70% reduction in imaging volumes and this will be the case for the next few months to come. Administrators and radiologists should proactively take measures to device strategies and plans to tide over this crisis. Eventually, this pandemic will end, and life will have a \"New Normal.\" Medical aid that is being deferred today will be sought out later. Alternate means of reporting like teleradiology and artificial intelligence should be strongly pursued and providing education regarding these to their staff and the younger generation of radiologists should be of prime concern.", "journal": "The Indian journal of radiology & imaging", "date": "2021-04-06", "authors": ["GauriAhuja", "MitushaVerma", "DeepakPatkar"], "doi": "10.4103/ijri.IJRI_305_20"}
{"title": "Artificial intelligence and radiology: Combating the COVID-19 conundrum.", "abstract": "The COVID-19 pandemic has necessitated rapid testing and diagnosis to manage its spread. While reverse transcriptase polymerase chain reaction (RT-PCR) is being used as the gold standard method to diagnose COVID-19, many scientists and doctors have pointed out some challenges related to the variability, accuracy, and affordability of this technique. At the same time, radiological methods, which were being used to diagnose COVID-19 in the early phase of the pandemic in China, were sidelined by many primarily due to their low specificity and the difficulty in conducting a differential diagnosis. However, the utility of radiological methods cannot be neglected. Indeed, over the past few months, healthcare consultants and radiologists in India have been using or advising the use of high-resolution computed tomography (HRCT) of the chest for early diagnosis and tracking of COVID-19, particularly in preoperative and asymptomatic patients. At the same time, scientists have been trying to improve upon the radiological method of COVID-19 diagnosis and monitoring by using artificial intelligence (AI)-based interpretation models. This review is an effort to compile and compare such efforts. To this end, the latest scientific literature on the use of radiology and AI-assisted radiology for the diagnosis and monitoring of COVID-19 has been reviewed and presented, highlighting the strengths and limitations of such techniques.", "journal": "The Indian journal of radiology & imaging", "date": "2021-04-06", "authors": ["MayurPankhania"], "doi": "10.4103/ijri.IJRI_618_20"}
{"title": "Pneumocystis pneumonia: An important consideration when investigating artificial intelligence-based methods in the radiological diagnosis of COVID-19.", "abstract": null, "journal": "Clinical imaging", "date": "2021-04-04", "authors": ["TemiLampejo"], "doi": "10.1016/j.clinimag.2021.02.044\n10.1016/j.clinimag.2021.01.019\n10.1148/radiol.2020200905\n10.1259/bjr.20200703\n10.1002/jia2.25533\n10.7861/CLINMED.2020-0565\n10.1016/j.medin.2020.07.007\n10.21203/rs.3.rs-53350/v1\n10.4414/smw.2020.20312\n10.1136/bmj.m1808\n10.1016/j.cmi.2020.12.007\n10.1128/CMR.00013-12\n10.1093/ofid/ofaa633"}
{"title": "A Few-Shot U-Net Deep Learning Model for COVID-19 Infected Area Segmentation in CT Images.", "abstract": "Recent studies indicate that detecting radiographic patterns on CT chest scans can yield high sensitivity and specificity for COVID-19 identification. In this paper, we scrutinize the effectiveness of deep learning models for semantic segmentation of pneumonia-infected area segmentation in CT images for the detection of COVID-19. Traditional methods for CT scan segmentation exploit a supervised learning paradigm, so they (a) require large volumes of data for their training, and (b) assume fixed (static) network weights once the training procedure has been completed. Recently, to overcome these difficulties, few-shot learning (FSL) has been introduced as a general concept of network model training using a very small amount of samples. In this paper, we explore the efficacy of few-shot learning in U-Net architectures, allowing for a dynamic fine-tuning of the network weights as new few samples are being fed into the U-Net. Experimental results indicate improvement in the segmentation accuracy of identifying COVID-19 infected regions. In particular, using 4-fold cross-validation results of the different classifiers, we observed an improvement of 5.388 \u00b1 3.046% for all test data regarding the IoU metric and a similar increment of 5.394 \u00b1 3.015% for the F1 score. Moreover, the statistical significance of the improvement obtained using our proposed few-shot U-Net architecture compared with the traditional U-Net model was confirmed by applying the Kruskal-Wallis test (", "journal": "Sensors (Basel, Switzerland)", "date": "2021-04-04", "authors": ["AthanasiosVoulodimos", "EftychiosProtopapadakis", "IasonKatsamenis", "AnastasiosDoulamis", "NikolaosDoulamis"], "doi": "10.3390/s21062215\n10.1016/S0140-6736(20)30183-5\n10.2139/ssrn.3557504\n10.1148/radiol.2020200432\n10.1109/RBME.2020.2987975\n10.1016/j.neunet.2020.03.007\n10.1109/TMI.2020.2995965\n10.1101/2020.05.08.20094664\n10.1145/3386252\n10.2214/AJR.20.22954\n10.1016/j.ejrad.2020.109009\n10.1016/j.jinf.2020.04.004\n10.1155/2018/7068349\n10.1021/acs.jproteome.9b00411\n10.1148/radiol.2020200905\n10.1016/j.imu.2020.100412\n10.1109/TMI.2020.2996645\n10.1016/j.ejrad.2020.109041\n10.1016/j.neucom.2019.01.110\n10.1038/s41598-020-76282-0\n10.1016/j.compbiomed.2020.103795\n10.1007/s10096-020-03901-z\n10.14299/ijser.2020.03.02\n10.1109/ACCESS.2020.3005510\n10.1007/s10044-020-00950-0\n10.1016/j.eswa.2020.114142\n10.1016/j.knosys.2020.106647\n10.21037/atm-20-2464\n10.1088/1361-6560/abe838\n10.1007/s10489-018-01396-y\n10.5281/zenodo.3757476\n10.1016/j.cmpb.2020.105581\n10.1017/S0950268820001727\n10.1504/IJDMMM.2019.10019369\n10.1007/s11042-015-2512-x\n10.1109/ACCESS.2017.2776349\n10.1016/j.eswa.2005.09.019\n10.1016/j.compbiomed.2020.103805\n10.1080/01621459.1952.10483441\n10.1016/j.scitotenv.2016.06.201\n10.1016/j.patrec.2005.10.010"}
{"title": "Role of Hybrid Deep Neural Networks (HDNNs), Computed Tomography, and Chest X-rays for the Detection of COVID-19.", "abstract": "COVID-19 syndrome has extensively escalated worldwide with the induction of the year 2020 and has resulted in the illness of millions of people. COVID-19 patients bear an elevated risk once the symptoms deteriorate. Hence, early recognition of diseased patients can facilitate early intervention and avoid disease succession. This article intends to develop a hybrid deep neural networks (HDNNs), using computed tomography (CT) and X-ray imaging, to predict the risk of the onset of disease in patients suffering from COVID-19. To be precise, the subjects were classified into 3 categories namely normal, Pneumonia, and COVID-19. Initially, the CT and chest X-ray images, denoted as 'hybrid images' (with resolution 1080 \u00d7 1080) were collected from different sources, including GitHub, COVID-19 radiography database, Kaggle, COVID-19 image data collection, and Actual Med COVID-19 Chest X-ray Dataset, which are open source and publicly available data repositories. The 80% hybrid images were used to train the hybrid deep neural network model and the remaining 20% were used for the testing purpose. The capability and prediction accuracy of the HDNNs were calculated using the confusion matrix. The hybrid deep neural network showed a 99% classification accuracy on the test set data.", "journal": "International journal of environmental research and public health", "date": "2021-04-04", "authors": ["MuhammadIrfan", "Muhammad AksamIftikhar", "SanaYasin", "UmarDraz", "TariqAli", "ShafiqHussain", "SarahBukhari", "Abdullah SaeedAlwadie", "SaifurRahman", "AdamGlowacz", "FaisalAlthobiani"], "doi": "10.3390/ijerph18063056\n10.1093/cid/ciaa461\n10.1148/radiol.2020200642\n10.1148/radiol.2020200905\n10.1007/s00330-019-06163-2\n10.1080/14737159.2020.1757437\n10.21106/ijma.421\n10.1101/2020.10.29.339317\n10.1007/s10916-020-1536-6\n10.2214/AJR.18.20509\n10.1016/j.compmedimag.2019.101688\n10.1016/j.measurement.2019.05.028\n10.1109/TMI.2019.2963248\n10.1007/s13246-020-00957-1\n10.1108/WJE-10-2020-0529\n10.1016/j.neuroimage.2011.01.008\n10.3390/diagnostics10080565\n10.1001/jamasurg.2020.4998\n10.1016/j.neucom.2015.09.034\n10.1038/s41568-018-0016-5\n10.1007/s00247-017-3943-5\n10.1101/2020.06.12.20129643\n10.1016/j.compbiomed.2020.103792\n10.1101/2020.08.14.20170290\n10.1101/2020.02.23.20026930\n10.1101/2020.07.11.20151332\n10.3389/fbioe.2020.00898\n10.1007/s00500-020-05275-y\n10.1016/j.cell.2020.04.045\n10.1038/s42256-019-0057-9\n10.1109/TMI.2018.2832217\n10.1109/ACCESS.2020.3009908\n10.14569/IJACSA.2018.090543\n10.3390/e22121370"}
{"title": "Artificial Intelligence Applied to Chest X-ray for Differential Diagnosis of COVID-19 Pneumonia.", "abstract": "We assessed the role of artificial intelligence applied to chest X-rays (CXRs) in supporting the diagnosis of COVID-19. We trained and cross-validated a model with an ensemble of 10 convolutional neural networks with CXRs of 98 COVID-19 patients, 88 community-acquired pneumonia (CAP) patients, and 98 subjects without either COVID-19 or CAP, collected in two Italian hospitals. The system was tested on two independent cohorts, namely, 148 patients (COVID-19, CAP, or negative) collected by one of the two hospitals (independent testing I) and 820 COVID-19 patients collected by a multicenter study (independent testing II). On the training and cross-validation dataset, sensitivity, specificity, and area under the curve (AUC) were 0.91, 0.87, and 0.93 for COVID-19 versus negative subjects, 0.85, 0.82, and 0.94 for COVID-19 versus CAP. On the independent testing I, sensitivity, specificity, and AUC were 0.98, 0.88, and 0.98 for COVID-19 versus negative subjects, 0.97, 0.96, and 0.98 for COVID-19 versus CAP. On the independent testing II, the system correctly diagnosed 652 COVID-19 patients versus negative subjects (0.80 sensitivity) and correctly differentiated 674 COVID-19 versus CAP patients (0.82 sensitivity). This system appears promising for the diagnosis and differential diagnosis of COVID-19, showing its potential as a second opinion tool in conditions of the variable prevalence of different types of infectious pneumonia.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-04-04", "authors": ["ChristianSalvatore", "MatteoInterlenghi", "Caterina BMonti", "DavideIppolito", "DavideCapra", "AndreaCozzi", "SimoneSchiaffino", "AnnalisaPolidori", "DavideGandola", "MarcoAl\u00ec", "IsabellaCastiglioni", "CristinaMessa", "FrancescoSardanelli"], "doi": "10.3390/diagnostics11030530\n10.2807/1560-7917.ES.2020.25.8.2000170\n10.2214/AJR.20.23119\n10.7326/M20-1301\n10.1164/rccm.2020C1\n10.1136/bmj.m1328\n10.1186/s13578-020-00404-4\n10.1259/bjr.20200515\n10.2214/AJR.20.22969\n10.2214/AJR.20.23418\n10.1148/radiol.2020203173\n10.1148/radiol.2020201365\n10.1097/RTI.0000000000000516\n10.1148/radiol.2020201343\n10.1148/radiol.2020201326\n10.1148/radiol.2020200988\n10.1186/s41747-020-00182-1\n10.21037/qims-20-771\n10.1148/rg.2020200097\n10.1097/RTI.0000000000000533\n10.1016/j.ejrad.2020.109272\n10.1148/radiol.2020201160\n10.1148/radiol.2020200463\n10.3390/app10020559\n10.1097/RTI.0000000000000498\n10.1101/2020.04.21.20063263\n10.1371/journal.pone.0235187\n10.1148/radiol.2020200905\n10.1097/RTI.0000000000000512\n10.1016/j.cmpb.2020.105532\n10.1007/s40846-020-00529-4\n10.1016/j.compbiomed.2020.104037\n10.1007/s00259-020-04953-1\n10.1186/s41747-020-00203-z\n10.1101/2020.05.09.20096560\n10.1016/j.media.2020.101794\n10.3233/XST-200715\n10.1016/j.mehy.2020.109761\n10.1007/s00138-020-01101-5\n10.1109/RBME.2020.2987975\n10.1007/s12553-021-00520-2\n10.1016/j.ejmp.2021.02.006\n10.1148/radiol.2020191145\n10.1016/S2468-2667(21)00006-2\n10.1136/bmjgh-2020-003098\n10.1016/S2468-2667(20)30308-X\n10.1016/S2214-109X(20)30264-3\n10.1001/jama.2020.27124\n10.1001/jama.2021.2088\n10.1001/jama.2021.1114\n10.1016/j.clinimag.2020.06.031\n10.1007/s00330-020-06918-2\n10.1097/RTI.0000000000000347\n10.1148/rg.2018170048\n10.1148/radiol.11092149\n10.1148/radiol.2020201845\n10.1016/j.jclinepi.2008.04.007\n10.1097/RTI.0000000000000453"}
{"title": "Hyperparameter Optimization for COVID-19 Pneumonia Diagnosis Based on Chest CT.", "abstract": "Convolutional Neural Networks (CNNs) have been successfully applied in the medical diagnosis of different types of diseases. However, selecting the architecture and the best set of hyperparameters among the possible combinations can be a significant challenge. The purpose of this work is to investigate the use of the Hyperband optimization algorithm in the process of optimizing a CNN applied to the diagnosis of SARS-Cov2 disease (COVID-19). The test was performed with the Optuna framework, and the optimization process aimed to optimize four hyperparameters: (1) backbone architecture, (2) the number of inception modules, (3) the number of neurons in the fully connected layers, and (4) the learning rate. CNNs were trained on 2175 computed tomography (CT) images. The CNN that was proposed by the optimization process was a VGG16 with five inception modules, 128 neurons in the two fully connected layers, and a learning rate of 0.0027. The proposed method achieved a sensitivity, precision, and accuracy of 97%, 82%, and 88%, outperforming the sensitivity of the Real-Time Polymerase Chain Reaction (RT-PCR) tests (53-88%) and the accuracy of the diagnosis performed by human experts (72%).", "journal": "Sensors (Basel, Switzerland)", "date": "2021-04-04", "authors": ["PauloLacerda", "BrunoBarros", "C\u00e9lioAlbuquerque", "AuraConci"], "doi": "10.3390/s21062174\n10.1590/s1678-9946202062044\n10.1016/j.rmed.2020.105980\n10.1038/s41568-018-0016-5\n10.1016/j.eswa.2019.01.060\n10.1007/s10044-020-00950-0\n10.1038/s41591-020-0931-3\n10.1016/j.eng.2020.04.010\n10.1016/j.inffus.2020.11.005\n10.3390/diagnostics10010024\n10.1016/j.apacoust.2020.107549\n10.1007/s11263-015-0816-y\n10.1016/j.drudis.2018.01.039\n10.1016/j.media.2017.06.015\n10.1148/radiol.2020200463\n10.1007/s00330-020-07347-x\n10.1148/radiol.2020201237\n10.1162/neco.1997.9.8.1735"}
{"title": "SARS-CoV-2 Is a Culprit for Some, but Not All Acute Ischemic Strokes: A Report from the Multinational COVID-19 Stroke Study Group.", "abstract": "SARS-CoV-2 infected patients are suggested to have a higher incidence of thrombotic events such as acute ischemic strokes (AIS). This study aimed at exploring vascular comorbidity patterns among SARS-CoV-2 infected patients with subsequent stroke. We also investigated whether the comorbidities and their frequencies under each subclass of TOAST criteria were similar to the AIS population studies prior to the pandemic.\nThis is a report from the Multinational COVID-19 Stroke Study Group. We present an original dataset of SASR-CoV-2 infected patients who had a subsequent stroke recorded through our multicenter prospective study. In addition, we built a dataset of previously reported patients by conducting a systematic literature review. We demonstrated distinct subgroups by clinical risk scoring models and unsupervised machine learning algorithms, including hierarchical K-Means (ML-K) and Spectral clustering (ML-S).\nThis study included 323 AIS patients from 71 centers in 17 countries from the original dataset and 145 patients reported in the literature. The unsupervised clustering methods suggest a distinct cohort of patients (ML-K: 36% and ML-S: 42%) with no or few comorbidities. These patients were more than 6 years younger than other subgroups and more likely were men (ML-K: 59% and ML-S: 60%). The majority of patients in this subgroup suffered from an embolic-appearing stroke on imaging (ML-K: 83% and ML-S: 85%) and had about 50% risk of large vessel occlusions (ML-K: 50% and ML-S: 53%). In addition, there were two cohorts of patients with large-artery atherosclerosis (ML-K: 30% and ML-S: 43% of patients) and cardioembolic strokes (ML-K: 34% and ML-S: 15%) with consistent comorbidity and imaging patterns. Binominal logistic regression demonstrated that ischemic heart disease (odds ratio (OR), 4.9; 95% confidence interval (CI), 1.6-14.7), atrial fibrillation (OR, 14.0; 95% CI, 4.8-40.8), and active neoplasm (OR, 7.1; 95% CI, 1.4-36.2) were associated with cardioembolic stroke.\nAlthough a cohort of young and healthy men with cardioembolic and large vessel occlusions can be distinguished using both clinical sub-grouping and unsupervised clustering, stroke in other patients may be explained based on the existing comorbidities.", "journal": "Journal of clinical medicine", "date": "2021-04-04", "authors": ["ShimaShahjouei", "MichelleAnyaehie", "EricKoza", "GeorgiosTsivgoulis", "SoheilNaderi", "AshkanMowla", "VenkateshAvula", "AlirezaVafaei Sadr", "DurgeshChaudhary", "GhasemFarahmand", "ChristophGriessenauer", "Mahmoud RezaAzarpazhooh", "DebdiptoMisra", "JiangLi", "VidaAbedi", "RaminZand", "NoneThe Multinational Covid-Stroke Study Group"], "doi": "10.3390/jcm10050931\n10.1056/NEJMc2009787\n10.1161/STROKEAHA.120.030574\n10.1007/s00415-020-09885-2\n10.1056/NEJMc2007575\n10.1016/S2215-0366(20)30287-X\n10.1007/s00415-020-10012-4\n10.1016/j.medcli.2020.04.012\n10.1016/j.avsg.2020.05.031\n10.1016/j.neurad.2020.04.003\n10.1111/ene.14380\n10.1056/NEJMc2015630\n10.1016/j.amjcard.2020.08.003\n10.1161/JAHA.120.018379\n10.1016/j.lanwpc.2020.100056\n10.1016/S2468-2667(20)30188-2\n10.1002/ehf2.13075\n10.1016/j.jaccas.2020.05.024\n10.1016/j.amjcard.2020.06.063\n10.7759/cureus.8426\n10.1016/j.ebiom.2020.102939\n10.1177/1747493020972922\n10.1016/j.jstrokecerebrovasdis.2020.105288\n10.1161/STROKEAHA.120.031217\n10.1001/jama.2011.1615\n10.1097/CCM.0000000000002325\n10.1161/STROKEAHA.116.016162\n10.1016/j.dsx.2020.07.005\n10.1161/CIRCRESAHA.116.308398\n10.1161/STROKEAHA.115.012052\n10.1159/000502446\n10.1161/01.STR.24.1.35\n10.1016/j.ijsu.2014.07.013\n10.1186/2046-4053-4-1\n10.1007/s11222-007-9033-z\n10.1108/eb003648\n10.1161/strokeaha.120.033160\n10.1016/j.jstrokecerebrovasdis.2020.105321\n10.1161/STROKEAHA.120.031208\n10.1177/1747493020937189\n10.1136/jnnp-2020-323586\n10.1016/j.jstrokecerebrovasdis.2011.05.022\n10.1016/j.jstrokecerebrovasdis.2019.104503\n10.1161/STROKEAHA.107.510933\n10.1161/STROKEAHA.116.015169"}
{"title": "Lessons from the COVID-19 Pandemic on the Use of Artificial Intelligence in Digital Radiology: The Submission of a Survey to Investigate the Opinion of Insiders.", "abstract": "The development of artificial intelligence (AI) during the COVID-19 pandemic is there for all to see, and has undoubtedly mainly concerned the activities of digital radiology. Nevertheless, the strong perception in the research and clinical application environment is that AI in radiology is like a hammer in search of a nail. Notable developments and opportunities do not seem to be combined, now, in the time of the COVID-19 pandemic, with a stable, effective, and concrete use in clinical routine; the use of AI often seems limited to use in research applications. This study considers the future perceived integration of AI with digital radiology after the COVID-19 pandemic and proposes a methodology that, by means of a wide interaction of the involved actors, allows a positioning exercise for acceptance evaluation using a general purpose electronic survey. The methodology was tested on a first category of professionals, the medical radiology technicians (MRT), and allowed to (i) collect their impressions on the issue in a structured way, and (ii) collect their suggestions and their comments in order to create a specific tool for this professional figure to be used in scientific societies. This study is useful for the stakeholders in the field, and yielded several noteworthy observations, among them (iii) the perception of great development in thoracic radiography and CT, but a loss of opportunity in integration with non-radiological technologies; (iv) the belief that it is appropriate to invest in training and infrastructure dedicated to AI; and (v) the widespread idea that AI can become a strong complementary tool to human activity. From a general point of view, the study is a clear invitation to face the last yard of AI in digital radiology, a last yard that depends a lot on the opinion and the ability to accept these technologies by the operators of digital radiology.", "journal": "Healthcare (Basel, Switzerland)", "date": "2021-04-04", "authors": ["DanieleGiansanti", "IvanoRossi", "LisaMonoscalco"], "doi": "10.3390/healthcare9030331\n10.1016/j.jinf.2020.03.041\n10.1016/S0140-6736(20)30183-5\n10.1016/j.cca.2021.01.011\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.3390/jcm9030623\n10.1002/wrna.46\n10.1186/s40779-020-0233-6\n10.1515/cclm-2020-0198\n10.1515/cclm-2020-0285\n10.1128/CMR.19.1.165-256.2006\n10.1515/cclm-2016-0810\n10.1515/cclm-2019-1089\n10.1128/JCM.00512-20\n10.26355/eurrev_202011_23640\n10.1155/2020/9756518\n10.1007/s13755-020-00135-3\n10.1148/radiol.2020204238\n10.1148/radiol.2020203511\n10.1148/radiol.2020201874\n10.1148/radiol.2020202944\n10.1145/3065386\n10.1148/radiol.2020204226\n10.3390/healthcare9010030\n10.21037/mhealth.2019.11.02\n10.21037/mhealth-19-188"}
{"title": "COVID-19 Recognition Using Ensemble-CNNs in Two New Chest X-ray Databases.", "abstract": "The recognition of COVID-19 infection from X-ray images is an emerging field in the learning and computer vision community. Despite the great efforts that have been made in this field since the appearance of COVID-19 (2019), the field still suffers from two drawbacks. First, the number of available X-ray scans labeled as COVID-19-infected is relatively small. Second, all the works that have been carried out in the field are separate; there are no unified data, classes, and evaluation protocols. In this work, based on public and newly collected data, we propose two X-ray COVID-19 databases, which are three-class COVID-19 and five-class COVID-19 datasets. For both databases, we evaluate different deep learning architectures. Moreover, we propose an Ensemble-CNNs approach which outperforms the deep learning architectures and shows promising results in both databases. In other words, our proposed Ensemble-CNNs achieved a high performance in the recognition of COVID-19 infection, resulting in accuracies of 100% and 98.1% in the three-class and five-class scenarios, respectively. In addition, our approach achieved promising results in the overall recognition accuracy of 75.23% and 81.0% for the three-class and five-class scenarios, respectively. We make our databases of COVID-19 X-ray scans publicly available to encourage other researchers to use it as a benchmark for their studies and comparisons.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-04-04", "authors": ["EdoardoVantaggiato", "EmanuelaPaladini", "FaresBougourzi", "CosimoDistante", "AbdenourHadid", "AbdelmalikTaleb-Ahmed"], "doi": "10.3390/s21051742\n10.1007/s10489-020-01888-w\n10.7326/M20-1495\n10.1148/radiol.2020200527\n10.3389/fmed.2020.00427\n10.1038/s41598-020-76550-z\n10.3390/app10093233\n10.1016/j.patrec.2021.01.010\n10.1155/2020/8889023\n10.1016/j.eswa.2020.113459\n10.1049/iet-ipr.2018.6235\n10.1007/s13246-020-00865-4\n10.1145/3065386\n10.1007/BF00994018\n10.1038/s41598-020-71294-2\n10.1016/j.cell.2018.02.010\n10.1148/ryai.2019180041\n10.1088/1757-899X/428/1/012043\n10.1016/j.inffus.2020.11.007"}
{"title": "A Novel Computational Approach for the Discovery of Drug Delivery System Candidates for COVID-19.", "abstract": "In order to treat Coronavirus Disease 2019 (COVID-19), we predicted and implemented a drug delivery system (DDS) that can provide stable drug delivery through a computational approach including a clustering algorithm and the Schr\u00f6dinger software. Six carrier candidates were derived by the proposed method that could find molecules meeting the predefined conditions using the molecular structure and its functional group positional information. Then, just one compound named glycyrrhizin was selected as a candidate for drug delivery through the Schr\u00f6dinger software. Using glycyrrhizin, nafamostat mesilate (NM), which is known for its efficacy, was converted into micelle nanoparticles (NPs) to improve drug stability and to effectively treat COVID-19. The spherical particle morphology was confirmed by transmission electron microscopy (TEM), and the particle size and stability of 300-400 nm were evaluated by measuring DLSand the zeta potential. The loading of NM was confirmed to be more than 90% efficient using the UV spectrum.", "journal": "International journal of molecular sciences", "date": "2021-04-04", "authors": ["TaeheumCho", "Hyo-SangHan", "JunhyukJeong", "Eun-MiPark", "Kyu-SikShim"], "doi": "10.3390/ijms22062815\n10.1016/j.ijid.2020.10.093\n10.1186/2052-0492-2-20\n10.3390/v12060629\n10.1016/j.cell.2020.02.052\n10.4155/tde.14.69\n10.1021/jp037487t\n10.4236/jbise.2016.91002\n10.1093/nar/gkx1037\n10.1038/nrd1468\n10.7150/thno.38425\n10.1208/s12249-019-1333-z\n10.1016/j.addr.2019.05.001\n10.1371/journal.pone.0031724\n10.3390/biom10060913\n10.1016/j.pharmthera.2020.107618\n10.1073/pnas.0905468106\n10.2307/2346830\n10.1007/s11222-007-9033-z\n10.1080/01621459.1963.10500845\n10.1007/s13042-017-0756-7"}
{"title": "Volume-of-Interest Aware Deep Neural Networks for Rapid Chest CT-Based COVID-19 Patient Risk Assessment.", "abstract": "Since December 2019, the world has been devastated by the Coronavirus Disease 2019 (COVID-19) pandemic. Emergency Departments have been experiencing situations of urgency where clinical experts, without long experience and mature means in the fight against COVID-19, have to rapidly decide the most proper patient treatment. In this context, we introduce an artificially intelligent tool for effective and efficient Computed Tomography (CT)-based risk assessment to improve treatment and patient care. In this paper, we introduce a data-driven approach built on top of volume-of-interest aware deep neural networks for automatic COVID-19 patient risk assessment (discharged, hospitalized, intensive care unit) based on lung infection quantization through segmentation and, subsequently, CT classification. We tackle the high and varying dimensionality of the CT input by detecting and analyzing only a sub-volume of the CT, the Volume-of-Interest (VoI). Differently from recent strategies that consider infected CT slices without requiring any spatial coherency between them, or use the whole lung volume by applying abrupt and lossy volume down-sampling, we assess only the \"most infected volume\" composed of slices at its original spatial resolution. To achieve the above, we create, present and publish a new labeled and annotated CT dataset with 626 CT samples from COVID-19 patients. The comparison against such strategies proves the effectiveness of our VoI-based approach. We achieve remarkable performance on patient risk assessment evaluated on balanced data by reaching 88.88%, 89.77%, 94.73% and 88.88% accuracy, sensitivity, specificity and F1-score, respectively.", "journal": "International journal of environmental research and public health", "date": "2021-04-04", "authors": ["AnargyrosChatzitofis", "PierandreaCancian", "VasileiosGkitsas", "AlessandroCarlucci", "PanagiotisStalidis", "GeorgiosAlbanis", "AntonisKarakottas", "TheodorosSemertzidis", "PetrosDaras", "CaterinaGiannitto", "ElenaCasiraghi", "Federica MrakicSposta", "GiuliaVatteroni", "AngelaAmmirabile", "LudovicaLofino", "PasqualaRagucci", "Maria ElenaLaino", "AntonioVoza", "AntonioDesai", "MaurizioCecconi", "LucaBalzarini", "ArturoChiti", "DimitriosZarpalas", "VictorSavevski"], "doi": "10.3390/ijerph18062842\n10.1038/s41591-020-0820-9\n10.21037/atm.2020.02.06\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200230\n10.1148/radiol.2020200343\n10.1007/s11547-020-01269-w\n10.1109/ACCESS.2020.3034032\n10.7150/thno.45985\n10.14245/ns.1938396.198\n10.1038/s41591-018-0316-z\n10.1109/ACCESS.2020.2971576\n10.1136/bmj.m1328\n10.1148/radiol.2018172986\n10.1148/ryai.2019180091\n10.1038/s41591-020-0931-3\n10.1007/s10916-020-01562-1\n10.1038/s41467-020-17971-2\n10.1148/ryct.2020200034\n10.1007/s13042-017-0645-0\n10.1016/j.acra.2019.07.006\n10.1155/2020/9756518\n10.1109/TMI.2020.2994908\n10.1148/radiol.2020202439\n10.1038/s41591-019-0447-x\n10.1148/radiol.2020201473\n10.1016/j.eng.2020.04.010\n10.1148/radiol.2020200905\n10.1016/j.media.2020.101860\n10.1109/5254.708428\n10.1016/j.bj.2020.08.003\n10.1023/A:1010933404324\n10.1016/j.jksuci.2020.12.010\n10.1038/s41598-020-71294-2\n10.1007/s10044-020-00950-0\n10.1016/j.media.2020.101844\n10.1109/TMI.2020.2996645\n10.1007/s10462-020-09825-6\n10.1016/j.bbe.2019.03.001\n10.1007/s00330-020-07013-2"}
{"title": "Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images.", "abstract": "Computer-aided diagnosis for the reliable and fast detection of coronavirus disease (COVID-19) has become a necessity to prevent the spread of the virus during the pandemic to ease the burden on the healthcare system. Chest X-ray (CXR) imaging has several advantages over other imaging and detection techniques. Numerous works have been reported on COVID-19 detection from a smaller set of original X-ray images. However, the effect of image enhancement and lung segmentation of a large dataset in COVID-19 detection was not reported in the literature. We have compiled a large X-ray dataset (COVQU) consisting of 18,479 CXR images with 8851 normal, 6012 non-COVID lung infections, and 3616 COVID-19 CXR images and their corresponding ground truth lung masks. To the best of our knowledge, this is the largest public COVID positive database and the lung masks. Five different image enhancement techniques: histogram equalization (HE), contrast limited adaptive histogram equalization (CLAHE), image complement, gamma correction, and balance contrast enhancement technique (BCET) were used to investigate the effect of image enhancement techniques on COVID-19 detection. A novel U-Net model was proposed and compared with the standard U-Net model for lung segmentation. Six different pre-trained Convolutional Neural Networks (CNNs) (ResNet18, ResNet50, ResNet101, InceptionV3, DenseNet201, and ChexNet) and a shallow CNN model were investigated on the plain and segmented lung CXR images. The novel U-Net model showed an accuracy, Intersection over Union (IoU), and Dice coefficient of 98.63%, 94.3%, and 96.94%, respectively for lung segmentation. The gamma correction-based enhancement technique outperforms other techniques in detecting COVID-19 from the plain and the segmented lung CXR images. Classification performance from plain CXR images is slightly better than the segmented lung CXR images; however, the reliability of network performance is significantly improved for the segmented lung images, which was observed using the visualization technique. The accuracy, precision, sensitivity, F1-score, and specificity were 95.11%, 94.55%, 94.56%, 94.53%, and 95.59% respectively for the segmented lung images. The proposed approach with very reliable and comparable performance will boost the fast and robust COVID-19 detection using chest X-ray images.", "journal": "Computers in biology and medicine", "date": "2021-04-03", "authors": ["TawsifurRahman", "AmithKhandakar", "YazanQiblawey", "AnasTahir", "SerkanKiranyaz", "Saad BinAbul Kashem", "Mohammad TariqulIslam", "SomayaAl Maadeed", "Susu MZughaier", "Muhammad SalmanKhan", "Muhammad E HChowdhury"], "doi": "10.1016/j.compbiomed.2021.104319\n10.1016/j.chest.2020.04.010\n10.1109/ACCESS.2020.3010287"}
{"title": "Quantitative Analysis and Automated Lung Ultrasound Scoring for Evaluating COVID-19 Pneumonia With Neural Networks.", "abstract": "As being radiation-free, portable, and capable of repetitive use, ultrasonography is playing an important role in diagnosing and evaluating the COVID-19 Pneumonia (PN) in this epidemic. By virtue of lung ultrasound scores (LUSS), lung ultrasound (LUS) was used to estimate the excessive lung fluid that is an important clinical manifestation of COVID-19 PN, with high sensitivity and specificity. However, as a qualitative method, LUSS suffered from large interobserver variations and requirement for experienced clinicians. Considering this limitation, we developed a quantitative and automatic lung ultrasound scoring system for evaluating the COVID-19 PN. A total of 1527 ultrasound images prospectively collected from 31 COVID-19 PN patients with different clinical conditions were evaluated and scored with LUSS by experienced clinicians. All images were processed via a series of computer-aided analysis, including curve-to-linear conversion, pleural line detection, region-of-interest (ROI) selection, and feature extraction. A collection of 28 features extracted from the ROI was specifically defined for mimicking the LUSS. Multilayer fully connected neural networks, support vector machines, and decision trees were developed for scoring LUS images using the fivefold cross validation. The model with 128\u00d7256 two fully connected layers gave the best accuracy of 87%. It is concluded that the proposed method could assess the ultrasound images by assigning LUSS automatically with high accuracy, potentially applicable to the clinics.", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2021-04-03", "authors": ["JiangangChen", "ChaoHe", "JintaoYin", "JiaweiLi", "XiaoqianDuan", "YuchengCao", "LiSun", "MenghanHu", "WenfangLi", "QingliLi"], "doi": "10.1109/TUFFC.2021.3070696"}
{"title": "Semi-supervised learning for an improved diagnosis of COVID-19 in CT images.", "abstract": "Coronavirus disease 2019 (COVID-19) has been spread out all over the world. Although a real-time reverse-transcription polymerase chain reaction (RT-PCR) test has been used as a primary diagnostic tool for COVID-19, the utility of CT based diagnostic tools have been suggested to improve the diagnostic accuracy and reliability. Herein we propose a semi-supervised deep neural network for an improved detection of COVID-19. The proposed method utilizes CT images in a supervised and unsupervised manner to improve the accuracy and robustness of COVID-19 diagnosis. Both labeled and unlabeled CT images are employed. Labeled CT images are used for supervised leaning. Unlabeled CT images are utilized for unsupervised learning in a way that the feature representations are invariant to perturbations in CT images. To systematically evaluate the proposed method, two COVID-19 CT datasets and three public CT datasets with no COVID-19 CT images are employed. In distinguishing COVID-19 from non-COVID-19 CT images, the proposed method achieves an overall accuracy of 99.83%, sensitivity of 0.9286, specificity of 0.9832, and positive predictive value (PPV) of 0.9192. The results are consistent between the COVID-19 challenge dataset and the public CT datasets. For discriminating between COVID-19 and common pneumonia CT images, the proposed method obtains 97.32% accuracy, 0.9971 sensitivity, 0.9598 specificity, and 0.9326 PPV. Moreover, the comparative experiments with respect to supervised learning and training strategies demonstrate that the proposed method is able to improve the diagnostic accuracy and robustness without exhaustive labeling. The proposed semi-supervised method, exploiting both supervised and unsupervised learning, facilitates an accurate and reliable diagnosis for COVID-19, leading to an improved patient care and management.", "journal": "PloS one", "date": "2021-04-02", "authors": ["Chang HeeHan", "MisukKim", "Jin TaeKwak"], "doi": "10.1371/journal.pone.0249450\n10.1056/NEJMoa2001017\n10.1038/s41591-020-0820-9\n10.1002/jmv.25689\n10.1016/S0140-6736(20)30185-9\n10.3346/jkms.2020.35.e223\n10.1148/radiol.2020200642\n10.1148/radiol.2020200274\n10.2214/AJR.20.22954\n10.1148/radiol.2020200463\n10.1148/radiol.2020200330\n10.1148/radiol.2020200343\n10.1146/annurev-bioeng-071516-044442\n10.1109/TMI.2016.2528162\n10.1016/S0140-6736(18)31645-3\n10.1001/jama.2017.18152\n10.1001/jama.2017.14585\n10.1016/j.media.2019.101563\n10.1038/nature14539\n10.1109/RBME.2020.2987975\n10.2196/19569\n10.1016/j.eng.2020.04.010\n10.1183/13993003.00775-2020\n10.1109/TMI.2020.2994908\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2996256\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2996645\n10.1148/ryct.2020200075\n10.1007/s10994-019-05855-6\n10.1109/TMI.2018.2876510\n10.1016/j.chaos.2020.110153"}
{"title": "Automated Detection of COVID-19 Cases on Radiographs using Shape-Dependent Fibonacci-p Patterns.", "abstract": "The coronavirus (COVID-19) pandemic has been adversely affecting people's health globally. To diminish the effect of this widespread pandemic, it is essential to detect COVID-19 cases as quickly as possible. Chest radiographs are less expensive and are a widely available imaging modality for detecting chest pathology compared with CT images. They play a vital role in early prediction and developing treatment plans for suspected or confirmed COVID-19 chest infection patients. In this paper, a novel shape-dependent Fibonacci-p patterns-based feature descriptor using a machine learning approach is proposed. Computer simulations show that the presented system (1) increases the effectiveness of differentiating COVID-19, viral pneumonia, and normal conditions, (2) is effective on small datasets, and (3) has faster inference time compared to deep learning methods with comparable performance. Computer simulations are performed on two publicly available datasets; (a) the Kaggle dataset, and (b) the COVIDGR dataset. To assess the performance of the presented system, various evaluation parameters, such as accuracy, recall, specificity, precision, and f1-score are used. Nearly 100% differentiation between normal and COVID-19 radiographs is observed for the three-class classification scheme using the lung area-specific Kaggle radiographs. While Recall of 72.65\u00a0\u00b1\u00a06.83 and specificity of 77.72\u00a0\u00b1\u00a08.06 is observed for the COVIDGR dataset.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-04-01", "authors": ["KarenPanetta", "ForamSanghavi", "SosAgaian", "NeelMadan"], "doi": "10.1109/JBHI.2021.3069798\n10.1016/j.compbiomed.2020.103792"}
{"title": "Machine learning is the key to diagnose COVID-19: a proof-of-concept study.", "abstract": "The reverse transcription-polymerase chain reaction (RT-PCR) assay is the accepted standard for coronavirus disease 2019 (COVID-19) diagnosis. As any test, RT-PCR provides false negative results that can be rectified by clinicians by confronting clinical, biological and imaging data. The combination of RT-PCR and chest-CT could improve diagnosis performance, but this would requires considerable resources for its rapid use in all patients with suspected COVID-19. The potential contribution of machine learning in this situation has not been fully evaluated. The objective of this study was to develop and evaluate machine learning models using routine clinical and laboratory data to improve the performance of RT-PCR and chest-CT for COVID-19 diagnosis among post-emergency hospitalized patients. All adults admitted to the ED for suspected COVID-19, and then hospitalized at Rennes academic hospital, France, between March 20, 2020 and May 5, 2020 were included in the study. Three model types were created: logistic regression, random forest, and neural network. Each model was trained to diagnose COVID-19 using different sets of variables. Area under the receiving operator characteristics curve (AUC) was the primary outcome to evaluate model's performances. 536 patients were included in the study: 106 in the COVID group, 430 in the NOT-COVID group. The AUC values of chest-CT and RT-PCR increased from 0.778 to 0.892 and from 0.852 to 0.930, respectively, with the contribution of machine learning. After generalization, machine learning models will allow increasing chest-CT and RT-PCR performances for COVID-19 diagnosis.", "journal": "Scientific reports", "date": "2021-04-01", "authors": ["CedricGangloff", "SoniaRafi", "GuillaumeBouzill\u00e9", "LouisSoulat", "MarcCuggia"], "doi": "10.1038/s41598-021-86735-9\n10.1001/jama.2020.12839\n10.1056/NEJMoa2001316\n10.3346/jkms.2020.35.e132\n10.34172/aim.2020.10\n10.1016/j.jhin.2020.03.036\n10.1038/s41467-020-16670-2\n10.1016/j.jiph.2020.05.019\n10.1186/s13017-020-00304-5\n10.1016/j.chest.2020.03.063\n10.1093/cid/ciaa760\n10.1002/jmv.25786\n10.1148/radiol.2020201365\n10.3390/diagnostics10070464\n10.1080/22221751.2020.1775132\n10.1016/S2213-2600(17)30292-8\n10.1148/radiol.2020201237\n10.1148/radiol.2020200230\n10.1016/S0304-3800(02)00257-0\n10.1186/1471-2105-12-77\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1007/s11606-020-05762-w\n10.1056/NEJMc2010419\n10.1001/jama.2020.1585\n10.1007/s00405-020-05965-1\n10.1111/ene.14273\n10.1001/jama.2016.0289\n10.1136/fmch-2020-000406\n10.1111/all.14364\n10.1016/j.cca.2020.03.022\n10.1002/14651858.CD010864.pub2\n10.1055/s-0040-1714350\n10.1056/NEJMc2009787\n10.1111/jth.14859\n10.1056/NEJMc2007575\n10.5694/mja11.11159\n10.1186/s13054-020-03019-w\n10.1016/j.ejrad.2020.108961\n10.1016/j.cca.2020.03.009\n10.1007/s00330-020-06827-4\n10.1007/s10916-020-01582-x\n10.1002/hpm.2987\n10.1016/j.ejrad.2020.109041\n10.1109/TMI.2020.2992546\n10.1371/journal.pone.0242759\n10.1038/s41598-020-76550-z\n10.1016/j.dsx.2020.05.008\n10.1371/journal.pone.0239474\n10.2196/24478\n10.1186/s13049-020-00808-8\n10.2196/24048\n10.1007/s10140-020-01821-1"}
{"title": "Federated deep learning for detecting COVID-19 lung abnormalities in CT: a privacy-preserving multinational validation study.", "abstract": "Data privacy mechanisms are essential for rapidly scaling medical training databases to capture the heterogeneity of patient data distributions toward robust and generalizable machine learning systems. In the current COVID-19 pandemic, a major focus of artificial intelligence (AI) is interpreting chest CT, which can be readily used in the assessment and management of the disease. This paper demonstrates the feasibility of a federated learning method for detecting COVID-19 related CT abnormalities with external validation on patients from a multinational study. We recruited 132 patients from seven multinational different centers, with three internal hospitals from Hong Kong for training and testing, and four external, independent datasets from Mainland China and Germany, for validating model generalizability. We also conducted case studies on longitudinal scans for automated estimation of lesion burden for hospitalized COVID-19 patients. We explore the federated learning algorithms to develop a privacy-preserving AI model for COVID-19 medical image diagnosis with good generalization capability on unseen multinational datasets. Federated learning could provide an effective mechanism during pandemics to rapidly develop clinically useful AI across institutions and countries overcoming the burden of central aggregation of large amounts of sensitive data.", "journal": "NPJ digital medicine", "date": "2021-03-31", "authors": ["QiDou", "Tiffany YSo", "MeiruiJiang", "QuandeLiu", "VarutVardhanabhuti", "GeorgiosKaissis", "ZejuLi", "WeixinSi", "Heather H CLee", "KevinYu", "ZuxinFeng", "LiDong", "EgonBurian", "FriederikeJungmann", "RickmerBraren", "MarcusMakowski", "BernhardKainz", "DanielRueckert", "BenGlocker", "Simon C HYu", "Pheng AnnHeng"], "doi": "10.1038/s41746-021-00431-6\n10.1038/s41591-020-0824-5\n10.1038/s42256-020-0181-6\n10.1038/s42256-020-0184-3\n10.1038/s41591-018-0107-6\n10.1038/s42256-020-0186-1\n10.1038/s41746-020-00323-1\n10.1038/s41746-019-0148-3\n10.1038/s41591-018-0316-z\n10.1038/s41598-020-69250-1\n10.1038/s42256-020-0180-7\n10.1038/s42256-020-0185-2\n10.3390/jcm9051514\n10.1148/radiol.2020200463\n10.1016/j.cell.2020.04.045\n10.1109/RBME.2020.2987975\n10.1117/1.JMI.5.3.036501\n10.1148/radiology.143.1.7063747\n10.2307/2531595\n10.1093/biomet/26.4.404\n10.1109/TMI.2020.2974574\n10.1371/journal.pmed.1002683\n10.1148/rg.2017170077\n10.1186/s41747-020-00173-2"}
{"title": "Deep learning for diagnosis of COVID-19 using 3D CT scans.", "abstract": "A new pneumonia-type coronavirus, COVID-19, recently emerged in Wuhan, China. COVID-19 has subsequently infected many people and caused many deaths worldwide. Isolating infected people is one of the methods of preventing the spread of this virus. CT scans provide detailed imaging of the lungs and assist radiologists in diagnosing COVID-19 in hospitals. However, a person's CT scan contains hundreds of slides, and the diagnosis of COVID-19 using such scans can lead to delays in hospitals. Artificial intelligence techniques could assist radiologists with rapidly and accurately detecting COVID-19 infection from these scans. This paper proposes an artificial intelligence (AI) approach to classify COVID-19 and normal CT volumes. The proposed AI method uses the ResNet-50 deep learning model to predict COVID-19 on each CT image of a 3D CT scan. Then, this AI method fuses image-level predictions to diagnose COVID-19 on a 3D CT volume. We show that the proposed deep learning model provides 96% AUC value for detecting COVID-19 on CT scans.", "journal": "Computers in biology and medicine", "date": "2021-03-30", "authors": ["SertanSerte", "HasanDemirel"], "doi": "10.1016/j.compbiomed.2021.104306"}
{"title": "A Cascade-SEME network for COVID-19 detection in chest x-ray images.", "abstract": "The worldwide spread of the SARS-CoV-2 virus poses unprecedented challenges to medical resources and infection prevention and control measures around the world. In this case, a rapid and effective detection method for COVID-19 can not only relieve the pressure of the medical system but find and isolate patients in time, to a certain extent, slow down the development of the epidemic. In this paper, we propose a method that can quickly and accurately diagnose whether pneumonia is viral pneumonia, and classify viral pneumonia in a fine-grained way to diagnose COVID-19.\nWe proposed a Cascade Squeeze-Excitation and Moment Exchange (Cascade-SEME) framework that can effectively detect COVID-19 cases by evaluating the chest x-ray images, where SE is the structure we designed in the network which has attention mechanism, and ME is a method for image enhancement from feature dimension. The framework integrates a model for a coarse level detection of virus cases among other forms of lung infection, and a model for fine-grained categorisation of pneumonia types identifying COVID-19 cases. In addition, a Regional Learning approach is proposed to mitigate the impact of non-lesion features on network training. The network output is also visualised, highlighting the likely areas of lesion, to assist experts' assessment and diagnosis of COVID-19.\nThree datasets were used: a set of Chest x-ray Images for Classification with bacterial pneumonia, viral pneumonia and normal chest x-rays, a COVID chest x-ray dataset with COVID-19, and a Lung Segmentation dataset containing 1000 chest x-rays with masks in the lung region. We evaluated all the models on the test set. The results shows the proposed SEME structure significantly improves the performance of the models: in the task of pneumonia infection type diagnosis, the sensitivity, specificity, accuracy and F1 score of ResNet50 with SEME structure are significantly improved in each category, and the accuracy and AUC of the whole test set are also enhanced; in the detection task of COVID-19, the evaluation results shows that when SEME structure was added to the task, the sensitivities, specificities, accuracy and F1 scores of ResNet50 and DenseNet169 are improved. Although the sensitivities and specificities are not significantly promoted, SEME well balanced these two significant indicators. Regional learning also plays an important role. Experiments show that Regional Learning can effectively correct the impact of non-lesion features on the network, which can be seen in the Grad-CAM method.\nExperiments show that after the application of SEME structure in the network, the performance of SEME-ResNet50 and SEME-DenseNet169 in both two datasets show a clear enhancement. And the proposed regional learning method effectively directs the network's attention to focus on relevant pathological regions in the lung radiograph, ensuring the performance of the proposed framework even when a small training set is used. The visual interpretation step using Grad-CAM finds that the region of attention on radiographs of different types of pneumonia are located in different regions of the lungs.", "journal": "Medical physics", "date": "2021-03-30", "authors": ["DailinLv", "YaqiWang", "ShuaiWang", "QianniZhang", "WutengQi", "YunxiangLi", "LinglingSun"], "doi": "10.1002/mp.14711\n10.1007/s11263-019-01228-7\n10.1109/Confluence47617.2020.9057809"}
{"title": "Quantitative Burden of COVID-19 Pneumonia on Chest CT Predicts Adverse Outcomes: A Post-Hoc Analysis of a Prospective International Registry.", "abstract": "To examine the independent and incremental value of CT-derived quantitative burden and attenuation of COVID-19 pneumonia for the prediction of clinical deterioration or death.\nThis was a retrospective analysis of a prospective international registry of consecutive patients with laboratory-confirmed COVID-19 and chest CT imaging, admitted to four centers between January 10 and May 6, 2020. Total burden (expressed as a percentage) and mean attenuation of ground glass opacities (GGO) and consolidation were quantified from CT using semi-automated research software. The primary outcome was clinical deterioration (intensive care unit admission, invasive mechanical ventilation, or vasopressor therapy) or in-hospital death. Logistic regression was performed to assess the predictive value of clinical and CT parameters for the primary outcome.\nThe final population comprised 120 patients (mean age 64 \u00b1 16 years, 78 men), of whom 39 (32.5%) experienced clinical deterioration or death. In multivariable regression of clinical and CT parameters, consolidation burden (odds ratio [OR], 3.4; 95% confidence interval [CI]: 1.7, 6.9 per doubling; \nQuantitative burden of consolidation or GGO on chest CT independently predict clinical deterioration or death in patients with COVID-19 pneumonia. CT-derived measures have incremental prognostic value over and above clinical parameters, and may be useful for risk stratifying patients with COVID-19.", "journal": "Radiology. Cardiothoracic imaging", "date": "2021-03-30", "authors": ["KajetanGrodecki", "AndrewLin", "SebastienCadet", "Priscilla AMcElhinney", "AryabodRazipour", "CatoChan", "BarryPressman", "PeterJulien", "PalMaurovich-Horvat", "NicolaGaibazzi", "UditThakur", "ElisabettaMancini", "CeciliaAgalbato", "RobertoMen\u00e8", "GianfrancoParati", "FrancoCernigliaro", "NiteshNerlekar", "CamillaTorlasco", "GianlucaPontone", "Piotr JSlomka", "DaminiDey"], "doi": "10.1148/ryct.2020200389"}
{"title": "Regarding \"Serial Quantitative Chest CT Assessment of COVID-19: Deep-Learning Approach\".", "abstract": null, "journal": "Radiology. Cardiothoracic imaging", "date": "2021-03-30", "authors": ["Marcelo StrausTakahashi", "MatheusRibeiro Furtado de Mendon\u00e7a", "IanPan", "Rogerio ZaiaPinetti", "Felipe CKitamura"], "doi": "10.1148/ryct.2020200242"}
{"title": "Longitudinal Assessment of COVID-19 Using a Deep Learning-based Quantitative CT Pipeline: Illustration of Two Cases.", "abstract": null, "journal": "Radiology. Cardiothoracic imaging", "date": "2021-03-30", "authors": ["YukunCao", "ZhanweiXu", "JianjiangFeng", "ChengJin", "XiaoyuHan", "HanpingWu", "HeshuiShi"], "doi": "10.1148/ryct.2020200082"}
{"title": "Serial Quantitative Chest CT Assessment of COVID-19: A Deep Learning Approach.", "abstract": "To quantitatively evaluate lung burden changes in patients with coronavirus disease 2019 (COVID-19) by using serial CT scan by an automated deep learning method.\nPatients with COVID-19, who underwent chest CT between January 1 and February 3, 2020, were retrospectively evaluated. The patients were divided into mild, moderate, severe, and critical types, according to their baseline clinical, laboratory, and CT findings. CT lung opacification percentages of the whole lung and five lobes were automatically quantified by a commercial deep learning software and compared with those at follow-up CT scans. Longitudinal changes of the CT quantitative parameter were also compared among the four clinical types.\nA total of 126 patients with COVID-19 (mean age, 52 years \u00b1 15 [standard deviation]; 53.2% males) were evaluated, including six mild, 94 moderate, 20 severe, and six critical cases. CT-derived opacification percentage was significantly different among clinical groups at baseline, gradually progressing from mild to critical type (all \nThe quantification of lung opacification in COVID-19 measured at chest CT by using a commercially available deep learning-based tool was significantly different among groups with different clinical severity. This approach could potentially eliminate the subjectivity in the initial assessment and follow-up of pulmonary findings in COVID-19.", "journal": "Radiology. Cardiothoracic imaging", "date": "2021-03-30", "authors": ["LuHuang", "RuiHan", "TaoAi", "PengxinYu", "HanKang", "QianTao", "LimingXia"], "doi": "10.1148/ryct.2020200075\n10.1016/S2213-2600(20)30076-x"}
{"title": "Imaging Profile of the COVID-19 Infection: Radiologic Findings and Literature Review.", "abstract": "To present the findings of 21 coronavirus disease 2019 (COVID-19) cases from two Chinese centers with CT and chest radiographic findings, as well as follow-up imaging in five cases.\nThis was a retrospective study in Shenzhen and Hong Kong. Patients with COVID-19 infection were included. A systematic review of the published literature on radiologic features of COVID-19 infection was conducted.\nThe predominant imaging pattern was of ground-glass opacification with occasional consolidation in the peripheries. Pleural effusions and lymphadenopathy were absent in all cases. Patients demonstrated evolution of the ground-glass opacities into consolidation and subsequent resolution of the airspace changes. Ground-glass and consolidative opacities visible on CT are sometimes undetectable on chest radiography, suggesting that CT is a more sensitive imaging modality for investigation. The systematic review identified four other studies confirming the findings of bilateral and peripheral ground glass with or without consolidation as the predominant finding at CT chest examinations.\nPulmonary manifestation of COVID-19 infection is predominantly characterized by ground-glass opacification with occasional consolidation on CT. Radiographic findings in patients presenting in Shenzhen and Hong Kong are in keeping with four previous publications from other sites.\u00a9 RSNA, 2020See editorial by Kay and Abbara in this issue.", "journal": "Radiology. Cardiothoracic imaging", "date": "2021-03-30", "authors": ["Ming-YenNg", "Elaine Y PLee", "JinYang", "FangfangYang", "XiaLi", "HongxiaWang", "Macy Mei-SzeLui", "Christine Shing-YenLo", "BarryLeung", "Pek-LanKhong", "Christopher Kim-MingHui", "Kwok-YungYuen", "Michael DKuo"], "doi": "10.1148/ryct.2020200034"}
{"title": "PSSPNN: PatchShuffle Stochastic Pooling Neural Network for an Explainable Diagnosis of COVID-19 with Multiple-Way Data Augmentation.", "abstract": "COVID-19 has caused large death tolls all over the world. Accurate diagnosis is of significant importance for early treatment.\nIn this study, we proposed a novel PSSPNN model for classification between COVID-19, secondary pulmonary tuberculosis, community-captured pneumonia, and healthy subjects. PSSPNN entails five improvements: we first proposed the n-conv stochastic pooling module. Second, a novel stochastic pooling neural network was proposed. Third, PatchShuffle was introduced as a regularization term. Fourth, an improved multiple-way data augmentation was used. Fifth, Grad-CAM was utilized to interpret our AI model.\nThe 10 runs with random seed on the test set showed our algorithm achieved a microaveraged F1 score of 95.79%. Moreover, our method is better than nine state-of-the-art approaches.\nThis proposed PSSPNN will help assist radiologists to make diagnosis more quickly and accurately on COVID-19 cases.", "journal": "Computational and mathematical methods in medicine", "date": "2021-03-30", "authors": ["Shui-HuaWang", "YinZhang", "XiaochunCheng", "XinZhang", "Yu-DongZhang"], "doi": "10.1155/2021/6633755\n10.1097/ICO.0000000000002267\n10.3201/eid2608.201843\n10.1148/radiol.2020200642\n10.1007/s10489-020-01902-1\n10.1053/j.jvca.2020.04.062\n10.1007/s10278-019-00214-2\n10.3233/FI-2017-1492\n10.1166/jmihi.2019.2804\n10.3390/brainsci9090212\n10.1109/ICASID.2019.8925267\n10.7759/cureus.9448\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2995965\n10.1109/JSEN.2020.3025855\n10.1016/j.inffus.2020.11.005\n10.1007/s12652-020-02612-9\n10.1007/s11042-019-08345-y\n10.1016/j.scienta.2019.109133\n10.1016/j.cjph.2018.06.010\n10.1016/j.neuroimage.2020.117328\n10.1364/OSAC.387102\n10.1007/s10489-020-01901-2\n10.1109/ACCESS.2018.2848903\n10.1186/s40537-019-0197-0\n10.1016/j.inffus.2020.10.004\n10.23919/CCC50068.2020.9188681\n10.1007/s11263-019-01228-7\n10.3390/app10062050\n10.1109/TITS.2019.2940547\n10.1109/MNET.011.2000458\n10.1097/PAP.0000000000000264\n10.1109/JSYST.2015.2470644\n10.1109/JIOT.2017.2772959\n10.1007/s40747-020-00161-4\n10.1007/s11036-019-01358-9\n10.1007/s00521-020-05021-3\n10.1109/TFUZZ.2020.3006520\n10.1016/j.ymssp.2019.106537\n10.1109/TGRS.2019.2924221\n10.1109/TGRS.2018.2817507\n10.1109/TCI.2016.2612945\n10.1109/TII.2020.3007174\n10.1109/TITS.2020.2987724\n10.1109/TCBB.2020.2973978"}
{"title": "Exercise and Use of Enhancement Drugs at the Time of the COVID-19 Pandemic: A Multicultural Study on Coping Strategies During Self-Isolation and Related Risks.", "abstract": "", "journal": "Frontiers in psychiatry", "date": "2021-03-30", "authors": ["Artemisa RDores", "Irene PCarvalho", "JuliusBurkauskas", "PierluigiSimonato", "IlariaDe Luca", "RoisinMooney", "KonstantinosIoannidis", "M \u00c1ngelesG\u00f3mez-Mart\u00ednez", "ZsoltDemetrovics", "Krisztina Edina\u00c1bel", "AttilaSzabo", "HironobuFujiwara", "MamiShibata", "Alejandra Rebeca MeleroVentola", "Eva MariaArroyo-Anll\u00f3", "Ricardo MSantos-Labrador", "IngaGriskova-Bulanova", "AistePranckeviciene", "KeiKobayashi", "GiovanniMartinotti", "Naomi AFineberg", "FernandoBarbosa", "OrnellaCorazza"], "doi": "10.3389/fpsyt.2021.648501\n10.1016/S0140-6736(20)30308-1\n10.1016/j.bbi.2020.06.008\n10.1177/0020764020915212\n10.25561/77482\n10.3389/fpubh.2020.553345\n10.1098/rsos.200742\n10.1101/2020.05.04.20091017\n10.1037/amp0000660\n10.3390/ijerph17207663\n10.23736/S0026-4725.20.05309-8\n10.1002/jclp.23082\n10.1016/S0140-6736(20)30460-8\n10.4178/epih.e2020038\n10.4178/epih.e2016048\n10.1016/j.puhe.2020.03.007\n10.1037/tra0000592\n10.1016/S2215-0366(20)30168-1\n10.1371/journal.pone.0213060\n10.3109/16066359709005257\n10.1002/hup.2619\n10.1037/ppm0000089\n10.1002/eat.22254\n10.1089/cyber.2013.0305\n10.1080/10410236.2016.1140273\n10.2196/jmir.6368\n10.1186/s12889-018-5930-7\n10.1080/09515079608256358\n10.1177/1359105306069091\n10.1002/eat.10071\n10.1016/S1469-0292(00)00015-7\n10.1016/s1471-0153(03)00022-9\n10.1556/JBA.2.2013.014\n10.1177/0163278710380124\n10.1002/hup.2299\n10.3109/10826084.2014.912232\n10.1016/j.ecl.2009.10.009\n10.1016/j.peh.2014.05.001\n10.1016/j.euroneuro.2016.04.011\n10.1016/j.jpsychires.2020.11.004\n10.5772/intechopen.76446\n10.1136/bmj.j3208\n10.14195/1647-8606_54_8\n10.1002/cpp.1974\n10.1136/bmj.j1745\n10.3389/fphys.2020.572718\n10.1007/s11469-020-00433-7\n10.1080/16066350310001637363\n10.1136/bjsm.2004.017020\n10.1080/14659890500114359\n10.1186/s40798-014-0005-5\n10.1017/s1352465813000556\n10.1080/15298860309027\n10.3390/ijerph8104069\n10.3305/nh.2015.31.6.8934\n10.4321/S1578-84232015000200003\n10.7334/psicothema2013.21\n10.1006/appe.2001.0424\n10.1007/s40519-018-0544-8\n10.3389/fpsyt.2020.00790\n10.3389/fpsyt.2020.575755"}
{"title": "Deep-chest: Multi-classification deep learning model for diagnosing COVID-19, pneumonia, and lung cancer chest diseases.", "abstract": "Corona Virus Disease (COVID-19) has been announced as a pandemic and is spreading rapidly throughout the world. Early detection of COVID-19 may protect many infected people. Unfortunately, COVID-19 can be mistakenly diagnosed as pneumonia or lung cancer, which with fast spread in the chest cells, can lead to patient death. The most commonly used diagnosis methods for these three diseases are chest X-ray and computed tomography (CT) images. In this paper, a multi-classification deep learning model for diagnosing COVID-19, pneumonia, and lung cancer from a combination of chest x-ray and CT images is proposed. This combination has been used because chest X-ray is less powerful in the early stages of the disease, while a CT scan of the chest is useful even before symptoms appear, and CT can precisely detect the abnormal features that are identified in images. In addition, using these two types of images will increase the dataset size, which will increase the classification accuracy. To the best of our knowledge, no other deep learning model choosing between these diseases is found in the literature. In the present work, the performance of four architectures are considered, namely: VGG19-CNN, ResNet152V2, ResNet152V2\u00a0+\u00a0Gated Recurrent Unit (GRU), and ResNet152V2\u00a0+\u00a0Bidirectional GRU (Bi-GRU). A comprehensive evaluation of different deep learning architectures is provided using public digital chest x-ray and CT datasets with four classes (i.e., Normal, COVID-19, Pneumonia, and Lung cancer). From the results of the experiments, it was found that the VGG19 +CNN model outperforms the three other proposed models. The VGG19+CNN model achieved 98.05% accuracy (ACC), 98.05% recall, 98.43% precision, 99.5% specificity (SPC), 99.3% negative predictive value (NPV), 98.24% F1 score, 97.7% Matthew's correlation coefficient (MCC), and 99.66% area under the curve (AUC) based on X-ray and CT images.", "journal": "Computers in biology and medicine", "date": "2021-03-29", "authors": ["Dina MIbrahim", "Nada MElshennawy", "Amany MSarhan"], "doi": "10.1016/j.compbiomed.2021.104348"}
{"title": "Prognostication of patients with COVID-19 using artificial intelligence based on chest x-rays and clinical data: a retrospective study.", "abstract": "Chest x-ray is a relatively accessible, inexpensive, fast imaging modality that might be valuable in the prognostication of patients with COVID-19. We aimed to develop and evaluate an artificial intelligence system using chest x-rays and clinical data to predict disease severity and progression in patients with COVID-19.\nWe did a retrospective study in multiple hospitals in the University of Pennsylvania Health System in Philadelphia, PA, USA, and Brown University affiliated hospitals in Providence, RI, USA. Patients who presented to a hospital in the University of Pennsylvania Health System via the emergency department, with a diagnosis of COVID-19 confirmed by RT-PCR and with an available chest x-ray from their initial presentation or admission, were retrospectively identified and randomly divided into training, validation, and test sets (7:1:2). Using the chest x-rays as input to an EfficientNet deep neural network and clinical data, models were trained to predict the binary outcome of disease severity (ie, critical or non-critical). The deep-learning features extracted from the model and clinical data were used to build time-to-event models to predict the risk of disease progression. The models were externally tested on patients who presented to an independent multicentre institution, Brown University affiliated hospitals, and compared with severity scores provided by radiologists.\n1834 patients who presented via the University of Pennsylvania Health System between March 9 and July 20, 2020, were identified and assigned to the model training (n=1285), validation (n=183), or testing (n=366) sets. 475 patients who presented via the Brown University affiliated hospitals between March 1 and July 18, 2020, were identified for external testing of the models. When chest x-rays were added to clinical data for severity prediction, area under the receiver operating characteristic curve (ROC-AUC) increased from 0\u00b7821 (95% CI 0\u00b7796-0\u00b7828) to 0\u00b7846 (0\u00b7815-0\u00b7852; p<0\u00b70001) on internal testing and 0\u00b7731 (0\u00b7712-0\u00b7738) to 0\u00b7792 (0\u00b7780-0 \u00b7803; p<0\u00b70001) on external testing. When deep-learning features were added to clinical data for progression prediction, the concordance index (C-index) increased from 0\u00b7769 (0\u00b7755-0\u00b7786) to 0\u00b7805 (0\u00b7800-0\u00b7820; p<0\u00b70001) on internal testing and 0\u00b7707 (0\u00b7695-0\u00b7729) to 0\u00b7752 (0\u00b7739-0\u00b7764; p<0\u00b70001) on external testing. The image and clinical data combined model had significantly better prognostic performance than combined severity scores and clinical data on internal testing (C-index 0\u00b7805 vs 0\u00b7781; p=0\u00b70002) and external testing (C-index 0\u00b7752 vs 0\u00b7715; p<0\u00b70001).\nIn patients with COVID-19, artificial intelligence based on chest x-rays had better prognostic performance than clinical data or radiologist-derived severity scores. Using artificial intelligence, chest x-rays can augment clinical data in predicting the risk of progression to critical illness in patients with COVID-19.\nBrown University, Amazon Web Services Diagnostic Development Initiative, Radiological Society of North America, National Cancer Institute and National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health.", "journal": "The Lancet. Digital health", "date": "2021-03-29", "authors": ["ZhichengJiao", "Ji WhaeChoi", "KaseyHalsey", "Thi My LinhTran", "BenHsieh", "DongcuiWang", "FeyisopeEweje", "RobinWang", "KenChang", "JingWu", "Scott ACollins", "Thomas YYi", "Andrew TDelworth", "TaoLiu", "Terrance THealey", "ShaoleiLu", "JianxinWang", "XueFeng", "Michael KAtalay", "LiYang", "MichaelFeldman", "Paul J LZhang", "Wei-HuaLiao", "YongFan", "Harrison XBai"], "doi": "10.1016/S2589-7500(21)00039-X\n10.1101/2020.05.20.20108159\n10.1101/2020.05.09.20096560"}
{"title": "COVID-19 in CXR: From Detection and Severity Scoring to Patient Disease Monitoring.", "abstract": "This work estimates the severity of pneumonia in COVID-19 patients and reports the findings of a longitudinal study of disease progression. It presents a deep learning model for simultaneous detection and localization of pneumonia in chest Xray (CXR) images, which is shown to generalize to COVID-19 pneumonia. The localization maps are utilized to calculate a \"Pneumonia Ratio\" which indicates disease severity. The assessment of disease severity serves to build a temporal disease extent profile for hospitalized patients. To validate the model's applicability to the patient monitoring task, we developed a validation strategy which involves a synthesis of Digital Reconstructed Radiographs (DRRs - synthetic Xray) from serial CT scans; we then compared the disease progression profiles that were generated from the DRRs to those that were generated from CT volumes.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-03-27", "authors": ["MaayanFrid-Adar", "RulaAmer", "OphirGozes", "JannetteNassar", "HayitGreenspan"], "doi": "10.1109/JBHI.2021.3069169\n10.1109/RBME.2020.2987975\n10.1109/ACCESS.2020.3003810\n10.3390/electronics9091439\n10.1007/s10489-020-01867-1"}
{"title": "Generalized chest CT and lab curves throughout the course of COVID-19.", "abstract": "A better understanding of temporal relationships between chest CT and labs may provide a reference for disease severity over the disease course. Generalized curves of lung opacity volume and density over time can be used as standardized references from well before symptoms develop to over a month after recovery, when residual lung opacities remain. 739 patients with COVID-19 underwent CT and RT-PCR in an outbreak setting between January 21st and April 12th, 2020. 29 of 739 patients had serial exams (121 CTs and 279 laboratory measurements) over 50\u2009\u00b1\u200916\u00a0days, with an average of 4.2 sequential CTs each. Sequential volumes of total lung, overall opacity and opacity subtypes (ground glass opacity [GGO] and consolidation) were extracted using deep learning and manual segmentation. Generalized temporal curves of CT and laboratory measurements were correlated. Lung opacities appeared 3.4\u2009\u00b1\u20092.2\u00a0days prior to symptom onset. Opacity peaked 1\u00a0day after symptom onset. GGO onset was earlier and resolved later than consolidation. Lactate dehydrogenase, and C-reactive protein peaked earlier than procalcitonin and leukopenia. The temporal relationships of quantitative CT features and clinical labs have distinctive patterns and peaks in relation to symptom onset, which may inform early clinical course in patients with mild COVID-19 pneumonia, or may shed light upon chronic lung effects or mechanisms of medical countermeasures in clinical trials.", "journal": "Scientific reports", "date": "2021-03-27", "authors": ["Michael TKassin", "NicoleVarble", "MaximeBlain", "ShengXu", "Evrim BTurkbey", "StephanieHarmon", "DongYang", "ZiyueXu", "HolgerRoth", "DaguangXu", "MonaFlores", "AmelAmalou", "KaiyunSun", "SameerKadri", "FrancescaPatella", "MaurizioCariati", "AliceScarabelli", "ElviraStellato", "Anna MariaIerardi", "GianpaoloCarrafiello", "PengAn", "BarisTurkbey", "Bradford JWood"], "doi": "10.1038/s41598-021-85694-5\n10.1148/radiol.2020201365\n10.1016/S0140-6736(20)30728-5\n10.1148/radiol.2020203173\n10.7326/M20-1495\n10.1148/radiol.2020201433\n10.7150/thno.46465\n10.1007/s00330-020-06817-6\n10.1097/RLI.0000000000000689\n10.1007/s42058-020-00034-2\n10.1148/radiol.2020200843\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1016/j.ejrad.2020.109009\n10.1148/radiol.2020200230\n10.1001/jama.2020.8259\n10.1007/s00330-020-07013-2\n10.1016/j.ijid.2020.05.006\n10.7150/thno.45985\n10.3348/kjr.2020.0293\n10.1016/j.ejrad.2020.109202\n10.1038/s41598-020-68057-4\n10.1007/s00330-020-06976-6\n10.1038/s41591-020-0869-5\n10.2214/ajr.175.5.1751329\n10.1371/journal.pone.0205490\n10.1016/j.ejcts.2004.02.004\n10.1148/radiol.2016142975\n10.1007/s00330-016-4317-3"}
{"title": "Metaheuristic-based Deep COVID-19 Screening Model from Chest X-Ray Images.", "abstract": "COVID-19 has affected the whole world drastically. A huge number of people have lost their lives due to this pandemic. Early detection of COVID-19 infection is helpful for treatment and quarantine. Therefore, many researchers have designed a deep learning model for the early diagnosis of COVID-19-infected patients. However, deep learning models suffer from overfitting and hyperparameter-tuning issues. To overcome these issues, in this paper, a metaheuristic-based deep COVID-19 screening model is proposed for X-ray images. The modified AlexNet architecture is used for feature extraction and classification of the input images. Strength Pareto evolutionary algorithm-II (SPEA-II) is used to tune the hyperparameters of modified AlexNet. The proposed model is tested on a four-class (i.e., COVID-19, tuberculosis, pneumonia, or healthy) dataset. Finally, the comparisons are drawn among the existing and the proposed models.", "journal": "Journal of healthcare engineering", "date": "2021-03-26", "authors": ["ManjitKaur", "VijayKumar", "VaishaliYadav", "DilbagSingh", "NareshKumar", "Nripendra NarayanDas"], "doi": "10.1155/2021/8829829\n10.1148/radiol.2303030853\n10.1148/radiol.2282030593\n10.1007/11677437_22\n10.1109/tmi.2020.2993291\n10.3390/sym12040651\n10.2807/1560-7917.es.2020.25.3.2000045\n10.1109/tcbb.2020.2986544\n10.1016/j.radi.2020.04.017\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.compbiomed.2020.103792\n10.1016/j.irbm.2020.07.001\n10.1016/j.chemolab.2020.104054\n10.1016/j.compbiomed.2020.103805\n10.1016/j.compbiomed.2020.103869\n10.1101/2020.06.21.20136.598\n10.1016/j.imu.2020.100360\n10.3390/app10020559\n10.1007/s13246-020-00888-x\n10.1049/trit.2019.0028\n10.1049/trit.2019.0051\n10.1049/trit.2018.1006\n10.1504/ijhm.2019.098951\n10.1504/ijhm.2019.102893\n10.1504/ijhm.2019.098949\n10.1016/j.compeleceng.2019.03.004\n10.1109/4235.797969\n10.1016/j.camwa.2012.01.063\n10.1007/s12652-020-02669-6"}
{"title": "The clinical classification of patients with COVID-19 pneumonia was predicted by Radiomics using chest CT.", "abstract": "In 2020, the new type of coronal pneumonitis became a pandemic in the world, and has firstly been reported in Wuhan, China. Chest CT is a vital component in the diagnostic algorithm for patients with suspected or confirmed COVID-19 infection. Therefore, it is necessary to conduct automatic and accurate detection of COVID-19 by chest CT.The clinical classification of patients with COVID-19 pneumonia was predicted by Radiomics using chest CT.From the COVID-19 cases in our institution, 136 moderate patients and 83 severe patients were screened, and their clinical and laboratory data on admission were collected for statistical analysis. Initial CT Radiomics were modeled by automatic machine learning, and diagnostic performance was evaluated according to AUC, TPR, TNR, PPV and NPV of the subjects. At the same time, the initial CT main features of the two groups were analyzed semi-quantitatively, and the results were statistically analyzed.There was a statistical difference in age between the moderate group and the severe group. The model cohort showed TPR 96.9%, TNR 99.1%, PPV98.4%, NPV98.2%, and AUC 0.98. The test cohort showed TPR 94.4%, TNR100%, PPV100%, NPV96.2%, and AUC 0.97. There was statistical difference between the two groups with grade 1 score (P\u200a=\u200a.001), the AUC of grade 1 score, grade 2 score, grade 3 score and CT score were 0.619, 0.519, 0.478 and 0.548, respectively.Radiomics' Auto ML model was built by CT image of initial COVID -19 pneumonia, and it proved to be effectively used to predict the clinical classification of COVID-19 pneumonia. CT features have limited ability to predict the clinical typing of Covid-19 pneumonia.", "journal": "Medicine", "date": "2021-03-26", "authors": ["FeiXiong", "YeWang", "TaoYou", "Han HanLi", "Ting TingFu", "HuibinTan", "WeicaiHuang", "YuanliangJiang"], "doi": "10.1097/MD.0000000000025307"}
{"title": "The characteristics and evolution of pulmonary fibrosis in COVID-19 patients as assessed by AI-assisted chest HRCT.", "abstract": "The characteristics and evolution of pulmonary fibrosis in patients with coronavirus disease 2019 (COVID-19) have not been adequately studied. AI-assisted chest high-resolution computed tomography (HRCT) was used to investigate the proportion of COVID-19 patients with pulmonary fibrosis, the relationship between the degree of fibrosis and the clinical classification of COVID-19, the characteristics of and risk factors for pulmonary fibrosis, and the evolution of pulmonary fibrosis after discharge. The incidence of pulmonary fibrosis in patients with severe or critical COVID-19 was significantly higher than that in patients with moderate COVID-19. There were significant differences in the degree of pulmonary inflammation and the extent of the affected area among patients with mild, moderate and severe pulmonary fibrosis. The IL-6 level in the acute stage and albumin level were independent risk factors for pulmonary fibrosis. Ground-glass opacities, linear opacities, interlobular septal thickening, reticulation, honeycombing, bronchiectasis and the extent of the affected area were significantly improved 30, 60 and 90 days after discharge compared with at discharge. The more severe the clinical classification of COVID-19, the more severe the residual pulmonary fibrosis was; however, in most patients, pulmonary fibrosis was improved or even resolved within 90 days after discharge.", "journal": "PloS one", "date": "2021-03-24", "authors": ["Jia-NiZou", "LiuSun", "Bin-RuWang", "YouZou", "ShanXu", "Yong-JunDing", "Li-JunShen", "Wen-CaiHuang", "Xiao-JingJiang", "Shi-MingChen"], "doi": "10.1371/journal.pone.0248957\n10.1016/j.pharmthera.2015.04.005\n10.1016/j.chest.2019.10.032\n10.1378/chest.128.4.2247\n10.1111/j.1440-1843.2010.01720.x\n10.4103/ijri.IJRI_469_16\n10.3390/v12020244\n10.1016/S2213-2600(20)30225-3\n10.1002/path.1440\n10.1016/j.jrid.2020.04.001\n10.36416/1806-3756/e20200226\n10.21037/atm.2020.02.91\n10.1093/infdis/jiaa301\n10.1148/ryct.2020200047\n10.1378/chest.06-1401\n10.1007/s00330-017-5053-z\n10.7326/0003-4819-139-9-200311040-00005\n10.1016/S0140-6736(20)30566-3\n10.1086/432007\n10.1111/bjh.16659\n10.1097/MCP.0000000000000136\n10.1111/eci.13427\n10.1038/s41413-020-0084-5"}
{"title": "Mini-COVIDNet: Efficient Lightweight Deep Neural Network for Ultrasound Based Point-of-Care Detection of COVID-19.", "abstract": "Lung ultrasound (US) imaging has the potential to be an effective point-of-care test for detection of COVID-19, due to its ease of operation with minimal personal protection equipment along with easy disinfection. The current state-of-the-art deep learning models for detection of COVID-19 are heavy models that may not be easy to deploy in commonly utilized mobile platforms in point-of-care testing. In this work, we develop a lightweight mobile friendly efficient deep learning model for detection of COVID-19 using lung US images. Three different classes including COVID-19, pneumonia, and healthy were included in this task. The developed network, named as Mini-COVIDNet, was bench-marked with other lightweight neural network models along with state-of-the-art heavy model. It was shown that the proposed network can achieve the highest accuracy of 83.2% and requires a training time of only 24 min. The proposed Mini-COVIDNet has 4.39 times less number of parameters in the network compared to its next best performing network and requires a memory of only 51.29 MB, making the point-of-care detection of COVID-19 using lung US imaging plausible on a mobile platform. Deployment of these lightweight networks on embedded platforms shows that the proposed Mini-COVIDNet is highly versatile and provides optimal performance in terms of being accurate as well as having latency in the same order as other lightweight networks. The developed lightweight models are available at https://github.com/navchetan-awasthi/Mini-COVIDNet.", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2021-03-24", "authors": ["NavchetanAwasthi", "AveenDayal", "Linga ReddyCenkeramaddi", "Phaneendra KYalavarthy"], "doi": "10.1109/TUFFC.2021.3068190"}
{"title": "COVID_SCREENET: COVID-19 Screening in Chest Radiography Images Using Deep Transfer Stacking.", "abstract": "Infectious diseases are highly contagious due to rapid transmission and very challenging to diagnose in the early stage. Artificial Intelligence and Machine Learning now become a strategic weapon in assisting infectious disease prevention, rapid-response in diagnosis, surveillance, and management. In this paper, a bifold COVID_SCREENET architecture is introduced for providing COVID-19 screening solutions using Chest Radiography (CR) images. Transfer learning using nine pre-trained ImageNet models to extract the features of Normal, Pneumonia, and COVID-19 images is adapted in the first fold and classified using baseline Convolutional Neural Network (CNN). A Modified Stacked Ensemble Learning (MSEL) is proposed in the second fold by stacking the top five pre-trained models, and then the predictions resulted. Experimentation is carried out in two folds: In first fold, open-source samples are considered and in second fold 2216 real-time samples collected from Tamilnadu Government Hospitals, India, and the screening results for COVID data is 100% accurate in both the cases. The proposed approach is also validated and blind reviewed with the help of two radiologists at Thanjavur Medical College & Hospitals by collecting 2216 chest X-ray images between the month of April and May. Based on the reports, the measures are calculated for COVID_SCREENET and it showed 100% accuracy in performing multi-class classification.", "journal": "Information systems frontiers : a journal of research and innovation", "date": "2021-03-24", "authors": ["RElakkiya", "PandiVijayakumar", "MarimuthuKaruppiah"], "doi": "10.1007/s10796-021-10123-x\n10.1016/j.asoc.2020.106642\n10.1016/j.techfore.2020.120431\n10.1016/j.knosys.2020.106647\n10.1007/s10916-017-0861-x\n10.1016/j.jpdc.2017.08.014\n10.1093/clinchem/hvaa029\n10.1016/j.cell.2018.02.010\n10.1007/s10796-020-10028-1\n10.1109/TKDE.2009.191\n10.1007/s10796-020-10023-6"}
{"title": "Deep Learning in the Detection and Diagnosis of COVID-19 Using Radiology Modalities: A Systematic Review.", "abstract": "The early detection and diagnosis of COVID-19 and the accurate separation of non-COVID-19 cases at the lowest cost and in the early stages of the disease are among the main challenges in the current COVID-19 pandemic. Concerning the novelty of the disease, diagnostic methods based on radiological images suffer from shortcomings despite their many applications in diagnostic centers. Accordingly, medical and computer researchers tend to use machine-learning models to analyze radiology images. \nThis review study provides an overview of the current state of all models for the detection and diagnosis of COVID-19 through radiology modalities and their processing based on deep learning. According to the findings, deep learning-based models have an extraordinary capacity to offer an accurate and efficient system for the detection and diagnosis of COVID-19, the use of which in the processing of modalities would lead to a significant increase in sensitivity and specificity values.\nThe application of deep learning in the field of COVID-19 radiologic image processing reduces false-positive and negative errors in the detection and diagnosis of this disease and offers a unique opportunity to provide fast, cheap, and safe diagnostic services to patients.", "journal": "Journal of healthcare engineering", "date": "2021-03-23", "authors": ["MustafaGhaderzadeh", "FarkhondehAsadi"], "doi": "10.1155/2021/6677314\n10.1016/s0140-6736(20)30211-7\n10.1016/s0140-6736(20)30183-5\n10.1590/0100-3984.2020.53.2e1\n10.1002/14651858.CD013665\n10.1016/j.xinn.2020.04.001\n10.1056/nejmoa2001316\n10.21037/atm.2018.04.02\n10.1515/cclm-2020-0285\n10.1038/d41587-020-00002-2\n10.1097/moh.0000000000000322\n10.1148/radiol.2020200642\n10.1590/S1678-9946202062044\n10.1148/ryct.2020200034\n10.1101/2020.02.14.20023028\n10.1016/j.diii.2020.11.008\n10.2214/AJR.20.23034\n10.1183/13993003.00775-2020\n10.1016/j.jacr.2007.03.002\n10.36416/1806-3756/e20200226\n10.1016/j.chest.2020.04.003\n10.1016/j.crad.2018.12.015\n10.3390/s20040957\n10.1126/science.1127647\n10.1001/jamanetworkopen.2019.7416\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103795\n10.1016/j.cmpb.2020.105608\n10.3390/s20113089\n10.1080/07391102.2020.1788642\n10.1007/s00330-020-07044-9\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1007/s00259-020-04929-1\n10.1007/s00264-020-04609-7\n10.1109/access.2020.2994762\n10.1097/RTI.0000000000000532\n10.1007/s40846-020-00529-4\n10.1007/s10489-020-01714-3\n10.1007/s13246-020-00888-x\n10.3390/e22050517\n10.2196/19569\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103869\n10.1038/s41591-020-0931-3\n10.1016/j.cmpb.2020.105532\n10.1016/j.imu.2020.100360\n10.33889/ijmems.2020.5.4.052\n10.1016/j.compbiomed.2020.103805\n10.1016/j.mehy.2020.109761\n10.1007/s00330-020-06976-6\n10.1016/j.ejrad.2020.109041\n10.1016/j.cmpb.2020.105581\n10.1080/07391102.2020.1767212\n10.21037/atm.2020.03.132\n10.1371/journal.pone.0235187\n10.1007/s12559-020-09751-3\n10.9781/ijimai.2020.04.003\n10.3390/sym12040651\n10.18517/ijaseit.10.2.11446\n10.1016/j.irbm.2020.05.003"}
{"title": "Computer-Aided Diagnosis of COVID-19 CT Scans Based on Spatiotemporal Information Fusion.", "abstract": "Coronavirus disease (COVID-19) is highly contagious and pathogenic. Currently, the diagnosis of COVID-19 is based on nucleic acid testing, but it has false negatives and hysteresis. The use of lung CT scans can help screen and effectively monitor diagnosed cases. The application of computer-aided diagnosis technology can reduce the burden on doctors, which is conducive to rapid and large-scale diagnostic screening. In this paper, we proposed an automatic detection method for COVID-19 based on spatiotemporal information fusion. Using the segmentation network in the deep learning method to segment the lung area and the lesion area, the spatiotemporal information features of multiple CT scans are extracted to perform auxiliary diagnosis analysis. The performance of this method was verified on the collected dataset. We achieved the classification of COVID-19 CT scans and non-COVID-19 CT scans and analyzed the development of the patients' condition through the CT scans. The average accuracy rate is 96.7%, sensitivity is 95.2%, and F1 score is 95.9%. Each scan takes about 30 seconds for detection.", "journal": "Journal of healthcare engineering", "date": "2021-03-23", "authors": ["TianyiLi", "WeiWei", "LidanCheng", "ShengjieZhao", "ChuanjunXu", "XiaZhang", "YiZeng", "JihuaGu"], "doi": "10.1155/2021/6649591\n10.1056/nejmoa2001017\n10.1126/science.367.6475.234\n10.1016/j.jgar.2020.02.021\n10.1016/j.jaut.2020.102433\n10.1016/j.ajem.2020.03.036\n10.1148/radiol.2020200330\n10.1148/ryct.2020200034\n10.1016/j.jinf.2020.03.007\n10.1177/0846537120913033\n10.1109/tbme.2018.2845706\n10.1109/tmi.2019.2951439\n10.1109/tbme.2018.2814538\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200236\n10.1148/radiol.2020200823\n10.1148/radiol.2020200343\n10.1148/radiol.2020200463\n10.21203/rs.3.rs-96782/v1\n10.1016/j.media.2017.06.015\n10.1007/978-3-319-24574-4_28"}
{"title": "A new approach for computer-aided detection of coronavirus (COVID-19) from CT and X-ray images using machine learning methods.", "abstract": "The COVID-19 outbreak has been causing a global health crisis since December 2019. Due to this virus declared by the World Health Organization as a pandemic, the health authorities of the countries are constantly trying to reduce the spread rate of the virus by emphasizing the rules of masks, social distance, and hygiene. COVID-19 is highly contagious and spreads rapidly globally and early detection is of paramount importance. Any technological tool that can provide rapid detection of COVID-19 infection with high accuracy can be very useful to medical professionals. The disease findings on COVID-19 images, such as computed tomography (CT) and X-rays, are similar to other lung infections, making it difficult for medical professionals to distinguish COVID-19. Therefore, computer-aided diagnostic solutions are being developed to facilitate the identification of positive COVID-19 cases. The method currently used as a gold standard in detecting the virus is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Due to the high false-negative rate of this test and the delays in the test results, alternative solutions are sought. This study was conducted to investigate the contribution of machine learning and image processing to the rapid and accurate detection of COVID-19 from two of the most widely used different medical imaging modes, chest X-ray and CT images. The main purpose of this study is to support early diagnosis and treatment to end the coronavirus epidemic as soon as possible. One of the primary aims of the study is to provide support to medical professionals who are most worn out and working under intense stress during COVID-19 through smart learning methods and image classification models. The proposed approach was applied to three different public COVID-19 data sets and consists of five basic steps: data set acquisition, pre-processing, feature extraction, dimension reduction, and classification stages. Each stage has its sub-operations. The proposed model performs in considerable levels of COVID-19 detection for dataset-1 (CT), dataset-2 (X-ray) and dataset-3 (CT) with the accuracy of 89.41%, 99.02%, 98.11%, respectively. On the other hand, in the X-ray data set, an accuracy of 85.96% was obtained for COVID-19 (+), COVID-19\u00a0(-), and those with Pneumonia but not COVID-19 classes. As a result of the study, it has been shown that COVID-19 can be detected with a high success rate in about less than one minute with image processing and classical learning methods. In the light of the findings, it is possible to say that the proposed system will help radiologists in their decisions, will be useful in the early diagnosis of the virus, and can distinguish pneumonia caused by the COVID-19 virus from the pneumonia of other diseases.", "journal": "Applied soft computing", "date": "2021-03-23", "authors": ["AhmetSayg\u0131l\u0131"], "doi": "10.1016/j.asoc.2021.107323"}
{"title": "Machining learning predicts the need for escalated care and mortality in COVID-19 patients from clinical variables.", "abstract": "", "journal": "International journal of medical sciences", "date": "2021-03-23", "authors": ["WeiHou", "ZirunZhao", "AnneChen", "HaifangLi", "Tim QDuong"], "doi": "10.7150/ijms.51235"}
{"title": "Domain adaptation based self-correction model for COVID-19 infection segmentation in CT images.", "abstract": "The capability of generalization to unseen domains is crucial for deep learning models when considering real-world scenarios. However, current available medical image datasets, such as those for COVID-19 CT images, have large variations of infections and domain shift problems. To address this issue, we propose a prior knowledge driven domain adaptation and a dual-domain enhanced self-correction learning scheme. Based on the novel learning scheme, a domain adaptation based self-correction model (DASC-Net) is proposed for COVID-19 infection segmentation on CT images. DASC-Net consists of a novel attention and feature domain enhanced domain adaptation model (AFD-DA) to solve the domain shifts and a self-correction learning process to refine segmentation results. The innovations in AFD-DA include an image-level activation feature extractor with attention to lung abnormalities and a multi-level discrimination module for hierarchical feature domain alignment. The proposed self-correction learning process adaptively aggregates the learned model and corresponding pseudo labels for the propagation of aligned source and target domain information to alleviate the overfitting to noises caused by pseudo labels. Extensive experiments over three publicly available COVID-19 CT datasets demonstrate that DASC-Net consistently outperforms state-of-the-art segmentation, domain shift, and coronavirus infection segmentation methods. Ablation analysis further shows the effectiveness of the major components in our model. The DASC-Net enriches the theory of domain adaptation and self-correction learning in medical imaging and can be generalized to multi-site COVID-19 infection segmentation on CT images for clinical deployment.", "journal": "Expert systems with applications", "date": "2021-03-23", "authors": ["QiangguoJin", "HuiCui", "ChangmingSun", "ZhaopengMeng", "LeyiWei", "RanSu"], "doi": "10.1016/j.eswa.2021.114848"}
{"title": "Evaluation of lung involvement in COVID-19 pneumonia based on ultrasound images.", "abstract": "Lung ultrasound (LUS) can be an important imaging tool for the diagnosis and assessment of lung involvement. Ultrasound sonograms have been confirmed to illustrate damage to a person's lungs, which means that the correct classification and scoring of a patient's sonogram can be used to assess lung involvement.\nThe purpose of this study was to establish a lung involvement assessment model based on deep learning. A novel multimodal channel and receptive field attention network combined with ResNeXt (MCRFNet) was proposed to classify sonograms, and the network can automatically fuse shallow features and determine the importance of different channels and respective fields. Finally, sonogram classes were transformed into scores to evaluate lung involvement from the initial diagnosis to rehabilitation.\nUsing multicenter and multimodal ultrasound data from 104 patients, the diagnostic model achieved 94.39% accuracy, 82.28% precision, 76.27% sensitivity, and 96.44% specificity. The lung involvement severity and the trend of COVID-19 pneumonia were evaluated quantitatively.", "journal": "Biomedical engineering online", "date": "2021-03-22", "authors": ["ZhaoyuHu", "ZhenhuaLiu", "YijieDong", "JianjianLiu", "BinHuang", "AihuaLiu", "JingjingHuang", "XujuanPu", "XiaShi", "JinhuaYu", "YangXiao", "HuiZhang", "JianqiaoZhou"], "doi": "10.1186/s12938-021-00863-x\n10.1016/j.tmaid.2020.101627\n10.1148/radiol.2020200463\n10.1148/radiol.2020200370\n10.1378/chest.1806646\n10.1136/emermed-2013-203039\n10.1016/j.jcrc.2014.11.021\n10.21037/jtd.2016.09.38\n10.1007/s13246-020-00865-4\n10.1101/2020.03.12.20027185\n10.1080/03772063.2019.1575292\n10.1007/s40031-019-00398-9\n10.1109/Cvpr.2016.90\n10.1109/Cvpr.2017.634\n10.1109/TPAMI.2019.2913372\n10.1007/s00134-012-2513-4\n10.1164/rccm.201802-0227LE\n10.1186/s13054-019-2569-4\n10.1109/JBHI.2019.2936151\n10.3791/58990\n10.1590/S1679-45082016MD3557\n10.1109/cvpr.2015.7298594\n10.1097/Mcp.0000000000000468"}
{"title": "Quantification of COVID-19 Opacities on Chest CT - Evaluation of a Fully Automatic AI-approach to Noninvasively Differentiate Critical Versus Noncritical Patients.", "abstract": "To evaluate the potential of a fully automatic artificial intelligence (AI)-driven computed tomography (CT) software prototype to quantify severity of COVID-19 infection on chest CT in relationship with clinical and laboratory data.\nWe retrospectively analyzed 50 patients with laboratory confirmed COVID-19 infection who had received chest CT between March and July 2020. Pulmonary opacifications were automatically evaluated by an AI-driven software and correlated with clinical and laboratory parameters using Spearman-Rho and linear regression analysis. We divided the patients into sub cohorts with or without necessity of intensive care unit (ICU) treatment. Sub cohort differences were evaluated employing Wilcoxon-Mann-Whitney-Test.\nWe included 50 CT examinations (mean age, 57.24 years), of whom 24 (48%) had an ICU stay. Extent of COVID-19 like opacities on chest CT showed correlations (all p < 0.001 if not otherwise stated) with occurrence of ICU stay (R\u202f=\u202f0.74), length of ICU stay (R\u202f=\u202f0.81), lethal outcome (R\u202f=\u202f0.56) and length of hospital stay (R\u202f=\u202f0.33, p < 0.05). The opacities extent was correlated with laboratory parameters: neutrophil count (NEU) (R\u202f=\u202f0.60), lactate dehydrogenase (LDH) (R\u202f=\u202f0.60), troponin (TNTHS) (R\u202f=\u202f0.55) and c-reactive protein (CRP) (R\u202f=\u202f0.51). Differences (p < 0.001) between ICU group and non-ICU group concerned longer length of hospital stay (24.04 vs. 10.92 days), higher opacity score (12.50 vs. 4.96) and severity of laboratory data changes such as c-reactive protein (11.64 vs. 5.07 mg/dl, p < 0.01).\nAutomatically AI-driven quantification of opacities on chest CT correlates with laboratory and clinical data in patients with confirmed COVID-19 infection and may serve as non-invasive predictive marker for clinical course of COVID-19.", "journal": "Academic radiology", "date": "2021-03-21", "authors": ["ChristophMader", "SimonBernatz", "SabineMichalik", "VitaliKoch", "Simon SMartin", "ScherwinMahmoudi", "LajosBasten", "Leon DGr\u00fcnewald", "AndreasBucher", "Moritz HAlbrecht", "Thomas JVogl", "ChristianBooz"], "doi": "10.1016/j.acra.2021.03.001"}
{"title": "A comparison between manual and artificial intelligence-based automatic positioning in CT imaging for COVID-19 patients.", "abstract": "To analyze and compare the imaging workflow, radiation dose, and image quality for COVID-19 patients examined using either the conventional manual positioning (MP) method or an AI-based automatic positioning (AP) method.\nOne hundred twenty-seven adult COVID-19 patients underwent chest CT scans on a CT scanner using the same scan protocol except with the manual positioning (MP group) for the initial scan and an AI-based automatic positioning method (AP group) for the follow-up scan. Radiation dose, patient positioning time, and off-center distance of the two groups were recorded and compared. Image noise and signal-to-noise ratio (SNR) were assessed by three experienced radiologists and were compared between the two groups.\nThe AP operation was successful for all patients in the AP group and reduced the total positioning time by 28% compared with the MP group. Compared with the MP group, the AP group had significantly less patient off-center distance (AP 1.56\u00a0cm \u00b1 0.83 vs. MP 4.05\u00a0cm \u00b1 2.40, p < 0.001) and higher proportion of positioning accuracy (AP 99% vs. MP 92%), resulting in 16% radiation dose reduction (AP 6.1\u00a0mSv \u00b1 1.3 vs. MP 7.3\u00a0mSv \u00b1 1.2, p <\u00a00.001) and 9% image noise reduction in erector spinae and lower noise and higher SNR for lesions in the pulmonary peripheral areas.\nThe AI-based automatic positioning and centering in CT imaging is a promising new technique for reducing radiation dose and optimizing imaging workflow and image quality in imaging the chest.\n\u2022 The AI-based automatic positioning (AP) operation was successful for all patients in our study. \u2022 AP method reduced the total positioning time by 28% compared with the manual positioning (MP). \u2022 AP method had less patient off-center distance and higher proportion of positioning accuracy than MP method, resulting in 16% radiation dose reduction and 9% image noise reduction in erector spinae.", "journal": "European radiology", "date": "2021-03-20", "authors": ["YadongGang", "XiongfengChen", "HuanLi", "HanlunWang", "JianyingLi", "YingGuo", "JunjieZeng", "QiangHu", "JinxiangHu", "HaiboXu"], "doi": "10.1007/s00330-020-07629-4\n10.1002/mp.12137\n10.1016/j.ejrad.2018.12.017\n10.1002/mp.12519\n10.1118/1.2748113\n10.1016/j.ebiom.2020.102724\n10.1097/RLI.000000000000048\n10.1378/chest.11.6.511\n10.1016/0003-4975(93)90507-e\n10.1016/j.thorsurg\n10.2214/AJR.16.17215\n10.1016/j.ejmp.2011.06.002\n10.1148/radiol.2020200230"}
{"title": "Lung Lesion Localization of COVID-19 From Chest CT Image: A Novel Weakly Supervised Learning Method.", "abstract": "Chest computed tomography (CT) image data is necessary for early diagnosis, treatment, and prognosis of Coronavirus Disease 2019 (COVID-19). Artificial intelligence has been tried to help clinicians in improving the diagnostic accuracy and working efficiency of CT. Whereas, existing supervised approaches on CT image of COVID-19 pneumonia require voxel-based annotations for training, which take a lot of time and effort. This paper proposed a weakly-supervised method for COVID-19 lesion localization based on generative adversarial network (GAN) with image-level labels only. We first introduced a GAN-based framework to generate normal-looking CT slices from CT slices with COVID-19 lesions. We then developed a novel feature match strategy to improve the reality of generated images by guiding the generator to capture the complex texture of chest CT images. Finally, the localization map of lesions can be easily obtained by subtracting the output image from its corresponding input image. By adding a classifier branch to the GAN-based framework to classify localization maps, we can further develop a diagnosis system with improved classification accuracy. Three CT datasets from hospitals of Sao Paulo, Italian Society of Medical and Interventional Radiology, and China Medical University about COVID-19 were collected in this article for evaluation. Our weakly supervised learning method obtained AUC of 0.883, dice coefficient of 0.575, accuracy of 0.884, sensitivity of 0.647, specificity of 0.929, and F1-score of 0.640, which exceeded other widely used weakly supervised object localization methods by a significant margin. We also compared the proposed method with fully supervised learning methods in COVID-19 lesion segmentation task, the proposed weakly supervised method still leads to a competitive result with dice coefficient of 0.575. Furthermore, we also analyzed the association between illness severity and visual score, we found that the common severity cohort had the largest sample size as well as the highest visual score which suggests our method can help rapid diagnosis of COVID-19 patients, especially in massive common severity cohort. In conclusion, we proposed this novel method can serve as an accurate and efficient tool to alleviate the bottleneck of expert annotation cost and advance the progress of computer-aided COVID-19 diagnosis.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-03-20", "authors": ["ZiduoYang", "LuZhao", "ShuyuWu", "Calvin Yu-ChianChen"], "doi": "10.1109/JBHI.2021.3067465\n10.1148/radiol.2020201491\n10.1109/tmi.2020.2996645\n10.3156/jsoft.29.5_177_2\n10.1016/j.jpha.2020.03.004"}
{"title": "Machine Learning-Based Prediction of COVID-19 Severity and Progression to Critical Illness Using CT Imaging and Clinical Data.", "abstract": "To develop a machine learning (ML) pipeline based on radiomics to predict Coronavirus Disease 2019 (COVID-19) severity and the future deterioration to critical illness using CT and clinical variables.\nClinical data were collected from 981 patients from a multi-institutional international cohort with real-time polymerase chain reaction-confirmed COVID-19. Radiomics features were extracted from chest CT of the patients. The data of the cohort were randomly divided into training, validation, and test sets using a 7:1:2 ratio. A ML pipeline consisting of a model to predict severity and time-to-event model to predict progression to critical illness were trained on radiomics features and clinical variables. The receiver operating characteristic area under the curve (ROC-AUC), concordance index (C-index), and time-dependent ROC-AUC were calculated to determine model performance, which was compared with consensus CT severity scores obtained by visual interpretation by radiologists.\nAmong 981 patients with confirmed COVID-19, 274 patients developed critical illness. Radiomics features and clinical variables resulted in the best performance for the prediction of disease severity with a highest test ROC-AUC of 0.76 compared with 0.70 (0.76 vs. 0.70, \nCT radiomics features combined with clinical variables were predictive of COVID-19 severity and progression to critical illness with fairly high accuracy.", "journal": "Korean journal of radiology", "date": "2021-03-20", "authors": ["SubhanikPurkayastha", "YanheXiao", "ZhichengJiao", "RujapaThepumnoeysuk", "KaseyHalsey", "JingWu", "Thi My LinhTran", "BenHsieh", "Ji WhaeChoi", "DongcuiWang", "MartinValli\u00e8res", "RobinWang", "ScottCollins", "XueFeng", "MichaelFeldman", "Paul JZhang", "MichaelAtalay", "RonnieSebro", "LiYang", "YongFan", "Wei HuaLiao", "Harrison XBai"], "doi": "10.3348/kjr.2020.1104"}
{"title": "Transfer learning-based ensemble support vector machine model for automated COVID-19 detection using lung computerized tomography scan data.", "abstract": "The novel discovered disease coronavirus popularly known as COVID-19 is caused due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and declared a pandemic by the World Health Organization (WHO). An early-stage detection of COVID-19 is crucial for the containment of the pandemic it has caused. In this study, a transfer learning-based COVID-19 screening technique is proposed. The motivation of this study is to design an automated system that can assist medical staff especially in areas where trained staff are outnumbered. The study investigates the potential of transfer learning-based models for automatically diagnosing diseases like COVID-19 to assist the medical force, especially in times of an outbreak. In the proposed work, a deep learning model, i.e., truncated VGG16 (Visual Geometry Group from Oxford) is implemented to screen COVID-19 CT scans. The VGG16 architecture is fine-tuned and used to extract features from CT scan images. Further principal component analysis (PCA) is used for feature selection. For the final classification, four different classifiers, namely deep convolutional neural network (DCNN), extreme learning machine (ELM), online sequential ELM, and bagging ensemble with support vector machine (SVM) are compared. The best performing classifier bagging ensemble with SVM within 385 ms achieved an accuracy of 95.7%, the precision of 95.8%, area under curve (AUC) of 0.958, and an F1 score of 95.3% on 208 test images. The results obtained on diverse datasets prove the superiority and robustness of the proposed work. A pre-processing technique has also been proposed for radiological data. The study further compares pre-trained CNN architectures and classification models against the proposed technique.", "journal": "Medical & biological engineering & computing", "date": "2021-03-20", "authors": ["MukulSingh", "ShreyBansal", "SakshiAhuja", "Rahul KumarDubey", "Bijaya KetanPanigrahi", "NilanjanDey"], "doi": "10.1007/s11517-020-02299-2\n10.1007/s10916-020-01582-x\n10.1080/14737159.2020.1757437\n10.1148/radiol.2020200642\n10.1016/j.cmpb.2020.105617\n10.1142/S0219622020500285\n10.1007/s00330-020-06801-0\n10.1016/j.jiph.2020.06.028\n10.1038/s41598-019-56847-4\n10.1007/s10096-020-03901-z\n10.1016/j.ejrad.2020.108941\n10.1148/radiol.2020200527\n10.1186/s40537-019-0197-0\n10.1016/j.imu.2020.100427\n10.1109/ACCESS.2020.3016780\n10.1007/s13246-020-00888-x\n10.1109/ACCESS.2020.3016780\n10.1109/ACCESS.2020.3019095\n10.1016/j.neucom.2005.12.126\n10.1109/TNN.2006.880583"}
{"title": "AI detection of mild COVID-19 pneumonia from chest CT scans.", "abstract": "An artificial intelligence model was adopted to identify mild COVID-19 pneumonia from computed tomography (CT) volumes, and its diagnostic performance was then evaluated.\nIn this retrospective multicenter study, an atrous convolution-based deep learning model was established for the computer-assisted diagnosis of mild COVID-19 pneumonia. The dataset included 2087 chest CT exams collected from four hospitals between 1 January 2019 and 31 May 2020. The true positive rate, true negative rate, receiver operating characteristic curve, area under the curve (AUC) and convolutional feature map were used to evaluate the model.\nThe proposed deep learning model was trained on 1538 patients and tested on an independent testing cohort of 549 patients. The overall sensitivity was 91.5% (195/213; p < 0.001, 95% CI: 89.2-93.9%), the overall specificity was 90.5% (304/336; p < 0.001, 95% CI: 88.0-92.9%) and the general AUC value was 0.955 (p < 0.001).\nA deep learning model can accurately detect COVID-19 and serve as an important supplement to the COVID-19 reverse transcription-polymerase chain reaction (RT-PCR) test.\n\u2022 The implementation of a deep learning model to identify mild COVID-19 pneumonia was confirmed to be effective and feasible. \u2022 The strategy of using a binary code instead of the region of interest label to identify mild COVID-19 pneumonia was verified. \u2022 This AI model can assist in the early screening of COVID-19 without interfering with normal clinical examinations.", "journal": "European radiology", "date": "2021-03-20", "authors": ["Jin-CaoYao", "TaoWang", "Guang-HuaHou", "DiOu", "WeiLi", "Qiao-DanZhu", "Wen-CongChen", "ChenYang", "Li-JingWang", "Li-PingWang", "Lin-YinFan", "Kai-YuanShi", "JieZhang", "DongXu", "Ya-QingLi"], "doi": "10.1007/s00330-021-07797-x\n10.1016/S0140-6736(20)30183-5\n10.1038/s41586-020-2179-y\n10.1001/jama.2020.2648\n10.1126/science.abb4557\n10.1002/jmv.26060\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020200463\n10.1007/s00330-020-06748-2\n10.1056/NEJMoa2001017\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2996645\n10.1148/radiol.2019182465\n10.1016/j.media.2017.06.014\n10.1016/j.media.2019.07.004\n10.1109/TMI.2019.2934577\n10.1109/TPAMI.2017.2699184\n10.1007/s00330-020-07044-9\n10.1007/s00330-020-07042-x"}
{"title": "Integration of CNN, CBMIR, and Visualization Techniques for Diagnosis and Quantification of Covid-19 Disease.", "abstract": "Diagnosis techniques based on medical image modalities have higher sensitivities compared to conventional RT-PCT tests. We propose two methods for diagnosing COVID-19 disease using X-ray images and differentiating it from viral pneumonia. The diagnosis section is based on deep neural networks, and the discriminating uses an image retrieval approach. Both units were trained by healthy, pneumonia, and COVID-19 images. In COVID-19 patients, the maximum intensity projection of the lung CT is visualized to a physician, and the CT Involvement Score is calculated. The performance of the CNN and image retrieval algorithms were improved by transfer learning and hashing functions. We achieved an accuracy of 97% and an overall prec@10 of 87%, respectively, concerning the CNN and the retrieval methods.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-03-19", "authors": ["SaeedMohagheghi", "MehdiAlizadeh", "Seyed MahdiSafavi", "Amir HForuzan", "Yen-WeiChen"], "doi": "10.1109/JBHI.2021.3067333"}
{"title": "Diagnosis and risk stratification in hypertrophic cardiomyopathy using machine learning wall thickness measurement: a comparison with human test-retest performance.", "abstract": "Left ventricular maximum wall thickness (MWT) is central to diagnosis and risk stratification of hypertrophic cardiomyopathy, but human measurement is prone to variability. We developed an automated machine learning algorithm for MWT measurement and compared precision (reproducibility) with that of 11 international experts, using a dataset of patients with hypertrophic cardiomyopathy.\n60 adult patients with hypertrophic cardiomyopathy, including those carrying hypertrophic cardiomyopathy gene mutations, were recruited at three institutes in the UK from August, 2018, to September, 2019: Barts Heart Centre, University College London Hospital (The Heart Hospital), and Leeds Teaching Hospitals NHS Trust. Participants had two cardiovascular magnetic resonance scans (test and retest) on the same day, ensuring no biological variability, using four cardiac MRI scanner models represented across two manufacturers and two field strengths. End-diastolic short-axis MWT was measured in test and retest by 11 international experts (from nine centres in six countries) and an automated machine learning method, which was trained to segment endocardial and epicardial contours on an independent, multicentre, multidisease dataset of 1923 patients. Machine learning MWT measurement was done with a method based on solving Laplace's equation. To assess test-retest reproducibility, we estimated the absolute test-retest MWT difference (precision), the coefficient of variation (CoV) for duplicate measurements, and the number of patients reclassified between test and retest according to different thresholds (MWT >15 mm and >30 mm). We calculated the sample size required to detect a prespecified MWT change between pairs of scans for machine learning and each expert.\n1440 MWT measurements were analysed, corresponding to two scans from 60 participants by 12 observers (11 experts and machine learning). Experts differed in the MWT they measured, ranging from 14\u00b79 mm (SD 4\u00b72) to 19\u00b70 mm (4\u00b77; p<0\u00b70001 for trend). Machine learning-measured mean MWT was 16\u00b78 mm (4\u00b71). Machine learning precision was superior, with a test-retest difference of 0\u00b77 mm (0\u00b76) compared with experts, who ranged from 1\u00b71 mm (0\u00b79) to 3\u00b77 mm (2\u00b70; p values for machine learning vs expert comparison ranging from <0\u00b70001 to 0\u00b70073) and a significantly lower CoV than for all experts (4\u00b73% [95% CI 3\u00b73-5\u00b71] vs 5\u00b77-12\u00b71% across experts). On average, 38 (64%) patients were designated as having MWT greater than 15 mm by machine learning compared with 27 (45%) to 50 (83%) patients by experts; five (8%) patients were reclassified in test-retest by machine learning compared with four (7%) to 12 (20%) by experts. With a cutoff point of more than 30 mm for implantable cardioverter-defibrillator, three experts would have changed recommendations between tests a total of four times, but machine learning was consistent. Using machine learning, a clinical trial to detect a 2 mm MWT change would need 2\u00b73 times (range 1\u00b76-4\u00b76) fewer patients.\nIn this preliminary study, machine learning MWT measurement in hypertrophic cardiomyopathy is superior to human experts with potential implications for diagnosis, risk stratification, and clinical trials.\nEuropean Regional Development Fund and Barts Charity.", "journal": "The Lancet. Digital health", "date": "2021-03-19", "authors": ["Jo\u00e3o BAugusto", "Rhodri HDavies", "Anish NBhuva", "Kristopher DKnott", "AndreasSeraphim", "MashaelAlfarih", "ClementLau", "Rebecca KHughes", "Lu\u00eds RLopes", "HunainShiwani", "Thomas ATreibel", "Bernhard LGerber", "ChristianHamilton-Craig", "Ntobeko A BNtusi", "GianlucaPontone", "Milind YDesai", "John PGreenwood", "Peter PSwoboda", "GabriellaCaptur", "Jo\u00e3oCavalcante", "ChiaraBucciarelli-Ducci", "Steffen EPetersen", "ErikSchelbert", "CharlotteManisty", "James CMoon"], "doi": "10.1016/S2589-7500(20)30267-3"}
{"title": "Thoracic imaging tests for the diagnosis of COVID-19.", "abstract": "The respiratory illness caused by SARS-CoV-2 infection continues to present diagnostic challenges. Our 2020 edition of this review showed thoracic (chest) imaging to be sensitive and moderately specific in the diagnosis of coronavirus disease 2019 (COVID-19). In this update, we include new relevant studies, and have removed studies with case-control designs, and those not intended to be diagnostic test accuracy studies.\nTo evaluate the diagnostic accuracy of thoracic imaging (computed tomography (CT), X-ray and ultrasound) in people with suspected COVID-19.\nWe searched the COVID-19 Living Evidence Database from the University of Bern, the Cochrane COVID-19 Study Register, The Stephen B. Thacker CDC Library, and repositories of COVID-19 publications through to 30 September 2020. We did not apply any language restrictions.\nWe included studies of all designs, except for case-control, that recruited participants of any age group suspected to have COVID-19 and that reported estimates of test accuracy or provided data from which we could compute estimates.\nThe review authors independently and in duplicate screened articles, extracted data and assessed risk of bias and applicability concerns using the QUADAS-2 domain-list. We presented the results of estimated sensitivity and specificity using paired forest plots, and we summarised pooled estimates in tables. We used a bivariate meta-analysis model where appropriate. We presented the uncertainty of accuracy estimates using 95% confidence intervals (CIs).\nWe included 51 studies with 19,775 participants suspected of having COVID-19, of whom 10,155 (51%) had a final diagnosis of COVID-19. Forty-seven studies evaluated one imaging modality each, and four studies evaluated two imaging modalities each. All studies used RT-PCR as the reference standard for the diagnosis of COVID-19, with 47 studies using only RT-PCR and four studies using a combination of RT-PCR and other criteria (such as clinical signs, imaging tests, positive contacts, and follow-up phone calls) as the reference standard. Studies were conducted in Europe (33), Asia (13), North America (3) and South America (2); including only adults (26), all ages (21), children only (1), adults over 70 years (1), and unclear (2); in inpatients (2), outpatients (32), and setting unclear (17). Risk of bias was high or unclear in thirty-two (63%) studies with respect to participant selection, 40 (78%) studies with respect to reference standard, 30 (59%) studies with respect to index test, and 24 (47%) studies with respect to participant flow. For chest CT (41 studies, 16,133 participants, 8110 (50%) cases), the sensitivity ranged from 56.3% to 100%, and specificity ranged from 25.4% to 97.4%. The pooled sensitivity of chest CT was 87.9% (95% CI 84.6 to 90.6) and the pooled specificity was 80.0% (95% CI 74.9 to 84.3). There was no statistical evidence indicating that reference standard conduct and definition for index test positivity were sources of heterogeneity for CT studies. Nine chest CT studies (2807 participants, 1139 (41%) cases) used the COVID-19 Reporting and Data System (CO-RADS) scoring system, which has five thresholds to define index test positivity. At a CO-RADS threshold of 5 (7 studies), the sensitivity ranged from 41.5% to 77.9% and the pooled sensitivity was 67.0% (95% CI 56.4 to 76.2); the specificity ranged from 83.5% to 96.2%; and the pooled specificity was 91.3% (95% CI 87.6 to 94.0). At a CO-RADS threshold of 4 (7 studies), the sensitivity ranged from 56.3% to 92.9% and the pooled sensitivity was 83.5% (95% CI 74.4 to 89.7); the specificity ranged from 77.2% to 90.4% and the pooled specificity was 83.6% (95% CI 80.5 to 86.4). For chest X-ray (9 studies, 3694 participants, 2111 (57%) cases) the sensitivity ranged from 51.9% to 94.4% and specificity ranged from 40.4% to 88.9%. The pooled sensitivity of chest X-ray was 80.6% (95% CI 69.1 to 88.6) and the pooled specificity was 71.5% (95% CI 59.8 to 80.8). For ultrasound of the lungs (5 studies, 446 participants, 211 (47%) cases) the sensitivity ranged from 68.2% to 96.8% and specificity ranged from 21.3% to 78.9%. The pooled sensitivity of ultrasound was 86.4% (95% CI 72.7 to 93.9) and the pooled specificity was 54.6% (95% CI 35.3 to 72.6). Based on an indirect comparison using all included studies, chest CT had a higher specificity than ultrasound. For indirect comparisons of chest CT and chest X-ray, or chest X-ray and ultrasound, the data did not show differences in specificity or sensitivity.\nOur findings indicate that chest CT is sensitive and moderately specific for the diagnosis of COVID-19. Chest X-ray is moderately sensitive and moderately specific for the diagnosis of COVID-19. Ultrasound is sensitive but not specific for the diagnosis of COVID-19. Thus, chest CT and ultrasound may have more utility for excluding COVID-19 than for differentiating SARS-CoV-2 infection from other causes of respiratory illness. Future diagnostic accuracy studies should pre-define positive imaging findings, include direct comparisons of the various modalities of interest in the same participant population, and implement improved reporting practices.", "journal": "The Cochrane database of systematic reviews", "date": "2021-03-17", "authors": ["NayaarIslam", "SanamEbrahimzadeh", "Jean-PaulSalameh", "SakibKazi", "NicholasFabiano", "LeeTreanor", "MarissaAbsi", "ZacharyHallgrimson", "Mariska MgLeeflang", "LottyHooft", "Christian Bvan der Pol", "RossPrager", "Samanjit SHare", "CaroleDennie", "Ren\u00e9Spijker", "Jonathan JDeeks", "JacquelineDinnes", "KevinJenniskens", "Dani\u00ebl AKorevaar", "J\u00e9r\u00e9mie FCohen", "AnnVan den Bruel", "YemisiTakwoingi", "Jannekevan de Wijgert", "Johanna AagDamen", "JunfengWang", "Matthew DfMcInnes", "NoneNone"], "doi": "10.1002/14651858.CD013639.pub4\n10.5152/dir.2020.20350\n10.1007/s00330-020-07273-y\n10.4081/monaldi.2020.1446\n10.1007/s00330-020-07154-4\n10.1371/journal.pone.0235844\n10.5152/dir.2020.20270\n10.5811/westjem.2020.5.47743\n10.3238/arztebl.2020.0389\n10.1101/2020.02.19.20025023\n10.1007/s00330-020-06829-2\n10.5152/ejbh.2020.010420\n10.1148/radiol.2020201433\n10.1016/j.ejrad.2020.109009\n10.2214/AJR.20.23035\n10.1016/j.tmaid.2020.101627\n10.1056/NEJMc2005073\n10.1148/radiol.2020200847\n10.2214/AJR.20.23232\n10.1101/2020.03.19.20027078\n10.1002/ijgo.13165\n10.1002/jmv.25904\n10.1007/s00259-020-04720-2\n10.1183/13993003.00407-2020\n10.1148/radiol.2015151516\n10.1001/jamainternmed.2020.8876\n10.1016/j.jclinepi.2006.06.011\n10.1016/j.jclinepi.2016.04.013\n10.1001/jama.2020.22717\n10.1002/14651858.CD013652\n10.1002/14651858.CD013705\n10.1002/jmri.25797\n10.1016/0895-4356(94)00099-c\n10.7326/M20-1495\n10.1056/NEJM199207233270406\n10.1002/jmv.25786\n10.1080/22221751.2020.1745095\n10.1371/journal.pmed.1000097\n10.1148/ryct.2020200034\n10.1016/j.jclinepi.2005.02.022\n10.1148/radiol.2020201365\n10.1136/bmj.m2632\n10.2214/AJR.20.23034\n10.1016/S1473-3099(20)30086-4\n10.1148/ryct.2020200152\n10.1002/14651858.CD013787\n10.1002/14651858.CD013665\n10.1002/14651858.CD013639.pub3\n10.1002/14651858.CD013639\n10.1002/14651858.CD013639.pub2"}
{"title": "Identification of Images of COVID-19 from Chest X-rays Using Deep Learning: Comparing COGNEX VisionPro Deep Learning 1.0\u2122 Software with Open Source Convolutional Neural Networks.", "abstract": "The novel Coronavirus, COVID-19, pandemic is being considered the most crucial health calamity of the century. Many organizations have come together during this crisis and created various Deep Learning models for the effective diagnosis of COVID-19 from chest radiography images. For example, The University of Waterloo, along with Darwin AI-a start-up spin-off of this department, has designed the Deep Learning model 'COVID-Net' and created a dataset called 'COVIDx' consisting of 13,975 images across 13,870 patient cases. In this study, COGNEX's Deep Learning Software, VisionPro Deep Learning\u2122, \u00a0is used to classify these Chest X-rays from the COVIDx dataset. The results are compared with the results of COVID-Net and various other state-of-the-art Deep Learning models from the open-source community. Deep Learning tools are often referred to as black boxes because humans cannot interpret how or why a model is classifying an image into a particular class. This problem is addressed by testing VisionPro Deep Learning with two settings, first, by selecting the entire image as the Region of Interest (ROI), and second, by segmenting the lungs in the first step, and then doing the classification step on the segmented lungs only, instead of using the entire image. VisionPro Deep Learning results: on the entire image as the ROI it achieves an overall F score of 94.0%, and on the segmented lungs, it gets an ", "journal": "SN computer science", "date": "2021-03-16", "authors": ["ArjunSarkar", "JoergVandenhirtz", "JozsefNagy", "DavidBacsa", "MitchellRiley"], "doi": "10.1007/s42979-021-00496-w\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.1148/ryct.2020200034\n10.1056/nejmoa2002032\n10.1183/13993003.00607-2020\n10.1016/j.virol.2007.09.045\n10.1093/infdis/jis455\n10.1097/RLI.0000000000000670\n10.1016/j.clinimag.2020.04.001\n10.1038/s41598-020-76550-z\n10.1109/ACCESS.2020.3010287\n10.1145/3065386\n10.1007/s10916-020-01562-1\n10.1007/s10489-020-01943-6\n10.1007/s13246-020-00888-x\n10.1016/j.cmpb.2020.105581\n10.1007/s10489-020-01867-1\n10.1007/s42979-020-00209-9"}
{"title": "Quantitative CT imaging and advanced visualization methods: potential application in novel coronavirus disease 2019 (COVID-19) pneumonia.", "abstract": "Increasingly, quantitative lung computed tomography (qCT)-derived metrics are providing novel insights into chronic inflammatory lung diseases, including chronic obstructive pulmonary disease, asthma, interstitial lung disease, and more. Metrics related to parenchymal, airway, and vascular anatomy together with various measures associated with lung function including regional parenchymal mechanics, air trapping associated with functional small airways disease, and dual-energy derived measures of perfused blood volume are offering the ability to characterize disease phenotypes associated with the chronic inflammatory pulmonary diseases. With the emergence of COVID-19, together with its widely varying degrees of severity, its rapid progression in some cases, and the potential for lengthy post-COVID-19 morbidity, there is a new role in applying well-established qCT-based metrics. Based on the utility of qCT tools in other lung diseases, previously validated supervised classical machine learning methods, and emerging unsupervised machine learning and deep-learning approaches, we are now able to provide desperately needed insight into the acute and the chronic phases of this inflammatory lung disease. The potential areas in which qCT imaging can be beneficial include improved accuracy of diagnosis, identification of clinically distinct phenotypes, improvement of disease prognosis, stratification of care, and early objective evaluation of intervention response. There is also a potential role for qCT in evaluating an increasing population of post-COVID-19 lung parenchymal changes such as fibrosis. In this work, we discuss the basis of various lung qCT methods, using case-examples to highlight their potential application as a tool for the exploration and characterization of COVID-19, and offer scanning protocols to serve as templates for imaging the lung such that these established qCT analyses have the best chance at yielding the much needed new insights.", "journal": "BJR open", "date": "2021-03-16", "authors": ["PrashantNagpal", "JunfengGuo", "Kyung MinShin", "Jae-KwangLim", "Ki BeomKim", "Alejandro PComellas", "David WKaczka", "SamuelPeterson", "Chang HyunLee", "Eric AHoffman"], "doi": "10.1259/bjro.20200043\n10.1056/NEJMoa2001017\n10.1164/ajrccm.166.1.at1102\n10.1148/radiol.2020200230\n10.5582/bst.2020.01043\n10.1056/NEJMp2006141\n10.1016/j.ijantimicag.2020.105924\n10.1148/radiol.2020201365\n10.1164/rccm.202003-0817LE\n10.1148/radiol.2020200843\n10.1148/radiol.2020200642\n10.1164/ajrccm.160.2.9804094\n10.1164/rccm.201607-1385OC\n10.1002/art.30438\n10.1056/NEJMoa0900928\n10.1513/AnnalsATS.201310-364OC\n10.1016/j.chest.2016.11.033\n10.1378/chest.129.3.558\n10.3109/15412555.2012.661000\n10.1016/j.rmed.2016.08.023\n10.1378/chest.06-1401\n10.1111/crj.12451\n10.1164/rccm.201803-0444PP\n10.1148/radiol.2231010315\n10.1002/jmri.25010\n10.1016/j.jaci.2011.04.051\n10.1164/rccm.201506-1208PP\n10.1183/13993003.00547-2020\n10.1148/radiol.2020200905\n10.1016/S2213-2600(20)30003-5\n10.1016/j.crad.2020.01.010\n10.1109/TMI.2018.2858202\n10.1148/radiol.2020201491\n10.1101/2020.05.10.20096073\n10.1007/s11547-020-01195-x\n10.1148/ryai.2020200048\n10.1007/s11432-020-2849-3\n10.1016/S1076-6332(97)80080-3\n10.1016/j.acra.2004.09.005\n10.1513/pats.200603-086MS\n10.1002/wsbm.1234\n10.1016/j.jaci.2016.11.009\n10.1164/rccm.201807-1351SO\n10.1148/ryai.2020200053\n10.1016/j.acra.2017.01.016\n10.1152/jappl.1985.59.2.468\n10.1164/ajrccm.152.2.7633722\n10.1152/jappl.1995.79.5.1525\n10.1016/j.acra.2008.12.024\n10.1148/radiol.2431060194\n10.1148/radiol.2483071434\n10.1016/j.acra.2009.12.002\n10.1007/s00330-012-2683-z\n10.1016/j.rmed.2010.02.023\n10.1164/rccm.200912-1843CC\n10.3109/15412550903341513\n10.1164/rccm.200510-1677OC\n10.1056/NEJMoa030287\n10.1152/jappl.1994.76.1.271\n10.1016/S1076-6332(03)00330-1\n10.1111/resp.13783\n10.1164/rccm.201703-0555OC\n10.1186/s12931-017-0527-8\n10.1183/13993003.00129-2016\n10.1097/RTI.0000000000000220\n10.1371/journal.pone.0224772\n10.1148/radiol.10091446\n10.1259/bjr.20200538\n10.1016/j.media.2019.01.012\n10.1007/s00330-020-07013-2\n10.1016/j.ijid.2020.06.033\n10.1148/radiol.2020201433\n10.1164/ajrccm.159.2.9707145\n10.1164/ajrccm.156.1.9606093\n10.1016/j.acra.2006.04.017\n10.1109/TMI.2006.870889\n10.1016/j.ejrad.2020.108852\n10.1164/rccm.201711-2174OC\n10.1007/s00330-019-06402-6\n10.7326/M20-2003\n10.1161/CIRCULATIONAHA.120.047430\n10.1016/S2665-9913(20)30121-1\n10.1097/00004728-197908000-00002\n10.1148/radiographics.17.2.9084083\n10.1038/nm.2971\n10.1159/000478865\n10.1097/ALN.0000000000002583\n10.1136/thoraxjnl-2013-203897\n10.1183/09031936.00165410\n10.1002/mp.12436\n10.1097/RLI.0000000000000093\n10.1097/RTI.0b013e31829f6796\n10.1007/s00134-020-06033-2\n10.1001/jama.2012.5669\n10.1513/AnnalsATS.202004-376RL\n10.1016/j.ejmp.2017.05.071\n10.1088/0031-9155/61/13/R150\n10.1259/bjr.20170926\n10.1016/j.ejca.2011.11.036\n10.21037/tlcr.2019.12.19\n10.1186/s12931-019-1121-z\n10.1001/jama.2020.6918\n10.1073/pnas.1715564115\n10.1089/jamp.2018.1487\n10.1186/s12931-018-0888-7\n10.1136/bmjresp-2017-000252\n10.1016/j.jaci.2016.11.053\n10.1016/j.jaci.2016.02.001\n10.1016/j.jaci.2013.09.039"}
{"title": "Deep metric learning-based image retrieval system for chest radiograph and its clinical applications in COVID-19.", "abstract": "In recent years, deep learning-based image analysis methods have been widely applied in computer-aided detection, diagnosis and prognosis, and has shown its value during the public health crisis of the novel coronavirus disease 2019 (COVID-19) pandemic. Chest radiograph (CXR) has been playing a crucial role in COVID-19 patient triaging, diagnosing and monitoring, particularly in the United States. Considering the mixed and unspecific signals in CXR, an image retrieval model of CXR that provides both similar images and associated clinical information can be more clinically meaningful than a direct image diagnostic model. In this work we develop a novel CXR image retrieval model based on deep metric learning. Unlike traditional diagnostic models which aim at learning the direct mapping from images to labels, the proposed model aims at learning the optimized embedding space of images, where images with the same labels and similar contents are pulled together. The proposed model utilizes multi-similarity loss with hard-mining sampling strategy and attention mechanism to learn the optimized embedding space, and provides similar images, the visualizations of disease-related attention maps and useful clinical information to assist clinical decisions. The model is trained and validated on an international multi-site COVID-19 dataset collected from 3 different sources. Experimental results of COVID-19 image retrieval and diagnosis tasks show that the proposed model can serve as a robust solution for CXR analysis and patient management for COVID-19. The model is also tested on its transferability on a different clinical decision support task for COVID-19, where the pre-trained model is applied to extract image features from a new dataset without any further training. The extracted features are then combined with COVID-19 patient's vitals, lab tests and medical histories to predict the possibility of airway intubation in 72 hours, which is strongly associated with patient prognosis, and is crucial for patient care and hospital resource planning. These results demonstrate our deep metric learning based image retrieval model is highly efficient in the CXR retrieval, diagnosis and prognosis, and thus has great clinical value for the treatment and management of COVID-19 patients.", "journal": "Medical image analysis", "date": "2021-03-13", "authors": ["AoxiaoZhong", "XiangLi", "DufanWu", "HuiRen", "KyungsangKim", "YounggonKim", "VarunBuch", "NirNeumark", "BernardoBizzo", "Won YoungTak", "Soo YoungPark", "Yu RimLee", "Min KyuKang", "Jung GilPark", "Byung SeokKim", "Woo JinChung", "NingGuo", "IttaiDayan", "Mannudeep KKalra", "QuanzhengLi"], "doi": "10.1016/j.media.2021.101993"}
{"title": "An Interpretable Model-Based Prediction of Severity and Crucial Factors in Patients with COVID-19.", "abstract": "This study established an interpretable machine learning model to predict the severity of coronavirus disease 2019 (COVID-19) and output the most crucial deterioration factors. Clinical information, laboratory tests, and chest computed tomography (CT) scans at admission were collected. Two experienced radiologists reviewed the scans for the patterns, distribution, and CT scores of lung abnormalities. Six machine learning models were established to predict the severity of COVID-19. After parameter tuning and performance comparison, the optimal model was explained using Shapley Additive explanations to output the crucial factors. This study enrolled and classified 198 patients into mild (", "journal": "BioMed research international", "date": "2021-03-13", "authors": ["BowenZheng", "YongCai", "FengxiaZeng", "MinLin", "JunZheng", "WeiguoChen", "GenggengQin", "YiGuo"], "doi": "10.1155/2021/8840835\n10.1038/s41591-020-0931-3\n10.1148/radiol.2020200230\n10.1186/s40779-020-00270-8\n10.1001/jama.2020.2648\n10.1056/NEJMoa2002032\n10.1148/radiol.2303030853\n10.1148/radiol.2020200463\n10.1038/s42256-019-0138-9\n10.1148/radiol.2020200823\n10.1148/ryct.2020204001\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020200843\n10.1613/jair.1.12162\n10.1016/j.dsx.2020.04.012\n10.1016/j.jcot.2019.06.012\n10.1016/j.dsx.2020.03.002\n10.1101/2020.02.19.955484v1.abstract\n10.3389/fimmu.2020.01581\n10.1097/RCT.0000000000001094\n10.1016/j.ijid.2016.06.015\n10.1148/ryct.2020200126\n10.1016/S0140-6736(20)30183-5\n10.1148/rg.2018170048\n10.1172/JCI60331\n10.1007/s10096-020-03901-z\n10.1148/ryct.2020200075"}
{"title": "Automatic deep learning-based pleural effusion classification in lung ultrasound images for respiratory pathology diagnosis.", "abstract": "Lung ultrasound (LUS) imaging as a point-of-care diagnostic tool for lung pathologies has been proven superior to X-ray and comparable to CT, enabling earlier and more accurate diagnosis in real-time at the patient's bedside. The main limitation to widespread use is its dependence on the operator training and experience. COVID-19 lung ultrasound findings predominantly reflect a pneumonitis pattern, with pleural effusion being infrequent. However, pleural effusion is easy to detect and to quantify, therefore it was selected as the subject of this study, which aims to develop an automated system for the interpretation of LUS of pleural effusion. A LUS dataset was collected at the Royal Melbourne Hospital which consisted of 623 videos containing 99,209 2D ultrasound images of 70 patients using a phased array transducer. A standardized protocol was followed that involved scanning six anatomical regions providing complete coverage of the lungs for diagnosis of respiratory pathology. This protocol combined with a deep learning algorithm using a Spatial Transformer Network provides a basis for automatic pathology classification on an image-based level. In this work, the deep learning model was trained using supervised and weakly supervised approaches which used frame- and video-based ground truth labels respectively. The reference was expert clinician image interpretation. Both approaches show comparable accuracy scores on the test set of 92.4% and 91.1%, respectively, not statistically significantly different. However, the video-based labelling approach requires significantly less effort from clinical experts for ground truth labelling.", "journal": "Physica medica : PM : an international journal devoted to the applications of physics to medicine and biology : official journal of the Italian Association of Biomedical Physics (AIFB)", "date": "2021-03-12", "authors": ["Chung-HanTsai", "Jeroenvan der Burgt", "DamjanVukovic", "NancyKaur", "LibertarioDemi", "DavidCanty", "AndrewWang", "AlistairRoyse", "ColinRoyse", "KaviHaji", "JasonDowling", "GirijaChetty", "DavideFontanarosa"], "doi": "10.1016/j.ejmp.2021.02.023"}
{"title": "Deep Learning Enables Accurate Diagnosis of Novel Coronavirus (COVID-19) With CT Images.", "abstract": "A novel coronavirus (COVID-19) recently emerged as an acute respiratory syndrome, and has caused a pneumonia outbreak world-widely. As the COVID-19 continues to spread rapidly across the world, computed tomography (CT) has become essentially important for fast diagnoses. Thus, it is urgent to develop an accurate computer-aided method to assist clinicians to identify COVID-19-infected patients by CT images. Here, we have collected chest CT scans of 88 patients diagnosed with COVID-19 from hospitals of two provinces in China, 100 patients infected with bacteria pneumonia, and 86 healthy persons for comparison and modeling. Based on the data, a deep learning-based CT diagnosis system was developed to identify patients with COVID-19. The experimental results showed that our model could accurately discriminate the COVID-19 patients from the bacteria pneumonia patients with an AUC of 0.95, recall (sensitivity) of 0.96, and precision of 0.79. When integrating three types of CT images, our model achieved a recall of 0.93 with precision of 0.86 for discriminating COVID-19 patients from others. Moreover, our model could extract main lesion features, especially the ground-glass opacity (GGO), which are visually helpful for assisted diagnoses by doctors. An online server is available for online diagnoses with CT images by our server (http://biomed.nscc-gz.cn/model.php). Source codes and datasets are available at our GitHub (https://github.com/SY575/COVID19-CT).", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "date": "2021-03-12", "authors": ["YingSong", "ShuangjiaZheng", "LiangLi", "XiangZhang", "XiaodongZhang", "ZiwangHuang", "JianwenChen", "RuixuanWang", "HuiyingZhao", "YutianChong", "JunShen", "YunfeiZha", "YuedongYang"], "doi": "10.1109/TCBB.2021.3065361"}
{"title": "Machine learning-based prognostic modeling using clinical data and quantitative radiomic features from chest CT images in COVID-19 patients.", "abstract": "To develop prognostic models for survival (alive or deceased status) prediction of COVID-19 patients using clinical data (demographics and history, laboratory tests, visual scoring by radiologists) and lung/lesion radiomic features extracted from chest CT images.\nOverall, 152 patients were enrolled in this study protocol. These were divided into 106 training/validation and 46 test datasets (untouched during training), respectively. Radiomic features were extracted from the segmented lungs and infectious lesions separately from chest CT images. Clinical data, including patients' history and demographics, laboratory tests and radiological scores were also collected. Univariate analysis was first performed (q-value reported after false discovery rate (FDR) correction) to determine the most predictive features among all imaging and clinical data. Prognostic modeling of survival was performed using radiomic features and clinical data, separately or in combination. Maximum relevance minimum redundancy (MRMR) and XGBoost were used for feature selection and classification. The receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC), sensitivity, specificity, and accuracy were used to assess the prognostic performance of the models on the test datasets.\nFor clinical data, cancer comorbidity (q-value\u00a0<\u00a00.01), consciousness level (q-value\u00a0<\u00a00.05) and radiological score involved zone (q-value\u00a0<\u00a00.02) were found to have high correlated features with outcome. Oxygen saturation (AUC\u00a0=\u00a00.73, q-value\u00a0<\u00a00.01) and Blood Urea Nitrogen (AUC\u00a0=\u00a00.72, q-value\u00a0=\u00a00.72) were identified as high clinical features. For lung radiomic features, SAHGLE (AUC\u00a0=\u00a00.70) and HGLZE (AUC\u00a0=\u00a00.67) from GLSZM were identified as most prognostic features. Amongst lesion radiomic features, RLNU from GLRLM (AUC\u00a0=\u00a00.73), HGLZE from GLSZM (AUC\u00a0=\u00a00.73) had the highest performance. In multivariate analysis, combining lung, lesion and clinical features was determined to provide the most accurate prognostic model (AUC\u00a0=\u00a00.95\u00a0\u00b1\u00a00.029 (95%CI: 0.95-0.96), accuracy\u00a0=\u00a00.88\u00a0\u00b1\u00a00.046 (95% CI: 0.88-0.89), sensitivity\u00a0=\u00a00.88\u00a0\u00b1\u00a00.066 (95% CI\u00a0=\u00a00.87-0.9) and specificity\u00a0=\u00a00.89\u00a0\u00b1\u00a00.07 (95% CI\u00a0=\u00a00.87-0.9)).\nCombination of radiomic features and clinical data can effectively predict outcome in COVID-19 patients. The developed model has significant potential for improved management of COVID-19 patients.", "journal": "Computers in biology and medicine", "date": "2021-03-11", "authors": ["IsaacShiri", "MajidSorouri", "ParhamGeramifar", "MostafaNazari", "MohammadAbdollahi", "YazdanSalimi", "BardiaKhosravi", "DariushAskari", "LeilaAghaghazvini", "GhasemHajianfar", "AmirKasaeian", "HamidAbdollahi", "HosseinArabi", "ArmanRahmim", "Amir RezaRadmard", "HabibZaidi"], "doi": "10.1016/j.compbiomed.2021.104304\n10.1016/j.matpr.2020.09.352"}
{"title": "Prediction of Patient Management in COVID-19 Using Deep Learning-Based Fully Automated Extraction of Cardiothoracic CT Metrics and Laboratory Findings.", "abstract": "To extract pulmonary and cardiovascular metrics from chest CTs of patients with coronavirus disease 2019 (COVID-19) using a fully automated deep learning-based approach and assess their potential to predict patient management.\nAll initial chest CTs of patients who tested positive for severe acute respiratory syndrome coronavirus 2 at our emergency department between March 25 and April 25, 2020, were identified (n = 120). Three patient management groups were defined: group 1 (outpatient), group 2 (general ward), and group 3 (intensive care unit [ICU]). Multiple pulmonary and cardiovascular metrics were extracted from the chest CT images using deep learning. Additionally, six laboratory findings indicating inflammation and cellular damage were considered. Differences in CT metrics, laboratory findings, and demographics between the patient management groups were assessed. The potential of these parameters to predict patients' needs for intensive care (yes/no) was analyzed using logistic regression and receiver operating characteristic curves. Internal and external validity were assessed using 109 independent chest CT scans.\nWhile demographic parameters alone (sex and age) were not sufficient to predict ICU management status, both CT metrics alone (including both pulmonary and cardiovascular metrics; area under the curve [AUC] = 0.88; 95% confidence interval [CI] = 0.79-0.97) and laboratory findings alone (C-reactive protein, lactate dehydrogenase, white blood cell count, and albumin; AUC = 0.86; 95% CI = 0.77-0.94) were good classifiers. Excellent performance was achieved by a combination of demographic parameters, CT metrics, and laboratory findings (AUC = 0.91; 95% CI = 0.85-0.98). Application of a model that combined both pulmonary CT metrics and demographic parameters on a dataset from another hospital indicated its external validity (AUC = 0.77; 95% CI = 0.66-0.88).\nChest CT of patients with COVID-19 contains valuable information that can be accessed using automated image analysis. These metrics are useful for the prediction of patient management.", "journal": "Korean journal of radiology", "date": "2021-03-10", "authors": ["ThomasWeikert", "SaikiranRapaka", "SasaGrbic", "ThomasRe", "ShikhaChaganti", "David JWinkel", "ConstantinAnastasopoulos", "TiloNiemann", "Benedikt JWiggli", "JensBremerich", "RaphaelTwerenbold", "GregorSommer", "DorinComaniciu", "Alexander WSauter"], "doi": "10.3348/kjr.2020.0994"}
{"title": "Novel coronavirus (COVID-19) diagnosis using computer vision and artificial intelligence techniques: a review.", "abstract": "The universal transmission of pandemic COVID-19 (Coronavirus) causes an immediate need to commit in the fight across the whole human population. The emergencies for human health care are limited for this abrupt outbreak and abandoned environment. In this situation, inventive automation like computer vision (machine learning, deep learning, artificial intelligence), medical imaging (computed tomography, X-Ray) has developed an encouraging solution against COVID-19. In recent months, different techniques using image processing are done by various researchers. In this paper, a major review on image acquisition, segmentation, diagnosis, avoidance, and management are presented. An analytical comparison of the various proposed algorithm by researchers for coronavirus has been carried out. Also, challenges and motivation for research in the future to deal with coronavirus are indicated. The clinical impact and use of computer vision and deep learning were discussed and we hope that dermatologists may have better understanding of these areas from the study.", "journal": "Multimedia tools and applications", "date": "2021-03-10", "authors": ["AnujaBhargava", "AtulBansal"], "doi": "10.1007/s11042-021-10714-5\n10.1007/s10916-019-1466-3\n10.1007/s11042-019-07875-9\n10.1007/s00521-019-04514-0\n10.1007/s10916-019-1413-3"}
{"title": "QIBA guidance: Computed tomography imaging for COVID-19 quantitative imaging applications.", "abstract": "As the COVID-19 pandemic impacts global populations, computed tomography (CT) lung imaging is being used in many countries to help manage patient care as well as to rapidly identify potentially useful quantitative COVID-19 CT imaging biomarkers. Quantitative COVID-19 CT imaging applications, typically based on computer vision modeling and artificial intelligence algorithms, include the potential for better methods to assess COVID-19 extent and severity, assist with differential diagnosis of COVID-19 versus other respiratory conditions, and predict disease trajectory. To help accelerate the development of robust quantitative imaging algorithms and tools, it is critical that CT imaging is obtained following best practices of the quantitative lung CT imaging community. Toward this end, the Radiological Society of North America's (RSNA) Quantitative Imaging Biomarkers Alliance (QIBA) CT Lung Density Profile Committee and CT Small Lung Nodule Profile Committee developed a set of best practices to guide clinical sites using quantitative imaging solutions and to accelerate the international development of quantitative CT algorithms for COVID-19. This guidance document provides quantitative CT lung imaging recommendations for COVID-19 CT imaging, including recommended CT image acquisition settings for contemporary CT scanners. Additional best practice guidance is provided on scientific publication reporting of quantitative CT imaging methods and the importance of contributing COVID-19 CT imaging datasets to open science research databases.", "journal": "Clinical imaging", "date": "2021-03-09", "authors": ["Ricardo SAvila", "Sean BFain", "ChuckHatt", "Samuel GArmato", "James LMulshine", "DavidGierada", "MarioSilva", "David ALynch", "Eric AHoffman", "Frank NRanallo", "John RMayo", "DavidYankelevitz", "Raul San JoseEstepar", "RajaSubramaniam", "Claudia IHenschke", "AlexGuimaraes", "Daniel CSullivan"], "doi": "10.1016/j.clinimag.2021.02.017"}
{"title": "An integrated autoencoder-based hybrid CNN-LSTM model for COVID-19 severity prediction from lung ultrasound.", "abstract": "The COVID-19 pandemic has become one of the biggest threats to the global healthcare system, creating an unprecedented condition worldwide. The necessity of rapid diagnosis calls for alternative methods to predict the condition of the patient, for which disease severity estimation on the basis of Lung Ultrasound (LUS) can be a safe, radiation-free, flexible, and favorable option. In this paper, a frame-based 4-score disease severity prediction architecture is proposed with the integration of deep convolutional and recurrent neural networks to consider both spatial and temporal features of the LUS frames. The proposed convolutional neural network (CNN) architecture implements an autoencoder network and separable convolutional branches fused with a modified DenseNet-201 network to build a vigorous, noise-free classification model. A five-fold cross-validation scheme is performed to affirm the efficacy of the proposed network. In-depth result analysis shows a promising improvement in the classification performance by introducing the Long Short-Term Memory (LSTM) layers after the proposed CNN architecture by an average of 7-12%, which is approximately 17% more than the traditional DenseNet architecture alone. From an extensive analysis, it is found that the proposed end-to-end scheme is very effective in detecting COVID-19 severity scores from LUS images.", "journal": "Computers in biology and medicine", "date": "2021-03-09", "authors": ["Ankan GhoshDastider", "FarhanSadik", "Shaikh AnowarulFattah"], "doi": "10.1016/j.compbiomed.2021.104296"}
{"title": "Temporal changes of quantitative CT findings from 102 patients with COVID-19 in Wuhan, China: A longitudinal study.", "abstract": "Computed tomography (CT) imaging combined with artificial intelligence is important in the diagnosis and prognosis of lung diseases.\nThis study aimed to investigate temporal changes of quantitative CT findings in patients with COVID-19 in three clinic types, including moderate, severe, and non-survivors, and to predict severe cases in the early stage from the results.\nOne hundred and two patients with confirmed COVID-19 were included in this study. Based on the time interval between onset of symptoms and the CT scan, four stages were defined in this study: Stage-1 (0 \u223c7 days); Stage-2 (8 \u223c 14 days); Stage-3 (15 \u223c 21days); Stage-4 (> 21 days). Eight parameters, the infection volume and percentage of the whole lung in four different Hounsfield (HU) ranges, ((-, -750), [-750, -300), [-300, 50) and [50, +)), were calculated and compared between different groups.\nThe infection volume and percentage of four HU ranges peaked in Stage-2. The highest proportion of HU [-750, 50) was found in the infected regions in non-survivors among three groups.\nThe findings indicate rapid deterioration in the first week since the onset of symptoms in non-survivors. Higher proportion of HU [-750, 50) in the lesion area might be a potential bio-marker for poor prognosis in patients with COVID-19.", "journal": "Technology and health care : official journal of the European Society for Engineering and Medicine", "date": "2021-03-09", "authors": ["XiaohuiChen", "WenboSun", "DanXu", "JiaojiaoMa", "FengXiao", "HaiboXu"], "doi": "10.3233/THC-218027"}
{"title": "COVID-19 classification using deep feature concatenation technique.", "abstract": "Detecting COVID-19 from medical images is a challenging task that has excited scientists around the world. COVID-19 started in China in 2019, and it is still spreading even now. Chest X-ray and Computed Tomography (CT) scan are the most important imaging techniques for diagnosing COVID-19. All researchers are looking for effective solutions and fast treatment methods for this epidemic. To reduce the need for medical experts, fast and accurate automated detection techniques are introduced. Deep learning convolution neural network (DL-CNN) technologies are showing remarkable results for detecting cases of COVID-19. In this paper, deep feature concatenation (DFC) mechanism is utilized in two different ways. In the first one, DFC links deep features extracted from X-ray and\u00a0CT\u00a0scan using a simple proposed CNN. The other way depends on DFC to combine features extracted from either X-ray or CT scan\u00a0using the proposed CNN architecture and two modern pre-trained CNNs:\u00a0ResNet and GoogleNet. The DFC mechanism is applied to form a definitive classification descriptor. The proposed CNN architecture consists of three deep layers to overcome the problem of large time consumption. For each image type, the proposed CNN performance is studied using different optimization algorithms and different values for the maximum number of epochs, the learning rate (LR), and mini-batch (M-B)\u00a0size. Experiments have demonstrated the superiority of the proposed approach compared to other modern and state-of-the-art methodologies in terms of accuracy, precision, recall and f_score.", "journal": "Journal of ambient intelligence and humanized computing", "date": "2021-03-09", "authors": ["WaleedSaad", "Wafaa AShalaby", "MonaShokair", "Fathi AbdEl-Samie", "MoawadDessouky", "EssamAbdellatef"], "doi": "10.1007/s12652-021-02967-7\n10.1504/IJHPCN.2018.096726\n10.1001/jamanetworkopen.2019.2561\n10.1109/ACCESS.2014.2325029\n10.1038/s41598-016-0001-8\n10.1016/j.patrec.2019.11.015\n10.1109/MCOM.2018.1700817\n10.4018/IJSSCI.2018040103\n10.1016/S0140-6736(20)30183-5\n10.1109/34.216733\n10.1109/JSEN.2019.2913281\n10.1109/ACCESS.2019.2936215\n10.1109/TIP.2007.908073\n10.1186/s40537-014-0007-7\n10.1109/ACCESS.2020.2978629\n10.1504/IJHPCN.2019.10022723\n10.1016/j.imu.2020.100360\n10.1016/j.jocs.2018.12.003\n10.3389/fnins.2019.00095\n10.1007/s12098-020-03263-6\n10.1148/radiol.2020200343\n10.1148/radiol.2020200343\n10.1016/j.eng.2020.04.010\n10.1109/ACCESS.2017.2775038"}
{"title": "Deep Learning-Driven Automated Detection of COVID-19 from Radiography Images: a Comparative Analysis.", "abstract": "The COVID-19 pandemic has wreaked havoc on the whole world, taking over half a million lives and capsizing the world economy in unprecedented magnitudes. With the world scampering for a possible vaccine, early detection and containment are the only redress. Existing diagnostic technologies with high accuracy like RT-PCRs are expensive and sophisticated, requiring skilled individuals for specimen collection and screening, resulting in lower outreach. So, methods excluding direct human intervention are much sought after, and artificial intelligence-driven automated diagnosis, especially with radiography images, captured the researchers' interest. This survey marks a detailed inspection of the deep learning-based automated detection of COVID-19 works done to date, a comparison of the available datasets, methodical challenges like imbalanced datasets and others, along with probable solutions with different preprocessing methods, and scopes of future exploration in this arena. We also benchmarked the performance of 315 deep models in diagnosing COVID-19, normal, and pneumonia from X-ray images of a custom dataset created from four others. The dataset is publicly available at https://github.com/rgbnihal2/COVID-19-X-ray-Dataset. Our results show that DenseNet201 model with Quadratic SVM classifier performs the best (accuracy: 98.16%, sensitivity: 98.93%, specificity: 98.77%) and maintains high accuracies in other similar architectures as well. This proves that even though radiography images might not be conclusive for radiologists, but it is so for deep learning algorithms for detecting COVID-19. We hope this extensive review will provide a comprehensive guideline for researchers in this field.", "journal": "Cognitive computation", "date": "2021-03-09", "authors": ["SejutiRahman", "SujanSarker", "Md Abdullah AlMiraj", "Ragib AminNihal", "A K MNadimul Haque", "Abdullah AlNoman"], "doi": "10.1007/s12559-020-09779-5\n10.1056/NEJMoa2002032\n10.1002/aisy.202000071\n10.7326/M20-0504\n10.1016/S1473-3099(20)30237-1\n10.1093/cid/ciaa310\n10.1016/j.jrid.2020.04.001\n10.1148/radiol.2020200905\n10.1016/j.compbiomed.2020.103795\n10.1007/s10489-020-01770-9\n10.1016/j.jacr.2020.02.008\n10.1097/RLI.0000000000000670\n10.1148/radiol.2020200432\n10.1016/j.clinimag.2020.04.001\n10.1016/j.neucom.2015.09.116\n10.1016/j.mehy.2020.109761\n10.1145/3386252\n10.1145/1007730.1007733\n10.1613/jair.953\n10.1109/TMI.2020.2992546\n10.1007/s40846-020-00529-4\n10.5455/jjee.204-1585312246\n10.1007/s13246-020-00865-4\n10.1016/j.media.2016.11.002\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2020.105532\n10.1093/oxfordjournals.pan.a004868\n10.1016/j.procs.2015.08.087\n10.22237/jmasm/1051747320\n10.1109/21.97458"}
{"title": "DeepAlign, a 3D alignment method based on regionalized deep learning for Cryo-EM.", "abstract": "Cryo Electron Microscopy (Cryo-EM) is currently one of the main tools to reveal the structural information of biological specimens at high resolution. Despite the great development of the techniques involved to solve the biological structures with Cryo-EM in the last years, the reconstructed 3D maps can present lower resolution due to errors committed while processing the information acquired by the microscope. One of the main problems comes from the 3D alignment step, which is an error-prone part of the reconstruction workflow due to the very low signal-to-noise ratio (SNR) common in Cryo-EM imaging. In fact, as we will show in this work, it is not unusual to find a disagreement in the alignment parameters in approximately 20-40% of the processed images, when outputs of different alignment algorithms are compared. In this work, we present a novel method to align sets of single particle images in the 3D space, called DeepAlign. Our proposal is based on deep learning networks that have been successfully used in plenty of problems in image classification. Specifically, we propose to design several deep neural networks on a regionalized basis to classify the particle images in sub-regions and, then, make a refinement of the 3D alignment parameters only inside that sub-region. We show that this method results in accurately aligned images, improving the Fourier shell correlation (FSC) resolution obtained with other state-of-the-art methods while decreasing computational time.", "journal": "Journal of structural biology", "date": "2021-03-07", "authors": ["AJim\u00e9nez-Moreno", "DSt\u0159el\u00e1k", "JFilipovi\u010d", "J MCarazo", "C O SSorzano"], "doi": "10.1016/j.jsb.2021.107712"}
{"title": "Efficient COVID-19 Segmentation from CT Slices Exploiting Semantic Segmentation with Integrated Attention Mechanism.", "abstract": "Coronavirus (COVID-19) is a pandemic, which caused suddenly unexplained pneumonia cases and caused a devastating effect on global public health. Computerized tomography (CT) is one of the most effective tools for COVID-19 screening. Since some specific patterns such as bilateral, peripheral, and basal predominant ground-glass opacity, multifocal patchy consolidation, crazy-paving pattern with a peripheral distribution can be observed in CT images and these patterns have been declared as the findings of COVID-19 infection. For patient monitoring, diagnosis and segmentation of COVID-19, which spreads into the lung, expeditiously and accurately from CT, will provide vital information about the stage of the disease. In this work, we proposed a SegNet-based network using the attention gate (AG) mechanism for the automatic segmentation of COVID-19 regions in CT images. AGs can be easily integrated into standard convolutional neural network (CNN) architectures with a minimum computing load as well as increasing model precision and predictive accuracy. Besides, the success of the proposed network has been evaluated based on dice, Tversky, and focal Tversky loss functions to deal with low sensitivity arising from the small lesions. The experiments were carried out using a fivefold cross-validation technique on a COVID-19 CT segmentation database containing 473 CT images. The obtained sensitivity, specificity, and dice scores were reported as 92.73%, 99.51%, and 89.61%, respectively. The superiority of the proposed method has been highlighted by comparing with the results reported in previous studies and it is thought that it will be an auxiliary tool that accurately detects automatic COVID-19 regions from CT images.", "journal": "Journal of digital imaging", "date": "2021-03-07", "authors": ["\u00dcmitBudak", "Musa\u00c7\u0131buk", "ZaferC\u00f6mert", "Abdulkadir\u015eeng\u00fcr"], "doi": "10.1007/s10278-021-00434-5\n10.1148/radiol.2020200257\n10.1101/2020.03.12.20027185\n10.1016/j.mehy.2020.109761\n10.1016/j.jfma.2020.04.007\n10.1016/j.ejrad.2020.108972\n10.1016/j.jpha.2020.03.004\n10.1016/j.irbm.2019.10.006\n10.20944/preprints202003.0300.v1\n10.1016/j.compbiomed.2020.103805\n10.1016/j.mehy.2019.109431\n10.1109/TPAMI.2016.2644615\n10.1016/j.bspc.2019.101734\n10.1148/ryct.2020200082\n10.1148/ryct.2020200075\n10.1101/2020.03.19.20039354\n10.1016/j.mehy.2019.109426\n10.1016/j.asoc.2019.105765\n10.3174/ajnr.A5543\n10.1007/s13755-018-0057-x\n10.1109/TMI.2013.2265805\n10.1016/j.cmpb.2018.10.021\n10.1016/j.measurement.2018.05.003\n10.1037/0033-295X.84.4.327"}
{"title": "Development of a convolutional neural network to differentiate among the etiology of similar appearing pathological B lines on lung ultrasound: a deep learning study.", "abstract": "Lung ultrasound (LUS) is a portable, low-cost respiratory imaging tool but is challenged by user dependence and lack of diagnostic specificity. It is unknown whether the advantages of LUS implementation could be paired with deep learning (DL) techniques to match or exceed human-level, diagnostic specificity among similar appearing, pathological LUS images.\nA convolutional neural network (CNN) was trained on LUS images with B lines of different aetiologies. CNN diagnostic performance, as validated using a 10% data holdback set, was compared with surveyed LUS-competent physicians.\nTwo tertiary Canadian hospitals.\n612 LUS videos (121\u2009381 frames) of B lines from 243 distinct patients with either (1) COVID-19 (COVID), non-COVID acute respiratory distress syndrome (NCOVID) or (3) hydrostatic pulmonary edema (HPE).\nThe trained CNN performance on the independent dataset showed an ability to discriminate between COVID (area under the receiver operating characteristic curve (AUC) 1.0), NCOVID (AUC 0.934) and HPE (AUC 1.0) pathologies. This was significantly better than physician ability (AUCs of 0.697, 0.704, 0.967 for the COVID, NCOVID and HPE classes, respectively), p<0.01.\nA DL model can distinguish similar appearing LUS pathology, including COVID-19, that cannot be distinguished by humans. The performance gap between humans and the model suggests that subvisible biomarkers within ultrasound images could exist and multicentre research is merited.", "journal": "BMJ open", "date": "2021-03-07", "authors": ["RobertArntfield", "BlakeVanBerlo", "ThamerAlaifan", "NathanPhelps", "MatthewWhite", "RushilChaudhary", "JordanHo", "DerekWu"], "doi": "10.1136/bmjopen-2020-045120\n10.1097/MD.0000000000005713\n10.1378/chest.07-2800\n10.1016/s0196-0644(97)70341-x\n10.1016/S2213-2600(20)30120-X\n10.21037/jtd.2016.04.55\n10.1186/1476-7120-6-16\n10.1007/s00134-020-06005-6\n10.1002/jum.14627\n10.1016/S0140-6736(18)31645-3\n10.1001/jama.2016.17216\n10.1016/j.ejca.2019.04.001\n10.1038/s41551-018-0195-0\n10.1101/2020.02.23.20026930v1\n10.1016/j.jcrc.2014.12.006\n10.1007/s11548-018-1843-2\n10.1109/CVPR.2017.195\n10.1023/A:1020281327116\n10.1109/TUFFC.2020.3002249\n10.1109/TMI.2020.2994459\n10.1007/s13246-020-00865-4\n10.1159/000509223\n10.1186/s13054-019-2569-4\n10.1016/j.chest.2016.04.013\n10.1155/2017/8147075\n10.1007/s00134-020-06212-1\n10.1109/JBHI.2019.2936151\n10.1002/jum.15285"}
{"title": "COVID-19 Detection from Chest X-ray Images Using Feature Fusion and Deep Learning.", "abstract": "Currently, COVID-19 is considered to be the most dangerous and deadly disease for the human body caused by the novel coronavirus. In December 2019, the coronavirus spread rapidly around the world, thought to be originated from Wuhan in China and is responsible for a large number of deaths. Earlier detection of the COVID-19 through accurate diagnosis, particularly for the cases with no obvious symptoms, may decrease the patient's death rate. Chest X-ray images are primarily used for the diagnosis of this disease. This research has proposed a machine vision approach to detect COVID-19 from the chest X-ray images. The features extracted by the histogram-oriented gradient (HOG) and convolutional neural network (CNN) from X-ray images were fused to develop the classification model through training by CNN (VGGNet). Modified anisotropic diffusion filtering (MADF) technique was employed for better edge preservation and reduced noise from the images. A watershed segmentation algorithm was used in order to mark the significant fracture region in the input X-ray images. The testing stage considered generalized data for performance evaluation of the model. Cross-validation analysis revealed that a 5-fold strategy could successfully impair the overfitting problem. This proposed feature fusion using the deep learning technique assured a satisfactory performance in terms of identifying COVID-19 compared to the immediate, relevant works with a testing accuracy of 99.49%, specificity of 95.7% and sensitivity of 93.65%. When compared to other classification techniques, such as ANN, KNN, and SVM, the CNN technique used in this study showed better classification performance. K-fold cross-validation demonstrated that the proposed feature fusion technique (98.36%) provided higher accuracy than the individual feature extraction methods, such as HOG (87.34%) or CNN (93.64%).", "journal": "Sensors (Basel, Switzerland)", "date": "2021-03-07", "authors": ["Nur-A-Alam", "MominulAhsan", "Md AbdulBased", "JulfikarHaider", "MarcinKowalski"], "doi": "10.3390/s21041480\n10.1038/s41586-020-2008-3\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(20)30185-9\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1056/NEJMoa2001191\n10.1038/d41586-020-03441-8\n10.1038/d41586-020-03334-w\n10.1038/s41591-021-01230-y\n10.1016/S0140-6736(21)00234-8\n10.14218/ERHM.2020.00075\n10.3390/s18020556\n10.1016/j.neucom.2018.06.084\n10.1038/s41598-019-55972-4\n10.1038/nature21056\n10.1038/s41598-020-76550-z\n10.1016/j.jare.2020.08.002\n10.1136/bmj.d947\n10.1183/09031936.01.00213501\n10.1148/ryct.2020200034\n10.1109/RBME.2020.2987975\n10.1007/s13755-020-00119-3\n10.1007/s10489-020-01829-7\n10.1155/2020/8828855\n10.3390/electronics9091439\n10.3390/info11090419\n10.3390/sym12040651\n10.1016/j.media.2020.101794\n10.1177/2472630320958376\n10.3389/fmed.2020.00427\n10.1007/s11051-020-05041-z\n10.1371/journal.pone.0242535\n10.1038/s41598-020-71294-2\n10.1136/bmj.m2426\n10.3390/s19143164\n10.1109/TIP.2014.2371244\n10.1109/ACCESS.2018.2813395\n10.1007/s11042-020-09829-y\n10.1007/s42452-020-2944-4\n10.1016/j.cogsys.2019.09.007\n10.1016/j.precisioneng.2020.02.005\n10.1155/2017/4582948\n10.1016/j.jksuci.2019.12.013\n10.1007/s10489-020-02019-1\n10.1016/j.chaos.2020.109944\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105581\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103869\n10.18517/ijaseit.10.2.11446\n10.1016/j.chaos.2020.110122\n10.3390/app10165683\n10.3390/ijerph17186933"}
{"title": "A Deep Learning-Based Camera Approach for Vital Sign Monitoring Using Thermography Images for ICU Patients.", "abstract": "Infrared thermography for camera-based skin temperature measurement is increasingly used in medical practice, e.g., to detect fevers and infections, such as recently in the COVID-19 pandemic. This contactless method is a promising technology to continuously monitor the vital signs of patients in clinical environments. In this study, we investigated both skin temperature trend measurement and the extraction of respiration-related chest movements to determine the respiratory rate using low-cost hardware in combination with advanced algorithms. In addition, the frequency of medical examinations or visits to the patients was extracted. We implemented a deep learning-based algorithm for real-time vital sign extraction from thermography images. A clinical trial was conducted to record data from patients on an intensive care unit. The YOLOv4-Tiny object detector was applied to extract image regions containing vital signs (head and chest). The infrared frames were manually labeled for evaluation. Validation was performed on a hold-out test dataset of 6 patients and revealed good detector performance (0.75 intersection over union, 0.94 mean average precision). An optical flow algorithm was used to extract the respiratory rate from the chest region. The results show a mean absolute error of 2.69 bpm. We observed a computational performance of 47 fps on an NVIDIA Jetson Xavier NX module for YOLOv4-Tiny, which proves real-time capability on an embedded GPU system. In conclusion, the proposed method can perform real-time vital sign extraction on a low-cost system-on-module and may thus be a useful method for future contactless vital sign measurements.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-03-07", "authors": ["SimonLyra", "LeonMayer", "LiyangOu", "DavidChen", "PaddyTimms", "AndrewTay", "Peter YChan", "BergitaGanse", "SteffenLeonhardt", "ChristophHoog Antink"], "doi": "10.3390/s21041495\n10.1097/MD.0000000000016261\n10.1097/CCM.0b013e31822f061d\n10.1159/000505126\n10.5694/j.1326-5377.2008.tb01825.x\n10.1787/health_glance_eur-2018-12-en\n10.1016/j.aenj.2016.12.003\n10.1111/j.1552-6909.2001.tb01520.x\n10.1186/1475-925X-10-93\n10.1111/j.1469-8986.2010.01167.x\n10.1364/BOE.6.004378\n10.5772/intechopen.80652\n10.1183/13993003.congress-2015.PA1260\n10.1109/cvpr.2015.7298682\n10.3390/app9204405\n10.1016/j.infrared.2019.103117\n10.1088/2057-1976/ab7a54\n10.1371/journal.pone.0224747\n10.1364/BOE.397188\n10.1007/s10877-019-00437-2\n10.1007/s11517-020-02251-4\n10.1109/CVPRW50498.2020.00061\n10.1109/IJCNN.2018.8489367\n10.1117/1.JBO.25.9.097002"}
{"title": "Does non-COVID-19 lung lesion help? investigating transferability in COVID-19 CT image segmentation.", "abstract": "Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading all around the world. Deep learning has been adopted as an effective technique to aid COVID-19 detection and segmentation from computed tomography (CT) images. The major challenge lies in the inadequate public COVID-19 datasets. Recently, transfer learning has become a widely used technique that leverages the knowledge gained while solving one problem and applying it to a different but related problem. However, it remains unclear whether various non-COVID19 lung lesions could contribute to segmenting COVID-19 infection areas and how to better conduct this transfer procedure. This paper provides a way to understand the transferability of non-COVID19 lung lesions and a better strategy to train a robust deep learning model for COVID-19 infection segmentation.\nBased on a publicly available COVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four transfer learning methods using 3D U-Net as a standard encoder-decoder method. i) We introduce the multi-task learning method to get a multi-lesion pre-trained model for COVID-19 infection. ii) We propose and compare four transfer learning strategies with various performance gains and training time costs. Our proposed Hybrid-encoder Learning strategy introduces a Dedicated-encoder and an Adapted-encoder to extract COVID-19 infection features and general lung lesion features, respectively. An attention-based Selective Fusion unit is designed for dynamic feature selection and aggregation.\nExperiments show that trained with limited data, proposed Hybrid-encoder strategy based on multi-lesion pre-trained model achieves a mean DSC, NSD, Sensitivity, F1-score, Accuracy and MCC of 0.704, 0.735, 0.682, 0.707, 0.994 and 0.716, respectively, with better genetalization and lower over-fitting risks for segmenting COVID-19 infection.\nThe results reveal the benefits of transferring knowledge from non-COVID19 lung lesions, and learning from multiple lung lesion datasets can extract more general features, leading to accurate and robust pre-trained models. We further show the capability of the encoder to learn feature representations of lung lesions, which improves segmentation accuracy and facilitates training convergence. In addition, our proposed Hybrid-encoder learning method incorporates transferred lung lesion features from non-COVID19 datasets effectively and achieves significant improvement. These findings promote new insights into transfer learning for COVID-19 CT image segmentation, which can also be further generalized to other medical tasks.", "journal": "Computer methods and programs in biomedicine", "date": "2021-03-05", "authors": ["YixinWang", "YaoZhang", "YangLiu", "JiangTian", "ChengZhong", "ZhongchaoShi", "YangZhang", "ZhiqiangHe"], "doi": "10.1016/j.cmpb.2021.106004\n10.1128/JCM.00310-20\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2020.105532\n10.1109/TMI.2020.3001810\n10.1148/radiol.2020200642\n10.1148/radiol.2020200230\n10.1148/radiol.2020200905\n10.1101/2020.03.19.20039354\n10.1109/TMI.2020.3000314\n10.1101/2020.03.12.20027185\n10.1049/iet-ipr.2019.0312\n10.1002/cnm.3225\n10.1109/IPTA.2019.8936087\n10.1016/j.compbiomed.2020.104118\n10.1109/IPTA.2019.8936083\n10.1007/978-3-030-32040-9_25\n10.1016/j.media.2019.03.009\n10.1101/2020.02.29.20029603\n10.1148/ryct.2020200075\n10.1109/jbhi.2017.2769800\n10.1080/21681163.2016.1138324\n10.1080/21681163.2015.1124249\n10.3390/app10020559\n10.1007/s13246-020-00865-4\n10.1101/2020.05.12.20098954\n10.5281/zenodo.3757476\n10.7937/K9/TCIA.2014.X7ONY6B1\n10.1038/sdata.2018.202\n10.1148/radiol.12111607\n10.1007/s10278-004-1014-6"}
{"title": "Artificial Intelligence Clinicians Can Use Chest Computed Tomography Technology to Automatically Diagnose Coronavirus Disease 2019 (COVID-19) Pneumonia and Enhance Low-Quality Images.", "abstract": "Nowadays, the number of patients with COVID-19 pneumonia worldwide is still increasing. The clinical diagnosis of COVID-19 pneumonia faces challenges, such as the difficulty to perform RT-PCR tests in real time, the lack of experienced radiologists, clinical low-quality images, and the similarity of imaging features of community-acquired pneumonia and COVID-19. Therefore, we proposed an artificial intelligence model GARCD that uses chest CT images to assist in the diagnosis of COVID-19 in real time. It can show better diagnostic performance even facing low-quality CT images.\nWe used 14,129 CT images from 104 patients. A total of 12,929 samples were used to build artificial intelligence models, and 1200 samples were used to test its performance. The image quality improvement module is based on the generative adversarial structure. It improves the quality of the input image under the joint drive of feature loss and content loss. The enhanced image is sent to the disease diagnosis model based on residual convolutional network. It automatically extracts the semantic features of the image and then infers the probability that the sample belongs to COVID-19. The ROC curve is used to evaluate the performance of the model.\nThis model can effectively enhance the low-quality image and make the image that is difficult to be recognized become recognizable. The model proposed in this paper reached 97.8% AUC, 96.97% sensitivity and 91.16% specificity in an independent test set. ResNet, GADCD, CNN, and DenseNet achieved 80.9%, 97.3%, 70.7% and 85.7% AUC in the same test set, respectively. By comparing the performance with related works, it is proved that the model proposed has stronger clinical usability.\nThe method proposed can effectively assist doctors in real-time detection of suspected cases of COVID-19 pneumonia even faces unclear image. It can quickly isolate patients in a targeted manner, which is of positive significance for preventing the further spread of COVID-19 pneumonia.", "journal": "Infection and drug resistance", "date": "2021-03-05", "authors": ["QuanZhang", "ZhuoChen", "GuohuaLiu", "WenjiaZhang", "QianDu", "JiayuanTan", "QianqianGao"], "doi": "10.2147/IDR.S296346\n10.1590/1806-9282.66.7.880\n10.2147/IDR.S258677\n10.1016/j.acra.2020.04.031\n10.1148/radiol.2020201473\n10.1002/jmv.25735\n10.1007/s00330-020-06975-7\n10.1093/cid/ciaa247\n10.1148/radiol.2020200280\n10.1007/s00259-020-04929-1\n10.1016/S0140-6736(19)32501-2\n10.1016/j.cell.2018.02.010\n10.1016/S0140-6736(18)31645-3\n10.2147/DMSO.S288419\n10.3390/diagnostics10110901\n10.1109/TMM.2020.3037535\n10.1109/TMM.2019.2943750\n10.21203/rs.3.rs-21834/v1\n10.1148/radiol.2020200490\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.1148/radiol.2020202439\n10.1148/radiol.2020203511\n10.1016/j.ejrad.2020.109402\n10.1016/j.ejrad.2020.108991\n10.1136/fmch-2020-000406"}
{"title": "Epidemiological Surveillance of the Impact of the COVID-19 Pandemic on Stroke Care Using Artificial Intelligence.", "abstract": "The degree to which the coronavirus disease 2019 (COVID-19) pandemic has affected systems of care, in particular, those for time-sensitive conditions such as stroke, remains poorly quantified. We sought to evaluate the impact of COVID-19 in the overall screening for acute stroke utilizing a commercial clinical artificial intelligence platform.\nData were derived from the Viz Platform, an artificial intelligence application designed to optimize the workflow of patients with acute stroke. Neuroimaging data on suspected patients with stroke across 97 hospitals in 20 US states were collected in real time and retrospectively analyzed with the number of patients undergoing imaging screening serving as a surrogate for the amount of stroke care. The main outcome measures were the number of computed tomography (CT) angiography, CT perfusion, large vessel occlusions (defined according to the automated software detection), and severe strokes on CT perfusion (defined as those with hypoperfusion volumes >70 mL) normalized as number of patients per day per hospital. Data from the prepandemic (November 4, 2019 to February 29, 2020) and pandemic (March 1 to May 10, 2020) periods were compared at national and state levels. Correlations were made between the inter-period changes in imaging screening, stroke hospitalizations, and thrombectomy procedures using state-specific sampling.\nA total of 23\u2009223 patients were included. The incidence of large vessel occlusion on CT angiography and severe strokes on CT perfusion were 11.2% (n=2602) and 14.7% (n=1229/8328), respectively. There were significant declines in the overall number of CT angiographies (-22.8%; 1.39-1.07 patients/day per hospital, \nA significant decline in stroke imaging screening has occurred during the COVID-19 pandemic. This analysis underscores the broader application of artificial intelligence neuroimaging platforms for the real-time monitoring of stroke systems of care.", "journal": "Stroke", "date": "2021-03-05", "authors": ["Raul GNogueira", "Jason MDavies", "RishiGupta", "Ameer EHassan", "ThomasDevlin", "Diogo CHaussen", "Mahmoud HMohammaden", "Christopher PKellner", "AdamArthur", "LucasElijovich", "KumikoOwada", "DinaBegun", "MukundNarayan", "NadiaMordenfeld", "Wondwossen GTekle", "FadiNahab", "Tudor GJovin", "DonFrei", "Adnan HSiddiqui", "Michael RFrankel", "JMocco"], "doi": "10.1161/STROKEAHA.120.031960"}
{"title": "Correction to: Decoding COVID-19 pneumonia: comparison of deep learning and radiomics CT image signatures.", "abstract": null, "journal": "European journal of nuclear medicine and molecular imaging", "date": "2021-03-04", "authors": ["HongmeiWang", "LuWang", "Edward HLee", "JimmyZheng", "WeiZhang", "SafwanHalabi", "ChunleiLiu", "KexueDeng", "JiangdianSong", "Kristen WYeom"], "doi": "10.1007/s00259-021-05268-5"}
{"title": "Prognostic Implications of CT Feature Analysis in Patients with COVID-19: a Nationwide Cohort Study.", "abstract": "Few studies have classified chest computed tomography (CT) findings of coronavirus disease 2019 (COVID-19) and analyzed their correlations with prognosis. The present study aimed to evaluate retrospectively the clinical and chest CT findings of COVID-19 and to analyze CT findings and determine their relationships with clinical severity.\nChest CT and clinical features of 271 COVID-19 patients were assessed. The presence of CT findings and distribution of parenchymal abnormalities were evaluated, and CT patterns were classified as bronchopneumonia, organizing pneumonia (OP), or diffuse alveolar damage (DAD). Total extents were assessed using a visual scoring system and artificial intelligence software. Patients were allocated to two groups based on clinical outcomes, that is, to a severe group (requiring O\u2082 therapy or mechanical ventilation, n = 55) or a mild group (not requiring O\u2082 therapy or mechanical ventilation, n = 216). Clinical and CT features of these two groups were compared and univariate and multivariate logistic regression analyses were performed to identify independent prognostic factors.\nAge, lymphocyte count, levels of C-reactive protein, and procalcitonin were significantly different in the two groups. Forty-five of the 271 patients had normal chest CT findings. The most common CT findings among the remaining 226 patients were ground-glass opacity (98%), followed by consolidation (53%). CT findings were classified as OP (93%), DAD (4%), or bronchopneumonia (3%) and all nine patients with DAD pattern were included in the severe group. Uivariate and multivariate analyses showed an elevated procalcitonin (odds ratio [OR], 2.521; 95% confidence interval [CI], 1.001-6.303, \nCT findings of COVID-19 pneumonia can be classified into OP, DAD, or bronchopneumonia patterns and all patients with DAD pattern were included in severe group. Elevated inflammatory markers and higher CT scores were found to be significant predictors of poor prognosis in patients with COVID-19 pneumonia.", "journal": "Journal of Korean medical science", "date": "2021-03-03", "authors": ["Yeon JooJeong", "Bo DaNam", "Jin YoungYoo", "Kun IlKim", "HeeKang", "Jung HwaHwang", "Yun HyeonKim", "Kyung SooLee"], "doi": "10.3346/jkms.2021.36.e51"}
{"title": "Convolutional neural network model based on radiological images to support COVID-19 diagnosis: Evaluating database biases.", "abstract": "As SARS-CoV-2 has spread quickly throughout the world, the scientific community has spent major efforts on better understanding the characteristics of the virus and possible means to prevent, diagnose, and treat COVID-19. A valid approach presented in the literature is to develop an image-based method to support COVID-19 diagnosis using convolutional neural networks (CNN). Because the availability of radiological data is rather limited due to the novelty of COVID-19, several methodologies consider reduced datasets, which may be inadequate, biasing the model. Here, we performed an analysis combining six different databases using chest X-ray images from open datasets to distinguish images of infected patients while differentiating COVID-19 and pneumonia from 'no-findings' images. In addition, the performance of models created from fewer databases, which may imperceptibly overestimate their results, is discussed. Two CNN-based architectures were created to process images of different sizes (512 \u00d7 512, 768 \u00d7 768, 1024 \u00d7 1024, and 1536 \u00d7 1536). Our best model achieved a balanced accuracy (BA) of 87.7% in predicting one of the three classes ('no-findings', 'COVID-19', and 'pneumonia') and a specific balanced precision of 97.0% for 'COVID-19' class. We also provided binary classification with a precision of 91.0% for detection of sick patients (i.e., with COVID-19 or pneumonia) and 98.4% for COVID-19 detection (i.e., differentiating from 'no-findings' or 'pneumonia'). Indeed, despite we achieved an unrealistic 97.2% BA performance for one specific case, the proposed methodology of using multiple databases achieved better and less inflated results than from models with specific image datasets for training. Thus, this framework is promising for a low-cost, fast, and noninvasive means to support the diagnosis of COVID-19.", "journal": "PloS one", "date": "2021-03-02", "authors": ["Caio B SMaior", "Jo\u00e3o M MSantana", "Isis DLins", "M\u00e1rcio J CMoura"], "doi": "10.1371/journal.pone.0247839\n10.1016/j.ijantimicag.2020.105924\n10.1093/cid/ciaa344\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1016/j.mayocp.2020.04.004\n10.1148/radiol.2020200230\n10.1016/j.jinf.2020.03.007\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2002032\n10.2214/AJR.20.22954\n10.1148/radiol.2020200905\n10.1148/radiol.2020201160\n10.1164/rccm.201501-0017OC\n10.1097/SHK.0000000000001273\n10.11152/mu-1885\n10.1148/radiol.2019190613\n10.1016/j.jacr.2017.12.028\n10.1371/journal.pone.0235187\n10.1002/qre.1221\n10.17531/ein.2019.4.10\n10.1016/j.eswa.2020.113505\n10.1016/j.measurement.2016.07.054\n10.1038/nature14236\n10.1155/2017/5067651\n10.1371/journal.pone.0219570\n10.1177/1475921718788299\n10.1371/journal.pone.0233514\n10.1186/s40537-019-0276-2\n10.1016/j.imu.2020.100306\n10.1016/j.patcog.2018.05.014\n10.1016/j.eng.2020.04.010\n10.1007/s10096-020-03901-z\n10.1101/2020.02.14.20023028\n10.1007/s10489-020-01714-3\n10.3390/v12070769\n10.3390/ai1030027\n10.1016/j.eswa.2018.10.010\n10.1371/journal.pone.0242535\n10.1109/ACCESS.2020.3016780\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2020.103792\n10.3390/sym12040651\n10.1007/s13246-020-00865-4\n10.1016/j.mehy.2020.109761\n10.1148/ryai.2019180041\n10.1186/s40537-019-0197-0\n10.1109/ACCESS.2019.2919678\n10.1016/j.eswa.2017.11.028\n10.1016/j.neunet.2015.09.014\n10.1007/s00521-018-3924-0\n10.1016/j.biosystemseng.2016.08.024\n10.1080/2150704X.2016.1196837"}
{"title": "Accurately Discriminating COVID-19 from Viral and Bacterial Pneumonia According to CT Images Via Deep Learning.", "abstract": "Computed tomography (CT) is one of the most efficient diagnostic methods for rapid diagnosis of the widespread COVID-19. However, reading CT films brings a lot of concentration and time for doctors. Therefore, it is necessary to develop an automatic CT image diagnosis system to assist doctors in diagnosis. Previous studies devoted to COVID-19 in the past months focused mostly on discriminating COVID-19 infected patients from healthy persons and/or bacterial pneumonia patients, and have ignored typical viral pneumonia since it is hard to collect samples for viral pneumonia that is less frequent in adults. In addition, it is much more challenging to discriminate COVID-19 from typical viral pneumonia as COVID-19 is also a kind of virus. In this study, we have collected CT images of 262, 100, 219, and 78 persons for COVID-19, bacterial pneumonia, typical viral pneumonia, and healthy controls, respectively. To the best of our knowledge, this was the first study of quaternary classification to include also typical viral pneumonia. To effectively capture the subtle differences in CT images, we have constructed a new model by combining the ResNet50 backbone with SE blocks that was recently developed for fine image analysis. Our model was shown to outperform commonly used baseline models, achieving an overall accuracy of 0.94 with AUC of 0.96, recall of 0.94, precision of 0.95, and F1-score of 0.94. The model is available in https://github.com/Zhengfudan/COVID-19-Diagnosis-and-Pneumonia-Classification .", "journal": "Interdisciplinary sciences, computational life sciences", "date": "2021-03-01", "authors": ["FudanZheng", "LiangLi", "XiangZhang", "YingSong", "ZiwangHuang", "YutianChong", "ZhiguangChen", "HuilingZhu", "JiahaoWu", "WeifengChen", "YutongLu", "YuedongYang", "YunfeiZha", "HuiyingZhao", "JunShen"], "doi": "10.1007/s12539-021-00420-z\n10.1016/S0140-6736(20)30251-8\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200236\n10.1148/radiol.2020200269\n10.1148/radiol.2020200274\n10.1109/RBME.2020.2987975\n10.1101/2020.02.23.20026930\n10.1016/j.eng.2020.04.010\n10.1148/radiol.2020200905\n10.1038/s41598-020-76282-0\n10.1101/2020.03.12.20027185\n10.1101/2020.03.20.20039834\n10.1016/j.asoc.2020.106897\n10.1007/s12539-020-00403-6\n10.1007/s12539-020-00393-5\n10.1109/TMI.2020.2995508\n10.1109/TMI.2020.2996256\n10.1109/TMI.2020.2992546\n10.1007/s13246-020-00865-4\n10.1080/07391102.2020.1788642\n10.1109/TPAMI.2019.2913372\n10.1016/j.patrec.2005.10.010\n10.1093/bib/bbaa205\n10.1021/acs.jcim.9b00749"}
{"title": "Multicenter Assessment of CT Pneumonia Analysis Prototype for Predicting Disease Severity and Patient Outcome.", "abstract": "To perform a multicenter assessment of the CT Pneumonia Analysis prototype for predicting disease severity and patient outcome in COVID-19 pneumonia both without and with integration of clinical information. Our IRB-approved observational study included consecutive 241 adult patients (>\u200918\u00a0years; 105 females; 136 males) with RT-PCR-positive COVID-19 pneumonia who underwent non-contrast chest CT at one of the two tertiary care hospitals (site A: Massachusetts General Hospital, USA; site B: Firoozgar Hospital Iran). We recorded patient age, gender, comorbid conditions, laboratory values, intensive care unit (ICU) admission, mechanical ventilation, and final outcome (recovery or death). Two thoracic radiologists reviewed all chest CTs to record type, extent of pulmonary opacities based on the percentage of lobe involved, and severity of respiratory motion artifacts. Thin-section CT images were processed with the prototype (Siemens Healthineers) to obtain quantitative features including lung volumes, volume and percentage of all-type and high-attenuation opacities (\u2265\u2009-200 HU), and mean HU and standard deviation of opacities within a given lung region. These values are estimated for the total combined lung volume, and separately for each lung and each lung lobe. Multivariable analyses of variance (MANOVA) and multiple logistic regression were performed for data analyses. About 26% of chest CTs (62/241) had moderate to severe motion artifacts. There were no significant differences in the AUCs of quantitative features for predicting disease severity with and without motion artifacts (AUC 0.94-0.97) as well as for predicting patient outcome (AUC 0.7-0.77) (p\u2009>\u20090.5). Combination of the volume of all-attenuation opacities and the percentage of high-attenuation opacities (AUC 0.76-0.82, 95% confidence interval (CI) 0.73-0.82) had higher AUC for predicting ICU admission than the subjective severity scores (AUC 0.69-0.77, 95% CI 0.69-0.81). Despite a high frequency of motion artifacts, quantitative features of pulmonary opacities from chest CT can help differentiate patients with favorable and adverse outcomes.", "journal": "Journal of digital imaging", "date": "2021-02-27", "authors": ["FatemehHomayounieh", "Marcio AloisioBezerra Cavalcanti Rockenbach", "ShadiEbrahimian", "RuhaniDoda Khera", "Bernardo CBizzo", "VarunBuch", "RosaBabaei", "HadiKarimi Mobin", "ImanMohseni", "MatthiasMitschke", "MathisZimmermann", "FelixDurlak", "FranziskaRauch", "Subba RDigumarthy", "Mannudeep KKalra"], "doi": "10.1007/s10278-021-00430-9\n10.1503/cmaj.200715\n10.1073/pnas.2004064117\n10.1148/ryct.2020200047\n10.2214/AJR.20.22976\n10.1016/j.ejrad.2020.109041\n10.1016/j.compbiomed.2020.103795"}
{"title": "Monitoring social distancing under various low light conditions with deep learning and a single motionless time of flight camera.", "abstract": "The purpose of this work is to provide an effective social distance monitoring solution in low light environments in a pandemic situation. The raging coronavirus disease 2019 (COVID-19) caused by the SARS-CoV-2 virus has brought a global crisis with its deadly spread all over the world. In the absence of an effective treatment and vaccine the efforts to control this pandemic strictly rely on personal preventive actions, e.g., handwashing, face mask usage, environmental cleaning, and most importantly on social distancing which is the only expedient approach to cope with this situation. Low light environments can become a problem in the spread of disease because of people's night gatherings. Especially, in summers when the global temperature is at its peak, the situation can become more critical. Mostly, in cities where people have congested homes and no proper air cross-system is available. So, they find ways to get out of their homes with their families during the night to take fresh air. In such a situation, it is necessary to take effective measures to monitor the safety distance criteria to avoid more positive cases and to control the death toll. In this paper, a deep learning-based solution is proposed for the above-stated problem. The proposed framework utilizes the you only look once v4 (YOLO v4) model for real-time object detection and the social distance measuring approach is introduced with a single motionless time of flight (ToF) camera. The risk factor is indicated based on the calculated distance and safety distance violations are highlighted. Experimental results show that the proposed model exhibits good performance with 97.84% mean average precision (mAP) score and the observed mean absolute error (MAE) between actual and measured social distance values is 1.01 cm.", "journal": "PloS one", "date": "2021-02-26", "authors": ["AdinaRahim", "AyeshaMaqbool", "TauseefRana"], "doi": "10.1371/journal.pone.0247440\n10.3390/jcm9020596\n10.3390/ijerph17082932\n10.3390/app10144755\n10.1016/j.epidem.2019.02.004\n10.1109/TIP.2016.2639450\n10.1016/j.patcog.2016.06.008\n10.1109/TIP.2018.2810539\n10.1109/TIP.2019.2910412\n10.1016/j.neucom.2015.12.114\n10.1016/S2468-2667(20)30073-6\n10.12688/wellcomeopenres.15843.2\n10.1016/j.patrec.2012.07.005\n10.1016/j.puhe.2020.04.016\n10.3390/s141121247\n10.3390/s17051065\n10.1016/j.snb.2019.01.063\n10.3390/s140508895\n10.1016/j.compbiomed.2020.103792\n10.1109/TPAMI.2015.2437384\n10.3354/cr030079\n10.1016/j.cviu.2018.10.010\n10.14569/IJACSA.2018.090103\n10.1016/j.dss.2017.11.001\n10.1080/01691864.2017.1365009\n10.3390/mti2030047\n10.1017/ATSIP.2014.4"}
{"title": "A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19).", "abstract": "The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control.\nWe collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation.\nThe internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%.\nThese results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis.\n\u2022 The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season. \u2022 As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets. \u2022 The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.", "journal": "European radiology", "date": "2021-02-26", "authors": ["ShuaiWang", "BoKang", "JinluMa", "XianjunZeng", "MingmingXiao", "JiaGuo", "MengjiaoCai", "JingyiYang", "YaodongLi", "XiangfeiMeng", "BoXu"], "doi": "10.1007/s00330-021-07715-1\n10.1111/tmi.13383\n10.1007/s11517-019-01965-4\n10.1515/ijb-2018-0060\n10.14299/ijser.2020.03.02\n10.1089/omi.2018.0097"}
{"title": "SOM-LWL method for identification of COVID-19 on chest X-rays.", "abstract": "The outbreak of coronavirus disease 2019 (COVID-19) has had an immense impact on world health and daily life in many countries. Sturdy observing of the initial site of infection in patients is crucial to gain control in the struggle with COVID-19. The early automated detection of the recent coronavirus disease (COVID-19) will help to limit its dissemination worldwide. Many initial studies have focused on the identification of the genetic material of coronavirus and have a poor detection rate for long-term surgery. The first imaging procedure that played an important role in COVID-19 treatment was the chest X-ray. Radiological imaging is often used as a method that emphasizes the performance of chest X-rays. Recent findings indicate the presence of COVID-19 in patients with irregular findings on chest X-rays. There are many reports on this topic that include machine learning strategies for the identification of COVID-19 using chest X-rays. Other current studies have used non-public datasets and complex artificial intelligence (AI) systems. In our research, we suggested a new COVID-19 identification technique based on the locality-weighted learning and self-organization map (LWL-SOM) strategy for detecting and capturing COVID-19 cases. We first grouped images from chest X-ray datasets based on their similar features in different clusters using the SOM strategy in order to discriminate between the COVID-19 and non-COVID-19 cases. Then, we built our intelligent learning model based on the LWL algorithm to diagnose and detect COVID-19 cases. The proposed SOM-LWL model improved the correlation coefficient performance results between the Covid19, no-finding, and pneumonia cases; pneumonia and no-finding cases; Covid19 and pneumonia cases; and Covid19 and no-finding cases from 0.9613 to 0.9788, 0.6113 to 1 0.8783 to 0.9999, and 0.8894 to 1, respectively. The proposed LWL-SOM had better results for discriminating COVID-19 and non-COVID-19 patients than the current machine learning-based solutions using AI evaluation measures.", "journal": "PloS one", "date": "2021-02-25", "authors": ["Ahmed HamzaOsman", "Hani MoetqueAljahdali", "Sultan MenwerAltarrazi", "AliAhmed"], "doi": "10.1371/journal.pone.0247176\n10.1002/jmv.25678\n10.1016/j.compbiomed.2020.103792\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200490\n10.1148/radiol.2020200343\n10.1016/S1473-3099(20)30134-1\n10.1148/radiol.2020200463\n10.2214/AJR.20.22954\n10.1016/S0140-6736(20)30154-9\n10.2214/AJR.20.22976\n10.1016/j.chaos.2020.110337\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.cmpb.2018.04.005\n10.1016/j.compbiomed.2020.103726\n10.1007/s00521-020-05410-8\n10.2174/1573405617999210112193220\n10.1007/s00521-020-05636-6\n10.1016/j.compbiomed.2018.09.009\n10.1016/j.compbiomed.2017.08.022\n10.1016/j.compmedimag.2019.101673\n10.1016/j.cmpb.2019.06.005\n10.3390/v12070769\n10.1007/s13246-020-00865-4\n10.1371/journal.pone.0239474\n10.1371/journal.pone.0235187\n10.1109/72.846729\n10.1101/gr.634603\n10.1016/j.neunet.2012.09.018"}
{"title": "Association of Angiotensin-Converting Enzyme Inhibitors and Angiotensin Receptor Blockers With the Risk of Hospitalization and Death in Hypertensive Patients With COVID-19.", "abstract": "Background Despite its clinical significance, the risk of severe infection requiring hospitalization among outpatients with severe acute respiratory syndrome coronavirus 2 infection who receive angiotensin-converting enzyme (ACE) inhibitors and angiotensin receptor blockers (ARBs) remains uncertain. Methods and Results In a propensity score-matched outpatient cohort (January-May 2020) of 2263 Medicare Advantage and commercially insured individuals with hypertension and a positive outpatient SARS-CoV-2, we determined the association of ACE inhibitors and ARBs with COVID-19 hospitalization. In a concurrent inpatient cohort of 7933 hospitalized with COVID-19, we tested their association with in-hospital mortality. The robustness of the observations was assessed in a contemporary cohort (May-August). In the outpatient study, neither ACE inhibitors (hazard ratio [HR], 0.77; 0.53-1.13, ", "journal": "Journal of the American Heart Association", "date": "2021-02-25", "authors": ["RohanKhera", "CallahanClark", "YuanLu", "YinglongGuo", "ShengRen", "BrandonTruax", "Erica SSpatz", "KarthikMurugiah", "ZhenqiuLin", "Saad BOmer", "DeneenVojta", "Harlan MKrumholz"], "doi": "10.1161/JAHA.120.018086\n10.1016/S0140-6736(20)30566-3\n10.1016/S2213-2600(20)30116-8\n10.1001/jamacardio.2020.1286\n10.1056/NEJMsr2005760\n10.1038/s41569-020-0360-5\n10.1001/jamacardio.2020.1624\n10.7326/M20-1515\n10.1056/NEJMoa2006923\n10.1161/JAHA.120.017364\n10.1016/S2589-7500(20)30289-2\n10.1016/j.ijcard.2020.07.041\n10.1016/S0140-6736(20)31030-8\n10.1056/NEJMoa2008975\n10.1161/HYPERTENSIONAHA.120.15324\n10.1016/S2213-2600(20)30558-0\n10.1161/HYP.0000000000000065\n10.1016/S0140-6736(19)32317-7\n10.1002/pds.4767\n10.1001/jamainternmed.2014.7856\n10.1186/s12874-017-0338-0\n10.1161/CIRCULATIONAHA.115.017719\n10.1158/1078-0432.CCR-11-2097\n10.1001/jamasurg.2014.3490\n10.1161/CIRCRESAHA.120.317134\n10.1001/jamacardio.2020.1855\n10.1042/BJ20040634\n10.1038/nm1267\n10.1016/j.cell.2020.02.058\n10.1111/apha.13539\n10.1038/s41467-020-18319-6\n10.1080/22221751.2020.1746200\n10.1136/bmj.e4260\n10.1093/cid/cis733\n10.1056/NEJMe2012924\n10.1016/j.amjcard.2020.05.038\n10.1001/jama.2020.11301\n10.1002/ejhf.1924\n10.1073/pnas.1510502113\n10.1002/pds.4767"}
{"title": "Texture feature-based machine learning classifier could assist in the diagnosis of COVID-19.", "abstract": "Differentiating COVID-19 from other acute infectious pneumonias rapidly is challenging at present. This study aims to improve the diagnosis of COVID-19 using computed tomography (CT).\nCOVID-19 was confirmed mainly by virus nucleic acid testing and epidemiological history according to WHO interim guidance, while other infectious pneumonias were diagnosed by antigen testing. The texture features were extracted from CT images by two radiologists with 5 years of work experience using modified wavelet transform and matrix computation analyses. The random forest (RF) classifier was applied to identify COVID-19 patients and images.\nWe retrospectively analysed the data of 95 individuals (291 images) with COVID-19 and 96 individuals (279 images) with other acute infectious pneumonias, including 50 individuals (160 images) with influenza A/B. In total, 6 texture features showed a positive association with COVID-19, while 4 features were negatively associated. The mean AUROC, accuracy, sensitivity, and specificity values of the 5-fold test sets were 0.800, 0.722, 0.770, and 0.680 for image classification and 0.858, 0.826, 0.809, and 0.842 for individual classification, respectively. The feature 'Correlation' contributed most both at the image level and individual level, even compared with the clinical factors. In addition, the texture features could discriminate COVID-19 from influenza A/B, with an AUROC of 0.883 for images and 0.957 for individuals.\nThe developed texture feature-based RF classifier could assist in the diagnosis of COVID-19, which could be a rapid screening tool in the era of pandemic.", "journal": "European journal of radiology", "date": "2021-02-23", "authors": ["ZhiyuanWu", "LiLi", "RonghuaJin", "LianchunLiang", "ZhongjieHu", "LixinTao", "YongHan", "WeiFeng", "DiZhou", "WeimingLi", "QinbinLu", "WeiLiu", "LiqunFang", "JianHuang", "YuGu", "HongjunLi", "XiuhuaGuo"], "doi": "10.1016/j.ejrad.2021.109602"}
{"title": "CoVNet-19: A Deep Learning model for the detection and analysis of COVID-19 patients.", "abstract": "The ongoing fight with Novel Corona Virus, getting quick treatment, and rapid diagnosis reports have become an act of high priority. With millions getting infected daily and a fatality rate of 2%, we made it our motive to contribute a little to solve this real-world problem by accomplishing a significant and substantial method for diagnosing COVID-19 patients.\nThe Exponential growth of COVID-19 cases worldwide has severely affected the health care system of highly populated countries due to proportionally a smaller number of medical practitioners, testing kits, and other resources, thus becoming essential to identify the infected people. Catering to the above problems, the purpose of this paper is to formulate an accurate, efficient, and time-saving method for detecting positive corona patients.\nIn this paper, an Ensemble Deep Convolution Neural Network model \"CoVNet-19\" is being proposed that can unveil important diagnostic characteristics to find COVID-19 infected patients using X-ray images chest and help radiologists and medical experts to fight this pandemic.\nThe experimental results clearly show that the overall classification accuracy obtained with the proposed approach for three-class classification among COVID-19, Pneumonia, and Normal is 98.28%, along with an average precision and Recall of 98.33% and 98.33%, respectively. Besides this, for binary classification between Non-COVID and COVID Chest X-ray images, an overall accuracy of 99.71% was obtained.\nHaving a high diagnostic accuracy, our proposed ensemble Deep Learning classification model can be a productive and substantial contribution to detecting COVID-19 infected patients.", "journal": "Applied soft computing", "date": "2021-02-23", "authors": ["PriyanshKedia", "NoneAnjum", "RahulKatarya"], "doi": "10.1016/j.asoc.2021.107184\n10.1002/jmv.25681\n10.1016/j.ijsu.2020.02.034\n10.1016/j.ijsu.2020.04.001\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105581\n10.20944/preprints202003.0300.v1\n10.1016/j.irbm.2020.05.003\n10.1109/CVPR.2017.243\n10.1007/978-1-4842-4470-8_7\n10.1007/s11263-019-01228-7"}
{"title": "A rapid screening classifier for diagnosing COVID-19.", "abstract": "", "journal": "International journal of biological sciences", "date": "2021-02-23", "authors": ["YangXia", "WeixiangChen", "HongyiRen", "JianpingZhao", "LihuaWang", "RuiJin", "JiesenZhou", "QiyuanWang", "FuguiYan", "BinZhang", "JianLou", "ShaobinWang", "XiaomengLi", "JieZhou", "LimingXia", "ChengJin", "JianjiangFeng", "WenLi", "HuahaoShen"], "doi": "10.7150/ijbs.53982"}
{"title": "Screening of COVID-19 based on the extracted radiomics features from chest CT images.", "abstract": "Radiomics has been widely used in quantitative analysis of medical images for disease diagnosis and prognosis assessment. The objective of this study is to test a machine-learning (ML) method based on radiomics features extracted from chest CT images for screening COVID-19 cases.\nThe study is carried out on two groups of patients, including 138 patients with confirmed and 140 patients with suspected COVID-19. We focus on distinguishing pneumonia caused by COVID-19 from the suspected cases by segmentation of whole lung volume and extraction of 86 radiomics features. Followed by feature extraction, nine feature-selection procedures are used to identify valuable features. Then, ten ML classifiers are applied to classify and predict COVID-19 cases. Each ML models is trained and tested using a ten-fold cross-validation method. The predictive performance of each ML model is evaluated using the area under the curve (AUC) and accuracy.\nThe range of accuracy and AUC is from 0.32 (recursive feature elimination [RFE]+Multinomial Naive Bayes [MNB] classifier) to 0.984 (RFE+bagging [BAG], RFE+decision tree [DT] classifiers) and 0.27 (mutual information [MI]+MNB classifier) to 0.997 (RFE+k-nearest neighborhood [KNN] classifier), respectively. There is no direct correlation among the number of the selected features, accuracy, and AUC, however, with changes in the number of the selected features, the accuracy and AUC values will change. Feature selection procedure RFE+BAG classifier and RFE+DT classifier achieve the highest prediction accuracy (accuracy: 0.984), followed by MI+Gaussian Naive Bayes (GNB) and logistic regression (LGR)+DT classifiers (accuracy: 0.976). RFE+KNN classifier as a feature selection procedure achieve the highest AUC (AUC: 0.997), followed by RFE+BAG classifier (AUC: 0.991) and RFE+gradient boosting decision tree (GBDT) classifier (AUC: 0.99).\nThis study demonstrates that the ML model based on RFE+KNN classifier achieves the highest performance to differentiate patients with a confirmed infection caused by COVID-19 from the suspected cases.", "journal": "Journal of X-ray science and technology", "date": "2021-02-23", "authors": ["Seyed MasoudRezaeijo", "RazzaghAbedi-Firouzjah", "MohammadrezaGhorvei", "SamadSarnameh"], "doi": "10.3233/XST-200831"}
{"title": "Codeless Deep Learning of COVID-19 Chest X-Ray Image Dataset with KNIME Analytics Platform.", "abstract": "This paper proposes a method for computer-assisted diagnosis of coronavirus disease 2019 (COVID-19) through chest X-ray imaging using a deep learning model without writing a single line of code using the Konstanz Information Miner (KNIME) analytics platform.\nWe obtained 155 samples of posteroanterior chest X-ray images from COVID-19 open dataset repositories to develop a classification model using a simple convolutional neural network (CNN). All of the images contained diagnostic information for COVID-19 and other diseases. The model would classify whether a patient was infected with COVID-19 or not. Eighty percent of the images were used for model training, and the rest were used for testing. The graphic user interface-based programming in the KNIME enabled class label annotation, data preprocessing, CNN model training and testing, performance evaluation, and so on.\n1,000 epochs training were performed to test the simple CNN model. The lower and upper bounds of positive predictive value (precision), sensitivity (recall), specificity, and f-measure are 92.3% and 94.4%. Both bounds of the model's accuracies were equal to 93.5% and 96.6% of the area under the receiver operating characteristic curve for the test set.\nIn this study, a researcher who does not have basic knowledge of python programming successfully performed deep learning analysis of chest x-ray image dataset using the KNIME independently. The KNIME will reduce the time spent and lower the threshold for deep learning research applied to healthcare.", "journal": "Healthcare informatics research", "date": "2021-02-22", "authors": ["Jun YoungAn", "HoseokSeo", "Young-GonKim", "Kyu EunLee", "SungwanKim", "Hyoun-JoongKong"], "doi": "10.4258/hir.2021.27.1.82\n10.1101/2020.08.20.20178913"}
{"title": "Hybrid ensemble model for differential diagnosis between COVID-19 and common viral pneumonia by chest X-ray radiograph.", "abstract": "Chest X-ray radiography (CXR) has been widely considered as an accessible, feasible, and convenient method to evaluate suspected patients' lung involvement during the COVID-19 pandemic. However, with the escalating number of suspected cases, traditional diagnosis via CXR fails to deliver results within a short period of time. Therefore, it is crucial to employ artificial intelligence (AI) to enhance CXRs for obtaining quick and accurate diagnoses. Previous studies have reported the feasibility of utilizing deep learning methods to screen for COVID-19 using CXR and CT results. However, these models only use a single deep learning network for chest radiograph detection; the accuracy of this approach required further improvement.\nIn this study, we propose a three-step hybrid ensemble model, including a feature extractor, a feature selector, and a classifier. First, a pre-trained AlexNet with an improved structure extracts the original image features. Then, the ReliefF algorithm is adopted to sort the extracted features, and a trial-and-error approach is used to select the n most important features to reduce the feature dimension. Finally, an SVM classifier provides classification results based on the n selected features.\nCompared to five existing models (InceptionV3: 97.916\u00a0\u00b1\u00a00.408%; SqueezeNet: 97.189\u00a0\u00b1\u00a00.526%; VGG19: 96.520\u00a0\u00b1\u00a01.220%; ResNet50: 97.476\u00a0\u00b1\u00a00.513%; ResNet101: 98.241\u00a0\u00b1\u00a00.209%), the proposed model demonstrated the best performance in terms of overall accuracy rate (98.642\u00a0\u00b1\u00a00.398%). Additionally, compared to the existing models, the proposed model demonstrates a considerable improvement in classification time efficiency (SqueezeNet: 6.602\u00a0\u00b1\u00a00.001s; InceptionV3: 12.376\u00a0\u00b1\u00a00.002s; ResNet50: 10.952\u00a0\u00b1\u00a00.001s; ResNet101: 18.040\u00a0\u00b1\u00a00.002s; VGG19: 16.632\u00a0\u00b1\u00a00.002s; proposed model: 5.917\u00a0\u00b1\u00a00.001s).\nThe model proposed in this article is practical and effective, and can provide high-precision COVID-19 CXR detection. We demonstrated its suitability to aid medical professionals in distinguishing normal CXRs, viral pneumonia CXRs and COVID-19 CXRs efficiently on small sample sizes.", "journal": "Computers in biology and medicine", "date": "2021-02-21", "authors": ["WeiqiuJin", "ShuqinDong", "ChangziDong", "XiaodanYe"], "doi": "10.1016/j.compbiomed.2021.104252\n10.1007/s11547-020-01232-9\n10.1007/s00330-020-06967-7\n10.1016/j.ins.2020.09.041\n10.1016/j.compbiomed.2020.103792\n10.1007/s00138-020-01128-8\n10.1016/j.patrec.2020.10.001\n10.1016/j.chaos.2020.110153\n10.1016/j.chaos.2020.110170\n10.1016/j.media.2020.101794\n10.1038/s41598-020-76550-z\n10.3390/jpm10040213\n10.1016/j.chaos.2020.110245\n10.1016/j.bspc.2020.102365\n10.1016/j.ipm.2020.102411\n10.1007/978-3-030-13969-8_18\n10.1016/j.jocs.2018.11.008\n10.1016/j.mehy.2020.109577\n10.1016/j.jbi.2018.07.014\n10.1007/s13246-020-00865-4\n10.1007/s10489-020-01829-7"}
{"title": "RadTranslate: An Artificial Intelligence-Powered Intervention for Urgent Imaging to Enhance Care Equity for Patients With Limited English Proficiency During the COVID-19 Pandemic.", "abstract": "Disproportionally high rates of coronavirus disease 2019 (COVID-19) have been noted among communities with limited English proficiency, resulting in an unmet need for improved multilingual care and interpreter services. To enhance multilingual care, the authors created a freely available web application, RadTranslate, that provides multilingual radiology examination instructions. The purpose of this study was to evaluate the implementation of this intervention in radiology.\nThe device-agnostic web application leverages artificial intelligence text-to-speech technology to provide standardized, human-like spoken examination instructions in the patient's preferred language. Standardized phrases were collected from a consensus group consisting of technologists, radiologists, and ancillary staff members. RadTranslate was piloted in Spanish for chest radiography performed at a COVID-19 triage outpatient center that served a predominantly Spanish-speaking Latino community. Implementation included a tablet displaying the application in the chest radiography room. Imaging appointment duration was measured and compared between pre- and postimplementation groups.\nIn the 63-day test period after launch, there were 1,267 application uses, with technologists voluntarily switching exclusively to RadTranslate for Spanish-speaking patients. The most used phrases were a general explanation of the examination (30% of total), followed by instructions to disrobe and remove any jewelry (12%). There was no significant difference in imaging appointment duration (11 \u00b1 7 and 12 \u00b1 3 min for standard of care versus RadTranslate, respectively), but variability was significantly lower when RadTranslate was used (P\u00a0= .003).\nArtificial intelligence-aided multilingual audio instructions were successfully integrated into imaging workflows, reducing strain on medical interpreters and variance in throughput and resulting in more reliable average examination length.", "journal": "Journal of the American College of Radiology : JACR", "date": "2021-02-21", "authors": ["Daniel BChonde", "AliPourvaziri", "JoyWilliams", "JenniferMcGowan", "MargoMoskos", "CarmenAlvarez", "Anand KNarayan", "DaniaDaye", "Efren JFlores", "Marc DSucci"], "doi": "10.1016/j.jacr.2021.01.013"}
{"title": "CovidCTNet: an open-source deep learning approach to diagnose covid-19 using small cohort of CT images.", "abstract": "Coronavirus disease 2019 (Covid-19) is highly contagious with limited treatment options. Early and accurate diagnosis of Covid-19 is crucial in reducing the spread of the disease and its accompanied mortality. Currently, detection by reverse transcriptase-polymerase chain reaction (RT-PCR) is the gold standard of outpatient and inpatient detection of Covid-19. RT-PCR is a rapid method; however, its accuracy in detection is only ~70-75%. Another approved strategy is computed tomography (CT) imaging. CT imaging has a much higher sensitivity of ~80-98%, but similar accuracy of 70%. To enhance the accuracy of CT imaging detection, we developed an open-source framework, CovidCTNet, composed of a set of deep learning algorithms that accurately differentiates Covid-19 from community-acquired pneumonia (CAP) and other lung diseases. CovidCTNet increases the accuracy of CT imaging detection to 95% compared to radiologists (70%). CovidCTNet is designed to work with heterogeneous and small sample sizes independent of the CT imaging hardware. To facilitate the detection of Covid-19 globally and assist radiologists and physicians in the screening process, we are releasing all algorithms and model parameter details as open-source. Open-source sharing of CovidCTNet enables developers to rapidly improve and optimize services while preserving user privacy and data ownership.", "journal": "NPJ digital medicine", "date": "2021-02-20", "authors": ["TaherehJavaheri", "MortezaHomayounfar", "ZohrehAmoozgar", "RezaReiazi", "FatemehHomayounieh", "EngyAbbas", "AzadehLaali", "Amir RezaRadmard", "Mohammad HadiGharib", "Seyed Ali JavadMousavi", "OmidGhaemi", "RosaBabaei", "Hadi KarimiMobin", "MehdiHosseinzadeh", "RanaJahanban-Esfahlan", "KhaledSeidi", "Mannudeep KKalra", "GuanglanZhang", "L TChitkushev", "BenjaminHaibe-Kains", "RezaMalekzadeh", "RezaRawassizadeh"], "doi": "10.1038/s41746-021-00399-3\n10.1126/science.1176062\n10.3390/jcm9020580\n10.2807/1560-7917.ES.2020.25.4.2000058\n10.1016/S0140-6736(03)14630-2\n10.1016/j.idc.2010.04.009\n10.1371/journal.pone.0230548\n10.1148/radiol.2020200230\n10.1016/S1470-2045(19)30739-9\n10.1038/s41598-019-48995-4\n10.1038/s41598-019-55972-4\n10.1038/s41591-019-0447-x\n10.1016/j.jacr.2017.12.027\n10.1117/1.JMI.3.4.044506\n10.1145/325165.325247\n10.1016/j.patcog.2018.07.031\n10.1126/science.aba4456"}
{"title": "Assisting scalable diagnosis automatically via CT images in the combat against COVID-19.", "abstract": "The pandemic of Coronavirus Disease 2019 (COVID-19) is causing enormous loss of life globally. Prompt case identification is critical. The reference method is the real-time reverse transcription PCR (RT-PCR) assay, whose limitations may curb its prompt large-scale application. COVID-19 manifests with chest computed tomography (CT) abnormalities, some even before the onset of symptoms. We tested the hypothesis that the application of deep learning (DL) to 3D CT images could help identify COVID-19 infections. Using data from 920 COVID-19 and 1,073 non-COVID-19 pneumonia patients, we developed a modified DenseNet-264 model, COVIDNet, to classify CT images to either class. When tested on an independent set of 233 COVID-19 and 289 non-COVID-19 pneumonia patients, COVIDNet achieved an accuracy rate of 94.3% and an area under the curve of 0.98. As of March 23, 2020, the COVIDNet system had been used 11,966 times with a sensitivity of 91.12% and a specificity of 88.50% in six hospitals with PCR confirmation. Application of DL to CT images may improve both efficiency and capacity of case detection and long-term surveillance.", "journal": "Scientific reports", "date": "2021-02-20", "authors": ["BohanLiu", "PanLiu", "LutaoDai", "YanlinYang", "PengXie", "YiqingTan", "JichengDu", "WeiShan", "ChenghuiZhao", "QinZhong", "XixiangLin", "XizhouGuan", "NingXing", "YuhuiSun", "WenjunWang", "ZhibingZhang", "XiaFu", "YanqingFan", "MeifangLi", "NaZhang", "LinLi", "YaouLiu", "LinXu", "JingboDu", "ZhenhuaZhao", "XuelongHu", "WeipengFan", "RongpinWang", "ChongchongWu", "YongkangNie", "LiuquanCheng", "LinMa", "ZongrenLi", "QianJia", "MinchaoLiu", "HuayuanGuo", "GaoHuang", "HaipengShen", "LiangZhang", "PeifangZhang", "GangGuo", "HaoLi", "WeiminAn", "JianxinZhou", "KunlunHe"], "doi": "10.1038/s41598-021-83424-5\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1002/jmv.25678\n10.1016/S0140-6736(20)30154-9\n10.1136/bmj.m1090\n10.1126/science.abb5793\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1148/ryct.2020200034\n10.1016/S1473-3099(20)30086-4\n10.1016/S2589-7500(19)30058-5\n10.1016/S2589-7500(19)30159-1\n10.1016/S2589-7500(20)30025-X\n10.1016/S2213-2600(20)30003-5\n10.1148/radiol.2020200236\n10.1148/radiol.2020200905\n10.1016/S2213-2600(18)30286-8\n10.1056/NEJMp2015897\n10.1093/nsr/nwaa036"}
{"title": "Correlation between lung infection severity and clinical laboratory indicators in patients with COVID-19: a cross-sectional study based on machine learning.", "abstract": "Coronavirus disease 2019 (COVID-19) has caused a global pandemic that has raised worldwide concern. This study aims to investigate the correlation between the extent of lung infection and relevant clinical laboratory testing indicators in COVID-19 and to analyse its underlying mechanism.\nChest high-resolution computer tomography (CT) images and laboratory examination data of 31 patients with COVID-19 were extracted, and the lesion areas in CT images were quantitatively segmented and calculated using a deep learning (DL) system. A cross-sectional study method was carried out to explore the differences among the proportions of lung lobe infection and to correlate the percentage of infection (POI) of the whole lung in all patients with clinical laboratory examination values.\nNo significant difference in the proportion of infection was noted among various lung lobes (P\u2009>\u20090.05). The POI of total lung was negatively correlated with the peripheral blood lymphocyte percentage (L%) (r\u2009=\u2009-\u20090.633, P\u2009<\u20090.001) and lymphocyte (LY) count (r\u2009=\u2009-\u20090.555, P\u2009=\u20090.001) but positively correlated with the neutrophil percentage (N%) (r\u2009=\u20090.565, P\u2009=\u20090.001). Otherwise, the POI was not significantly correlated with the peripheral blood white blood cell (WBC) count, monocyte percentage (M%) or haemoglobin (HGB) content. In some patients, as the infection progressed, the L% and LY count decreased progressively accompanied by a continuous increase in the N%.\nLung lesions in COVID-19 patients are significantly correlated with the peripheral blood lymphocyte and neutrophil levels, both of which could serve as prognostic indicators that provide warning implications, and contribute to clinical interventions in patients.", "journal": "BMC infectious diseases", "date": "2021-02-20", "authors": ["XingruiWang", "QinglinChe", "XiaoxiaoJi", "XinyiMeng", "LangZhang", "RongrongJia", "HairongLyu", "WeixianBai", "LingjieTan", "YanjunGao"], "doi": "10.1186/s12879-021-05839-9\n10.1016/j.micinf.2020.01.004\n10.1148/radiol.2020200432\n10.1038/s41568-018-0016-5\n10.1016/S0140-6736(20)30183-5\n10.1016/S2213-2600(20)30076-X\n10.1002/jmv.25709\n10.1038/s41564-020-0688-y\n10.1002/path.1570\n10.1002/1521-4141(200209)32:9<2490::AID-IMMU2490>3.0.CO;2-G\n10.4049/jimmunol.176.7.4284\n10.1016/j.micinf.2005.06.007\n10.1002/path.1711260307\n10.7326/0003-4819-84-3-304\n10.1016/0091-6749(78)90200-2\n10.1016/S0140-6736(20)30317-2\n10.1378/chest.129.6.1441\n10.1016/S2213-2600(19)30417-5\n10.1038/ni762\n10.1111/j.1365-2249.2004.02415.x\n10.1111/j.1440-1843.2007.01102.x\n10.1164/rccm.201203-0508OC"}
{"title": "Federated semi-supervised learning for COVID region segmentation in chest CT using multi-national data from China, Italy, Japan.", "abstract": "The recent outbreak of Coronavirus Disease 2019 (COVID-19) has led to urgent needs for reliable diagnosis and management of SARS-CoV-2 infection. The current guideline is using RT-PCR for testing. As a complimentary tool with diagnostic imaging, chest Computed Tomography (CT) has been shown to be able to reveal visual patterns characteristic for COVID-19, which has definite value at several stages during the disease course. To facilitate CT analysis, recent efforts have focused on computer-aided characterization and diagnosis with chest CT scan, which has shown promising results. However, domain shift of data across clinical data centers poses a serious challenge when deploying learning-based models. A common way to alleviate this issue is to fine-tune the model locally with the target domains local data and annotations. Unfortunately, the availability and quality of local annotations usually varies due to heterogeneity in equipment and distribution of medical resources across the globe. This impact may be pronounced in the detection of COVID-19, since the relevant patterns vary in size, shape, and texture. In this work, we attempt to find a solution for this challenge via federated and semi-supervised learning. A multi-national database consisting of 1704 scans from three countries is adopted to study the performance gap, when training a model with one dataset and applying it to another. Expert radiologists manually delineated 945 scans for COVID-19 findings. In handling the variability in both the data and annotations, a novel federated semi-supervised learning technique is proposed to fully utilize all available data (with or without annotations). Federated learning avoids the need for sensitive data-sharing, which makes it favorable for institutions and nations with strict regulatory policy on data privacy. Moreover, semi-supervision potentially reduces the annotation burden under a distributed setting. The proposed framework is shown to be effective compared to fully supervised scenarios with conventional data sharing instead of model weight sharing.", "journal": "Medical image analysis", "date": "2021-02-19", "authors": ["DongYang", "ZiyueXu", "WenqiLi", "AndriyMyronenko", "Holger RRoth", "StephanieHarmon", "ShengXu", "BarisTurkbey", "EvrimTurkbey", "XiaosongWang", "WentaoZhu", "GianpaoloCarrafiello", "FrancescaPatella", "MaurizioCariati", "HirofumiObinata", "HitoshiMori", "KakuTamura", "PengAn", "Bradford JWood", "DaguangXu"], "doi": "10.1016/j.media.2021.101992"}
{"title": "Deep Learning-Based Measurement of Total Plaque Area in B-Mode Ultrasound Images.", "abstract": "Measurement of total-plaque-area (TPA) is important for determining long term risk for stroke and monitoring carotid plaque progression. Since delineation of carotid plaques is required, a deep learning method can provide automatic plaque segmentations and TPA measurements; however, it requires large datasets and manual annotations for training with unknown performance on new datasets. A UNet++ ensemble algorithm was proposed to segment plaques from 2D carotid ultrasound images, trained on three small datasets (n\u00a0=\u00a033, 33, 34 subjects) and tested on 44 subjects from the SPARC dataset (n\u00a0=\u00a0144, London, Canada). The ensemble was also trained on the entire SPARC dataset and tested with a different dataset (n\u00a0=\u00a0497, Zhongnan Hospital, China). Algorithm and manual segmentations were compared using Dice-similarity-coefficient (DSC), and TPAs were compared using the difference ( \u2206TPA), Pearson correlation coefficient (r) and Bland-Altman analyses. Segmentation variability was determined using the intra-class correlation coefficient (ICC) and coefficient-of-variation (CoV). For 44 SPARC subjects, algorithm DSC was 83.3-85.7%, and algorithm TPAs were strongly correlated (r\u00a0=\u00a00.985-0.988; p\u00a0<\u00a00.001) with manual results with marginal biases (0.73-6.75)\u00a0mm ", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-02-19", "authors": ["RanZhou", "FuminGuo", "M RezaAzarpazhooh", "SaminehHashemi", "XinyaoCheng", "J DavidSpence", "MingyueDing", "AaronFenster"], "doi": "10.1109/JBHI.2021.3060163"}
{"title": "JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation.", "abstract": "Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0% and a specificity of 93.0% on the classification test set, and 78.5% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.", "journal": "IEEE transactions on image processing : a publication of the IEEE Signal Processing Society", "date": "2021-02-19", "authors": ["Yu-HuanWu", "Shang-HuaGao", "JieMei", "JunXu", "Deng-PingFan", "Rong-GuoZhang", "Ming-MingCheng"], "doi": "10.1109/TIP.2021.3058783"}
{"title": "A deep learning integrated radiomics model for identification of coronavirus disease 2019 using computed tomography.", "abstract": "Since its first outbreak, Coronavirus Disease 2019 (COVID-19) has been rapidly spreading worldwide and caused a global pandemic. Rapid and early detection is essential to contain COVID-19. Here, we first developed a deep learning (DL) integrated radiomics model for end-to-end identification of COVID-19 using CT scans and then validated its clinical feasibility. We retrospectively collected CT images of 386 patients (129 with COVID-19 and 257 with other community-acquired pneumonia) from three medical centers to train and externally validate the developed models. A pre-trained DL algorithm was utilized to automatically segment infected lesions (ROIs) on CT images which were used for feature extraction. Five feature selection methods and four machine learning algorithms were utilized to develop radiomics models. Trained with features selected by L1 regularized logistic regression, classifier multi-layer perceptron (MLP) demonstrated the optimal performance with AUC of 0.922 (95% CI 0.856-0.988) and 0.959 (95% CI 0.910-1.000), the same sensitivity of 0.879, and specificity of 0.900 and 0.887 on internal and external testing datasets, which was equivalent to the senior radiologist in a reader study. Additionally, diagnostic time of DL-MLP was more efficient than radiologists (38\u00a0s vs 5.15\u00a0min). With an adequate performance for identifying COVID-19, DL-MLP may help in screening of suspected cases.", "journal": "Scientific reports", "date": "2021-02-18", "authors": ["XiaoguoZhang", "DaweiWang", "JiangShao", "SongTian", "WeixiongTan", "YanMa", "QingnanXu", "XiaomanMa", "DashengLi", "JunChai", "DingjunWang", "WenwenLiu", "LingboLin", "JiangfenWu", "ChenXia", "ZhongfaZhang"], "doi": "10.1038/s41598-021-83237-6\n10.3348/kjr.2020.0146\n10.3348/kjr.2020.0195\n10.1016/j.ejrad.2020.108961\n10.1016/j.acra.2020.09.004\n10.1038/s41467-020-18685-1\n10.7150/thno.46428\n10.1007/s00330-020-07032-z\n10.21037/atm-20-3026\n10.1136/bmj.m1328\n10.1038/nrclinonc.2017.141\n10.1016/j.canlet.2017.06.004\n10.3389/fonc.2016.00071\n10.1038/srep13087\n10.1016/j.compbiomed.2020.104037\n10.5152/dir.2019.19321\n10.3389/fnhum.2015.00353\n10.1148/radiology.143.1.7063747\n10.7189/jogh.10.010347\n10.1016/j.tmaid.2020.101627\n10.1016/j.rmed.2020.105980\n10.1038/s41467-020-17971-2\n10.1016/j.ejrad.2020.109041\n10.1007/s11548-020-02286-w\n10.1007/s11548-013-0913-8\n10.1016/j.ejrad.2020.109402\n10.1038/s41598-020-76282-0"}
{"title": "Data science in unveiling COVID-19 pathogenesis and diagnosis: evolutionary origin to drug repurposing.", "abstract": "The outbreak of novel severe acute respiratory syndrome coronavirus (SARS-CoV-2, also known as COVID-19) in Wuhan has attracted worldwide attention. SARS-CoV-2 causes severe inflammation, which can be fatal. Consequently, there has been a massive and rapid growth in research aimed at throwing light on the mechanisms of infection and the progression of the disease. With regard to this data science is playing a pivotal role in in silico analysis to gain insights into SARS-CoV-2 and the outbreak of COVID-19 in order to forecast, diagnose and come up with a drug to tackle the virus. The availability of large multiomics, radiological, bio-molecular and medical datasets requires the development of novel exploratory and predictive models, or the customisation of existing ones in order to fit the current problem. The high number of approaches generates the need for surveys to guide data scientists and medical practitioners in selecting the right tools to manage their clinical data.\nFocusing on data science methodologies, we conduct a detailed study on the state-of-the-art of works tackling the current pandemic scenario. We consider various current COVID-19 data analytic domains such as phylogenetic analysis, SARS-CoV-2 genome identification, protein structure prediction, host-viral protein interactomics, clinical imaging, epidemiological research and drug discovery. We highlight data types and instances, their generation pipelines and the data science models currently in use. The current study should give a detailed sketch of the road map towards handling COVID-19 like situations by leveraging data science experts in choosing the right tools. We also summarise our review focusing on prime challenges and possible future research directions.\nhguzzi@unicz.it, sroy01@cus.ac.in.", "journal": "Briefings in bioinformatics", "date": "2021-02-17", "authors": ["JayantaKumar Das", "GiuseppeTradigo", "PierangeloVeltri", "PietroH Guzzi", "SwarupRoy"], "doi": "10.1093/bib/bbaa420"}
{"title": "Coronavirus (COVID-19) detection from chest radiology images using convolutional neural networks.", "abstract": "Coronavirus disease (Covid-19) has been spreading all over the world and its diagnosis is attracting more research every moment. It is need of the hour to develop automated methods, which could detect this disease at its early stage, in a non-invasive way and within lesser time. Currently, medical specialists are analyzing Computed Tomography (CT), X-Ray, and Ultrasound (US) images or conducting Polymerase Chain Reaction (PCR) for its confirmation on manual basis. In Pakistan, CT scanners are available in most hospitals at district level, while X-Ray machines are available in all tehsil (large urban towns) level hospitals. Being widely used imaging modalities to analyze chest related diseases, produce large volume of medical data each moment clinical environments. Since automatic, time efficient and reliable methods for Covid-19 detection are required as alternate methods, therefore an automatic method of Covid-19 detection using Convolutional Neural Networks (CNN) has been proposed. Three publically available and a locally developed dataset, obtained from Department of Radiology (Diagnostics), Bahawal Victoria Hospital, Bahawalpur (BVHB), Pakistan have been used. The proposed method achieved on average accuracy (96.68 %), specificity (95.65 %), and sensitivity (96.24 %). Proposed model is trained on a large dataset and is being used at the Radiology Department, (BVHB), Pakistan.", "journal": "Biomedical signal processing and control", "date": "2021-02-17", "authors": ["GhulamGilanie", "Usama IjazBajwa", "Mustansar MahmoodWaraich", "MutyybaAsghar", "RehanaKousar", "AdnanKashif", "Rabab ShereenAslam", "Muhammad MohsinQasim", "HamzaRafique"], "doi": "10.1016/j.bspc.2021.102490"}
{"title": "A novel multiple instance learning framework for COVID-19 severity assessment via data augmentation and self-supervised learning.", "abstract": "How to fast and accurately assess the severity level of COVID-19 is an essential problem, when millions of people are suffering from the pandemic around the world. Currently, the chest CT is regarded as a popular and informative imaging tool for COVID-19 diagnosis. However, we observe that there are two issues - weak annotation and insufficient data that may obstruct automatic COVID-19 severity assessment with CT images. To address these challenges, we propose a novel three-component method, i.e., 1) a deep multiple instance learning component with instance-level attention to jointly classify the bag and also weigh the instances, 2) a bag-level data augmentation component to generate virtual bags by reorganizing high confidential instances, and 3) a self-supervised pretext component to aid the learning process. We have systematically evaluated our method on the CT images of 229 COVID-19 cases, including 50 severe and 179 non-severe cases. Our method could obtain an average accuracy of 95.8%, with 93.6% sensitivity and 96.4% specificity, which outperformed previous works.", "journal": "Medical image analysis", "date": "2021-02-16", "authors": ["ZekunLi", "WeiZhao", "FengShi", "LeiQi", "XingzhiXie", "YingWei", "ZhongxiangDing", "YangGao", "ShangjieWu", "JunLiu", "YinghuanShi", "DinggangShen"], "doi": "10.1016/j.media.2021.101978"}
{"title": "DON: Deep Learning and Optimization-Based Framework for Detection of Novel Coronavirus Disease Using X-ray Images.", "abstract": "In the hospital, a limited number of COVID-19 test kits are available due to the spike in cases every day. For this reason, a rapid alternative diagnostic option should be introduced as an automated detection method to prevent COVID-19 spreading among individuals. This article proposes multi-objective optimization and a deep-learning methodology for the detection of infected coronavirus patients with X-rays. J48 decision tree method classifies the deep characteristics of affected X-ray corona images to detect the contaminated patients effectively. Eleven different convolutional neuronal network-based (CNN) models were developed in this study to detect infected patients with coronavirus pneumonia using X-ray images (AlexNet, VGG16, VGG19, GoogleNet, ResNet18, ResNet500, ResNet101, InceptionV3, InceptionResNetV2, DenseNet201 and XceptionNet). In addition, the parameters of the CNN profound learning model are described using an emperor penguin optimizer with several objectives (MOEPO). A broad review reveals that the proposed model can categorise the X-ray images at the correct rates of precision, accuracy, recall, specificity and F1-score. Extensive test results show that the proposed model outperforms competitive models with well-known efficiency metrics. The proposed model is, therefore, useful for the real-time classification of X-ray chest images of COVID-19 disease.", "journal": "Interdisciplinary sciences, computational life sciences", "date": "2021-02-16", "authors": ["GauravDhiman", "VVinoth Kumar", "AmandeepKaur", "AshutoshSharma"], "doi": "10.1007/s12539-021-00418-7\n10.1101/2020.02.27.20028027\n10.1016/S0140-6736(20)30183-5\n10.1136/bmj.m641\n10.1148/radiol.2020200642\n10.1101/2020.02.14.20023028\n10.1016/j.ejrnm.2015.11.004\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30154-9\n10.3390/jcm9020388\n10.3390/jcm9020419\n10.3390/jcm9020462\n10.3390/jcm9020498\n10.3390/jcm9020523\n10.1016/j.engappai.2020.104008\n10.1016/j.advengsoft.2017.05.014\n10.1016/j.knosys.2018.03.011\n10.1016/j.knosys.2018.11.024\n10.1016/j.engappai.2019.03.021\n10.1016/j.engappai.2020.103541\n10.1016/j.knosys.2018.06.001\n10.1016/j.compbiomed.2019.103387\n10.1016/j.patrec.2020.03.011"}
{"title": "Current limitations to identify COVID-19 using artificial intelligence with chest X-ray imaging.", "abstract": "The scientific community has joined forces to mitigate the scope of the current COVID-19 pandemic. The early identification of the disease, as well as the evaluation of its evolution is a primary task for the timely application of medical protocols. The use of medical images of the chest provides valuable information to specialists. Specifically, chest X-ray images have been the focus of many investigations that apply artificial intelligence techniques for the automatic classification of this disease. The results achieved to date on the subject are promising. However, some results of these investigations contain errors that must be corrected to obtain appropriate models for clinical use. This research discusses some of the problems found in the current scientific literature on the application of artificial intelligence techniques in the automatic classification of COVID-19. It is evident that in most of the reviewed works an incorrect evaluation protocol is applied, which leads to overestimating the results.", "journal": "Health and technology", "date": "2021-02-16", "authors": ["Jos\u00e9 DanielL\u00f3pez-Cabrera", "Rub\u00e9nOrozco-Morales", "Jorge ArmandoPortal-Diaz", "OrlandoLovelle-Enr\u00edquez", "Marl\u00e9nP\u00e9rez-D\u00edaz"], "doi": "10.1007/s12553-021-00520-2\n10.1016/j.ijantimicag.2020.105924\n10.1016/j.mehy.2020.109689\n10.1016/j.ijid.2020.03.071\n10.1016/j.cca.2020.03.009\n10.1148/radiol.2020200642\n10.1148/radiol.2020200527\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3010287\n10.1016/j.imu.2020.100412\n10.1016/S2589-7500(20)30079-0\n10.1016/j.jiph.2020.06.028\n10.15212/bioi-2020-0015\n10.1109/JBHI.2020.3037127\n10.1109/ACCESS.2018.2830661\n10.1371/journal.pmed.1002683\n10.1148/ryai.2019180031\n10.2196/19673\n10.1148/radiol.2020200847\n10.1148/ryct.2020200034\n10.1016/j.mehy.2020.109761\n10.1016/j.compbiomed.2020.103805\n10.1109/ACCESS.2020.2990893\n10.1148/ryai.2020200053\n10.1371/journal.pone.0235187\n10.1016/j.bbe.2020.08.008\n10.3892/etm.2020.8797\n10.1016/S0734-189X(87)80186-X\n10.1007/s11263-015-0816-y\n10.1016/j.cell.2018.02.010\n10.1148/ryai.2019180041\n10.1016/j.media.2020.101797"}
{"title": "Transfer learning for establishment of recognition of COVID-19 on CT imaging using small-sized training datasets.", "abstract": "The coronavirus disease, called COVID-19, which is spreading fast worldwide since the end of 2019, and has become a global challenging pandemic. Until 27th May 2020, it caused more than 5.6 million individuals infected throughout the world and resulted in greater than 348,145 deaths. CT images-based classification technique has been tried to use the identification of COVID-19 with CT imaging by hospitals, which aims to minimize the possibility of virus transmission and alleviate the burden of clinicians and radiologists. Early diagnosis of COVID-19, which not only prevents the disease from spreading further but allows more reasonable allocation of limited medical resources. Therefore, CT images play an essential role in identifying cases of COVID-19 that are in great need of intensive clinical care. Unfortunately, the current public health emergency, which has caused great difficulties in collecting a large set of precise data for training neural networks. To tackle this challenge, our first thought is transfer learning, which is a technique that aims to transfer the knowledge from one or more source tasks to a target task when the latter has fewer training data. Since the training data is relatively limited, so a transfer learning-based DensNet-121 approach for the identification of COVID-19 is established. The proposed method is inspired by the precious work of predecessors such as CheXNet for identifying common Pneumonia, which was trained using the large Chest X-ray14 dataset, and the dataset contains 112,120 frontal chest X-rays of 14 different chest diseases (including Pneumonia) that are individually labeled and achieved good performance. Therefore, CheXNet as the pre-trained network was used for the target task (COVID-19 classification) by fine-tuning the network weights on the small-sized dataset in the target task. Finally, we evaluated our proposed method on the COVID-19-CT dataset. Experimentally, our method achieves state-of-the-art performance for the accuracy (ACC) and F1-score. The quantitative indicators show that the proposed method only uses a GPU can reach the best performance, up to 0.87 and 0.86, respectively, compared with some widely used and recent deep learning methods, which are helpful for COVID-19 diagnosis and patient triage. The codes used in this manuscript are publicly available on GitHub at (https://github.com/lichun0503/CT-Classification).", "journal": "Knowledge-based systems", "date": "2021-02-16", "authors": ["ChunLi", "YunyunYang", "HuiLiang", "BoyingWu"], "doi": "10.1016/j.knosys.2021.106849"}
{"title": "An Uncertainty-Aware Transfer Learning-Based Framework for COVID-19 Diagnosis.", "abstract": "The early and reliable detection of COVID-19 infected patients is essential to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not available in many countries, and also, there are genuine concerns about their reliability and performance. Motivated by these shortcomings, this article proposes a deep uncertainty-aware transfer learning framework for COVID-19 detection using medical images. Four popular convolutional neural networks (CNNs), including VGG16, ResNet50, DenseNet121, and InceptionResNetV2, are first applied to extract deep features from chest X-ray and computed tomography (CT) images. Extracted features are then processed by different machine learning and statistical modeling techniques to identify COVID-19 cases. We also calculate and report the epistemic uncertainty of classification results to identify regions where the trained models are not confident about their decisions (out of distribution problem). Comprehensive simulation results for X-ray and CT image data sets indicate that linear support vector machine and neural network models achieve the best results as measured by accuracy, sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC). Also, it is found that predictive uncertainty estimates are much higher for CT images compared to X-ray images.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-02-12", "authors": ["AfsharShamsi", "HamzehAsgharnezhad", "Shirin ShamsiJokandan", "AbbasKhosravi", "Parham MKebria", "DariusNahavandi", "SaeidNahavandi", "DiptiSrinivasan"], "doi": "10.1109/TNNLS.2021.3054306"}
{"title": "Artificial intelligence-based myocardial texture analysis in etiological differentiation of left ventricular hypertrophy.", "abstract": "Transthoracic echocardiography (TTE) is widely used in clinics to evaluate left ventricular hypertrophy (LVH). However, TTE is usually insufficient for the etiological diagnoses when morphological and functional features are nonspecific. With the booming of computer science and artificial intelligence (AI), previous literature has reported the application of radiomics based on cardiac magnetic resonance imaging, cardiac computed tomography and TTE in diagnosing several myocardial abnormalities, such as myocardial infarction, myocarditis, cardiac amyloidosis, and hypertrophic cardiomyopathy (HCM). In this study, we explored the possibility of using myocardial texture features in differentiating HCM, hypertensive heart disease (HHD) and uremic cardiomyopathy (UCM) based on echocardiography. To our knowledge, this was the first study to explore TTE myocardial texture analysis for multiple LVH etiology differentiation.\nTTE images were reviewed retrospectively from January 2018 to collect 50 cases for each group of HHD, HCM and UCM. The apical four chamber view was retrieved. Seventeen first-order statistics and 60 gray level co-occurrence matrix (GLCM) features were extracted for statistics and classification test by support vector machine (SVM).\nOf all the parameters, entropy of brightness (EtBrt), standard deviation (Std), coefficient of variation (CoV), skewness (Skew), contrast7 (Cont7) and homogeneity5 (Hm5) were found statistically significant among the three groups (all P<0.05) and with acceptable reproducibility (intraobserver and interobserver ICC >0.50). As a result, HCM showed the most homogeneous myocardial texture, and was significantly different from HHD and UCM (all six features: P\u22640.005). HHD appeared slightly more homogeneous than UCM, as only EtBrt and CoV were significant (P=0.011 and P=0.008). According to higher areas under the receiver operating characteristic curve (AUC) (>0.50), EtBrt, Std, and CoV were selected for test of classification as a combination of features. The AUC derived from SVM model was slightly improved compared with those of EtBrt, Std and CoV individually.\nAI-based myocardial texture analysis using ultrasonic images may be a potential approach to aiding LVH etiology differentiation.", "journal": "Annals of translational medicine", "date": "2021-02-12", "authors": ["FeiYu", "HaiboHuang", "QihuiYu", "YuqingMa", "QiZhang", "BoZhang"], "doi": "10.21037/atm-20-4891\n10.1007/s11886-010-0104-y\n10.1097/HJH.0000000000000658\n10.1016/j.echo.2010.05.020\n10.1097/HJH.0b013e32835ac71b\n10.1016/j.crad.2018.09.016\n10.2214/AJR.19.21986\n10.1097/RLI.0000000000000448\n10.1016/j.ebiom.2020.102726\n10.1111/j.0742-2822.2003.01126.x\n10.1016/j.echo.2014.10.003\n10.1093/eurheartj/ehu284\n10.1093/ehjci/jev329\n10.1016/j.jash.2015.11.006\n10.1007/s00467-015-3198-z\n10.1046/j.1523-1755.61.s80.7.x\n10.1016/S0140-6736(16)30508-6\n10.1016/0735-1097(89)90108-3\n10.1161/01.CIR.68.4.834\n10.1177/016173468400600206\n10.1161/CIRCRESAHA.117.311059\n10.1016/j.jacc.2019.02.065\n10.1053/j.ajkd.2019.02.013\n10.1016/j.bone.2016.08.022"}
{"title": "Artificial intelligence and cardiac surgery during COVID-19 era.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has increased the burden on hospital staff world-wide. Through the redistribution of scarce resources to these high-priority cases, the cardiac sector has fallen behind. In efforts to reduce transmission, reduction in direct patient-physician contact has led to a backlog of cardiac cases. However, this accumulation of postponed or cancelled nonurgent cardiac care seems to be resolvable with the assistance of technology. From telemedicine to artificial intelligence (AI), technology has transformed healthcare systems nationwide. Telemedicine enables patient monitoring from a distance, while AI unveils a whole new realm of possibilities in clinical practice, examples include: traditional systems replacement with more efficient and accurate processing machines; automation of clerical process; and triage assistance through risk predictions. These possibilities are driven by deep and machine learning. The two subsets of AI are explored and limitations regarding \"big data\" are discussed. The aims of this review are to explore AI: the advancements in methodology; current integration in cardiac surgery or other clinical scenarios; and potential future roles, which are innately nearing as the COVID-19 era urges alternative approaches for care.", "journal": "Journal of cardiac surgery", "date": "2021-02-11", "authors": ["Raveena KKhalsa", "ArwaKhashkhusha", "SaraZaidi", "AmerHarky", "MohamadBashir"], "doi": "10.1111/jocs.15417\n10.1016/j.jcmg.2020.05.004\n10.1007/s11886-013-0441-8\n10.1177/2047487320922926\n10.1080/00015385.2020.1787636\n10.1016/j.jtcvs.2018.09.124\n10.1007/s10916-018-1029-z\n10.1007/s00146-020-00978-0\n10.1007/s00068-020-01444-8"}
{"title": "Classification of COVID-19 by Compressed Chest CT Image through Deep Learning on a Large Patients Cohort.", "abstract": "Corona Virus Disease (COVID-19) has spread globally quickly, and has resulted in a large number of causalities and medical resources insufficiency in many countries. Reverse-transcriptase polymerase chain reaction (RT-PCR) testing is adopted as biopsy tool for confirmation of virus infection. However, its accuracy is as low as 60-70%, which is inefficient to uncover the infected. In comparison, the chest CT has been considered as the prior choice in diagnosis and monitoring progress of COVID-19 infection. Although the COVID-19 diagnostic systems based on artificial intelligence have been developed for assisting doctors in diagnosis, the small sample size and the excessive time consumption limit their applications. To this end, this paper proposed a diagnosis prototype system for COVID-19 infection testing. The proposed deep learning model is trained and is tested on 2267 CT sequences from 1357 patients clinically confirmed with COVID-19 and 1235 CT sequences from non-infected people. The main highlights of the prototype system are: (1) no data augmentation is needed to accurately discriminate the COVID-19 from normal controls with the specificity of 0.92 and sensitivity of 0.93; (2) the raw DICOM image is not necessary in testing. Highly compressed image like Jpeg can be used to allow a quick diagnosis; and (3) it discriminates the virus infection within 6 seconds and thus allows an online test with light cost. We also applied our model on 48 asymptomatic patients diagnosed with COVID-19. We found that: (1) the positive rate of RT-PCR assay is 63.5% (687/1082). (2) 45.8% (22/48) of the RT-PCR assay is negative for asymptomatic patients, yet the accuracy of CT scans is 95.8%. The online detection system is available: http://212.64.70.65/covid .", "journal": "Interdisciplinary sciences, computational life sciences", "date": "2021-02-11", "authors": ["ZiweiZhu", "ZhangXingming", "GuihuaTao", "TingtingDan", "JiaoLi", "XijieChen", "YangLi", "ZhichaoZhou", "XiangZhang", "JinzhaoZhou", "DongpeiChen", "HanchunWen", "HongminCai"], "doi": "10.1007/s12539-020-00408-1\n10.1148/radiol.2020200642\n10.1109/TIP.2014.2298981\n10.1148/radiol.2020200230\n10.1002/ppul.24885\n10.1148/radiol.2020200432\n10.1016/j.jinf.2020.03.041\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200330\n10.1148/radiol.2020200527\n10.1016/j.cmi.2020.04.040\n10.1016/s1473-3099(20)30134-1\n10.1148/radiol.2020200905\n10.1109/TMI.2020.2976825\n10.1109/TNSRE.2020.2973434\n10.1109/TNSRE.2019.2915621\n10.1001/jamanetworkopen.2020.10182"}
{"title": "The value of artificial intelligence and imaging diagnosis in the fight against COVID-19.", "abstract": "The outbreak of the new type of coronavirus pneumonia (COVID-19) has caused a huge impact on the world. In this case, only by adhering to the prevention and control methods of early diagnosis, early isolation, and early treatment, can the spread of the virus be prevented to the greatest extent. This article uses artificial intelligence-assisted medical imaging diagnosis as the research object, combines artificial intelligence and CT medical imaging diagnosis, introduces an intelligent COVID-19 detection system, and uses it to achieve COVID-19 disease screening and lesion evaluation. CT examination has the advantages of fast speed and high accuracy, which can provide a favorable basis for clinical diagnosis. This article collected 32 lung CT scan images of patients with confirmed COVID-19. Two professional radiologists analyzed the CT images using traditional imaging diagnostic methods and artificial intelligence-assisted imaging diagnostic methods, and the comparison showed the gap between the two methods. According to experiments, CT imaging diagnosis assisted by artificial intelligence only takes 0.744 min on average, which can save a lot of time and cost compared with the average time of 3.623 min for conventional diagnosis. In terms of comprehensive test accuracy, it can be concluded that the combination of artificial intelligence and imaging diagnosis has extremely high application value in COVID-19 diagnosis.", "journal": "Personal and ubiquitous computing", "date": "2021-02-11", "authors": ["DandanZhang", "XiaoyaLiu", "MingyueShao", "YapingSun", "QingyuanLian", "HongmeiZhang"], "doi": "10.1007/s00779-021-01522-7\n10.1111/j.1365-2133.2004.06210.x\n10.14366/usg.15052\n10.1007/s10278-016-9884-y\n10.1016/j.cmpb.2015.10.010\n10.1007/s11042-015-2826-8\n10.1166/jmihi.2019.2623\n10.1016/j.eswa.2015.02.005\n10.1183/23120541.00182-2020\n10.4236/ojrad.2020.103012\n10.1111/apt.15874\n10.1166/asl.2018.10697\n10.1016/j.compedu.2017.06.001\n10.14445/22315381/IJETT-V39P209\n10.1002/ima.22115\n10.1007/s00521-018-3877-3\n10.1007/s00521-020-04905-8"}
{"title": "Deep Learning Algorithm Trained with COVID-19 Pneumonia Also Identifies Immune Checkpoint Inhibitor Therapy-Related Pneumonitis.", "abstract": "Coronavirus disease 2019 (COVID-19) pneumonia and immune checkpoint inhibitor (ICI) therapy-related pneumonitis share common features. The aim of this study was to determine on chest computed tomography (CT) images whether a deep convolutional neural network algorithm is able to solve the challenge of differential diagnosis between COVID-19 pneumonia and ICI therapy-related pneumonitis.\nWe enrolled three groups: a pneumonia-free group (\nThe algorithm showed low specificity in distinguishing COVID-19 from ICI therapy-related pneumonitis (sensitivity 97.1%, specificity 14.3%, area under the curve (AUC) = 0.62). ICI therapy-related pneumonitis was identified by the AI when compared to pneumonia-free controls (sensitivity = 85.7%, specificity 100%, AUC = 0.97).\nThe deep learning algorithm is not able to distinguish between COVID-19 pneumonia and ICI therapy-related pneumonitis. Awareness must be increased among clinicians about imaging similarities between COVID-19 and ICI therapy-related pneumonitis. ICI therapy-related pneumonitis can be applied as a challenge population for cross-validation to test the robustness of AI models used to analyze interstitial pneumonias of variable etiology.", "journal": "Cancers", "date": "2021-02-11", "authors": ["Carlo AugustoMallio", "AndreaNapolitano", "GennaroCastiello", "Francesco MariaGiordano", "PasqualeD'Alessio", "MarioIozzino", "YipengSun", "SilviaAngeletti", "MarcoRussano", "DanieleSantini", "GiuseppeTonini", "Bruno BeomonteZobel", "BrunoVincenzi", "Carlo CosimoQuattrocchi"], "doi": "10.3390/cancers13040652\n10.1056/NEJMe2002387\n10.1101/2020.02.07.937862\n10.1148/radiol.2020200230\n10.1148/radiol.2020201237\n10.1148/radiol.2020201365\n10.1148/radiol.2020200905\n10.1148/rg.2019190036\n10.1136/esmoopen-2017-000213\n10.1158/1078-0432.CCR-15-0685\n10.1200/JCO.2016.68.2005\n10.1001/jamaoncol.2016.2453\n10.1053/j.seminoncol.2010.09.003\n10.3892/ol.2017.6919\n10.2217/imt-2020-0067\n10.1002/jmv.25897\n10.1177/1078155217745144\n10.1016/S0140-6736(13)60250-0\n10.1016/j.cell.2018.02.010\n10.1097/RLI.0000000000000127\n10.1109/TMI.2016.2535865\n10.1148/ryct.2020200075\n10.21037/qims-20-782\n10.1016/S2589-7500(20)30199-0\n10.1097/RTI.0000000000000524\n10.1148/ryct.2020200047\n10.1148/ryct.2020204001\n10.1148/radiol.2020200843\n10.1136/jclinpath-2020-206522\n10.3390/cancers11030305\n10.1080/14712598.2020.1789097\n10.21037/qims-20-1306"}
{"title": "Multiscale Attention Guided Network for COVID-19 Diagnosis Using Chest X-Ray Images.", "abstract": "Coronavirus disease 2019 (COVID-19) is one of the most destructive pandemic after millennium, forcing the world to tackle a health crisis. Automated lung infections classification using chest X-ray (CXR) images could strengthen diagnostic capability when handling COVID-19. However, classifying COVID-19 from pneumonia cases using CXR image is a difficult task because of shared spatial characteristics, high feature variation and contrast diversity between cases. Moreover, massive data collection is impractical for a newly emerged disease, which limited the performance of data thirsty deep learning models. To address these challenges, Multiscale Attention Guided deep network with Soft Distance regularization (MAG-SD) is proposed to automatically classify COVID-19 from pneumonia CXR images. In MAG-SD, MA-Net is used to produce prediction vector and attention from multiscale feature maps. To improve the robustness of trained model and relieve the shortage of training data, attention guided augmentations along with a soft distance regularization are posed, which aims at generating meaningful augmentations and reduce noise. Our multiscale attention model achieves better classification performance on our pneumonia CXR image dataset. Plentiful experiments are proposed for MAG-SD which demonstrates its unique advantage in pneumonia classification over cutting-edge models. The code is available at https://github.com/JasonLeeGHub/MAG-SD.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-02-10", "authors": ["JingxiongLi", "YaqiWang", "ShuaiWang", "JunWang", "JunLiu", "QunJin", "LinglingSun"], "doi": "10.1109/JBHI.2021.3058293\n10.1109/TCBB.2019.2911947"}
{"title": "COVID-19 Imaging: What We Know Now and What Remains Unknown.", "abstract": "Infection with SARS-CoV-2 ranges from an asymptomatic condition to a severe and sometimes fatal disease, with mortality most frequently being the result of acute lung injury. The role of imaging has evolved during the pandemic, with CT initially being an alternative and possibly superior testing method compared with reverse transcriptase-polymerase chain reaction (RT-PCR) testing and evolving to having a more limited role based on specific indications. Several classification and reporting schemes were developed for chest imaging early during the pandemic for patients suspected of having COVID-19 to aid in triage when the availability of RT-PCR testing was limited and its level of performance was unclear. Interobserver agreement for categories with findings typical of COVID-19 and those suggesting an alternative diagnosis is high across multiple studies. Furthermore, some studies looking at the extent of lung involvement on chest radiographs and CT images showed correlations with critical illness and a need for mechanical ventilation. In addition to pulmonary manifestations, cardiovascular complications such as thromboembolism and myocarditis have been ascribed to COVID-19, sometimes contributing to neurologic and abdominal manifestations. Finally, artificial intelligence has shown promise for use in determining both the diagnosis and prognosis of COVID-19 pneumonia with respect to both radiography and CT.", "journal": "Radiology", "date": "2021-02-10", "authors": ["Jeffrey PKanne", "HarrisonBai", "AdamBernheim", "MichaelChung", "Linda BHaramati", "David FKallmes", "Brent PLittle", "Geoffrey DRubin", "NicolaSverzellati"], "doi": "10.1148/radiol.2021204522"}
{"title": "COVID-19 lung CT image segmentation using deep learning methods: U-Net versus SegNet.", "abstract": "Currently, there is an urgent need for efficient tools to assess the diagnosis of COVID-19 patients. In this paper, we present feasible solutions for detecting and labeling infected tissues on CT lung images of such patients. Two structurally-different deep learning techniques, SegNet and U-NET, are investigated for semantically segmenting infected tissue regions in CT lung images.\nWe propose to use two known deep learning networks, SegNet and U-NET, for image tissue classification. SegNet is characterized as a scene segmentation network and U-NET as a medical segmentation tool. Both networks were exploited as binary segmentors to discriminate between infected and healthy lung tissue, also as multi-class segmentors to learn the infection type on the lung. Each network is trained using seventy-two data images, validated on ten images, and tested against the left eighteen images. Several statistical scores are calculated for the results and tabulated accordingly.\nThe results show the superior ability of SegNet in classifying infected/non-infected tissues compared to the other methods (with 0.95 mean accuracy), while the U-NET shows better results as a multi-class segmentor (with 0.91 mean accuracy).\nSemantically segmenting CT scan images of COVID-19 patients is a crucial goal because it would not only assist in disease diagnosis, also help in quantifying the severity of the illness, and hence, prioritize the population treatment accordingly. We propose computer-based techniques that prove to be reliable as detectors for infected tissue in lung CT scans. The availability of such a method in today's pandemic would help automate, prioritize, fasten, and broaden the treatment of COVID-19 patients globally.", "journal": "BMC medical imaging", "date": "2021-02-10", "authors": ["AdnanSaood", "IyadHatem"], "doi": "10.1186/s12880-020-00529-5\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06801-0\n10.1016/j.procs.2020.03.295\n10.1016/j.eswa.2019.112855\n10.1016/j.imu.2020.100357\n10.1016/j.procs.2018.01.104\n10.13053/cys-22-3-2526\n10.3390/s20051516\n10.1016/j.neucom.2018.12.085\n10.1016/j.neunet.2020.01.005\n10.1016/j.ultras.2019.03.014\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100360\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.104037\n10.1186/s41747-020-00173-2\n10.1109/TPAMI.2016.2644615\n10.1016/j.icte.2020.04.010"}
{"title": "COVIDetection-Net: A tailored COVID-19 detection from chest radiography images using deep learning.", "abstract": "In this study, a medical system based on Deep Learning (DL) which we called \"COVIDetection-Net\" is proposed for automatic detection of new corona virus disease 2019 (COVID-19) infection from chest radiography images (CRIs). The proposed system is based on ShuffleNet and SqueezeNet architecture to extract deep learned features and Multiclass Support Vector Machines (MSVM) for detection and classification. Our dataset contains 1200 CRIs that collected from two different publicly available databases. Extensive experiments were carried out using the proposed model. The highest detection accuracy of 100 % for COVID/NonCOVID, 99.72 % for COVID/Normal/pneumonia and 94.44 % for COVID/Normal/Bacterial pneumonia/Viral pneumonia have been obtained. The proposed system superior all published methods in recall, specificity, precision, F1-Score and accuracy. Confusion Matrix (CM) and Receiver Operation Characteristics (ROC) analysis are also used to depict the performance of the proposed model. Hence the proposed COVIDetection-Net can serve as an efficient system in the current state of COVID-19 pandemic and can be used in everywhere that are facing shortage of test kits.", "journal": "Optik", "date": "2021-02-09", "authors": ["Ahmed SElkorany", "Zeinab FElsharkawy"], "doi": "10.1016/j.ijleo.2021.166405"}
{"title": "A narrative review on characterization of acute respiratory distress syndrome in COVID-19-infected lungs using artificial intelligence.", "abstract": "COVID-19 has infected 77.4 million people worldwide and has caused 1.7 million fatalities as of December 21, 2020. The primary cause of death due to COVID-19 is Acute Respiratory Distress Syndrome (ARDS). According to the World Health Organization (WHO), people who are at least 60 years old or have comorbidities that have primarily been targeted are at the highest risk from SARS-CoV-2. Medical imaging provides a non-invasive, touch-free, and relatively safer alternative tool for diagnosis during the current ongoing pandemic. Artificial intelligence (AI) scientists are developing several intelligent computer-aided diagnosis (CAD) tools in multiple imaging modalities, i.e., lung computed tomography (CT), chest X-rays, and lung ultrasounds. These AI tools assist the pulmonary and critical care clinicians through (a) faster detection of the presence of a virus, (b) classifying pneumonia types, and (c) measuring the severity of viral damage in COVID-19-infected patients. Thus, it is of the utmost importance to fully understand the requirements of for a fast and successful, and timely lung scans analysis. This narrative review first presents the pathological layout of the lungs in the COVID-19 scenario, followed by understanding and then explains the comorbid statistical distributions in the ARDS framework. The novelty of this review is the approach to classifying the AI models as per the by school of thought (SoTs), exhibiting based on segregation of techniques and their characteristics. The study also discusses the identification of AI models and its extension from non-ARDS lungs (pre-COVID-19) to ARDS lungs (post-COVID-19). Furthermore, it also presents AI workflow considerations of for medical imaging modalities in the COVID-19 framework. Finally, clinical AI design considerations will be discussed. We conclude that the design of the current existing AI models can be improved by considering comorbidity as an independent factor. Furthermore, ARDS post-processing clinical systems must involve include (i) the clinical validation and verification of AI-models, (ii) reliability and stability criteria, and (iii) easily adaptable, and (iv) generalization assessments of AI systems for their use in pulmonary, critical care, and radiological settings.", "journal": "Computers in biology and medicine", "date": "2021-02-08", "authors": ["Jasjit SSuri", "SushantAgarwal", "Suneet KGupta", "AnudeepPuvvula", "MainakBiswas", "LucaSaba", "ArindamBit", "Gopal STandel", "MohitAgarwal", "AnubhavPatrick", "GavinoFaa", "Inder MSingh", "RonaldOberleitner", "MonikaTurk", "Paramjit SChadha", "Amer MJohri", "JMiguel Sanches", "Narendra NKhanna", "KlaudijaViskovic", "SophieMavrogeni", "John RLaird", "GyanPareek", "MartinMiner", "David WSobel", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "AthanasiosProtogerou", "Durga PrasannaMisra", "VikasAgarwal", "George DKitas", "PuneetAhluwalia", "JagjitTeji", "MustafaAl-Maini", "Surinder KDhanjil", "MeyypanSockalingam", "AjitSaxena", "AndrewNicolaides", "AdityaSharma", "VijayRathore", "Janet N AAjuluchukwu", "MostafaFatemi", "AzraAlizad", "VijayViswanathan", "P KKrishnan", "SubbaramNaidu"], "doi": "10.1016/j.compbiomed.2021.104210\n10.1016/j.irbm.2020.1007.1001"}
{"title": "Modality alignment contrastive learning for severity assessment of COVID-19 from lung ultrasound and clinical information.", "abstract": "The outbreak of COVID-19 around the world has caused great pressure to the health care system, and many efforts have been devoted to artificial intelligence (AI)-based analysis of CT and chest X-ray images to help alleviate the shortage of radiologists and improve the diagnosis efficiency. However, only a few works focus on AI-based lung ultrasound (LUS) analysis in spite of its significant role in COVID-19. In this work, we aim to propose a novel method for severity assessment of COVID-19 patients from LUS and clinical information. Great challenges exist regarding the heterogeneous data, multi-modality information, and highly nonlinear mapping. To overcome these challenges, we first propose a dual-level supervised multiple instance learning module (DSA-MIL) to effectively combine the zone-level representations into patient-level representations. Then a novel modality alignment contrastive learning module (MA-CLR) is presented to combine representations of the two modalities, LUS and clinical information, by matching the two spaces while keeping the discriminative features. To train the nonlinear mapping, a staged representation transfer (SRT) strategy is introduced to maximumly leverage the semantic and discriminative information from the training data. We trained the model with LUS data of 233 patients, and validated it with 80 patients. Our method can effectively combine the two modalities and achieve accuracy of 75.0% for 4-level patient severity assessment, and 87.5% for the binary severe/non-severe identification. Besides, our method also provides interpretation of the severity assessment by grading each of the lung zone (with accuracy of 85.28%) and identifying the pathological patterns of each lung zone. Our method has a great potential in real clinical practice for COVID-19 patients, especially for pregnant women and children, in aspects of progress monitoring, prognosis stratification, and patient management.", "journal": "Medical image analysis", "date": "2021-02-08", "authors": ["WufengXue", "ChunyanCao", "JieLiu", "YilianDuan", "HaiyanCao", "JianWang", "XuminTao", "ZejianChen", "MengWu", "JinxiangZhang", "HuiSun", "YangJin", "XinYang", "RuobingHuang", "FeixiangXiang", "YueSong", "ManjieYou", "WenZhang", "LiliJiang", "ZimingZhang", "ShuangshuangKong", "YingTian", "LiZhang", "DongNi", "MingxingXie"], "doi": "10.1016/j.media.2021.101975\n10.1101/2020.05.11.20093732"}
{"title": "SARS-CoV-2 diagnosis using medical imaging techniques and artificial intelligence: A review.", "abstract": "SARS-CoV-2 is a worldwide health emergency with unrecognized clinical features. This paper aims to review the most recent medical imaging techniques used for the diagnosis of SARS-CoV-2 and their potential contributions to attenuate the pandemic. Recent researches, including artificial intelligence tools, will be described.\nWe review the main clinical features of SARS-CoV-2 revealed by different medical imaging techniques. First, we present the clinical findings of each technique. Then, we describe several artificial intelligence approaches introduced for the SARS-CoV-2 diagnosis.\nCT is the most accurate diagnostic modality of SARS-CoV-2. Additionally, ground-glass opacities and consolidation are the most common signs of SARS-CoV-2 in CT images. However, other findings such as reticular pattern, and crazy paving could be observed. We also found that pleural effusion and pneumothorax features are less common in SARS-CoV-2. According to the literature, the B lines artifacts and pleural line irregularities are the common signs of SARS-CoV-2 in ultrasound images. We have also stated the different studies, focusing on artificial intelligence tools, to evaluate the SARS-CoV-2 severity. We found that most of the reported works based on deep learning focused on the detection of SARS-CoV-2 from medical images while the challenge for the radiologists is how to differentiate between SARS-CoV-2 and other viral infections with the same clinical features.\nThe identification of SARS-CoV-2 manifestations on medical images is a key step in radiological workflow for the diagnosis of the virus and could be useful for researchers working on computer-aided diagnosis of pulmonary infections.", "journal": "Clinical imaging", "date": "2021-02-06", "authors": ["NarjesBenameur", "RamziMahmoudi", "SorayaZaid", "YounesArous", "BadiiHmida", "Mohamed HediBedoui"], "doi": "10.1016/j.clinimag.2021.01.019\n10.1038/s41591-020-0820-9\n10.1071/MA20013\n10.3201/eid2606.200239\n10.1007/s11606-020-05762-w\n10.1016/S2468-1253(20)30048-0\n10.1053/j.gastro.2020.02.054\n10.1038/s41591-020-0817-4\n10.1016/j.jinf.2020.03.041\n10.1007/s00428-020-02829-1\n10.1007/s00330-020-06854-1\n10.1053/j.gastro.2020.04.008\n10.1053/j.gastro.2020.02.055\n10.1007/s00134-020-05985-9\n10.1128/JVI.79.23.14614-14621.2005\n10.1007/s10067-020-05073-9\n10.1097/01.ju.0000047363.03411.6b\n10.1016/j.cmi.2020.04.001\n10.1007/s42058-020-00030-6\n10.1007/s00259-020-04735-9\n10.1016/j.clinimag.2020.04.001\n10.21037/tlcr.2017.01.02\n10.1148/radiol.2020201160\n10.1016/j.ejim.2020.04.037\n10.1148/ryct.2020200034\n10.1007/s00330-020-06823-8\n10.1016/j.ajem.2020.04.016\n10.1016/j.jinf.2020.03.035\n10.1016/j.jinf.2020.04.004\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.2214/AJR.20.22959\n10.2214/AJR.20.22954\n10.1016/S0140-6736(20)30211-7\n10.1007/s00330-020-06801-0\n10.1007/s13244-012-0207-7\n10.1007/s13244-010-0060-5\n10.2214/AJR.20.22975\n10.2214/AJR.08.1286\n10.1016/j.ejro.2020.100231\n10.1148/radiol.2020200236\n10.2214/AJR.20.22969\n10.1007/s00247-003-1081-8\n10.1016/j.jrid.2020.04.001\n10.1007/s00134-020-05996-6\n10.1186/s13089-020-00171-w\n10.1016/j.jcrc.2015.08.021\n10.1016/j.ultrasmedbio.2014.10.002\n10.1016/j.jen.2020.07.010\n10.1097/ALN.0000000000003303\n10.1148/radiol.2020200905\n10.1101/2020.03.20.20039834\n10.1101/2020.02.25.20021568\n10.1101/2020.03.12.20027185\n10.1101/2020.02.23.20026930\n10.1016/j.compbiomed.2020.103792"}
{"title": "Anam-Net: Anamorphic Depth Embedding-Based Lightweight CNN for Segmentation of Anomalies in COVID-19 Chest CT Images.", "abstract": "Chest computed tomography (CT) imaging has become indispensable for staging and managing coronavirus disease 2019 (COVID-19), and current evaluation of anomalies/abnormalities associated with COVID-19 has been performed majorly by the visual score. The development of automated methods for quantifying COVID-19 abnormalities in these CT images is invaluable to clinicians. The hallmark of COVID-19 in chest CT images is the presence of ground-glass opacities in the lung region, which are tedious to segment manually. We propose anamorphic depth embedding-based lightweight CNN, called Anam-Net, to segment anomalies in COVID-19 chest CT images. The proposed Anam-Net has 7.8 times fewer parameters compared to the state-of-the-art UNet (or its variants), making it lightweight capable of providing inferences in mobile or resource constraint (point-of-care) platforms. The results from chest CT images (test cases) across different experiments showed that the proposed method could provide good Dice similarity scores for abnormal and normal regions in the lung. We have benchmarked Anam-Net with other state-of-the-art architectures, such as ENet, LEDNet, UNet++, SegNet, Attention UNet, and DeepLabV3+. The proposed Anam-Net was also deployed on embedded systems, such as Raspberry Pi 4, NVIDIA Jetson Xavier, and mobile-based Android application (CovSeg) embedded with Anam-Net to demonstrate its suitability for point-of-care platforms. The generated codes, models, and the mobile application are available for enthusiastic users at https://github.com/NaveenPaluru/Segmentation-COVID-19.", "journal": "IEEE transactions on neural networks and learning systems", "date": "2021-02-06", "authors": ["NaveenPaluru", "AveenDayal", "Havard BjorkeJenssen", "TomasSakinis", "Linga ReddyCenkeramaddi", "JayaPrakash", "Phaneendra KYalavarthy"], "doi": "10.1109/TNNLS.2021.3054746"}
{"title": "Development and Validation of a Machine Learning Approach for Automated Severity Assessment of COVID-19 Based on Clinical and Imaging Data: Retrospective Study.", "abstract": "COVID-19 has overwhelmed health systems worldwide. It is important to identify severe cases as early as possible, such that resources can be mobilized and treatment can be escalated.\nThis study aims to develop a machine learning approach for automated severity assessment of COVID-19 based on clinical and imaging data.\nClinical data-including demographics, signs, symptoms, comorbidities, and blood test results-and chest computed tomography scans of 346 patients from 2 hospitals in the Hubei Province, China, were used to develop machine learning models for automated severity assessment in diagnosed COVID-19 cases. We compared the predictive power of the clinical and imaging data from multiple machine learning models and further explored the use of four oversampling methods to address the imbalanced classification issue. Features with the highest predictive power were identified using the Shapley Additive Explanations framework.\nImaging features had the strongest impact on the model output, while a combination of clinical and imaging features yielded the best performance overall. The identified predictive features were consistent with those reported previously. Although oversampling yielded mixed results, it achieved the best model performance in our study. Logistic regression models differentiating between mild and severe cases achieved the best performance for clinical features (area under the curve [AUC] 0.848; sensitivity 0.455; specificity 0.906), imaging features (AUC 0.926; sensitivity 0.818; specificity 0.901), and a combination of clinical and imaging features (AUC 0.950; sensitivity 0.764; specificity 0.919). The synthetic minority oversampling method further improved the performance of the model using combined features (AUC 0.960; sensitivity 0.845; specificity 0.929).\nClinical and imaging features can be used for automated severity assessment of COVID-19 and can potentially help triage patients with COVID-19 and prioritize care delivery to those at a higher risk of severe disease.", "journal": "JMIR medical informatics", "date": "2021-02-04", "authors": ["Juan CarlosQuiroz", "You-ZhenFeng", "Zhong-YuanCheng", "DanaRezazadegan", "Ping-KangChen", "Qi-TingLin", "LongQian", "Xiao-FangLiu", "ShlomoBerkovsky", "EnricoCoiera", "LeiSong", "XiaomingQiu", "SidongLiu", "Xiang-RanCai"], "doi": "10.2196/24572\n10.1016/j.jcv.2020.104357\n10.1001/jama.2020.7872\n10.1001/jama.2020.6775\n10.1001/jamacardio.2020.1286\n10.1136/bmj.m1966\n10.1016/j.eng.2020.03.002\n10.1148/ryct.2020200110\n10.1148/radiol.2020200642\n10.1101/2020.02.27.20028027v3\n10.1101/2020.02.27.20028027\n10.1007/s10489-020-01714-3\n10.1101/2020.02.25.20021568v2\n10.1101/2020.02.25.20021568\n10.1148/radiol.2020200905\n10.1101/2020.02.23.20026930v1\n10.1101/2020.02.23.20026930\n10.1101/2020.02.14.20023028\n10.1101/2020.08.03.20167007\n10.1148/radiol.2020201491\n10.1016/j.compbiomed.2020.103792\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-76550-z\n10.1038/s41591-020-0931-3\n10.3389/fbioe.2020.00898\n10.3389/fbioe.2020.00898\n10.1109/JBHI.2020.3030853\n10.1109/ACCESS.2020.3005510\n10.1109/JBHI.2020.3023246\n10.1109/access.2020.3025164\n10.1148/ryct.2020200034\n10.1136/bmj.m1328\n10.1109/RBME.2020.2987975\n10.1109/ACCESS.2020.3001973\n10.1016/j.chaos.2020.110059\n10.1016/S0140-6736(20)30183-5\n10.1016/S1473-3099(20)30086-4\n10.1145/2939672.2939785\n10.1613/jair.953\n10.1109/IJCNN.2008.4633969\n10.1016/j.ins.2019.06.007\n10.1016/j.ins.2019.06.007\n10.1038/s41598-020-64588-y\n10.1038/s41598-020-64588-y\n10.1186/s41747-020-00173-2\n10.3390/info11020108\n10.3390/info11020108\n10.1109/wacv.2017.58\n10.1002/1097-0142(1950)3:1<32::aid-cncr2820030106>3.0.co;2-3\n10.1016/j.artmed.2015.07.005\n10.1186/s12938-017-0420-1\n10.1186/s12938-017-0420-1\n10.1016/j.cie.2019.106266\n10.1016/j.cie.2019.106266\n10.1186/s12938-018-0604-3\n10.1186/s12938-018-0604-3\n10.1109/TPAMI.2013.50\n10.1145/1273496.1273592\n10.7326/M18-1376?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dpubmed\n10.7326/M18-1376"}
{"title": "Establishing Classifiers With Clinical Laboratory Indicators to Distinguish COVID-19 From Community-Acquired Pneumonia: Retrospective Cohort Study.", "abstract": "The initial symptoms of patients with COVID-19 are very much like those of patients with community-acquired pneumonia (CAP); it is difficult to distinguish COVID-19 from CAP with clinical symptoms and imaging examination.\nThe objective of our study was to construct an effective model for the early identification of COVID-19 that would also distinguish it from CAP.\nThe clinical laboratory indicators (CLIs) of 61 COVID-19 patients and 60 CAP patients were analyzed retrospectively. Random combinations of various CLIs (ie, CLI combinations) were utilized to establish COVID-19 versus CAP classifiers with machine learning algorithms, including random forest classifier (RFC), logistic regression classifier, and gradient boosting classifier (GBC). The performance of the classifiers was assessed by calculating the area under the receiver operating characteristic curve (AUROC) and recall rate in COVID-19 prediction using the test data set.\nThe classifiers that were constructed with three algorithms from 43 CLI combinations showed high performance (recall rate >0.9 and AUROC >0.85) in COVID-19 prediction for the test data set. Among the high-performance classifiers, several CLIs showed a high usage rate; these included procalcitonin (PCT), mean corpuscular hemoglobin concentration (MCHC), uric acid, albumin, albumin to globulin ratio (AGR), neutrophil count, red blood cell (RBC) count, monocyte count, basophil count, and white blood cell (WBC) count. They also had high feature importance except for basophil count. The feature combination (FC) of PCT, AGR, uric acid, WBC count, neutrophil count, basophil count, RBC count, and MCHC was the representative one among the nine FCs used to construct the classifiers with an AUROC equal to 1.0 when using the RFC or GBC algorithms. Replacing any CLI in these FCs would lead to a significant reduction in the performance of the classifiers that were built with them.\nThe classifiers constructed with only a few specific CLIs could efficiently distinguish COVID-19 from CAP, which could help clinicians perform early isolation and centralized management of COVID-19 patients.", "journal": "Journal of medical Internet research", "date": "2021-02-04", "authors": ["WanfaDai", "Pei-FengKe", "Zhen-ZhenLi", "Qi-ZhenZhuang", "WeiHuang", "YiWang", "YujuanXiong", "Xian-ZhangHuang"], "doi": "10.2196/23390\n10.1038/s41564-020-0695-z\n10.1016/j.ijid.2020.03.042\n10.1001/jama.2020.2648\n10.1186/s13054-020-2817-7\n10.1186/s13054-020-2817-7\n10.1056/NEJMoa2002032\n10.1056/NEJMoa2001316\n10.3390/jcm9020538\n10.1002/ajh.25829\n10.1093/cid/ciaa414\n10.1093/cid/ciaa538\n10.1093/cid/ciz545\n10.1097/01.CCM.0000235673.40874.BD\n10.3390/jcm8060843\n10.1111/joim.12822\n10.1111/joim.12822\n10.2196/21439\n10.1038/s41591-020-0916-2\n10.1039/c7lc00955k\n10.25080/majora-92bf1922-011\n10.1023/A:1010933404324.pdf\n10.1023/A:1010933404324\n10.1111/acem.14182\n10.1111/acem.14182\n10.3389/fcell.2020.00683\n10.3389/fcell.2020.00683\n10.1093/cid/ciaa443\n10.2196/21439\n10.1016/j.ijid.2020.04.041\n10.21037/atm.2016.03.42\n10.21037/atm.2016.03.42\n10.1007/s11255-017-1732-6\n10.3389/fcimb.2020.00322\n10.3389/fcimb.2020.00322\n10.1016/j.jinf.2012.12.007\n10.1002/hep.1840040516"}
{"title": "Six artificial intelligence paradigms for tissue characterisation and classification of non-COVID-19 pneumonia against COVID-19 pneumonia in computed tomography lungs.", "abstract": "COVID-19 pandemic has currently no vaccines. Thus, the only feasible solution for prevention relies on the detection of COVID-19-positive cases through quick and accurate testing. Since artificial intelligence (AI) offers the powerful mechanism to automatically extract the tissue features and characterise the disease, we therefore hypothesise that AI-based strategies can provide quick detection and classification, especially for radiological computed tomography (CT) lung scans.\nSix models, two traditional machine learning (ML)-based (k-NN and RF), two transfer learning (TL)-based (VGG19 and InceptionV3), and the last two were our custom-designed deep learning (DL) models (CNN and iCNN), were developed for classification between COVID pneumonia (CoP) and non-COVID pneumonia (NCoP). K10 cross-validation (90% training: 10% testing) protocol on an Italian cohort of 100 CoP and 30 NCoP patients was used for performance evaluation and bispectrum analysis for CT lung characterisation.\nUsing K10 protocol, our results showed the accuracy in the order of DL\u2009>\u2009TL\u2009>\u2009ML, ranging the six accuracies for k-NN, RF, VGG19, IV3, CNN, iCNN as 74.58\u2009\u00b1\u20092.44%, 96.84\u2009\u00b1\u20092.6, 94.84\u2009\u00b1\u20092.85%, 99.53\u2009\u00b1\u20090.75%, 99.53\u2009\u00b1\u20091.05%, and 99.69\u2009\u00b1\u20090.66%, respectively. The corresponding AUCs were 0.74, 0.94, 0.96, 0.99, 0.99, and 0.99 (p-values\u2009<\u20090.0001), respectively. Our Bispectrum-based characterisation system suggested CoP can be separated against NCoP using AI models. COVID risk severity stratification also showed a high correlation of 0.7270 (p\u2009<\u20090.0001) with clinical scores such as ground-glass opacities (GGO), further validating our AI models.\nWe prove our hypothesis by demonstrating that all the six AI models successfully classified CoP against NCoP due to the strong presence of contrasting features such as ground-glass opacities (GGO), consolidations, and pleural effusion in CoP patients. Further, our online system takes\u2009<\u20092\u00a0s for inference.", "journal": "International journal of computer assisted radiology and surgery", "date": "2021-02-04", "authors": ["LucaSaba", "MohitAgarwal", "AnubhavPatrick", "AnudeepPuvvula", "Suneet KGupta", "AlessandroCarriero", "John RLaird", "George DKitas", "Amer MJohri", "AntonellaBalestrieri", "ZenoFalaschi", "AlessioPasch\u00e8", "VijayViswanathan", "AymanEl-Baz", "IqbalAlam", "AbhinavJain", "SubbaramNaidu", "RonaldOberleitner", "Narendra NKhanna", "ArindamBit", "MostafaFatemi", "AzraAlizad", "Jasjit SSuri"], "doi": "10.1007/s11548-021-02317-0\n10.1164/rccm.202003-0527LE\n10.1097/ALN.0000000000003296\n10.1016/S0140-6736(20)30183-5\n10.1016/j.cpcardiol.2020.100618\n10.1109/ACCESS.2020.3005510\n10.1007/s00330-020-07042-x\n10.1038/s41598-019-56847-4\n10.1148/ryai.2020200048\n10.1080/07391102.2020.1788642\n10.3233/XST-190545\n10.1007/s10916-017-0797-1\n10.1016/j.measurement.2017.01.016\n10.1016/j.patrec.2020.10.001\n10.3390/e22050517\n10.21595/chs.2020.21263\n10.1016/j.patrec.2018.07.026\n10.1097/RTI.0000000000000534\n10.1016/j.compbiomed.2020.103804\n10.2741/4876"}
{"title": "Fast and Accurate Detection of COVID-19 Along With 14 Other Chest Pathologies Using a Multi-Level Classification: Algorithm Development and Validation Study.", "abstract": "COVID-19 has spread very rapidly, and it is important to build a system that can detect it in order to help an overwhelmed health care system. Many research studies on chest diseases rely on the strengths of deep learning techniques. Although some of these studies used state-of-the-art techniques and were able to deliver promising results, these techniques are not very useful if they can detect only one type of disease without detecting the others.\nThe main objective of this study was to achieve a fast and more accurate diagnosis of COVID-19. This study proposes a diagnostic technique that classifies COVID-19 x-ray images from normal x-ray images and those specific to 14 other chest diseases.\nIn this paper, we propose a novel, multilevel pipeline, based on deep learning models, to detect COVID-19 along with other chest diseases based on x-ray images. This pipeline reduces the burden of a single network to classify a large number of classes. The deep learning models used in this study were pretrained on the ImageNet dataset, and transfer learning was used for fast training. The lungs and heart were segmented from the whole x-ray images and passed onto the first classifier that checks whether the x-ray is normal, COVID-19 affected, or characteristic of another chest disease. If it is neither a COVID-19 x-ray image nor a normal one, then the second classifier comes into action and classifies the image as one of the other 14 diseases.\nWe show how our model uses state-of-the-art deep neural networks to achieve classification accuracy for COVID-19 along with 14 other chest diseases and normal cases based on x-ray images, which is competitive with currently used state-of-the-art models. Due to the lack of data in some classes such as COVID-19, we applied 10-fold cross-validation through the ResNet50 model. Our classification technique thus achieved an average training accuracy of 96.04% and test accuracy of 92.52% for the first level of classification (ie, 3 classes). For the second level of classification (ie, 14 classes), our technique achieved a maximum training accuracy of 88.52% and test accuracy of 66.634% by using ResNet50. We also found that when all the 16 classes were classified at once, the overall accuracy for COVID-19 detection decreased, which in the case of ResNet50 was 88.92% for training data and 71.905% for test data.\nOur proposed pipeline can detect COVID-19 with a higher accuracy along with detecting 14 other chest diseases based on x-ray images. This is achieved by dividing the classification task into multiple steps rather than classifying them collectively.", "journal": "Journal of medical Internet research", "date": "2021-02-03", "authors": ["SalehAlbahli", "Ghulam Nabi Ahmad HassanYar"], "doi": "10.2196/23693\n10.1016/s2213-2600(20)30079-5\n10.2174/1573405616666200604163954\n10.1007/s12553-018-0244-4\n10.1148/radiol.2019181960\n10.1002/mp.13245\n10.1109/icassp.2018.8461430\n10.1109/cvpr.2017.369\n10.18653/v1/2020.emnlp-main.117\n10.3390/app9194130\n10.1007/s13246-020-00865-4\n10.1007/s10489-020-01829-7\n10.1101/2020.02.25.20021568v2\n10.1101/2020.02.25.20021568\n10.1148/radiol.2020200905\n10.20944/preprints202003.0300.v1\n10.1101/2020.02.14.20023028v3.full.pdf\n10.1016/j.cmpb.2020.105608\n10.1038/s41598-020-76550-z\n10.1038/s41598-020-76550-z\n10.1101/2020.02.23.20026930v1\n10.1101/2020.02.23.20026930\n10.1101/2020.03.12.20027185v2\n10.1101/2020.03.12.20027185\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103795\n10.1109/cvpr.2018.00907\n10.1109/cvpr.2017.195\n10.1109/CVPR.2016.308\n10.5555/3298023.3298188\n10.1109/cvpr.2016.90"}
{"title": "Quantitative Assessment of Chest CT Patterns in COVID-19 and Bacterial Pneumonia Patients: a Deep Learning Perspective.", "abstract": "It is difficult to distinguish subtle differences shown in computed tomography (CT) images of coronavirus disease 2019 (COVID-19) and bacterial pneumonia patients, which often leads to an inaccurate diagnosis. It is desirable to design and evaluate interpretable feature extraction techniques to describe the patient's condition.\nThis is a retrospective cohort study of 170 confirmed patients with COVID-19 or bacterial pneumonia acquired at Yeungnam University Hospital in Daegu, Korea. The Lung and lesion regions were segmented to crop the lesion into 2D patches to train a classifier model that could differentiate between COVID-19 and bacterial pneumonia. The K-means algorithm was used to cluster deep features extracted by the trained model into 20 groups. Each lesion patch cluster was described by a characteristic imaging term for comparison. For each CT image containing multiple lesions, a histogram of lesion types was constructed using the cluster information. Finally, a Support Vector Machine classifier was trained with the histogram and radiomics features to distinguish diseases and severity.\nThe 20 clusters constructed from 170 patients were reviewed based on common radiographic appearance types. Two clusters showed typical findings of COVID-19, with two other clusters showing typical findings related to bacterial pneumonia. Notably, there is one cluster that showed bilateral diffuse ground-glass opacities (GGOs) in the central and peripheral lungs and was considered to be a key factor for severity classification. The proposed method achieved an accuracy of 91.2% for classifying COVID-19 and bacterial pneumonia patients with 95% reported for severity classification. The CT quantitative parameters represented by the values of cluster 8 were correlated with existing laboratory data and clinical parameters.\nDeep chest CT analysis with constructed lesion clusters revealed well-known COVID-19 CT manifestations comparable to manual CT analysis. The constructed histogram features improved accuracy for both diseases and severity classification, and showed correlations with laboratory data and clinical parameters. The constructed histogram features can provide guidance for improved analysis and treatment of COVID-19.", "journal": "Journal of Korean medical science", "date": "2021-02-03", "authors": ["MyeongkyunKang", "Kyung SooHong", "PhilipChikontwe", "MiguelLuna", "Jong GeolJang", "JongsooPark", "Kyeong CheolShin", "Sang HyunPark", "June HongAhn"], "doi": "10.3346/jkms.2021.36.e46\n10.1109/ICCV.2017.89\n10.1101/2020.05.20.20100362\n10.1109/CVPR.2016.90\n10.21037/atm-20-3026"}
{"title": "Machine learning applied on chest x-ray can aid in the diagnosis of COVID-19: a first experience from Lombardy, Italy.", "abstract": "We aimed to train and test a deep learning classifier to support the diagnosis of coronavirus disease 2019 (COVID-19) using chest x-ray (CXR) on a cohort of subjects from two hospitals in Lombardy, Italy.\nWe used for training and validation an ensemble of ten convolutional neural networks (CNNs) with mainly bedside CXRs of 250 COVID-19 and 250 non-COVID-19 subjects from two hospitals (Centres 1 and 2). We then tested such system on bedside CXRs of an independent group of 110 patients (74 COVID-19, 36 non-COVID-19) from one of the two hospitals. A retrospective reading was performed by two radiologists in the absence of any clinical information, with the aim to differentiate COVID-19 from non-COVID-19 patients. Real-time polymerase chain reaction served as the reference standard.\nAt 10-fold cross-validation, our deep learning model classified COVID-19 and non-COVID-19 patients with 0.78 sensitivity (95% confidence interval [CI] 0.74-0.81), 0.82 specificity (95% CI 0.78-0.85), and 0.89 area under the curve (AUC) (95% CI 0.86-0.91). For the independent dataset, deep learning showed 0.80 sensitivity (95% CI 0.72-0.86) (59/74), 0.81 specificity (29/36) (95% CI 0.73-0.87), and 0.81 AUC (95% CI 0.73-0.87). Radiologists' reading obtained 0.63 sensitivity (95% CI 0.52-0.74) and 0.78 specificity (95% CI 0.61-0.90) in Centre 1 and 0.64 sensitivity (95% CI 0.52-0.74) and 0.86 specificity (95% CI 0.71-0.95) in Centre 2.\nThis preliminary experience based on ten CNNs trained on a limited training dataset shows an interesting potential of deep learning for COVID-19 diagnosis. Such tool is in training with new CXRs to further increase its performance.", "journal": "European radiology experimental", "date": "2021-02-03", "authors": ["IsabellaCastiglioni", "DavideIppolito", "MatteoInterlenghi", "Caterina BeatriceMonti", "ChristianSalvatore", "SimoneSchiaffino", "AnnalisaPolidori", "DavideGandola", "CristinaMessa", "FrancescoSardanelli"], "doi": "10.1186/s41747-020-00203-z\n10.1016/S0140-6736(20)30183-5\n10.1016/j.clinimag.2020.06.031\n10.3348/kjr.2020.0132\n10.1007/s40846-020-00529-4\n10.3233/XST-200715\n10.1016/j.mehy.2020.109761\n10.1148/radiol.2020200642\n10.1186/s41747-018-0061-6"}
{"title": "TLCoV- An automated Covid-19 screening model using Transfer Learning from chest X-ray images.", "abstract": "The Coronavirus disease (Covid-19) has been declared a pandemic by World Health Organisation (WHO) and till date caused 585,727 numbers of deaths all over the world. The only way to minimize the number of death is to quarantine the patients tested Corona positive. The quick spread of this disease can be reduced by automatic screening to cover the lack of radiologists. Though the researchers already have done extremely well to design pioneering deep learning models for the screening of Covid-19, most of them results in low accuracy rate. In addition, over-fitting problem increases difficulties for those models to learn on existing Covid-19 datasets. In this paper, an automated Covid-19 screening model is designed to identify the patients suffering from this disease by using their chest X-ray images. The model classifies the images in three categories - Covid-19 positive, other pneumonia infection and no infection. Three learning schemes such as CNN, VGG-16 and ResNet-50 are separately used to learn the model. A standard Covid-19 radiography dataset from the repository of Kaggle is used to get the chest X-ray images. The performance of the model with all the three learning schemes has been evaluated and it shows VGG-16 performed better as compared to CNN and ResNet-50. The model with VGG-16 gives the accuracy of 97.67%, precision of 96.65%, recall of 96.54% and F1 score of 96.59%. The performance evaluation also shows that our model outperforms two existing models to screen the Covid-19.", "journal": "Chaos, solitons, and fractals", "date": "2021-02-03", "authors": ["Ayan KumarDas", "SidraKalam", "ChiranjeevKumar", "DitipriyaSinha"], "doi": "10.1016/j.chaos.2021.110713\n10.1016/j.compbiomed.2017.08.022\n10.1016/j.chaos.2020.110176\n10.1007/s10489-020-01714-3\n10.1016/j.patrec.2020.03.011\n10.1016/j.chaos.2020.110245\n10.1147/JRD.2017.2708299\n10.1117/12.2043872\n10.1109/BHI.2017.7897215\n10.1038/nature21056\n10.1002/jbio.201700003\n10.1109/TMI.2020.2996256\n10.1038/s41591-018-0268-3\n10.1109/CVPR.2016.90\n10.1016/S0140-6736(20)30183-5\n10.1101/2020.03.20.20039834\n10.1101/2020.03.19.20039354\n10.1109/TMI.2020.2992546\n10.1016/j.ijantimicag.2020.105924\n10.1002/jmv.25678\n10.1016/j.chaos.2020.109853\n10.1101/2020.04.08.20057679\n10.1016/j.chaos.2020.110027\n10.1109/ACCESS.2020.2997311\n10.1007/s12098-020-03263-6\n10.1016/j.cmpb.2019.06.005\n10.1016/j.chaos.2020.110122\n10.1016/j.chaos.2020.110072\n10.1016/j.compmedimag.2019.101673\n10.1016/j.ins.2017.08.050\n10.1016/j.chaos.2020.110153\n10.1001/jama.2020.1585\n10.1109/TMI.2020.2995965\n10.1016/j.compbiomed.2018.09.009\n10.1101/2020.03.12.20027185\n10.1016/j.chaos.2020.110137"}
{"title": "COVID-19 Chest CT Image Segmentation Network by Multi-Scale Fusion and Enhancement Operations.", "abstract": "A novel coronavirus disease 2019 (COVID-19) was detected and has spread rapidly across various countries around the world since the end of the year 2019. Computed Tomography (CT) images have been used as a crucial alternative to the time-consuming RT-PCR test. However, pure manual segmentation of CT images faces a serious challenge with the increase of suspected cases, resulting in urgent requirements for accurate and automatic segmentation of COVID-19 infections. Unfortunately, since the imaging characteristics of the COVID-19 infection are diverse and similar to the backgrounds, existing medical image segmentation methods cannot achieve satisfactory performance. In this article, we try to establish a new deep convolutional neural network tailored for segmenting the chest CT images with COVID-19 infections. We first maintain a large and new chest CT image dataset consisting of 165,667 annotated chest CT images from 861 patients with confirmed COVID-19. Inspired by the observation that the boundary of the infected lung can be enhanced by adjusting the global intensity, in the proposed deep CNN, we introduce a feature variation block which adaptively adjusts the global properties of the features for segmenting COVID-19 infection. The proposed FV block can enhance the capability of feature representation effectively and adaptively for diverse cases. We fuse features at different scales by proposing Progressive Atrous Spatial Pyramid Pooling to handle the sophisticated infection areas with diverse appearance and shapes. The proposed method achieves state-of-the-art performance. Dice similarity coefficients are 0.987 and 0.726 for lung and COVID-19 segmentation, respectively. We conducted experiments on the data collected in China and Germany and show that the proposed deep CNN can produce impressive performance effectively. The proposed network enhances the segmentation ability of the COVID-19 infection, makes the connection with other techniques and contributes to the development of remedying COVID-19 infection.", "journal": "IEEE transactions on big data", "date": "2021-02-02", "authors": ["QingsenYan", "BoWang", "DongGong", "ChuanLuo", "WeiZhao", "JianhuShen", "JingyangAi", "QinfengShi", "YanningZhang", "ShuoJin", "LiangZhang", "ZhengYou"], "doi": "10.1109/TBDATA.2021.3056564\n10.1109/JBHI.2020.3042069"}
{"title": "Diagnosis of COVID-19 using CT scan images and deep learning techniques.", "abstract": "Early diagnosis of the coronavirus disease in 2019 (COVID-19) is essential for controlling this pandemic. COVID-19 has been spreading rapidly all over the world. There is no vaccine available for this virus yet. Fast and accurate COVID-19 screening is possible using computed tomography (CT) scan images. The deep learning techniques used in the proposed method is based on a convolutional neural network (CNN). Our manuscript focuses on differentiating the CT scan images of COVID-19 and non-COVID 19 CT using different deep learning techniques. A self-developed model named CTnet-10 was designed for the COVID-19 diagnosis, having an accuracy of 82.1%. Also, other models that we tested are DenseNet-169, VGG-16, ResNet-50, InceptionV3, and VGG-19. The VGG-19 proved to be superior with an accuracy of 94.52% as compared to all other deep learning models. Automated diagnosis of COVID-19 from the CT scan pictures can be used by the doctors as a quick and efficient method for COVID-19 screening.", "journal": "Emergency radiology", "date": "2021-02-02", "authors": ["VruddhiShah", "RinkalKeniya", "AkankshaShridharani", "ManavPunjabi", "JainamShah", "NinadMehendale"], "doi": "10.1007/s10140-020-01886-y\n10.1016/S0140-6736(20)30185-9\n10.1016/j.ijid.2020.03.070\n10.1128/JCM.00512-20\n10.1007/s00392-020-01626-9\n10.1002/jmv.25726\n10.1016/S0140-6736(13)61492-0\n10.1146/annurev-bioeng-071516-044442\n10.1109/ACCESS.2017.2788044\n10.1164/rccm.201705-0860OC\n10.2214/AJR.20.22976\n10.1097/RLI.0000000000000670\n10.2214/AJR.07.5212"}
{"title": "Convolutional neural network use chest radiography images for identification of COVID-19.", "abstract": "The epic Covid sickness 2019 (COVID-19) has turned into the significant danger to humankind in year 2020. The pandemic COVID-19 flare-up has influenced more than 2.7 million individuals and caused around 187 thousand fatalities worldwide [1] inside scarcely any months of its first appearance in Wuhan city of China and the number is developing quickly in various pieces of world. As researcher everywhere on the world are battling to discover the fix and treatment for COVID-19, the urgent advance fighting against COVID-19 is the screening of immense number of associated cases for disconnection and isolate with the patients. One of the key methodologies in screening of COVID-19 can be chest radiological imaging. The early investigations on the patients influenced by COVID-19 shows the attributes variations from the norm in chest radiography pictures. This introduced a chance to utilize distinctive counterfeit clever (AI) frameworks dependent on profound picking up utilizing chest radiology pictures for the recognition of COVID-19 and numerous such framework were proposed indicating promising outcomes. In this paper, we proposed a profound learning based convolution neural organization to characterize COVID-19, Pneumonia and Normal cases from chest radiology pictures. The proposed convolution neural organization (CNN) grouping model had the option to accomplish exactness of 94.85% on test dataset. The trial was completed utilizing the subset of information accessible in GitHub and Kaggle.", "journal": "Materials today. Proceedings", "date": "2021-02-02", "authors": ["DMurali", "EBhuvaneswari", "SParvathi", "A NSanjeev Kumar"], "doi": "10.1016/j.matpr.2020.10.866"}
{"title": "Automatic Screening of COVID-19 Using an Optimized Generative Adversarial Network.", "abstract": "The quick spread of coronavirus disease (COVID-19) has resulted in a global pandemic and more than fifteen million confirmed cases. To battle this spread, clinical imaging techniques, for example, computed tomography (CT), can be utilized for diagnosis. Automatic identification software tools are essential for helping to screen COVID-19 using CT images. However, there are few datasets available, making it difficult to train deep learning (DL) networks. To address this issue, a generative adversarial network (GAN) is proposed in this work to generate more CT images. The Whale Optimization Algorithm (WOA) is used to optimize the hyperparameters\u00a0of GAN's generator. The proposed method is tested and validated with different classification and meta-heuristics algorithms using the SARS-CoV-2 CT-Scan dataset, consisting of COVID-19 and non-COVID-19 images. The performance metrics of the proposed optimized model, including accuracy (99.22%), sensitivity (99.78%), specificity (97.78%), F1-score (98.79%), positive predictive value (97.82%), and negative predictive value (99.77%), as well as its confusion matrix and receiver operating characteristic (ROC) curves, indicate that it performs better than state-of-the-art methods. This proposed model will help in the automatic screening of COVID-19 patients and decrease the burden on medicinal services frameworks.", "journal": "Cognitive computation", "date": "2021-02-02", "authors": ["TriptiGoel", "RMurugan", "SeyedaliMirjalili", "Deba KumarChakrabartty"], "doi": "10.1007/s12559-020-09785-7\n10.1001/jama.2020.2565\n10.1016/j.chaos.2020.110170\n10.1016/j.compbiomed.2020.103795\n10.1177/0846537120913033\n10.3348/kjr.2020.0112\n10.1016/j.advengsoft.2016.01.008\n10.1038/scientificamerican0792-66\n10.1145/321062.321069\n10.1016/j.advengsoft.2013.12.007\n10.1186/s40779-019-0229-2"}
{"title": "COVID-19 classification by CCSHNet with deep fusion using transfer learning and discriminant correlation analysis.", "abstract": ": COVID-19 is a disease caused by a new strain of coronavirus. Up to 18th October 2020, worldwide there have been 39.6 million confirmed cases resulting in more than 1.1 million deaths. To improve diagnosis, we aimed to design and develop a novel advanced AI system for COVID-19 classification based on chest CT (CCT) images.\n: Our dataset from local hospitals consisted of 284 COVID-19 images, 281 community-acquired pneumonia images, 293 secondary pulmonary tuberculosis images; and 306 healthy control images. We first used pretrained models (PTMs) to learn features, and proposed a novel (L, 2) transfer feature learning algorithm to extract features, with a hyperparameter of number of layers to be removed (NLR, symbolized as \n: On the test set, CCSHNet achieved sensitivities of four classes of 95.61%, 96.25%, 98.30%, and 97.86%, respectively. The precision values of four classes were 97.32%, 96.42%, 96.99%, and 97.38%, respectively. The F1 scores of four classes were 96.46%, 96.33%, 97.64%, and 97.62%, respectively. The MA F1 score was 97.04%. In addition, CCSHNet outperformed 12 state-of-the-art COVID-19 detection methods.\n: CCSHNet is effective in detecting COVID-19 and other lung infectious diseases using first-line clinical imaging and can therefore assist radiologists in making accurate diagnoses based on CCTs.", "journal": "An international journal on information fusion", "date": "2021-02-02", "authors": ["Shui-HuaWang", "Deepak RanjanNayak", "David SGuttery", "XinZhang", "Yu-DongZhang"], "doi": "10.1016/j.inffus.2020.11.005\n10.1109/JSEN.2020.3025855\n10.1111/srt.12891\n10.1016/j.inffus.2020.10.004\n10.1109/TNSE.2020.2990963\n10.1109/TITS.2020.2990214"}
{"title": "Machine learning predictive model for severe COVID-19.", "abstract": "To develop a modified predictive model for severe COVID-19 in people infected with Sars-Cov-2. We developed the predictive model for severe patients of COVID-19 based on the clinical date from the Tumor Center of Union Hospital affiliated with Tongji Medical College, China. A total of 151 cases from Jan. 26 to Mar. 20, 2020, were included. Then we followed 5 steps to predict and evaluate the model: data preprocessing, data splitting, feature selection, model building, prevention of overfitting, and Evaluation, and combined with artificial neural network algorithms. We processed the results in the 5 steps. In feature selection, ALB showed a strong negative correlation (r\u00a0=\u00a00.771, P\u00a0<\u00a00.001) whereas GLB (r\u00a0=\u00a00.661, P\u00a0<\u00a00.001) and BUN (r\u00a0=\u00a00.714, P\u00a0<\u00a00.001) showed a strong positive correlation with severity of COVID-19. TensorFlow was subsequently applied to develop a neural network model. The model achieved good prediction performance, with an area under the curve value of 0.953(0.889-0.982). Our results showed its outstanding performance in prediction. GLB and BUN may be two risk factors for severe COVID-19. Our findings could be of great benefit in the future treatment of patients with COVID-19 and will help to improve the quality of care in the long term. This model has great significance to rationalize early clinical interventions and improve the cure rate.", "journal": "Infection, genetics and evolution : journal of molecular epidemiology and evolutionary genetics in infectious diseases", "date": "2021-01-31", "authors": ["JianhongKang", "TingChen", "HongheLuo", "YifengLuo", "GuipengDu", "MiaJiming-Yang"], "doi": "10.1016/j.meegid.2021.104737"}
{"title": "Deep COVID DeteCT: an international experience on COVID-19 lung detection and prognosis using chest CT.", "abstract": "The Coronavirus disease 2019 (COVID-19) presents open questions in how we clinically diagnose and assess disease course. Recently, chest computed tomography (CT) has shown utility for COVID-19 diagnosis. In this study, we developed Deep COVID DeteCT (DCD), a deep learning convolutional neural network (CNN) that uses the entire chest CT volume to automatically predict COVID-19 (COVID+) from non-COVID-19 (COVID-) pneumonia and normal controls. We discuss training strategies and differences in performance across 13 international institutions and 8 countries. The inclusion of non-China sites in training significantly improved classification performance with area under the curve (AUCs) and accuracies above 0.8 on most test sites. Furthermore, using available follow-up scans, we investigate methods to track patient disease course and predict prognosis.", "journal": "NPJ digital medicine", "date": "2021-01-31", "authors": ["Edward HLee", "JimmyZheng", "ErrolColak", "MaryamMohammadzadeh", "GolnazHoushmand", "NicholasBevins", "FelipeKitamura", "EmreAltinmakas", "Eduardo PontesReis", "Jae-KwangKim", "ChadKlochko", "MichelleHan", "SadeghMoradian", "AliMohammadzadeh", "HashemSharifian", "HassanHashemi", "KavousFirouznia", "HossienGhanaati", "MasoumehGity", "HakanDo\u011fan", "HojjatSalehinejad", "HenriqueAlves", "JayneSeekins", "NitamarAbdala", "\u00c7etinAtasoy", "HamidrezaPouraliakbar", "MajidMaleki", "S SimonWong", "Kristen WYeom"], "doi": "10.1038/s41746-020-00369-1\n10.1038/s41586-020-2012-7\n10.1148/radiol.2020202439\n10.1148/radiol.2020200230"}
{"title": "Chest Imaging of Patients with Sarcoidosis and SARS-CoV-2 Infection. Current Evidence and Clinical Perspectives.", "abstract": "The recent COVID-19 pandemic has dramatically changed the world in the last months, leading to a serious global emergency related to a novel coronavirus infection that affects both sexes of all ages ubiquitously. Advanced age, cardiovascular comorbidity, and viral load have been hypothesized as some of the risk factors for severity, but their role in patients affected with other diseases, in particular immune disorders, such as sarcoidosis, and the specific interaction between these two diseases remains unclear. The two conditions might share similar imaging findings but have distinctive features that are here described. The recent development of complex imaging softwares, called deep learning techniques, opens new scenarios for the diagnosis and management.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-01-31", "authors": ["ClaudioTana", "CesareMantini", "FrancescoCipollone", "Maria AdeleGiamberardino"], "doi": "10.3390/diagnostics11020183\n10.4103/jgid.jgid_86_20\n10.1016/j.diagmicrobio.2020.115094\n10.1016/j.ejim.2014.10.009\n10.2174/1573405614666180522074320\n10.1136/bmj.2.5261.1165\n10.1148/rg.306105512\n10.1007/s11547-017-0830-y\n10.2174/1573405614666180806141415\n10.1097/MCP.0000000000000705\n10.1155/2020/6175964\n10.1007/s11547-020-01200-3\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200370\n10.1016/S1473-3099(20)30086-4\n10.1183/09031936.00047908\n10.1148/radiol.2020200241\n10.1148/rg.236035101\n10.1259/bjr/77712845\n10.1186/s13244-020-00933-z\n10.1259/bjr/29049682\n10.1016/S2213-2600(20)30120-X\n10.1111/anae.15082\n10.1002/jum.15415\n10.1016/j.it.2020.08.001\n10.1016/j.ccm.2015.08.001\n10.3949/ccjm.87a.ccc026\n10.1016/j.ijid.2020.11.184\n10.1016/j.cell.2020.02.052\n10.3389/fcvm.2020.585866\n10.1056/NEJMoa2028836\n10.1136/bmj.m1432\n10.1016/j.antiviral.2020.104762\n10.1016/j.jcrc.2020.03.005\n10.3389/fmed.2020.588527\n10.1136/annrheumdis-2020-217681\n10.2147/TCRM.S192922\n10.1002/14651858.CD001114.pub2\n10.1056/nejmoa2021436\n10.1177/039463201402700302\n10.1186/s13054-020-03240-7\n10.1056/NEJMsa2011686\n10.21037/atm-20-5731\n10.1177/039463201302600204\n10.1007/s10354-014-0269-x\n10.1093/cvr/cvaa325\n10.1080/14787210.2020.1822737\n10.3390/jcm9093028\n10.1016/j.ejrad.2020.109217\n10.1038/s41568-018-0016-5\n10.1016/S2213-2600(15)00140-X\n10.1016/j.chaos.2020.110495\n10.1371/journal.pone.0242535\n10.1038/s41598-020-76282-0\n10.1097/RLI.0000000000000748"}
{"title": "Integrating deep learning CT-scan model, biological and clinical variables to predict severity of COVID-19 patients.", "abstract": "The SARS-COV-2 pandemic has put pressure on intensive care units, so that identifying predictors of disease severity is a priority. We collect 58 clinical and biological variables, and chest CT scan data, from 1003 coronavirus-infected patients from two French hospitals. We train a deep learning model based on CT scans to predict severity. We then construct the multimodal AI-severity score that includes 5 clinical and biological variables (age, sex, oxygenation, urea, platelet) in addition to the deep learning model. We show that neural network analysis of CT-scans brings unique prognosis information, although it is correlated with other markers of severity (oxygenation, LDH, and CRP) explaining the measurable but limited 0.03 increase of AUC obtained when adding CT-scan information to clinical variables. Here, we show that when comparing AI-severity with 11 existing severity scores, we find significantly improved prognosis performance; AI-severity can therefore rapidly become a reference scoring approach.", "journal": "Nature communications", "date": "2021-01-29", "authors": ["NathalieLassau", "SamyAmmari", "EmilieChouzenoux", "HugoGortais", "PaulHerent", "MatthieuDevilder", "SamerSoliman", "OlivierMeyrignac", "Marie-PaulineTalabard", "Jean-PhilippeLamarque", "RemyDubois", "NicolasLoiseau", "PaulTrichelair", "EtienneBendjebbar", "GabrielGarcia", "CorinneBalleyguier", "MansouriaMerad", "AnnabelleStoclin", "SimonJegou", "FranckGriscelli", "NicolasTetelboum", "YingpingLi", "SagarVerma", "MatthieuTerris", "TasnimDardouri", "KavyaGupta", "AnaNeacsu", "FrankChemouni", "MeriemSefta", "PaulJehanno", "ImadBousaid", "YannickBoursin", "EmmanuelPlanchet", "MikaelAzoulay", "JocelynDachary", "FabienBrulport", "AdrianGonzalez", "OlivierDehaene", "Jean-BaptisteSchiratti", "KathrynSchutte", "Jean-ChristophePesquet", "HuguesTalbot", "ElodiePronier", "GillesWainrib", "ThomasClozel", "FabriceBarlesi", "Marie-FranceBellin", "Michael G BBlum"], "doi": "10.1038/s41467-020-20657-4\n10.1136/bmj.m1985\n10.1016/S2213-2600(20)30161-2\n10.1038/s42256-020-0180-7\n10.1136/bmj.m1328\n10.1136/bmj.m3339\n10.1016/j.mayocp.2020.04.006\n10.1016/S0140-6736(20)30566-3\n10.2214/AJR.20.22976\n10.1097/RLI.0000000000000670\n10.1038/s41591-019-0583-3\n10.1186/s12916-019-1466-7\n10.1038/s41467-020-18786-x\n10.1007/s00134-020-05991-x\n10.18632/aging.103000\n10.1016/j.dsx.2020.03.002\n10.1016/S1470-2045(20)30096-6\n10.1097/RLI.0000000000000672\n10.1007/s00277-020-04019-0\n10.1371/journal.pone.0230548\n10.1016/j.ejrad.2020.108941\n10.1016/S2213-2600(20)30076-X\n10.1016/j.jtho.2020.02.010\n10.1016/j.crad.2020.03.004\n10.1148/radiol.2462070712\n10.2307/2531595\n10.1136/thorax.58.5.377\n10.1038/s41467-020-17280-8"}
{"title": "Effect of MRI acquisition acceleration via compressed sensing and parallel imaging on brain volumetry.", "abstract": "To investigate the effect of compressed SENSE (CS), an acceleration technique combining parallel imaging and compressed sensing, on potential bias and precision of brain volumetry and evaluate it in the context of normative brain volumetry.\nIn total, 171 scans from scan-rescan experiments on three healthy subjects were analyzed. Each subject received 3D-T1-weighted brain MRI scans at increasing degrees of acceleration (CS-factor\u2009=\u20091/4/8/12/16/20/32). Single-scan acquisition times ranged from 00:41\u00a0min (CS-factor\u2009=\u200932) to 21:52\u00a0min (CS-factor\u2009=\u20091). Brain segmentation and volumetry was performed using two different software tools: md.brain, a proprietary software based on voxel-based morphometry, and FreeSurfer, an open-source software based on surface-based morphometry. Four sub-volumes were analyzed: brain parenchyma (BP), total gray matter, total white matter, and cerebrospinal fluid (CSF). Coefficient of variation (CoV) of the repeated measurements as a measure of intra-subject reliability was calculated. Intraclass correlation coefficient (ICC) with regard to increasing CS-factor was calculated as another measure of reliability. Noise-to-contrast ratio as a measure of image quality was calculated for each dataset to analyze the association between acceleration factor, noise and volumetric brain measurements.\nFor all sub-volumes, there is a systematic bias proportional to the CS-factor which is dependent on the utilized software and subvolume. Measured volumes deviated significantly from the reference standard (CS-factor\u2009=\u20091), e.g. ranging from 1 to 13% for BP. The CS-induced systematic bias is driven by increased image noise. Except for CSF, reliability of brain volumetry remains high, demonstrated by low CoV (<\u20091% for CS-factor up to 20) and good to excellent ICC for CS-factor up to 12.\nCS-acceleration has a systematic biasing effect on volumetric brain measurements.", "journal": "Magma (New York, N.Y.)", "date": "2021-01-28", "authors": ["MichaelDieckmeyer", "Abhijit GuhaRoy", "JyotirmaySenapati", "ChristianWachinger", "LiobaGrundl", "J\u00f6rgD\u00f6pfert", "Pere FerreraBertran", "AndreasLemke", "ClausZimmer", "Jan SKirschke", "Dennis MHedderich"], "doi": "10.1007/s10334-020-00906-9\n10.1259/bjr.20190365\n10.1002/jmri.23671\n10.3233/JAD-150334\n10.1016/j.nicl.2018.09.013\n10.1016/j.nicl.2018.08.004\n10.1002/hbm.23536\n10.1002/jmri.1880010509\n10.1088/0031-9155/60/21/R297\n10.1002/mrm.10171\n10.1002/mrm.25347\n10.1002/(SICI)1522-2594(199911)42:5<952::AID-MRM16>3.0.CO;2-S\n10.1109/TIT.2006.871582\n10.1002/mrm.21391\n10.3174/ajnr.A5905\n10.3174/ajnr.A6703\n10.1016/B978-0-12-397025-1.00304-3\n10.1006/nimg.2000.0582\n10.1006/nimg.2001.0961\n10.1006/nimg.1998.0395\n10.1016/S0896-6273(02)00569-X\n10.1016/j.neuroimage.2004.07.016\n10.1037/1082-989X.1.1.30\n10.1371/journal.pone.0219854\n10.1214/aoms/1177729885\n10.1016/j.jcm.2016.02.012\n10.1002/hbm.20599\n10.1371/journal.pone.0165719\n10.1016/j.ejmp.2019.07.016\n10.1186/1471-2342-9-15\n10.1016/j.jalz.2013.05.540\n10.1038/sdata.2014.37"}
{"title": "Multi-classifier-based identification of COVID-19 from chest computed tomography using generalizable and interpretable radiomics features.", "abstract": "To investigate the efficacy of radiomics in diagnosing patients with coronavirus disease (COVID-19) and other types of viral pneumonia with clinical symptoms and CT signs similar to those of COVID-19.\nBetween 18 January 2020 and 20 May 2020, 110 SARS-CoV-2 positive and 108 SARS-CoV-2 negative patients were retrospectively recruited from three hospitals based on the inclusion criteria. Manual segmentation of pneumonia lesions on CT scans was performed by four radiologists. The latest version of Pyradiomics was used for feature extraction. Four classifiers (linear classifier, k-nearest neighbour, least absolute shrinkage and selection operator [LASSO], and random forest) were used to differentiate SARS-CoV-2 positive and SARS-CoV-2 negative patients. Comparison of the performance of the classifiers and radiologists was evaluated by ROC curve and Kappa score.\nWe manually segmented 16,053 CT slices, comprising 32,625 pneumonia lesions, from the CT scans of all patients. Using Pyradiomics, 120 radiomic features were extracted from each image. The key radiomic features screened by different classifiers varied and lead to significant differences in classification accuracy. The LASSO achieved the best performance (sensitivity: 72.2%, specificity: 75.1%, and AUC: 0.81) on the external validation dataset and attained excellent agreement (Kappa score: 0.89) with radiologists (average sensitivity: 75.6%, specificity: 78.2%, and AUC: 0.81). All classifiers indicated that \"Original_Firstorder_RootMeanSquared\" and \"Original_Firstorder_Uniformity\" were significant features for this task.\nWe identified radiomic features that were significantly associated with the classification of COVID-19 pneumonia using multiple classifiers. The quantifiable interpretation of the differences in features between the two groups extends our understanding of CT imaging characteristics of COVID-19 pneumonia.", "journal": "European journal of radiology", "date": "2021-01-27", "authors": ["LuWang", "BrendanKelly", "Edward HLee", "HongmeiWang", "JimmyZheng", "WeiZhang", "SafwanHalabi", "JiningLiu", "YulongTian", "BaoqinHan", "ChuanbinHuang", "Kristen WYeom", "KexueDeng", "JiangdianSong"], "doi": "10.1016/j.ejrad.2021.109552\n10.1148/radiol.2020200343\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200823\n10.1148/radiol.2020200463\n10.1148/radiol.2020200490\n10.1038/s41591-020-0824-5\n10.1038/s41591-020-0931-3\n10.1007/s00146-020-00978-0\n10.1162/NECO_a_00990\n10.1038/s41598-020-76550-z\n10.1007/s00330-020-07012-3\n10.1007/s00330-020-07032-z\n10.7150/ijms.48432\n10.7150/thno.46428\n10.1158/0008-5472.CAN-17-0339\n10.1016/j.ejrad.2019.108716\n10.1007/s00330-018-5912-2\n10.1038/nrclinonc.2017.141\n10.1016/j.ejrad.2019.108718\n10.1259/bjr.20170498\n10.1186/s12859-018-2277-0\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1109/TMI.2020.2994459\n10.3348/kjr.2020.0146\n10.1093/bib/bbx044\n10.1098/rsif.2017.0387\n10.1200/JCO.2015.65.9128\n10.1158/1078-0432.CCR-16-2910\n10.1016/j.ebiom.2018.09.007\n10.1007/s00259-018-4139-4\n10.1007/s10637-017-0524-2\n10.1016/j.ejrad.2020.108991\n10.1016/j.ejmp.2017.05.071\n10.1097/rti.0000000000000544\n10.1007/s00259-020-05075-4\n10.1016/j.ejro.2020.100271"}
{"title": "A Novel Block Imaging Technique Using Nine Artificial Intelligence Models for COVID-19 Disease Classification, Characterization and Severity Measurement in Lung Computed Tomography Scans on an Italian Cohort.", "abstract": "Computer Tomography (CT) is currently being adapted for visualization of COVID-19 lung damage. Manual classification and characterization of COVID-19 may be biased depending on the expert's opinion. Artificial Intelligence has recently penetrated COVID-19, especially deep learning paradigms. There are nine kinds of classification systems in this study, namely one deep learning-based CNN, five kinds of transfer learning (TL) systems namely VGG16, DenseNet121, DenseNet169, DenseNet201 and MobileNet, three kinds of machine-learning (ML) systems, namely artificial neural network (ANN), decision tree (DT), and random forest (RF) that have been designed for classification of COVID-19 segmented CT lung against Controls. Three kinds of characterization systems were developed namely (a) Block imaging for COVID-19 severity index (CSI); (b) Bispectrum analysis; and (c) Block Entropy. A cohort of Italian patients with 30 controls (990 slices) and 30 COVID-19 patients (705 slices) was used to test the performance of three types of classifiers. Using K10 protocol (90% training and 10% testing), the best accuracy and AUC was for DCNN and RF pairs were 99.41\u2009\u00b1\u20095.12%, 0.991 (p\u00a0<\u20090.0001), and 99.41\u2009\u00b1\u20090.62%, 0.988 (p\u00a0<\u20090.0001), respectively, followed by other ML and TL classifiers. We show that diagnostics odds ratio (DOR) was higher for DL compared to ML, and both, Bispecturm and Block Entropy shows higher values for COVID-19 patients. CSI shows an association with Ground Glass Opacities (0.9146, p\u2009<\u20090.0001). Our hypothesis holds true that deep learning shows superior performance compared to machine learning models. Block imaging is a powerful novel approach for pinpointing COVID-19 severity and is clinically validated.", "journal": "Journal of medical systems", "date": "2021-01-27", "authors": ["MohitAgarwal", "LucaSaba", "Suneet KGupta", "AlessandroCarriero", "ZenoFalaschi", "AlessioPasch\u00e8", "PietroDanna", "AymanEl-Baz", "SubbaramNaidu", "Jasjit SSuri"], "doi": "10.1007/s10916-021-01707-w\n10.1186/s13578-020-00404-4\n10.1016/j.healun.2020.04.004\n10.1088/0031-9155/53/20/N03\n10.1016/j.compbiomed.2020.103958\n10.1016/j.media.2012.02.005\n10.1109/MSP.2010.936730\n10.5121/ijsc.2011.2103\n10.1148/rg.2017160130\n10.1016/j.cmpb.2017.09.004\n10.1007/s10916-018-0940-7\n10.1016/j.cmpb.2019.04.008\n10.1109/29.9037\n10.1016/S0895-4356(03)00177-X\n10.1007/s10916-015-0214-6\n10.1016/j.compbiomed.2017.08.014\n10.1016/j.cmpb.2016.03.016\n10.1118/1.4725759\n10.1007/s10916-017-0862-9\n10.1016/j.ultrasmedbio.2010.07.011\n10.7785/tcrt.2012.500381\n10.1177/1533034614547445\n10.7785/tcrtexpress.2013.600273\n10.1007/s11517-012-1019-0\n10.1109/TIM.2011.2174897\n10.1007/s10916-017-0745-0\n10.1016/j.cmpb.2012.09.008\n10.1177/0954411913480622\n10.7785/tcrt.2012.500272\n10.1016/j.bspc.2013.08.008\n10.7785/tcrt.2012.500346\n10.1016/j.cmpb.2013.07.012\n10.1016/j.cmpb.2015.11.013\n10.1016/j.cmpb.2013.08.017\n10.1002/jcu.22183\n10.1007/s10916-010-9645-2\n10.21037/cdt.2020.01.07\n10.1016/j.ihj.2020.06.004\n10.2741/4850\n10.4103/0974-7788.59946\n10.1038/nrmicro.2016.81\n10.1074/jbc.M111.325803\n10.1161/CIRCRESAHA.116.307708\n10.1002/path.1570\n10.1016/S0140-6736(20)30211-7\n10.1111/j.1365-2362.2009.02153.x\n10.1007/s00234-019-02327-5\n10.1007/s00234-018-2142-x\n10.1016/j.cmpb.2015.10.022\n10.1016/j.compbiomed.2017.10.019\n10.1016/j.cmpb.2016.02.004\n10.1007/s10916-015-0407-z\n10.1007/s10916-016-0504-7\n10.1142/S0219519409003115"}
{"title": "Machine Learning: The Next Paradigm Shift in Medical Education.", "abstract": "Machine learning (ML) algorithms are powerful prediction tools with immense potential in the clinical setting. There are a number of existing clinical tools that use ML, and many more are in development. Physicians are important stakeholders in the health care system, but most are not equipped to make informed decisions regarding deployment and application of ML technologies in patient care. It is of paramount importance that ML concepts are integrated into medical curricula to position physicians to become informed consumers of the emerging tools employing ML. This paradigm shift is similar to the evidence-based medicine (EBM) movement of the 1990s. At that time, EBM was a novel concept; now, EBM is considered an essential component of medical curricula and critical to the provision of high-quality patient care. ML has the potential to have a similar, if not greater, impact on the practice of medicine. As this technology continues its inexorable march forward, educators must continue to evaluate medical curricula to ensure that physicians are trained to be informed stakeholders in the health care of tomorrow.", "journal": "Academic medicine : journal of the Association of American Medical Colleges", "date": "2021-01-27", "authors": ["Cornelius AJames", "Kevin MWheelock", "James OWoolliscroft"], "doi": "10.1097/ACM.0000000000003943"}
{"title": "Synergistic learning of lung lobe segmentation and hierarchical multi-instance classification for automated severity assessment of COVID-19 in CT images.", "abstract": "Understanding chest CT imaging of the coronavirus disease 2019 (COVID-19) will help detect infections early and assess the disease progression. Especially, automated severity assessment of COVID-19 in CT images plays an essential role in identifying cases that are in great need of intensive clinical care. However, it is often challenging to accurately assess the severity of this disease in CT images, due to variable infection regions in the lungs, similar imaging biomarkers, and large inter-case variations. To this end, we propose a synergistic learning framework for automated severity assessment of COVID-19 in 3D CT images, by jointly performing lung lobe segmentation and multi-instance classification. Considering that only a few infection regions in a CT image are related to the severity assessment, we first represent each input image by a bag that contains a set of 2D image patches (with each cropped from a specific slice). A multi-task multi-instance deep network (called M ", "journal": "Pattern recognition", "date": "2021-01-27", "authors": ["KeleiHe", "WeiZhao", "XingzhiXie", "WenJi", "MingxiaLiu", "ZhenyuTang", "YinghuanShi", "FengShi", "YangGao", "JunLiu", "JunfengZhang", "DinggangShen"], "doi": "10.1016/j.patcog.2021.107828\n10.1148/ryct.2020200047"}
{"title": "Computer aid screening of COVID-19 using X-ray and CT scan images: An inner comparison.", "abstract": "The objective of this study is to conduct a critical analysis to investigate and compare a group of computer aid screening methods of COVID-19 using chest X-ray images and computed tomography (CT) images. The computer aid screening method includes deep feature extraction, transfer learning, and machine learning image classification approach. The deep feature extraction and transfer learning method considered 13 pre-trained CNN models. The machine learning approach includes three sets of handcrafted features and three classifiers. The pre-trained CNN models include AlexNet, GoogleNet, VGG16, VGG19, Densenet201, Resnet18, Resnet50, Resnet101, Inceptionv3, Inceptionresnetv2, Xception, MobileNetv2 and ShuffleNet. The handcrafted features are GLCM, LBP & HOG, and machine learning based classifiers are KNN, SVM & Naive Bayes. In addition, the different paradigms of classifiers are also analyzed. Overall, the comparative analysis is carried out in 65 classification models, i.e., 13 in deep feature extraction, 13 in transfer learning, and 39 in the machine learning approaches. Finally, all classification models perform better when applying to the chest X-ray image set as comparing to the use of CT scan image set. Among 65 classification models, the VGG19 with SVM achieved the highest accuracy of 99.81%when applying to the chest X-ray images. In conclusion, the findings of this analysis study are beneficial for the researchers who are working towards designing computer aid tools for screening COVID-19 infection diseases.", "journal": "Journal of X-ray science and technology", "date": "2021-01-26", "authors": ["Prabira KumarSethy", "Santi KumariBehera", "KommaAnitha", "ChankiPandey", "M RKhan"], "doi": "10.3233/XST-200784"}
{"title": "Current Available Computer-Aided Detection Catches Cancer but Requires a Human Operator.", "abstract": "This study intends to show that the current widely used computer-aided detection (CAD) may be helpful, but it is not an adequate replacement for the human input required to interpret mammograms accurately. However, this is not to discredit CAD's ability but to further encourage the adoption of artificial intelligence-based algorithms into the toolset of radiologists.\nThis study will use Hologic (Marlborough, MA, USA) and General Electric (Boston, MA, USA) CAD read images provided by patients found to be Breast Imaging Reporting and Data System (BI-RADS) 6 from 2019 to 2020. In addition, patient information will be pulled from our institution's emergency medical record to confirm the findings seen in the pathologist report and the radiology read.\nData from a total of 24 female breast cancer patients from January 31st 2019 to April 31st 2020, was gathered from our institution's emergency medical record with restrictions in patient numbers due to coronavirus disease 2019 (COVID-19).\u00a0Within our patient population, CAD imaging was shown to be statistically significant in misidentifying breast cancer, while radiologist interpretation still proves to be the most effective tool.\nDespite a low sample size due to COVID-19, this study found that CAD did have significant difficulty in differentiating benign vs. malignant lesions.\u00a0CAD should not be ignored, but it is not specific enough. Although CAD often marks cancer, it also marks several areas that are not cancer. CAD is currently best used as an additional tool for the radiologist.", "journal": "Cureus", "date": "2021-01-26", "authors": ["FlorentinoSaenz Rios", "GiriMovva", "HariMovva", "Quan DNguyen"], "doi": "10.7759/cureus.12177"}
{"title": "Comparing supervised and semi-supervised Machine Learning Models on Diagnosing Breast Cancer.", "abstract": "Breast cancer disease is the most common cancer in US women and the second cause of cancer death among women.\nTo compare and evaluate the performance and accuracy of the key supervised and semi-supervised machine learning algorithms for breast cancer prediction.\nWe have used nine machine learning classification algorithms for supervised (SL) and semi-supervised learning (SSL): 1) Logistic regression; 2) Gaussian Naive Bayes; 3) Linear Support vector machine; 4) RBF Support vector machine; 5) Decision Tree; 6) Random Forest; 7) Xgboost; 8) Gradient Boosting; 9) KNN. The Wisconsin Diagnosis Cancer dataset was used to train and test these models. To ensure the robustness of the model, we have applied K-fold cross-validation and optimized hyperparameters. We have evaluated and compared the models using accuracy, precision, recall, F1-score, and ROC curves.\nThe results of all models are inspiring using both SL and SSL. The SSL has high accuracy (90%-98%) with just half of the training data. The KNN model for the SL and logistic regression for the SSL achieved the highest accuracy of 98.\nThe accuracies of SSL algorithms are very close to the SL algorithms. The accuracies of all models are in the range of 91-98%. SSL is a promising and competitive approach to solve the problem. Using a small sample of labeled and low computational power, the SSL is fully capable of replacing SL algorithms in diagnosing tumor type.", "journal": "Annals of medicine and surgery (2012)", "date": "2021-01-26", "authors": ["NosaybaAl-Azzam", "IbrahemShatnawi"], "doi": "10.1016/j.amsu.2020.12.043"}
{"title": "Deep Ensemble Model for Classification of Novel Coronavirus in Chest X-Ray Images.", "abstract": "The novel coronavirus, SARS-CoV-2, can be deadly to people, causing COVID-19. The ease of its propagation, coupled with its high capacity for illness and death in infected individuals, makes it a hazard to the community. Chest X-rays are one of the most common but most difficult to interpret radiographic examination for early diagnosis of coronavirus-related infections. They carry a considerable amount of anatomical and physiological information, but it is sometimes difficult even for the expert radiologist to derive the related information they contain. Automatic classification using deep learning models can help in better assessing these infections swiftly. Deep CNN models, namely, MobileNet, ResNet50, and InceptionV3, were applied with different variations, including training the model from the start, fine-tuning along with adjusting learned weights of all layers, and fine-tuning with learned weights along with augmentation. Fine-tuning with augmentation produced the best results in pretrained models. Out of these, two best-performing models (MobileNet and InceptionV3) selected for ensemble learning produced accuracy and FScore of 95.18% and 90.34%, and 95.75% and 91.47%, respectively. The proposed hybrid ensemble model generated with the merger of these deep models produced a classification accuracy and FScore of 96.49% and 92.97%. For test dataset, which was separately kept, the model generated accuracy and FScore of 94.19% and 88.64%. Automatic classification using deep ensemble learning can help radiologists in the correct identification of coronavirus-related infections in chest X-rays. Consequently, this swift and computer-aided diagnosis can help in saving precious human lives and minimizing the social and economic impact on society.", "journal": "Computational intelligence and neuroscience", "date": "2021-01-26", "authors": ["FareedAhmad", "AmjadFarooq", "Muhammad UsmanGhani"], "doi": "10.1155/2021/8890226\n10.3201/eid2313.170418\n10.3389/fpubh.2014.00144\n10.1016/S0140-6736(16)31012-1\n10.1186/1471-2458-14-509\n10.1371/journal.pntd.0003257\n10.20506/rst.33.2.2292\n10.1016/j.jmii.2020.02.001\n10.1056/NEJMc2004973\n10.1038/d41586-020-00974-w\n10.1101/2020.03.08.982637\n10.1017/ice.2020.58\n10.1001/jama.2020.4756\n10.1016/S0140-6736(20)30788-1\n10.1038/d41586-020-00548-w\n10.3390/vetsci7010028\n10.1016/j.coviro.2017.01.002\n10.1148/radiol.2020200642\n10.1148/ryct.2020200034\n10.1016/s0140-6736(20)30183-5\n10.1148/radiol.2020200432\n10.1371/journal.pone.0184554\n10.1002/ima.22469\n10.1007/978-3-030-55258-9_17\n10.1155/2018/4168538\n10.1155/2017/3048181\n10.1109/ICABME.2015.7323305\n10.1155/2019/4629859\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2017.195\n10.1109/CVPR.2016.90\n10.1007/s11263-015-0816-y\n10.1016/j.eswa.2017.11.028\n10.1109/ICAIT47043.2019.8987286\n10.1007/3-540-57233-3_76\n10.1148/radiol.2017162326\n10.1148/radiol.2017162725\n10.1155/2019/4180949\n10.1109/tmi.2014.2350539\n10.1109/CCECE.2019.8861969\n10.1109/EBBT.2019.8741582\n10.1109/ICECCT.2019.8869364\n10.1109/CVPR.2017.369\n10.3390/app10020559\n10.1016/j.cell.2018.02.010\n10.1093/jamia/ocv080\n10.1101/2020.02.14.20023028\n10.1016/j.eng.2020.04.010\n10.1007/s13246-020-00865-4\n10.1016/j.patrec.2020.09.010\n10.1007/s10489-020-01829-7\n10.1016/j.mehy.2020.109761\n10.1016/j.cmpb.2020.105581\n10.1016/j.cmpb.2020.105532\n10.1109/TMI.2020.2993291\n10.1016/j.media.2020.101794\n10.1109/ACCESS.2020.3010287\n10.1109/ACCESS.2020.3003810\n10.17632/2fxz4px6d8.4\n10.1109/access.2020.2971257\n10.3390/app8101715\n10.1007/s11042-019-08453-9\n10.1186/s12859-017-1898-z\n10.1109/tmi.2016.2528162\n10.21037/atm.2019.08.54\n10.1109/access.2019.2946000\n10.1007/978-3-319-93000-8_92\n10.1088/1742-6596/1237/2/022026\n10.1007/s13246-019-00807-9"}
{"title": "Multi-Radiologist User Study for Artificial Intelligence-Guided Grading of COVID-19 Lung Disease Severity on Chest Radiographs.", "abstract": "Radiographic findings of COVID-19 pneumonia can be used for patient risk stratification; however, radiologist reporting of disease severity is inconsistent on chest radiographs (CXRs). We aimed to see if an artificial intelligence (AI) system could help improve radiologist interrater agreement.\nWe performed a retrospective multi-radiologist user study to evaluate the impact of an AI system, the PXS score model, on the grading of categorical COVID-19 lung disease severity on 154 chest radiographs into four ordinal grades (normal/minimal, mild, moderate, and severe). Four radiologists (two thoracic and two emergency radiologists) independently interpreted 154 CXRs from 154 unique patients with COVID-19 hospitalized at a large academic center, before and after using the AI system (median washout time interval was 16 days). Three different thoracic radiologists assessed the same 154 CXRs using an updated version of the AI system trained on more imaging data. Radiologist interrater agreement was evaluated using Cohen and Fleiss kappa where appropriate. The lung disease severity categories were associated with clinical outcomes using a previously published outcomes dataset using Fisher's exact test and Chi-square test for trend.\nUse of the AI system improved radiologist interrater agreement (Fleiss \u03ba\u202f=\u202f0.40 to 0.66, before and after use of the system). The Fleiss \u03ba for three radiologists using the updated AI system was 0.74. Severity categories were significantly associated with subsequent intubation or death within 3 days.\nAn AI system used at the time of CXR study interpretation can improve the interrater agreement of radiologists.", "journal": "Academic radiology", "date": "2021-01-25", "authors": ["Matthew DLi", "Brent PLittle", "Tarik KAlkasab", "Dexter PMendoza", "Marc DSucci", "Jo-Anne OShepard", "Michael HLev", "JayashreeKalpathy-Cramer"], "doi": "10.1016/j.acra.2021.01.016\n10.1148/radiol.2020201754\n10.1148/radiol.2020202602\n10.1148/ryai.2020200079\n10.1101/2020.09.15.20195453\n10.1371/journal.pone.0236621\n10.1007/s00330-020-07269-8\n10.1136/bmj.m1966\n10.1056/NEJMoa2021436"}
{"title": "Association of AI quantified COVID-19 chest CT and patient outcome.", "abstract": "Severity scoring is a key step in managing patients with COVID-19 pneumonia. However, manual quantitative analysis by radiologists is a time-consuming task, while qualitative evaluation may be fast but highly subjective. This study aims to develop artificial intelligence (AI)-based methods to quantify disease severity and predict COVID-19 patient outcome.\nWe develop an AI-based framework that employs deep neural networks to efficiently segment lung lobes and pulmonary opacities. The volume ratio of pulmonary opacities inside each lung lobe gives the severity scores of the lobes, which are then used to predict ICU admission and mortality with three different machine learning methods. The developed methods were evaluated on datasets from two hospitals (site A: Firoozgar Hospital, Iran, 105 patients; site B: Massachusetts General Hospital, USA, 88 patients).\nAI-based severity scores are strongly associated with those evaluated by radiologists (Spearman's rank correlation 0.837, [Formula: see text]). Using AI-based scores produced significantly higher ([Formula: see text]) area under the ROC curve (AUC) values. The developed AI method achieved the best performance of AUC = 0.813 (95% CI [0.729, 0.886]) in predicting ICU admission and AUC = 0.741 (95% CI [0.640, 0.837]) in mortality estimation on the two datasets.\nAccurate severity scores can be obtained using the developed AI methods over chest CT images. The computed severity scores achieved better performance than radiologists in predicting COVID-19 patient outcome by consistently quantifying image features. Such developed techniques of severity assessment may be extended to other lung diseases beyond the current pandemic.", "journal": "International journal of computer assisted radiology and surgery", "date": "2021-01-24", "authors": ["XiFang", "UweKruger", "FatemehHomayounieh", "HanqingChao", "JiajinZhang", "Subba RDigumarthy", "Chiara DArru", "Mannudeep KKalra", "PingkunYan"], "doi": "10.1007/s11548-020-02299-5\n10.1148/radiol.2020200642\n10.1016/j.media.2020.101844\n10.1109/TMI.2020.3001036\n10.1038/s41467-020-17971-2\n10.1186/s41747-020-00173-2\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020200905\n10.1148/ryai.2020200079\n10.1088/1361-6560/abbf9e\n10.1148/radiol.2020201754\n10.1148/radiol.2020201160\n10.1148/radiol.2020200343\n10.1097/RLI.0000000000000674\n10.1148/ryct.2020200047\n10.1371/journal.pone.0178944\n10.2214/AJR.20.22976\n10.1016/j.media.2020.101824"}
{"title": "Automated Detection and Quantification of COVID-19 Airspace Disease on Chest Radiographs: A Novel Approach Achieving Expert Radiologist-Level Performance Using a Deep Convolutional Neural Network Trained on Digital Reconstructed Radiographs From Computed Tomography-Derived Ground Truth.", "abstract": "The aim of this study was to leverage volumetric quantification of airspace disease (AD) derived from a superior modality (computed tomography [CT]) serving as ground truth, projected onto digitally reconstructed radiographs (DRRs) to (1) train a convolutional neural network (CNN) to quantify AD on paired chest radiographs (CXRs) and CTs, and (2) compare the DRR-trained CNN to expert human readers in the CXR evaluation of patients with confirmed COVID-19.\nWe retrospectively selected a cohort of 86 COVID-19 patients (with positive reverse transcriptase-polymerase chain reaction test results) from March to May 2020 at a tertiary hospital in the northeastern United States, who underwent chest CT and CXR within 48 hours. The ground-truth volumetric percentage of COVID-19-related AD (POv) was established by manual AD segmentation on CT. The resulting 3-dimensional masks were projected into 2-dimensional anterior-posterior DRR to compute area-based AD percentage (POa). A CNN was trained with DRR images generated from a larger-scale CT dataset of COVID-19 and non-COVID-19 patients, automatically segmenting lungs, AD, and quantifying POa on CXR. The CNN POa results were compared with POa quantified on CXR by 2 expert readers and to the POv ground truth, by computing correlations and mean absolute errors.\nBootstrap mean absolute error and correlations between POa and POv were 11.98% (11.05%-12.47%) and 0.77 (0.70-0.82) for average of expert readers and 9.56% to 9.78% (8.83%-10.22%) and 0.78 to 0.81 (0.73-0.85) for the CNN, respectively.\nOur CNN trained with DRR using CT-derived airspace quantification achieved expert radiologist level of accuracy in the quantification of AD on CXR in patients with positive reverse transcriptase-polymerase chain reaction test results for COVID-19.", "journal": "Investigative radiology", "date": "2021-01-23", "authors": ["Eduardo JMortani Barbosa", "Warren BGefter", "Florin CGhesu", "SiqiLiu", "BorisMailhe", "AwaisMansoor", "SasaGrbic", "SebastianVogt"], "doi": "10.1097/RLI.0000000000000763"}
{"title": "ADOPT: automatic deep learning and optimization-based approach for detection of novel coronavirus COVID-19 disease using X-ray images.", "abstract": "In the hospital, because of the rise in cases daily, there are a small number of COVID-19 test kits available. For this purpose, a rapid alternative diagnostic choice to prevent COVID-19 spread among individuals must be implemented as an automatic detection method. In this article, the multi-objective optimization and deep learning-based technique for identifying infected patients with coronavirus using X-rays is proposed. J48 decision tree approach classifies the deep feature of corona affected X-ray images for the efficient detection of infected patients. In this study, 11 different convolutional neural network-based (CNN) models (AlexNet, VGG16, VGG19, GoogleNet, ResNet18, ResNet50, ResNet101, InceptionV3, InceptionResNetV2, DenseNet201 and XceptionNet) are developed for detection of infected patients with coronavirus pneumonia using X-ray images. The efficiency of the proposed model is tested using k-fold cross-validation method. Moreover, the parameters of CNN deep learning model are tuned using multi-objective spotted hyena optimizer (MOSHO). Extensive analysis shows that the proposed model can classify the X-ray images at a good accuracy, precision, recall, specificity and F1-score rates. Extensive experimental results reveal that the proposed model outperforms competitive models in terms of well-known performance metrics. Hence, the proposed model is useful for real-time COVID-19 disease classification from X-ray chest images.Communicated by Ramaswamy H. Sarma.", "journal": "Journal of biomolecular structure & dynamics", "date": "2021-01-22", "authors": ["GauravDhiman", "VictorChang", "KrishnaKant Singh", "AchyutShankar"], "doi": "10.1080/07391102.2021.1875049\n10.1148/radiol.2020200642\n10.1016/j.patrec.2020.03.011\n10.1007/s10489-019-01522-4\n10.1016/j.engappai.2019.03.021\n10.1016/j.advengsoft.2017.05.014\n10.1016/j.knosys.2018.06.001\n10.1016/j.knosys.2018.03.011\n10.1016/j.knosys.2018.11.024\n10.1016/j.knosys.2020.106560\n10.1016/j.eswa.2020.114150\n10.1016/j.ejrnm.2015.11.004\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30183-5\n10.3390/jcm9020523\n10.1016/j.engappai.2020.104008\n10.1016/j.engappai.2020.103541\n10.1136/bmj.m641\n10.3390/jcm9020419\n10.1016/j.idm.2020.02.002\n10.3390/jcm9020462\n10.1101/2020.02.14.20023028\n10.1101/2020.02.27.20028027\n10.1016/j.compbiomed.2019.103387\n10.3390/jcm9020388"}
{"title": "Artificial Intelligence-assisted chest X-ray assessment scheme for COVID-19.", "abstract": "To study whether a trained convolutional neural network (CNN) can be of assistance to radiologists in differentiating Coronavirus disease (COVID)-positive from COVID-negative patients using chest X-ray (CXR) through an ambispective clinical study. To identify subgroups of patients where artificial intelligence (AI) can be of particular value and analyse what imaging features may have contributed to the performance of AI by means of visualisation techniques.\nCXR of 487 patients were classified into [4] categories-normal, classical COVID, indeterminate, and non-COVID by consensus opinion of 2 radiologists. CXR which were classified as \"normal\" and \"indeterminate\" were then subjected to analysis by AI, and final categorisation provided as guided by prediction of the network. Precision and recall of the radiologist alone and radiologist assisted by AI were calculated in comparison to reverse transcriptase-polymerase chain reaction (RT-PCR) as the gold standard. Attention maps of the CNN were analysed to understand regions in the CXR important to the AI algorithm in making a prediction.\nThe precision of radiologists improved from 65.9 to 81.9% and recall improved from 17.5 to 71.75 when assistance with AI was provided. AI showed 92% accuracy in classifying \"normal\" CXR into COVID or non-COVID. Analysis of attention maps revealed attention on the cardiac shadow in these \"normal\" radiographs.\nThis study shows how deployment of an AI algorithm can complement a human expert in the determination of COVID status. Analysis of the detected features suggests possible subtle cardiac changes, laying ground for further investigative studies into possible cardiac changes.\n\u2022 Through an ambispective clinical study, we show how assistance with an AI algorithm can improve recall (sensitivity) and precision (positive predictive value) of radiologists in assessing CXR for possible COVID in comparison to RT-PCR. \u2022 We show that AI achieves the best results in images classified as \"normal\" by radiologists. We conjecture that possible subtle cardiac in the CXR, imperceptible to the human eye, may have contributed to this prediction. \u2022 The reported results may pave the way for a human computer collaboration whereby the expert with some help from the AI algorithm achieves higher accuracy in predicting COVID status on CXR than previously thought possible when considering either alone.", "journal": "European radiology", "date": "2021-01-21", "authors": ["KrithikaRangarajan", "SumanyuMuku", "Amit KumarGarg", "PavanGabra", "Sujay HalkurShankar", "NeerajNischal", "Kapil DevSoni", "Ashu SeithBhalla", "AnantMohan", "PawanTiwari", "SushmaBhatnagar", "RaghavBansal", "AtinKumar", "ShivanandGamanagati", "RichaAggarwal", "UpendraBaitha", "AshutoshBiswas", "ArvindKumar", "PankajJorwal", "NoneShalimar", "AShariff", "NaveetWig", "RajeshwariSubramanium", "AnjanTrikha", "RajeshMalhotra", "RandeepGuleria", "VinayNamboodiri", "SubhashisBanerjee", "ChetanArora"], "doi": "10.1007/s00330-020-07628-5\n10.1016/j.jcv.2020.104455\n10.1016/j.jcv.2020.104500\n10.1056/NEJMp2017739\n10.1148/radiol.2020201160\n10.1007/s11263-015-0816-y\n10.1038/nature21056\n10.1109/ACCESS.2017.2788044\n10.1038/s41568-018-0016-5\n10.1038/s41746-018-0076-7\n10.1148/radiol.2020201874\n10.1016/S2589-7500(20)30162-X"}
{"title": "Automated processing of social media content for radiologists: applied deep learning to radiological content on twitter during COVID-19 pandemic.", "abstract": "The purpose of this study was to develop an automated process to analyze multimedia content on Twitter during the COVID-19 outbreak and classify content for radiological significance using deep learning (DL).\nUsing Twitter search features, all tweets containing keywords from both \"radiology\" and \"COVID-19\" were collected for the period January 01, 2020 up to April 24, 2020. The resulting dataset comprised of 8354 tweets. Images were classified as (i) images with text (ii) radiological content (e.g., CT scan snapshots, X-ray images), and (iii) non-medical content like personal images or memes. We trained our deep learning model using Convolutional Neural Networks (CNN) on training dataset of 1040 labeled images drawn from all three classes. We then trained another DL classifier for segmenting images into categories based on human anatomy. All software used is open-source and adapted for this research. The diagnostic performance of the algorithm was assessed by comparing results on a test set of 1885 images.\nOur analysis shows that in COVID-19 related tweets on radiology, nearly 32% had textual images, another 24% had radiological content, and 44% were not of radiological significance. Our results indicated a 92% accuracy in classifying images originally labeled as chest X-ray or chest CT and a nearly 99% accurate classification of images containing medically relevant text. With larger training dataset and algorithmic tweaks, the accuracy can be further improved.\nApplying DL on rich textual images and other metadata in tweets we can process and classify content for radiological significance in real time.", "journal": "Emergency radiology", "date": "2021-01-19", "authors": ["ShikharKhurana", "RohanChopra", "BhartiKhurana"], "doi": "10.1007/s10140-020-01885-z\n10.1016/j.jacr.2013.07.015\n10.1016/j.jmir.2016.09.001\n10.1016/j.jacr.2019.03.014\n10.1371/journal.pone.0210689\n10.1002/j.0022-0337.2016.80.2.tb06070.x\n10.1017/cem.2020.361\n10.1016/j.jacr.2017.09.010"}
{"title": "COVID-19 diagnosis from chest X-ray images using transfer learning: Enhanced performance by debiasing dataloader.", "abstract": "Chest X-ray imaging has been proved as a powerful diagnostic method to detect and diagnose COVID-19 cases due to its easy accessibility, lower cost and rapid imaging time.\nThis study aims to improve efficacy of screening COVID-19 infected patients using chest X-ray images with the help of a developed deep convolutional neural network model (CNN) entitled nCoV-NET.\nTo train and to evaluate the performance of the developed model, three datasets were collected from resources of \"ChestX-ray14\", \"COVID-19 image data collection\", and \"Chest X-ray collection from Indiana University,\" respectively. Overall, 299 COVID-19 pneumonia cases and 1,522 non-COVID 19 cases are involved in this study. To overcome the probable bias due to the unbalanced cases in two classes of the datasets, ResNet, DenseNet, and VGG architectures were re-trained in the fine-tuning stage of the process to distinguish COVID-19 classes using a transfer learning method. Lastly, the optimized final nCoV-NET model was applied to the testing dataset to verify the performance of the proposed model.\nAlthough the performance parameters of all re-trained architectures were determined close to each other, the final nCOV-NET model optimized by using DenseNet-161 architecture in the transfer learning stage exhibits the highest performance for classification of COVID-19 cases with the accuracy of 97.1 %. The Activation Mapping method was used to create activation maps that highlights the crucial areas of the radiograph to improve causality and intelligibility.\nThis study demonstrated that the proposed CNN model called nCoV-NET can be utilized for reliably detecting COVID-19 cases using chest X-ray images to accelerate the triaging and save critical time for disease control as well as assisting the radiologist to validate their initial diagnosis.", "journal": "Journal of X-ray science and technology", "date": "2021-01-19", "authors": ["\u00c7a\u011f\u00ednPolat", "OnurKaraman", "CerenKaraman", "G\u00fcneyKorkmaz", "Mehmet CanBalc\u0131", "Sevim ErcanKelek"], "doi": "10.3233/XST-200757"}
{"title": "Deep Learning Models for Predicting Severe Progression in COVID-19-Infected Patients: Retrospective Study.", "abstract": "Many COVID-19 patients rapidly progress to respiratory failure with a broad range of severities. Identification of high-risk cases is critical for early intervention.\nThe aim of this study is to develop deep learning models that can rapidly identify high-risk COVID-19 patients based on computed tomography (CT) images and clinical data.\nWe analyzed 297 COVID-19 patients from five hospitals in Daegu, South Korea. A mixed artificial convolutional neural network (ACNN) model, combining an artificial neural network for clinical data and a convolutional neural network for 3D CT imaging data, was developed to classify these cases as either high risk of severe progression (ie, event) or low risk (ie, event-free).\nUsing the mixed ACNN model, we were able to obtain high classification performance using novel coronavirus pneumonia lesion images (ie, 93.9% accuracy, 80.8% sensitivity, 96.9% specificity, and 0.916 area under the curve [AUC] score) and lung segmentation images (ie, 94.3% accuracy, 74.7% sensitivity, 95.9% specificity, and 0.928 AUC score) for event versus event-free groups.\nOur study successfully differentiated high-risk cases among COVID-19 patients using imaging and clinical features. The developed model can be used as a predictive tool for interventions in aggressive therapies.", "journal": "JMIR medical informatics", "date": "2021-01-19", "authors": ["Thao ThiHo", "JongminPark", "TaewooKim", "ByunggeonPark", "JaeheeLee", "Jin YoungKim", "Ki BeomKim", "SooyoungChoi", "Young HwanKim", "Jae-KwangLim", "SanghunChoi"], "doi": "10.2196/24973\n10.1056/NEJMp2000929\n10.1016/j.coviro.2018.01.001\n10.1073/pnas.2009637117\n10.1056/NEJMoa2002032\n10.1164/rccm.201908-1581ST\n10.1080/22221751.2020.1745095\n10.1080/22221751.2020.1745095\n10.1016/j.ejrad.2020.108961\n10.2196/19569\n10.7150/thno.45985\n10.1148/radiol.2020201365\n10.1101/2020.03.28.20045997\n10.1101/2020.02.29.20029603\n10.1101/2020.02.27.20028027\n10.1371/journal.pone.0230548\n10.1371/journal.pone.0230548\n10.1101/2020.03.25.20043331\n10.1136/bmj.m1328\n10.2196/24018\n10.1016/j.cell.2020.04.045\n10.1364/OE.18.015256\n10.1148/radiol.2020201433\n10.1109/cvpr.2016.90\n10.1109/cvpr.2016.308\n10.1109/cvpr.2017.243\n10.1145/3065386\n10.1038/nature14539\n10.1007/s11263-019-01228-7.pdf\n10.1007/s11263-019-01228-7\n10.1016/j.patcog.2007.04.009\n10.1016/j.ipm.2009.03.002\n10.1038/s41592-019-0686-2\n10.12688/f1000research.7035.2\n10.1016/s0140-6736(20)30183-5\n10.1016/j.ejrad.2004.01.005\n10.1016/j.media.2019.101628"}
{"title": "Multi-window back-projection residual networks for reconstructing COVID-19 CT super-resolution images.", "abstract": "With the increasing problem of coronavirus disease 2019 (COVID-19) in the world, improving the image resolution of COVID-19 computed tomography (CT) becomes a very important task. At present, single-image super-resolution (SISR) models based on convolutional neural networks (CNN) generally have problems such as the loss of high-frequency information and the large size of the model due to the deep network structure.\nIn this work, we propose an optimization model based on multi-window back-projection residual network (MWSR), which outperforms most of the state-of-the-art methods. Firstly, we use multi-window to refine the same feature map at the same time to obtain richer high/low frequency information, and fuse and filter out the features needed by the deep network. Then, we develop a back-projection network based on the dilated convolution, using up-projection and down-projection modules to extract image features. Finally, we merge several repeated and continuous residual modules with global features, merge the information flow through the network, and input them to the reconstruction module.\nThe proposed method shows the superiority over the state-of-the-art methods on the benchmark dataset, and generates clear COVID-19 CT super-resolution images.\nBoth subjective visual effects and objective evaluation indicators are improved, and the model specifications are optimized. Therefore, the MWSR method can improve the clarity of CT images of COVID-19 and effectively assist the diagnosis and quantitative assessment of COVID-19.", "journal": "Computer methods and programs in biomedicine", "date": "2021-01-18", "authors": ["DefuQiu", "YuhuCheng", "XuesongWang", "XiaoqiangZhang"], "doi": "10.1016/j.cmpb.2021.105934"}
{"title": "Distant Domain Transfer Learning for Medical Imaging.", "abstract": "Medical image processing is one of the most important topics in the Internet of Medical Things (IoMT). Recently, deep learning methods have carried out state-of-the-art performances on medical imaging tasks. In this paper, we propose a novel transfer learning framework for medical image classification. Moreover, we apply our method COVID-19 diagnosis with lung Computed Tomography (CT) images. However, well-labeled training data sets cannot be easily accessed due to the disease's novelty and privacy policies. The proposed method has two components: reduced-size Unet Segmentation model and Distant Feature Fusion (DFF) classification model. This study is related to a not well-investigated but important transfer learning problem, termed Distant Domain Transfer Learning (DDTL). In this study, we develop a DDTL model for COVID-19 diagnosis using unlabeled Office-31, Caltech-256, and chest X-ray image data sets as the source data, and a small set of labeled COVID-19 lung CT as the target data. The main contributions of this study are: 1) the proposed method benefits from unlabeled data in distant domains which can be easily accessed, 2) it can effectively handle the distribution shift between the training data and the testing data, 3) it has achieved 96% classification accuracy, which is 13% higher classification accuracy than \"non-transfer\" algorithms, and 8% higher than existing transfer and distant transfer algorithms.", "journal": "IEEE journal of biomedical and health informatics", "date": "2021-01-16", "authors": ["ShutengNiu", "MerylLiu", "YongxinLiu", "JianWang", "HoubingSong"], "doi": "10.1109/JBHI.2021.3051470"}
{"title": "Methodology to create 3D models of COVID-19 pathologies for virtual clinical trials.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-01-16", "authors": ["SunayRodr\u00edguez P\u00e9rez", "JohanCoolen", "Nicholas WMarshall", "LesleyCockmartin", "CharlotteBieba\u00fb", "JeroenDesmet", "WalterDe Wever", "LaraStruelens", "HildeBosmans"], "doi": "10.1117/1.JMI.8.S1.013501\n10.1007/s00330-020-06827-4\n10.1148/radiol.2020201160\n10.1148/radiol.2020201237\n10.1001/jamanetworkopen.2018.5474\n10.1117/1.JMI.7.4.042805\n10.1088/0031-9155/59/15/4275\n10.1097/HP.0000000000000805\n10.1088/0031-9155/59/18/R233\n10.1117/1.JMI.5.1.013504\n10.5114/pjr.2018.73406\n10.1038/nmeth.2089\n10.1145/37402.37422\n10.1186/1471-2105-11-274\n10.1016/j.clinimag.2020.04.001\n10.1111/1467-8659.00236\n10.1111/cgf.12153\n10.11648/j.ajpa.20180602.12\n10.1118/1.595715\n10.1118/1.3643029\n10.1118/1.3377772\n10.1016/S0146-6453(03)00002-2\n10.1088/0031-9155/41/1/009\n10.1118/1.598165\n10.1088/1361-6560/aa9599\n10.1118/1.598623\n10.1118/1.597627\n10.1118/1.3644845\n10.1093/rpd/nch540\n10.1118/1.4704525\n10.1093/rpd/ncq057\n10.1007/s00330-020-07013-2\n10.1109/TMI.2020.3000314\n10.2214/AJR.20.23429"}
{"title": "CT image segmentation for inflamed and fibrotic lungs using a multi-resolution convolutional neural network.", "abstract": "The purpose of this study was to develop a fully-automated segmentation algorithm, robust to various density enhancing lung abnormalities, to facilitate rapid quantitative analysis of computed tomography images. A polymorphic training approach is proposed, in which both specifically labeled left and right lungs of humans with COPD, and nonspecifically labeled lungs of animals with acute lung injury, were incorporated into training a single neural network. The resulting network is intended for predicting left and right lung regions in humans with or without diffuse opacification and consolidation. Performance of the proposed lung segmentation algorithm was extensively evaluated on CT scans of subjects with COPD, confirmed COVID-19, lung cancer, and IPF, despite no labeled training data of the latter three diseases. Lobar segmentations were obtained using the left and right lung segmentation as input to the LobeNet algorithm. Regional lobar analysis was performed using hierarchical clustering to identify radiographic subtypes of COVID-19. The proposed lung segmentation algorithm was quantitatively evaluated using semi-automated and manually-corrected segmentations in 87 COVID-19 CT images, achieving an average symmetric surface distance of [Formula: see text] mm and Dice coefficient of [Formula: see text]. Hierarchical clustering identified four radiographical phenotypes of COVID-19 based on lobar fractions of consolidated and poorly aerated tissue. Lower left and lower right lobes were consistently more afflicted with poor aeration and consolidation. However, the most severe cases demonstrated involvement of all lobes. The polymorphic training approach was able to accurately segment COVID-19 cases with diffuse consolidation without requiring COVID-19 cases for training.", "journal": "Scientific reports", "date": "2021-01-16", "authors": ["Sarah EGerard", "JacobHerrmann", "YiXin", "Kevin TMartin", "EmanueleRezoagli", "DavideIppolito", "GiacomoBellani", "MaurizioCereda", "JunfengGuo", "Eric AHoffman", "David WKaczka", "Joseph MReinhardt"], "doi": "10.1038/s41598-020-80936-4\n10.1016/S0140-6736(20)30566-3\n10.1016/S0140-6736(20)30183-5\n10.1109/42.929615\n10.1109/TMI.2010.2044799\n10.1148/rg.252045070\n10.1016/j.compmedimag.2006.06.002\n10.1109/TMI.2008.929101\n10.1109/TMI.2012.2219881\n10.1109/TMI.2009.2027117\n10.1109/TMI.2011.2171357\n10.1109/TMI.2005.851757\n10.1109/TMI.2005.859209\n10.1118/1.3147146\n10.1016/j.media.2019.101592\n10.1109/TMI.2018.2858202\n10.3109/15412550903499522\n10.1259/bjr.20200538\n10.1148/radiol.2020200370\n10.1148/ryct.2020200110\n10.1148/ryct.2020200075"}
{"title": "Federated Learning used for predicting outcomes in SARS-COV-2 patients.", "abstract": "'Federated Learning' (FL) is a method to train Artificial Intelligence (AI) models with data from multiple sources while maintaining anonymity of the data thus removing many barriers to data sharing. During the SARS-COV-2 pandemic, 20 institutes collaborated on a healthcare FL study to predict future oxygen requirements of infected patients using inputs of vital signs, laboratory data, and chest x-rays, constituting the \"EXAM\" (EMR CXR AI Model) model. EXAM achieved an average Area Under the Curve (AUC) of over 0.92, an average improvement of 16%, and a 38% increase in generalisability over local models. The FL paradigm was successfully applied to facilitate a rapid data science collaboration without data exchange, resulting in a model that generalised across heterogeneous, unharmonized datasets. This provided the broader healthcare community with a validated model to respond to COVID-19 challenges, as well as set the stage for broader use of FL in healthcare.", "journal": "Research square", "date": "2021-01-15", "authors": ["MonaFlores", "IttaiDayan", "HolgerRoth", "AoxiaoZhong", "AhmedHarouni", "AmilcareGentili", "AnasAbidin", "AndrewLiu", "AnthonyCosta", "BradfordWood", "Chien-SungTsai", "Chih-HungWang", "Chun-NanHsu", "C KLee", "ColleenRuan", "DaguangXu", "DufanWu", "EddieHuang", "FelipeKitamura", "GriffinLacey", "GustavoC\u00e9sar de Ant\u00f4nio Corradi", "Hao-HsinShin", "HirofumiObinata", "HuiRen", "JasonCrane", "JesseTetreault", "JiahuiGuan", "JohnGarrett", "Jung GilPark", "KeithDreyer", "KrishnaJuluru", "KristopherKersten", "Marcio AloisioBezerra Cavalcanti Rockenbach", "MariusLinguraru", "MasoomHaider", "MeenaAbdelMaseeh", "NicolaRieke", "PabloDamasceno", "Pedro MarioCruz E Silva", "PochuanWang", "ShengXu", "ShuichiKawano", "SiraSriswasdi", "Soo YoungPark", "ThomasGrist", "VarunBuch", "WatsamonJantarabenjakul", "WeichungWang", "Won YoungTak", "XiangLi", "XihongLin", "FredKwon", "FionaGilbert", "JoshKaggie", "QuanzhengLi", "AboodQuraini", "AndrewFeng", "AndrewPriest", "BarisTurkbey", "BenjaminGlicksberg", "BernardoBizzo", "Byung SeokKim", "CarlosTor-Diez", "Chia-ChengLee", "Chia-JungHsu", "ChinLin", "Chiu-LingLai", "ChristopherHess", "ColinCompas", "DeepiBhatia", "EricOermann", "EvanLeibovitz", "HisashiSasaki", "HitoshiMori", "IsaacYang", "Jae HoSohn", "Krishna NandKeshava Murthy", "Li-ChenFu", "Matheus RibeiroFurtado de Mendon\u00e7a", "MikeFralick", "Min KyuKang", "MohammadAdil", "NatalieGangai", "PeeraponVateekul", "PierreElnajjar", "SarahHickman", "SharmilaMajumdar", "ShelleyMcLeod", "SheridanReed", "StefanGraf", "StephanieHarmon", "TatsuyaKodama", "ThanyaweePuthanakit", "TonyMazzulli", "Vitorde Lima Lavor", "YothinRakvongthai", "Yu RimLee", "YuhongWen"], "doi": "10.21203/rs.3.rs-126892/v1\n10.26099/41xy-9m57\n10.1111/anae.15073\n10.1111/anae.15073\n10.1002/emp2.12071\n10.1101/2020.05.10.20096073\n10.1093/jamia/ocaa172\n10.1101/2020.08.11.20172809\n10.1109/cvpr.2016.90\n10.1145/3124749.3124754\n10.1007/978-1-4842-6699-1_1\n10.1007/978-3-030-32692-0_16\n10.1109/allerton.2015.7447103\n10.1145/3124749.3124754\n10.1007/978-3-030-32692-0_16"}
{"title": "Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study.", "abstract": "To compare the performance of artificial intelligence (AI) and Radiographic Assessment of Lung Edema (RALE) scores from frontal chest radiographs (CXRs) for predicting patient outcomes and the need for mechanical ventilation in COVID-19 pneumonia. Our IRB-approved study included 1367 serial CXRs from 405 adult patients (mean age 65\u2009\u00b1\u200916\u00a0years) from two sites in the US (Site A) and South Korea (Site B).\u00a0We recorded information pertaining to patient demographics (age, gender), smoking history, comorbid conditions (such as cancer, cardiovascular and other diseases), vital signs (temperature, oxygen saturation), and available laboratory data (such as WBC count and CRP). Two thoracic radiologists performed the qualitative assessment of all CXRs based on the RALE score for assessing the severity of lung involvement. All CXRs were processed with a commercial AI algorithm to obtain the percentage of the lung affected with findings related to COVID-19 (AI score). Independent t- and chi-square tests were used in addition to multiple logistic regression with Area Under the Curve (AUC) as output for predicting disease outcome and the need for mechanical ventilation. The RALE and AI scores had a strong positive correlation in CXRs from each site (r", "journal": "Scientific reports", "date": "2021-01-15", "authors": ["ShadiEbrahimian", "FatemehHomayounieh", "Marcio A B CRockenbach", "PreethamPutha", "TarunRaj", "IttaiDayan", "Bernardo CBizzo", "VarunBuch", "DufanWu", "KyungsangKim", "QuanzhengLi", "Subba RDigumarthy", "Mannudeep KKalra"], "doi": "10.1038/s41598-020-79470-0\n10.1136/thoraxjnl-2020-215091\n10.1128/JCM.00512-20\n10.1148/ryct.2020200034\n10.1148/radiol.2020201160\n10.1007/s11547-020-01200-3\n10.1007/s11547-020-01232-9\n10.1148/radiol.2020201874\n10.1136/thoraxjnl-2017-211280\n10.1109/TPAMI.2019.2913372\n10.1016/j.clim.2020.108509\n10.2214/AJR.20.23240\n10.1371/journal.pone.0204155"}
{"title": "Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning.", "abstract": "This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopted advanced deep network architectures and proposed a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conducted extensive sets of experiments on two CT image datasets, namely, the SARS-CoV-2 CT-scan and the COVID19-CT. The results show superior performances for our models compared with previous studies. Our best models achieved average accuracy, precision, sensitivity, specificity, and F1-score values of 99.4%, 99.6%, 99.8%, 99.6%, and 99.4% on the SARS-CoV-2 dataset, and 92.9%, 91.3%, 93.7%, 92.2%, and 92.5% on the COVID19-CT dataset, respectively. For better interpretability of the results, we applied visualization techniques to provide visual explanations for the models' predictions. Feature visualizations of the learned features show well-separated clusters representing CT images of COVID-19 and non-COVID-19 cases. Moreover, the visualizations indicate that our models are not only capable of identifying COVID-19 cases but also provide accurate localization of the COVID-19-associated regions, as indicated by well-trained radiologists.", "journal": "Sensors (Basel, Switzerland)", "date": "2021-01-15", "authors": ["HammamAlshazly", "ChristophLinse", "ErhardtBarth", "ThomasMartinetz"], "doi": "10.3390/s21020455\n10.3201/eid2606.200239\n10.1016/S0140-6736(20)30607-3\n10.1016/S0140-6736(20)30211-7\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1148/radiol.2020200241\n10.1007/s10916-020-01562-1\n10.1007/s11263-015-0816-y\n10.1016/j.sysarc.2020.101830\n10.1109/ACCESS.2020.3024116\n10.1109/TMI.2016.2528162\n10.1148/radiol.2020200905\n10.1038/s41598-020-76282-0\n10.1016/j.eng.2020.04.010\n10.1016/j.imu.2020.100427\n10.1007/s13246-020-00865-4\n10.1371/journal.pone.0235187\n10.1016/j.ejrad.2020.109041\n10.1007/s12652-020-02669-6\n10.1038/s41598-020-76550-z\n10.1109/JBHI.2020.3023246\n10.1007/s10489-020-01943-6\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110122\n10.1016/j.media.2020.101794\n10.1016/j.cmpb.2020.105608\n10.1101/2020.03.24.20043117\n10.1101/2020.02.23.20026930\n10.1080/07391102.2020.1788642\n10.1016/j.cmpb.2020.105532\n10.1101/2020.04.13.20063479\n10.3390/s19194139\n10.1101/2020.04.24.20078584\n10.1101/2020.04.13.20063941\n10.1016/j.chaos.2020.110190\n10.1155/2020/8843664\n10.36227/techrxiv.12476426.v1\n10.1007/s00330-020-06801-0\n10.1016/j.diii.2020.03.014"}
{"title": "Texture Analysis in the Evaluation of COVID-19 Pneumonia in Chest X-Ray Images: A Proof of Concept Study.", "abstract": "One of the most challenging aspects related to Covid-19 is to establish the presence of infection in an early phase of the disease. Texture analysis might be an additional tool for the evaluation of Chest X-ray in patients with clinical suspicion of Covid-19 related pneumonia.\nTo evaluate the diagnostic performance of texture analysis and machine learning models for the diagnosis of Covid-19 interstitial pneumonia in Chest X-ray images.\nChest X-ray images were accessed from a publicly available repository(https://www.kaggle. com/tawsifurrahman/covid19-radiography-database). Lung areas were manually segmented using a polygonal region of interest covering both lung areas, using MaZda, a freely available software for texture analysis. A total of 308 features per ROI was extracted. One hundred-ten Covid-19 Chest X-ray images were selected for the final analysis.\nSix models, namely NB, GLM, DL, GBT, ANN, and PLS-DA were selected and ensembled. According to Youden's index, the Covid-19 Ensemble Machine Learning Score showing the highest area under the curve (0.971\u00b10.015) was 132.57. Assuming this cut-off the Ensemble model performance was estimated by evaluating both true and false positive/negative, resulting in 91.8% accuracy with 93% sensitivity and 90% specificity. Moving the cut-off value to -100, although the accuracy resulted lower (90.6%), the Ensemble Machine Learning showed 100% sensitivity, with 80% specificity.\nTexture analysis of Chest X-ray images and machine learning algorithms may help in differentiating patients with Covid-19 pneumonia. Despite several limitations, this study can lay the ground for future research works in this field and help to develop more rapid and accurate screening tools for these patients.", "journal": "Current medical imaging", "date": "2021-01-14", "authors": ["Armando UgoCavallo", "JacopoTroisi", "MarcoForcina", "Pier-ValerioMari", "ValerioForte", "MassimilianoSperandio", "SergioPagano", "PierpaoloCavallo", "RobertoFloris", "FrancescoGaraci"], "doi": "10.2174/1573405617999210112195450"}
{"title": "An Efficient Method for Coronavirus Detection Through X-rays Using Deep Neural Network.", "abstract": "Coronavirus (COVID-19) is a group of infectious diseases caused by related viruses called coronaviruses. In humans, the seriousness of infection caused by a coronavirus in the respiratory tract can vary from mild to lethal. A serious illness can be developed in old people and those with underlying medical problems like diabetes, cardiovascular disease, cancer, and chronic respiratory disease. For the diagnosis of coronavirus disease, due to the growing number of cases, a limited number of test kits for COVID-19 are available in the hospitals. Hence, it is important to implement an automated system as an immediate alternative diagnostic option to pause the spread of COVID-19 in the population.\nThis paper proposes a deep learning model for the classification of coronavirus infected patient detection using chest X-ray radiographs.\nA fully connected convolutional neural network model is developed to classify healthy and diseased X-ray radiographs. The proposed neural network model consists of seven convolutional layers with the rectified linear unit, softmax (last layer) activation functions, and max-pooling layers which were trained using the publicly available COVID-19 dataset.\nFor validation of the proposed model, the publicly available chest X-ray radiograph dataset consisting of COVID-19 and normal patient's images were used. Considering the performance of the results that are evaluated based on various evaluation metrics such as precision, recall, MSE, RMSE and accuracy, it is seen that the accuracy of the proposed CNN model is 98.07%.", "journal": "Current medical imaging", "date": "2021-01-14", "authors": ["P SrinivasaRao", "PradeepBheemavarapu", "P S LathaKalyampudi", "T V MadhusudhanaRao"], "doi": "10.2174/1573405617999210112193220"}
{"title": "Diabetic Retinopathy Screening Using Artificial Intelligence and Handheld Smartphone-Based Retinal Camera.", "abstract": "Portable retinal cameras and deep learning (DL) algorithms are novel tools adopted by diabetic retinopathy (DR) screening programs. Our objective is to evaluate the diagnostic accuracy of a DL algorithm and the performance of portable handheld retinal cameras in the detection of DR in a large and heterogenous type 2 diabetes population in a real-world, high burden setting.\nParticipants underwent fundus photographs of both eyes with a portable retinal camera (Phelcom Eyer). Classification of DR was performed by human reading and a DL algorithm (PhelcomNet), consisting of a convolutional neural network trained on a dataset of fundus images captured exclusively with the portable device; both methods were compared. We calculated the area under the curve (AUC), sensitivity, and specificity for more than mild DR.\nA total of 824 individuals with type 2 diabetes were enrolled at Itabuna Diabetes Campaign, a subset of 679 (82.4%) of whom could be fully assessed. The algorithm sensitivity/specificity was 97.8 % (95% CI 96.7-98.9)/61.4 % (95% CI 57.7-65.1); AUC was 0\u00b789. All false negative cases were classified as moderate non-proliferative diabetic retinopathy (NPDR) by human grading.\nThe DL algorithm reached a good diagnostic accuracy for more than mild DR in a real-world, high burden setting. The performance of the handheld portable retinal camera was adequate, with over 80% of individuals presenting with images of sufficient quality. Portable devices and artificial intelligence tools may increase coverage of DR screening programs.", "journal": "Journal of diabetes science and technology", "date": "2021-01-14", "authors": ["Fernando KornMalerbi", "Rafael ErnaneAndrade", "Paulo HenriqueMorales", "Jos\u00e9 AugustoStuchi", "DiegoLencione", "Jean Vitorde Paulo", "Mayana PereiraCarvalho", "Fabr\u00edcia SilvaNunes", "Roseanne MontargilRocha", "Daniel AFerraz", "RubensBelfort"], "doi": "10.1177/1932296820985567\n10.1177/1932296820906212\n10.5935/0004-2749.20200070."}
{"title": "A novel deep learning-based quantification of serial chest computed tomography in Coronavirus Disease 2019 (COVID-19).", "abstract": "This study aims to explore and compare a novel deep learning-based quantification with the conventional semi-quantitative computed tomography (CT) scoring for the serial chest CT scans of COVID-19. 95 patients with confirmed COVID-19 and a total of 465 serial chest CT scans were involved, including 61 moderate patients (moderate group, 319 chest CT scans) and 34 severe patients (severe group, 146 chest CT scans). Conventional CT scoring and deep learning-based quantification were performed for all chest CT scans for two study goals: (1) Correlation between these two estimations; (2) Exploring the dynamic patterns using these two estimations between moderate and severe groups. The Spearman's correlation coefficient between these two estimation methods was 0.920 (p\u2009<\u20090.001). predicted pulmonary involvement (CT score and percent of pulmonary lesions calculated using deep learning-based quantification) increased more rapidly and reached a higher peak on 23rd days from symptom onset in severe group, which reached a peak on 18th days in moderate group with faster absorption of the lesions. The deep learning-based quantification for COVID-19 showed a good correlation with the conventional CT scoring and demonstrated a potential benefit in the estimation of disease severities of COVID-19.", "journal": "Scientific reports", "date": "2021-01-13", "authors": ["FengPan", "LinLi", "BoLiu", "TianheYe", "LingliLi", "DehanLiu", "ZezhenDing", "GuangfengChen", "BoLiang", "LianYang", "ChuanshengZheng"], "doi": "10.1038/s41598-020-80261-w\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343\n10.1148/radiol.2020200642\n10.1001/jama.2020.1585\n10.1148/radiol.2020200370\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200843\n10.2214/AJR.20.22976\n10.1097/RLI.0000000000000670\n10.1097/RLI.0000000000000672\n10.1148/radiol.11092149\n10.1148/rg.2018170048\n10.1148/radiol.2462070712\n10.1007/s11604-020-01010-7\n10.1148/radiol.2363040958\n10.1016/j.media.2017.06.014\n10.9734/BJMCS/2017/32229\n10.1007/s00330-016-4317-3\n10.1016/j.compmedimag.2008.04.005\n10.1109/TMI.2012.2219881\n10.3760/cma.j.issn.0254-6450.2020.02.003\n10.1056/NEJMoa2002032\n10.1016/S2213-2600(20)30370-2\n10.7150/ijms.46614\n10.1038/s41598-020-68057-4\n10.1016/S2213-2600(20)30453-7\n10.1016/j.ejrad.2020.109233\n10.3389/fpubh.2020.587937\n10.1038/s41551-020-00633-5"}
{"title": "COV19-CNNet and COV19-ResNet: Diagnostic Inference Engines for Early Detection of COVID-19.", "abstract": "Chest CT is used in the COVID-19 diagnosis process as a significant complement to the reverse transcription polymerase chain reaction (RT-PCR) technique. However, it has several drawbacks, including long disinfection and ventilation times, excessive radiation effects, and high costs. While X-ray radiography is more useful for detecting COVID-19, it is insensitive to the early stages of the disease. We have developed inference engines that will turn X-ray machines into powerful diagnostic tools by using deep learning technology to detect COVID-19. We named these engines COV19-CNNet and COV19-ResNet. The former is based on convolutional neural network architecture; the latter is on residual neural network (ResNet) architecture. This research is a retrospective study. The database consists of 210 COVID-19, 350 viral pneumonia, and 350 normal (healthy) chest X-ray (CXR) images that were created using two different data sources. This study was focused on the problem of multi-class classification (COVID-19, viral pneumonia, and normal), which is a rather difficult task for the diagnosis of COVID-19. The classification accuracy levels for COV19-ResNet and COV19-CNNet were 97.61% and 94.28%, respectively. The inference engines were developed from scratch using new and special deep neural networks without pre-trained models, unlike other studies in the field. These powerful diagnostic engines allow for the early detection of COVID-19 as well as distinguish it from viral pneumonia with similar radiological appearances. Thus, they can help in fast recovery at the early stages, prevent the COVID-19 outbreak from spreading, and contribute to reducing pressure on health-care systems worldwide.", "journal": "Cognitive computation", "date": "2021-01-12", "authors": ["AyturkKeles", "Mustafa BerkKeles", "AliKeles"], "doi": "10.1007/s12559-020-09795-5\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2017161659\n10.1148/radiol.2020200370\n10.1148/radiol.2481071451\n10.1016/j.tibtech.2018.08.005\n10.1016/j.zemedi.2018.11.002\n10.1016/j.zemedi.2018.12.003\n10.1007/s13246-020-00865-4\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103792\n10.1155/2019/4180949\n10.7717/peerj.6201\n10.1109/TMI.2016.2538465\n10.1016/j.cogsys.2018.12.007\n10.1038/nature21056\n10.1148/radiol.2020200463\n10.1148/radiol.2020200241\n10.1148/ryct.2020200034"}
{"title": "Modeling the progression of COVID-19 deaths using Kalman Filter and AutoML.", "abstract": "The COVID-19 pandemic continues to have a destructive effect on the health and well-being of the global population. A vital step in the battle against it is the successful screening of infected patients, together with one of the effective screening methods being radiology examination using chest radiography. Recognition of epidemic growth patterns across temporal and social factors can improve our capability to create epidemic transmission designs, including the critical job of predicting the estimated intensity of the outbreak morbidity or mortality impact at the end. The study's primary motivation is to be able to estimate with a certain level of accuracy the number of deaths due to COVID-19, managing to model the progression of the pandemic. Predicting the number of possible deaths from COVID-19 can provide governments and decision-makers with indicators for purchasing respirators and pandemic prevention policies. Thus, this work presents itself as an essential contribution to combating the pandemic. Kalman Filter is a widely used method for tracking and navigation and filtering and time series. Designing and tuning machine learning methods are a labor- and time-intensive task that requires extensive experience. The field of automated machine learning Auto Machine Learning relies on automating this task. Auto Machine Learning tools enable novice users to create useful machine learning units, while experts can use them to free up valuable time for other tasks. This paper presents an objective method of forecasting the COVID-19 outbreak using Kalman Filter and Auto Machine Learning. We use a COVID-19 dataset of Cear\u00e1, one of the 27 federative units in Brazil. Cear\u00e1 has more than 235,222 confirmed cases of COVID-19 and 8850 deaths due to the disease. The TPOT automobile model showed the best result with a 0.99 of ", "journal": "Soft computing", "date": "2021-01-12", "authors": ["TaoHan", "Francisco Nauber BernardoGois", "Rams\u00e9sOliveira", "Luan RochaPrates", "Magda Moura de AlmeidaPorto"], "doi": "10.1007/s00500-020-05503-5\n10.2139/ssrn.3580188\n10.1007/BF00169563\n10.1016/S0025-5564(96)00155-1\n10.3390/app10020559\n10.1216/RMJ-1979-9-1-31\n10.1109/tfuzz.2019.2949771\n10.1109/tcyb.2020.3000440\n10.1109/JSAC.2020.3020598\n10.1101/2020.07.13.20152983\n10.1016/j.chaos.2020.109761\n10.1002/for.1125\n10.1016/j.knosys.2020.106622\n10.1016/j.ijforecast.2003.09.015\n10.1016/j.procs.2010.04.136\n10.1080/00031305.1983.10482723\n10.1109/tnnls.2020.2995800\n10.22214/ijraset.2020.4040\n10.1109/JAS.2020.1003393\n10.2139/ssrn.3590821\n10.1016/j.chaos.2020.109853\n10.1109/ACCESS.2018.2817614\n10.1016/S0888-3270(03)00020-7\n10.1016/j.inffus.2019.06.004\n10.1093/imammb/1.2.169\n10.1109/tfuzz.2019.2961350\n10.1049/iet-com.2015.0368\n10.1016/j.future.2018.12.008\n10.1016/j.jclepro.2019.01.188\n10.1109/TITS.2020.3019227\n10.1016/S0895-7177(00)00040-6\n10.1080/00031305.2017.1380080\n10.1016/j.epidem.2016.01.002\n10.1109/tmi.2020.3000314\n10.1287/mnsc.6.3.324\n10.1136/bmj.m1328\n10.14778/3415478.3415542\n10.1371/journal.pcbi.1003583\n10.1101/2020.03.27.20045625\n10.1007/s00466-020-01911-4\n10.1101/2020.03.26.20044289\n10.1101/2020.03.29.20047118"}
{"title": "Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network.", "abstract": "Recently, the outbreak of the novel coronavirus disease 2019 (COVID-19) pandemic has seriously endangered human health and life. In fighting against COVID-19, effective diagnosis of infected patient is critical for preventing the spread of diseases. Due to limited availability of test kits, the need for auxiliary diagnostic approach has increased. Recent research has shown radiography of COVID-19 patient, such as CT and X-ray, contains salient information about the COVID-19 virus and could be used as an alternative diagnosis method. Chest X-ray (CXR) due to its faster imaging time, wide availability, low cost, and portability gains much attention and becomes very promising. In order to reduce intra- and inter-observer variability, during radiological assessment, computer-aided diagnostic tools have been used in order to supplement medical decision making and subsequent management. Computational methods with high accuracy and robustness are required for rapid triaging of patients and aiding radiologist in the interpretation of the collected data.\nIn this study, we design a novel multi-feature convolutional neural network (CNN) architecture for multi-class improved classification of COVID-19 from CXR images. CXR images are enhanced using a local phase-based image enhancement method. The enhanced images, together with the original CXR data, are used as an input to our proposed CNN architecture. Using ablation studies, we show the effectiveness of the enhanced images in improving the diagnostic accuracy. We provide quantitative evaluation on two datasets and qualitative results for visual inspection. Quantitative evaluation is performed on data consisting of 8851 normal (healthy), 6045 pneumonia, and 3323 COVID-19 CXR scans.\nIn Dataset-1, our model achieves 95.57% average accuracy for a three classes classification, 99% precision, recall, and F1-scores for COVID-19 cases. For Dataset-2, we have obtained 94.44% average accuracy, and 95% precision, recall, and F1-scores for detection of COVID-19.\nOur proposed multi-feature-guided CNN achieves improved results compared to single-feature CNN proving the importance of the local phase-based CXR image enhancement. Future work will involve further evaluation of the proposed method on a larger-size COVID-19 dataset as they become available.", "journal": "International journal of computer assisted radiology and surgery", "date": "2021-01-10", "authors": ["XiaoQi", "Lloyd GBrown", "David JForan", "JohnNosher", "IlkerHacihaliloglu"], "doi": "10.1007/s11548-020-02305-w\n10.1016/S1473-3099(20)30120-1\n10.1128/AEM.69.7.4116-4122.2003\n10.1016/j.compbiomed.2020.103792\n10.1016/j.mehy.2020.109761\n10.1109/78.969520\n10.1016/j.array.2019.100004\n10.1007/s11548-019-01934-0\n10.1109/TMI.2017.2712367\n10.1016/j.media.2017.07.005\n10.1007/s11263-019-01228-7"}
{"title": "Glycemic status affects the severity of coronavirus disease 2019 in patients with diabetes mellitus: an observational study of CT radiological manifestations using an artificial intelligence algorithm.", "abstract": "Increasing evidence suggests that poor glycemic control in diabetic individuals is associated with poor coronavirus disease 2019 (COVID-19) pneumonia outcomes and influences chest computed tomography (CT) manifestations. This study aimed to explore the impact of diabetes mellitus (DM) and glycemic control on chest CT manifestations, acquired using an artificial intelligence (AI)-based quantitative evaluation system, and COVID-19 disease severity and to investigate the association between CT lesions and clinical outcome.\nA total of 126 patients with COVID-19 were enrolled in this retrospective study. According to their clinical history of DM and glycosylated hemoglobin (HbA1c) level, the patients were divided into 3 groups: the non-DM group (Group 1); the well-controlled blood glucose (BG) group, with HbA1c\u2009<\u20097% (Group 2); and the poorly controlled BG group, with HbA1c\u2009\u2265\u20097% (Group 3). The chest CT images were analyzed with an AI-based quantitative evaluation system. Three main quantitative CT features representing the percentage of total lung lesion volume (PLV), percentage of ground-glass opacity volume (PGV) and percentage of consolidation volume (PCV) in bilateral lung fields were used to evaluate the severity of pneumonia lesions.\nPatients in Group 3 had the highest percentage of severe or critical illness, with 12 (32%) cases, followed by 6 (11%) and 7 (23%) cases in Groups 1 and 2, respectively (p\u2009=\u20090.042). The composite endpoints, including death or using mechanical ventilation or admission to the intensive care unit (ICU), were 3 (5%), 5 (16%) and 10 (26%) in Groups 1, 2 and 3, respectively (p\u2009=\u20090.013). The PLV, PGV and PCV in bilateral lung fields were significantly different among the three groups (all p\u2009<\u20090.001): the median PLVs were 12.5% (Group 3), 3.8% (Group 2) and 2.4% (Group 1); the median PGVs were 10.2% (Group 3), 3.6% (Group 2) and 1.9% (Group 1); and the median PCVs were 1.8% (Group 3), 0.3% (Group 2) and 0.1% (Group 1). In the linear regression analyses, which were adjusted for age, sex, BMI, and comorbidities, HbA1c remained positively associated with PLV (\u03b2\u2009=\u20090.401, p\u2009<\u20090.001), PGV (\u03b2\u2009=\u20090.364, p\u2009=\u20090.001) and PCV (\u03b2\u2009=\u20090.472, p\u2009<\u20090.001); this relationship was also observed between fasting blood glucose (FBG) and the three CT quantitative parameters. In the logistic regression analyses, PLV [OR 1.067 (1.032, 1.103)], PGV [OR 1.076 (1.034, 1.120)] and PCV [OR 1.280 (1.110, 1.476)] levels were independent predictors of the composite endpoints, as well as the areas under the ROC (AUCs) for PLV [AUC 0.796 (0.691, 0.900)], PGV [AUC 0.783 (0.678, 0.889)] and PCV [AUC 0.816 (0.722, 0.911)]; the ORs were still significant for CT lesions after adjusting for age, sex and poorly controlled diabetes.\nIncreased blood glucose level was correlated with the severity of lung involvement, as evidenced by certain chest CT parameters, and clinical prognosis in diabetic COVID-19 patients. There was a positive correlation between blood glucose level (both HbA1c and FBG) on admission and lung lesions. Moreover, the CT lesion severity by AI quantitative analysis was correlated with clinical outcomes.", "journal": "Acta diabetologica", "date": "2021-01-10", "authors": ["XiaotingLu", "ZhenhaiCui", "FengPan", "LingliLi", "LinLi", "BoLiang", "LianYang", "ChuanshengZheng"], "doi": "10.1007/s00592-020-01654-x\n10.1016/S2213-2600(20)30079-5\n10.1016/S0140-6736(20)30211-7\n10.1002/dmrr.3319\n10.1016/j.cmet.2020.04.021\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.1007/s00592-020-01546-0\n10.1016/j.diabres.2020.108346\n10.2337/dc20-S002\n10.2337/dc20-S006\n10.7150/thno.45985\n10.1183/13993003.01234-2020\n10.1007/s00330-020-06713-z\n10.1148/radiol.2020200843\n10.1016/S2213-2600(20)30132-6\n10.1007/s00330-020-06731-x\n10.1016/j.ejrad.2020.108941\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2020201491\n10.1148/ryct.2020200075\n10.1016/j.diabres.2020.108214\n10.1016/j.diabres.2020.108286\n10.1016/j.diabres.2020.108185\n10.1016/S2213-8587(20)30271-0\n10.1007/s00125-020-05180-x\n10.1172/jci.insight.131774\n10.1007/s00134-003-1961-2\n10.1002/dmrr.3159"}
{"title": "Artificial Intelligence Empowers Radiologists to Differentiate Pneumonia Induced by COVID-19 versus Influenza Viruses.", "abstract": "Given the current pandemic, differentiation between pneumonia induced by COVID-19 or influenza viruses is of utmost clinical significance in the patients' management. For this purpose, this study was conducted to develop sensitive artificial intelligence (AI) models to assist radiologists to decisively differentiate pneumonia due to COVID-19 versus influenza viruses.\nCross sectional chest CT images (N=12744) from well-evaluated cases of pneumonias induced by COVID-19 or H1N1 Influenza viruses, and normal individuals were collected. We examined the computer tomographic (CT) chest images from 137 individuals. Various pre-trained convolutional neural network models, such as ResNet-50, InceptionV3, Wide ResNet, SqueezNet, VGG 16 and VGG 19 were fine-tuned on our datasets. The datasets were used for training (60%), validation (20%), and testing (20%) of the final models. Also, the predictive power and means of precision and recall were determined for each model.\nFine-tuned \nFine-tuned and pre-trained image classifying models of AI enable radiologists to reliably differentiate the pneumonia induced by COVID-19 versus H1N1 influenza virus. For this purpose, ", "journal": "Acta informatica medica : AIM : journal of the Society for Medical Informatics of Bosnia & Herzegovina : casopis Drustva za medicinsku informatiku BiH", "date": "2021-01-09", "authors": ["HoumanSotoudeh", "MohsenTabatabaei", "BaharakTasorian", "KamranTavakol", "EhsanSotoudeh", "Abdol LatifMoini"], "doi": "10.5455/aim.2020.28.190-195"}
{"title": "Cascaded deep transfer learning on thoracic CT in COVID-19 patients treated with steroids.", "abstract": "", "journal": "Journal of medical imaging (Bellingham, Wash.)", "date": "2021-01-09", "authors": ["Jordan DFuhrman", "JunChen", "ZegangDong", "Fleming Y MLure", "ZheLuo", "Maryellen LGiger"], "doi": "10.1117/1.JMI.8.S1.014501\n10.1056/NEJMsb2005114\n10.1016/S2213-2600(20)30076-X\n10.3332/ecancer.2020.1023\n10.1001/jama.2020.17023\n10.1056/NEJMoa2021436\n10.1038/s41392-020-0158-2\n10.1093/cid/ciaa601\n10.1148/ryct.2020200152\n10.1148/ryct.2020200034\n10.1148/radiol.2020200642\n10.1146/annurev-bioeng-071516-044442\n10.1016/j.jacr.2017.12.028\n10.1016/j.media.2017.07.005\n10.1002/mp.13264\n10.1016/j.jacr.2017.12.028\n10.1002/mp.12453\n10.1109/TMI.2016.2535302\n10.1080/14786440109462720\n10.1016/j.neucom.2011.06.026\n10.1148/radiol.2533090280\n10.1006/jmps.1998.1218\n10.1080/01621459.1958.10501452\n10.1001/jamainternmed.2020.0994\n10.1016/S0140-6736(20)30566-3\n10.1056/NEJM199701233360402\n10.1148/radiology.143.1.7063747"}
{"title": "CT radiomics facilitates more accurate diagnosis of COVID-19 pneumonia: compared with CO-RADS.", "abstract": "Limited data was available for rapid and accurate detection of COVID-19 using CT-based machine learning model. This study aimed to investigate the value of chest CT radiomics for diagnosing COVID-19 pneumonia compared with clinical model and COVID-19 reporting and data system (CO-RADS), and develop an open-source diagnostic tool with the constructed radiomics model.\nThis study enrolled 115 laboratory-confirmed COVID-19 and 435 non-COVID-19 pneumonia patients (training dataset, n\u2009=\u2009379; validation dataset, n\u2009=\u2009131; testing dataset, n\u2009=\u200940). Key radiomics features extracted from chest CT images were selected to build a radiomics signature using least absolute shrinkage and selection operator (LASSO) regression. Clinical and clinico-radiomics combined models were constructed. The combined model was further validated in the viral pneumonia cohort, and compared with performance of two radiologists using CO-RADS. The diagnostic performance was assessed by receiver operating characteristics curve (ROC) analysis, calibration curve, and decision curve analysis (DCA).\nEight radiomics features and 5 clinical variables were selected to construct the combined radiomics model, which outperformed the clinical model in diagnosing COVID-19 pneumonia with an area under the ROC (AUC) of 0.98 and good calibration in the validation cohort. The combined model also performed better in distinguishing COVID-19 from other viral pneumonia with an AUC of 0.93 compared with 0.75 (P\u2009=\u20090.03) for clinical model, and 0.69 (P\u2009=\u20090.008) or 0.82 (P\u2009=\u20090.15) for two trained radiologists using CO-RADS. The sensitivity and specificity of the combined model can be achieved to 0.85 and 0.90. The DCA confirmed the clinical utility of the combined model. An easy-to-use open-source diagnostic tool was developed using the combined model.\nThe combined radiomics model outperformed clinical model and CO-RADS for diagnosing COVID-19 pneumonia, which can facilitate more rapid and accurate detection.", "journal": "Journal of translational medicine", "date": "2021-01-09", "authors": ["HuanhuanLiu", "HuaRen", "ZengbinWu", "HeXu", "ShuhaiZhang", "JinningLi", "LiangHou", "RunminChi", "HuiZheng", "YanhongChen", "ShaofengDuan", "HuiminLi", "ZongyuXie", "DengbinWang"], "doi": "10.1186/s12967-020-02692-3\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200274\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200370\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200823\n10.1148/radiol.2020201473\n10.1007/s00330-020-06863-0\n10.1158/1078-0432.CCR-17-1038\n10.3389/fonc.2020.00593\n10.1007/s00330-019-06026-w\n10.1148/radiol.2363040958\n10.1158/0008-5472.CAN-17-0339\n10.2307/2531595\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30211-7\n10.1148/radiol.2020200230\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020201491\n10.1148/radiol.2020200905\n10.1101/2020.02.14.20023028\n10.1016/j.asoc.2020.106897\n10.1007/s00330-019-06572-3\n10.1007/s00330-018-5802-7\n10.1148/rg.2018170048\n10.1148/radiol.2015151169\n10.1038/nrclinonc.2017.141"}
{"title": "Accurately Differentiating Between Patients With COVID-19, Patients With Other Viral Infections, and Healthy Individuals: Multimodal Late Fusion Learning Approach.", "abstract": "Effectively identifying patients with COVID-19 using nonpolymerase chain reaction biomedical data is critical for achieving optimal clinical outcomes. Currently, there is a lack of comprehensive understanding in various biomedical features and appropriate analytical approaches for enabling the early detection and effective diagnosis of patients with COVID-19.\nWe aimed to combine low-dimensional clinical and lab testing data, as well as high-dimensional computed tomography (CT) imaging data, to accurately differentiate between healthy individuals, patients with COVID-19, and patients with non-COVID viral pneumonia, especially at the early stage of infection.\nIn this study, we recruited 214 patients with nonsevere COVID-19, 148 patients with severe COVID-19, 198 noninfected healthy participants, and 129 patients with non-COVID viral pneumonia. The participants' clinical information (ie, 23 features), lab testing results (ie, 10 features), and CT scans upon admission were acquired and used as 3 input feature modalities. To enable the late fusion of multimodal features, we constructed a deep learning model to extract a 10-feature high-level representation of CT scans. We then developed 3 machine learning models (ie, k-nearest neighbor, random forest, and support vector machine models) based on the combined 43 features from all 3 modalities to differentiate between the following 4 classes: nonsevere, severe, healthy, and viral pneumonia.\nMultimodal features provided substantial performance gain from the use of any single feature modality. All 3 machine learning models had high overall prediction accuracy (95.4%-97.7%) and high class-specific prediction accuracy (90.6%-99.9%).\nCompared to the existing binary classification benchmarks that are often focused on single-feature modality, this study's hybrid deep learning-machine learning framework provided a novel and effective breakthrough for clinical applications. Our findings, which come from a relatively large sample size, and analytical workflow will supplement and assist with clinical decision support for current COVID-19 diagnostic methods and other clinical applications with high-dimensional multimodal biomedical features.", "journal": "Journal of medical Internet research", "date": "2021-01-07", "authors": ["MingXu", "LiuOuyang", "LeiHan", "KaiSun", "TingtingYu", "QianLi", "HuaTian", "LidaSafarnejad", "HengdongZhang", "YueGao", "Forrest ShengBao", "YuanfangChen", "PatrickRobinson", "YaorongGe", "BaoliZhu", "JieLiu", "ShiChen"], "doi": "10.2196/25535\n10.1002/jmv.25702\n10.1002/jmv.25721\n10.1016/j.ajic.2020.07.011\n10.1109/TMI.2020.3001810\n10.1002/hep.31446\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06975-7\n10.1097/RLI.0000000000000689\n10.1016/S0262-4079(20)30909-X\n10.1001/jama.2020.2648\n10.1056/NEJMp2005689\n10.1148/radiol.2020200432\n10.1038/s41591-020-0916-2\n10.2196/19822\n10.1016/j.crad.2020.03.008\n10.1093/cid/ciaa322\n10.1007/s10916-020-01597-4\n10.33321/cdi.2020.44.55\n10.33321/cdi.2020.44.55\n10.1056/NEJMe2009758\n10.3348/kjr.2020.0181\n10.1007/s00330-020-06925-3\n10.1007/s00330-020-07018-x\n10.1080/17843286.2020.1798668\n10.1016/S1473-3099(20)30460-6\n10.1038/s41598-019-42294-8\n10.1038/s41598-019-42294-8\n10.1148/radiol.2020200642\n10.1093/ofid/ofaa171\n10.1097/RLI.0000000000000670\n10.1016/j.jinf.2020.04.004\n10.2214/AJR.20.22959\n10.1016/j.jinf.2020.02.022\n10.1109/TPAMI.2018.2798607\n10.1164/rccm.201908-1581ST\n10.1016/j.compbiomed.2020.103795\n10.2196/19569\n10.1109/ACCESS.2020.3005510\n10.1148/radiol.2020200905\n10.1101/2020.05.18.20105841\n10.1371/journal.pone.0235187\n10.1371/journal.pone.0235187\n10.1093/cid/ciaa538\n10.1515/cclm-2020-0398\n10.1038/s41591-020-0931-3\n10.1148/radiol.2020200823\n10.1002/cyto.a.23990\n10.1089/omi.2020.0073\n10.1089/omi.2020.0093\n10.1016/j.dsx.2020.06.070\n10.1016/j.diabres.2020.108347\n10.1001/jamacardio.2020.1286\n10.3390/jcm9051407"}
{"title": "RICORD: A Precedent for Open AI in COVID-19 Image Analytics.", "abstract": null, "journal": "Radiology", "date": "2021-01-07", "authors": ["Harrison XBai", "Nicole MThomasian"], "doi": "10.1148/radiol.2020204214\n10.1016/S1473-3099(20)30120-1"}
{"title": "The RSNA International COVID-19 Open Radiology Database (RICORD).", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic is a global health care emergency. Although reverse-transcription polymerase chain reaction testing is the reference standard method to identify patients with COVID-19 infection, chest radiography and CT play a vital role in the detection and management of these patients. Prediction models for COVID-19 imaging are rapidly being developed to support medical decision making. However, inadequate availability of a diverse annotated data set has limited the performance and generalizability of existing models. To address this unmet need, the RSNA and Society of Thoracic Radiology collaborated to develop the RSNA International COVID-19 Open Radiology Database (RICORD). This database is the first multi-institutional, multinational, expert-annotated COVID-19 imaging data set. It is made freely available to the machine learning community as a research and educational resource for COVID-19 chest imaging. Pixel-level volumetric segmentation with clinical annotations was performed by thoracic radiology subspecialists for all COVID-19-positive thoracic CT scans. The labeling schema was coordinated with other international consensus panels and COVID-19 data annotation efforts, the European Society of Medical Imaging Informatics, the American College of Radiology, and the American Association of Physicists in Medicine. Study-level COVID-19 classification labels for chest radiographs were annotated by three radiologists, with majority vote adjudication by board-certified radiologists. RICORD consists of 240 thoracic CT scans and 1000 chest radiographs contributed from four international sites. It is anticipated that RICORD will ideally lead to prediction models that can demonstrate sustained performance across populations and health care systems.", "journal": "Radiology", "date": "2021-01-06", "authors": ["Emily BTsai", "ScottSimpson", "Matthew PLungren", "MichelleHershman", "LeonidRoshkovan", "ErrolColak", "Bradley JErickson", "GeorgeShih", "AnoukStein", "JayashreeKalpathy-Cramer", "JodyShen", "MonaHafez", "SusanJohn", "PrabhakarRajiah", "Brian PPogatchnik", "JohnMongan", "EmreAltinmakas", "Erik RRanschaert", "Felipe CKitamura", "LaurensTopff", "LindaMoy", "Jeffrey PKanne", "Carol CWu"], "doi": "10.1148/radiol.2021203957\n10.1148/ryai.2019180031"}
{"title": "Fast automated detection of COVID-19 from medical images using convolutional neural networks.", "abstract": "Coronavirus disease 2019 (COVID-19) is a global pandemic posing significant health risks. The diagnostic test sensitivity of COVID-19 is limited due to irregularities in specimen handling. We propose a deep learning framework that identifies COVID-19 from medical images as an auxiliary testing method to improve diagnostic sensitivity. We use pseudo-coloring methods and a platform for annotating X-ray and computed tomography images to train the convolutional neural network, which achieves a performance similar to that of experts and provides high scores for multiple statistical indices (F1 scores > 96.72% (0.9307, 0.9890) and specificity >99.33% (0.9792, 1.0000)). Heatmaps are used to visualize the salient features extracted by the neural network. The neural network-based regression provides strong correlations between the lesion areas in the images and five clinical indicators, resulting in high accuracy of the classification framework. The proposed method represents a potential computer-aided diagnosis method for COVID-19 in clinical practice.", "journal": "Communications biology", "date": "2021-01-06", "authors": ["ShuangLiang", "HuixiangLiu", "YuGu", "XiuhuaGuo", "HongjunLi", "LiLi", "ZhiyuanWu", "MengyangLiu", "LixinTao"], "doi": "10.1038/s42003-020-01535-7\n10.1101/2020.02.11.20021493v2\n10.1016/j.tvjl.2005.12.014\n10.1377/hlthaff.27.6.1491\n10.1021/acsnano.0c02624\n10.1162/neco_a_00990\n10.1109/TEVC.2019.2916183\n10.1109/TNNLS.2018.2876865\n10.1038/nature24270\n10.1109/MCI.2018.2840738\n10.1016/j.cmpb.2013.10.011\n10.1001/jama.2016.17216\n10.1145/358669.358692\n10.1007/s003300101100\n10.1038/nature14539\n10.1111/1467-9639.00050\n10.1007/BF02295996\n10.1016/j.media.2017.06.015\n10.1007/s10278-013-9622-7\n10.1007/s00259-020-04929-1"}
{"title": "Development and Validation of an Automated Radiomic CT Signature for Detecting COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) outbreak has reached pandemic status. Drastic measures of social distancing are enforced in society and healthcare systems are being pushed to and beyond their limits. To help in the fight against this threat on human health, a fully automated AI framework was developed to extract radiomics features from volumetric chest computed tomography (CT) exams. The detection model was developed on a dataset of 1381 patients (181 COVID-19 patients plus 1200 non COVID control patients). A second, independent dataset of 197 RT-PCR confirmed COVID-19 patients and 500 control patients was used to assess the performance of the model. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC). The model had an AUC of 0.882 (95% CI: 0.851-0.913) in the independent test dataset (641 patients). The optimal decision threshold, considering the cost of false negatives twice as high as the cost of false positives, resulted in an accuracy of 85.18%, a sensitivity of 69.52%, a specificity of 91.63%, a negative predictive value (NPV) of 94.46% and a positive predictive value (PPV) of 59.44%. Benchmarked against RT-PCR confirmed cases of COVID-19, our AI framework can accurately differentiate COVID-19 from routine clinical conditions in a fully automated fashion. Thus, providing rapid accurate diagnosis in patients suspected of COVID-19 infection, facilitating the timely implementation of isolation procedures and early intervention.", "journal": "Diagnostics (Basel, Switzerland)", "date": "2021-01-06", "authors": ["JulienGuiot", "AkshayaaVaidyanathan", "LouisDeprez", "FadilaZerka", "DenisDanthine", "Anne-No\u00eblleFrix", "MarieThys", "MoniqueHenket", "GregoryCanivet", "StephaneMathieu", "EvanthiaEftaxia", "PhilippeLambin", "NathanTsoutzidis", "BenjaminMiraglio", "SeanWalsh", "MichelMoutschen", "RenaudLouis", "PaulMeunier", "WimVos", "Ralph T HLeijenaar", "PierreLovinfosse"], "doi": "10.3390/diagnostics11010041\n10.1016/S0140-6736(20)30260-9\n10.1183/13993003.00407-2020\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30728-5\n10.1016/S1473-3099(20)30241-3\n10.1038/ncomms5006\n10.1016/S2213-2600(18)30286-8\n10.1038/s41586-019-1799-6\n10.1038/s41591-019-0447-x\n10.1002/mp.12967\n10.1016/j.ejca.2011.11.036\n10.1038/nrclinonc.2017.141\n10.1371/journal.pone.0102107\n10.1038/srep13087\n10.1007/s11263-019-01228-7\n10.1148/radiol.2020200343\n10.1038/d41587-020-00002-2\n10.1007/s00330-020-06865-y\n10.1007/s00259-020-04795-x\n10.1101/2020.02.23.20026930\n10.1101/2020.02.14.20023028\n10.1101/2020.02.25.20021568\n10.1259/bjr.20170498\n10.1016/j.tmaid.2020.101673\n10.1109/RBME.2020.2987975\n10.1148/ryai.2020200053\n10.1038/s41591-020-0931-3\n10.1038/s41467-020-17971-2\n10.1155/2020/9756518\n10.1016/j.diii.2020.06.001\n10.7326/M20-1382\n10.1148/radiol.2020201491\n10.1148/radiol.2020200905\n10.1016/j.ejro.2020.100271\n10.1007/s11432-020-2849-3\n10.1097/RTI.0000000000000544\n10.18383/j.tom.2016.00208"}
{"title": "Lung CT Segmentation to Identify Consolidations and Ground Glass Areas for Quantitative Assesment of SARS-CoV Pneumonia.", "abstract": "Segmentation is a complex task, faced by radiologists and researchers as radiomics and machine learning grow in potentiality. The process can either be automatic, semi-automatic, or manual, the first often not being sufficiently precise or easily reproducible, and the last being excessively time consuming when involving large districts with high-resolution acquisitions. A high-resolution CT of the chest is composed of hundreds of images, and this makes the manual approach excessively time consuming. Furthermore, the parenchymal alterations require an expert evaluation to be discerned from the normal appearance; thus, a semi-automatic approach to the segmentation process is, to the best of our knowledge, the most suitable when segmenting pneumonias, especially when their features are still unknown. For the studies conducted in our institute on the imaging of COVID-19, we adopted 3D Slicer, a freeware software produced by the Harvard University, and combined the threshold with the paint brush instruments to achieve fast and precise segmentation of aerated lung, ground glass opacities, and consolidations. When facing complex cases, this method still requires a considerable amount of time for proper manual adjustments, but provides an extremely efficient mean to define segments to use for further analysis, such as the calculation of the percentage of the affected lung parenchyma or texture analysis of the ground glass areas.", "journal": "Journal of visualized experiments : JoVE", "date": "2021-01-05", "authors": ["ArrigoCattabriga", "Maria AdrianaCocozza", "GiulioVara", "FrancescaCoppola", "RitaGolfieri"], "doi": "10.3791/61737"}
{"title": "Deployment of artificial intelligence for radiographic diagnosis of COVID-19 pneumonia in the emergency department.", "abstract": "The coronavirus disease 2019 pandemic has inspired new innovations in diagnosing, treating, and dispositioning patients during high census conditions with constrained resources. Our objective is to describe first experiences of physician interaction with a novel artificial intelligence (AI) algorithm designed to enhance physician abilities to identify ground-glass opacities and consolidation on chest radiographs.\nDuring the first wave of the pandemic, we deployed a previously developed and validated deep-learning AI algorithm for assisted interpretation of chest radiographs for use by physicians at an academic health system in Southern California. The algorithm overlays radiographs with \"heat\" maps that indicate pneumonia probability alongside standard chest radiographs at the point of care. Physicians were surveyed in real time regarding ease of use and impact on clinical decisionmaking.\nOf the 5125 total visits and 1960 chest radiographs obtained in the emergency department (ED) during the study period, 1855 were analyzed by the algorithm. Among these, emergency physicians were surveyed for their experiences on 202 radiographs. Overall, 86% either strongly agreed or somewhat agreed that the intervention was easy to use in their workflow. Of the respondents, 20% reported that the algorithm impacted clinical decisionmaking.\nTo our knowledge, this is the first published literature evaluating the impact of medical imaging AI on clinical decisionmaking in the emergency department setting. Urgent deployment of a previously validated AI algorithm clinically was easy to use and was found to have an impact on clinical decision making during the predicted surge period of a global pandemic.", "journal": "Journal of the American College of Emergency Physicians open", "date": "2021-01-05", "authors": ["MorganCarlile", "BrianHurt", "AlbertHsiao", "MichaelHogarth", "Christopher ALonghurst", "ChristianDameff"], "doi": "10.1002/emp2.12297"}
{"title": "Integration of cardiovascular risk assessment with COVID-19 using artificial intelligence.", "abstract": "Artificial Intelligence (AI), in general, refers to the machines (or computers) that mimic \"cognitive\" functions that we associate with our mind, such as \"learning\" and \"solving problem\". New biomarkers derived from medical imaging are being discovered and are then fused with non-imaging biomarkers (such as office, laboratory, physiological, genetic, epidemiological, and clinical-based biomarkers) in a big data framework, to develop AI systems. These systems can support risk prediction and monitoring. This perspective narrative shows the powerful methods of AI for tracking cardiovascular risks. We conclude that AI could potentially become an integral part of the COVID-19 disease management system. Countries, large and small, should join hands with the WHO in building biobanks for scientists around the world to build AI-based platforms for tracking the cardiovascular risk assessment during COVID-19 times and long-term follow-up of the survivors.", "journal": "Reviews in cardiovascular medicine", "date": "2021-01-04", "authors": ["Jasjit SSuri", "AnudeepPuvvula", "MishaMajhail", "MainakBiswas", "Ankush DJamthikar", "LucaSaba", "GavinoFaa", "Inder MSingh", "RonaldOberleitner", "MonikaTurk", "SaurabhSrivastava", "Paramjit SChadha", "Harman SSuri", "Amer MJohri", "VijayNambi", "J MiguelSanches", "Narendra NKhanna", "KlaudijaViskovic", "SophieMavrogeni", "John RLaird", "ArindamBit", "GyanPareek", "MartinMiner", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "AthanasiosProtogerou", "Durga PrasannaMisra", "VikasAgarwal", "George DKitas", "RaghuKolluri", "JagjitTeji", "MichelePorcu", "MustafaAl-Maini", "AnnAgbakoba", "MeyypanSockalingam", "AjitSexena", "AndrewNicolaides", "AdityaSharma", "VijayRathore", "VijayViswanathan", "SubbaramNaidu", "Deepak LBhatt"], "doi": "10.31083/j.rcm.2020.04.236"}
{"title": "A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images.", "abstract": "Corona virus disease (COVID-19) acknowledged as a pandemic by the WHO and mankind all over the world is vulnerable to this virus. Alternative tools are needed that can help in diagnosis of the coronavirus. Researchers of this article investigated the potential of machine learning methods for automatic diagnosis of corona virus with high accuracy from X-ray images. Two most commonly used classifiers were selected: logistic\u00a0regression (LR) and convolutional neural networks (CNN). The main reason was to make the system fast and efficient. Moreover, a dimensionality reduction approach was also investigated based on principal component analysis (PCA) to further speed up the learning process and improve the classification accuracy by selecting the highly discriminate features. The deep learning-based methods demand large amount of training samples compared to conventional approaches, yet adequate amount of labelled training samples was not available for COVID-19 X-ray images. Therefore, data augmentation technique using generative adversarial network (GAN) was employed to further increase the training samples and reduce the overfitting problem. We used the online available dataset and incorporated GAN to have 500 X-ray images in total for this study. Both CNN and LR showed encouraging results for COVID-19 patient identification. The LR and CNN models showed 95.2-97.6% overall accuracy without PCA and 97.6-100% with PCA for positive cases identification, respectively.", "journal": "Interdisciplinary sciences, computational life sciences", "date": "2021-01-03", "authors": ["JawadRasheed", "Alaa AliHameed", "ChawkiDjeddi", "AkhtarJamil", "FadiAl-Turjman"], "doi": "10.1007/s12539-020-00403-6\n10.1016/S0140-6736(66)92364-6\n10.1097/01.inf.0000188166.17324.60\n10.1007/s00038-020-01390-7\n10.1007/s10916-020-01585-8\n10.1186/s12916-020-01533-w\n10.3389/fdgth.2020.00008\n10.1109/UBMYK48245.2019.8965556\n10.1109/TSP.2019.8769040\n10.1109/access.2019.2928975\n10.1016/j.media.2016.06.032\n10.1016/j.patcog.2017.05.025\n10.1109/EBBT.2019.8741582\n10.1109/RIVF.2019.8713648\n10.1109/TMI.2016.2526687\n10.1109/ACCESS.2018.2831280\n10.1109/TMI.2016.2528162\n10.1007/978-981-13-8300-7_8\n10.1007/s11263-015-0816-y\n10.1109/TMI.2018.2881415\n10.1186/s40537-019-0197-0\n10.1109/DICTA.2018.8615771\n10.1109/TMI.2017.2743464\n10.1109/TMI.2017.2760978\n10.1109/TMI.2016.2528120\n10.1109/TMI.2016.2538465\n10.1109/TII.2019.2891738\n10.1109/JBHI.2017.2787595\n10.1002/wics.101\n10.1007/BF02293599\n10.1109/ICEngTechnol.2017.8308186\n10.1080/00220670209598786\n10.1016/j.compbiomed.2020.103805\n10.1016/j.chemolab.2020.104054\n10.1007/s13246-020-00865-4\n10.1016/j.cmpb.2020.105608\n10.1080/07391102.2020.1788642\n10.1007/s11356-020-10133-3\n10.1016/j.mehy.2020.109761\n10.2139/ssrn.3557984\n10.1016/j.chaos.2020.110122\n10.1016/j.chaos.2020.110071"}
{"title": "Microscopy-based assay for semi-quantitative detection of SARS-CoV-2 specific antibodies in human sera: A semi-quantitative, high throughput, microscopy-based assay expands existing approaches to measure SARS-CoV-2 specific antibody levels in human sera.", "abstract": "Emergence of the novel pathogenic coronavirus SARS-CoV-2 and its rapid pandemic spread presents challenges that demand immediate attention. Here, we describe the development of a semi-quantitative high-content microscopy-based assay for detection of three major classes (IgG, IgA, and IgM) of SARS-CoV-2 specific antibodies in human samples. The possibility to detect antibodies against the entire viral proteome together with a robust semi-automated image analysis workflow resulted in specific, sensitive and unbiased assay that complements the portfolio of SARS-CoV-2 serological assays. Sensitive, specific and quantitative serological assays are urgently needed for a better understanding of humoral immune response against the virus as a basis for developing public health strategies to control viral spread. The procedure described here has been used for clinical studies and provides a general framework for the application of quantitative high-throughput microscopy to rapidly develop serological assays for emerging virus infections.", "journal": "BioEssays : news and reviews in molecular, cellular and developmental biology", "date": "2020-12-31", "authors": ["ConstantinPape", "RomanRemme", "AdrianWolny", "SylviaOlberg", "SteffenWolf", "LorenzoCerrone", "MirkoCortese", "SeverinaKlaus", "BojanaLucic", "StephanieUllrich", "MariaAnders-\u00d6sswein", "StefanieWolf", "BeratiCerikan", "Christopher JNeufeldt", "MarkusGanter", "PaulSchnitzler", "UtaMerle", "MarinaLusic", "SteeveBoulant", "MeganStanifer", "RalfBartenschlager", "Fred AHamprecht", "AnnaKreshuk", "ChristianTischer", "Hans-GeorgKr\u00e4usslich", "BarbaraM\u00fcller", "ViborLaketa"], "doi": "10.1002/bies.202000257\n10.2139/ssrn.3668418"}
{"title": "The usage of deep neural network improves distinguishing COVID-19 from other suspected viral pneumonia by clinicians on chest CT: a real-world study.", "abstract": "Based on the current clinical routine, we aimed to develop a novel deep learning model to distinguish coronavirus disease 2019 (COVID-19) pneumonia from other types of pneumonia and validate it with a real-world dataset (RWD).\nA total of 563 chest CT scans of 380 patients (227/380 were diagnosed with COVID-19 pneumonia) from 5 hospitals were collected to train our deep learning (DL) model. Lung regions were extracted by U-net, then transformed and fed to pre-trained ResNet-50-based IDANNet (Identification and Analysis of New covid-19 Net) to produce a diagnostic probability. Fivefold cross-validation was employed to validate the application of our model. Another 318 scans of 316 patients (243/316 were diagnosed with COVID-19 pneumonia) from 2 other hospitals were enrolled prospectively as the RWDs to testify our DL model's performance and compared it with that from 3 experienced radiologists.\nA three-dimensional DL model was successfully established. The diagnostic threshold to differentiate COVID-19 and non-COVID-19 pneumonia was 0.685 with an AUC of 0.906 (95% CI: 0.886-0.913) in the internal validation group. In the RWD cohort, our model achieved an AUC of 0.868 (95% CI: 0.851-0.876) with the sensitivity of 0.811 and the specificity of 0.822, non-inferior to the performance of 3 experienced radiologists, suggesting promising clinical practical usage.\nThe established DL model was able to achieve accurate identification of COVID-19 pneumonia from other suspected ones in the real-world situation, which could become a reliable tool in clinical routine.\n\u2022 In an internal validation set, our DL model achieved the best performance to differentiate COVID-19 from non-COVID-19 pneumonia with a sensitivity of 0.836, a specificity of 0.800, and an AUC of 0.906 (95% CI: 0.886-0.913) when the threshold was set at 0.685. \u2022 In the prospective RWD cohort, our DL diagnostic model achieved a sensitivity of 0.811, a specificity of 0.822, and AUC of 0.868 (95% CI: 0.851-0.876), non-inferior to the performance of 3 experienced radiologists. \u2022 The attention heatmaps were fully generated by the model without additional manual annotation and the attention regions were highly aligned with the ROIs acquired by human radiologists for diagnosis.", "journal": "European radiology", "date": "2020-12-30", "authors": ["QiuchenXie", "YipingLu", "XianchengXie", "NanMei", "YunXiong", "XuanxuanLi", "YangyongZhu", "AnlingXiao", "BoYin"], "doi": "10.1007/s00330-020-07553-7\n10.1056/NEJMoa2002032\n10.1002/jmv.25689\n10.1002/jmv.25681\n10.1016/j.ijid.2020.03.071\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200370\n10.3390/s20041214\n10.1109/JBHI.2018.2841992\n10.1007/s10278-019-00254-8\n10.1001/jama.2016.17216\n10.1007/s10096-020-03901-z\n10.1136/bmj.m689\n10.1056/NEJMsb1609216\n10.14236/jhi.v22i3.177"}
{"title": "Accelerating bioinformatics research with International Conference on Intelligent Biology and Medicine 2020.", "abstract": "The International Association for Intelligent Biology and Medicine (IAIBM) is a nonprofit organization that promotes intelligent biology and medical science. It hosts an annual International Conference on Intelligent Biology and Medicine (ICIBM), which was initially established in 2012. Due to the coronavirus (COVID-19) pandemic, the ICIBM 2020 was held for the first time as a virtual online conference on August 9 to 10. The virtual conference had\u2009~\u2009300 registered participants and featured 41 online real-time presentations. ICIBM 2020 received a total of 75 manuscript submissions, and 12 were selected to be published in this special issue of BMC Bioinformatics. These 12 manuscripts cover a wide range of bioinformatics topics including network analysis, imaging analysis, machine learning, gene expression analysis, and sequence analysis.", "journal": "BMC bioinformatics", "date": "2020-12-30", "authors": ["YanGuo", "LiShen", "XinghuaShi", "KaiWang", "YulinDai", "ZhongmingZhao"], "doi": "10.1186/s12859-020-03890-y\n10.1186/s12859-020-03877-9\n10.1186/s12859-020-03849-z\n10.1186/s12859-020-03857-z\n10.1186/s12859-020-03885-9\n10.1186/s12859-020-03848-0\n10.1186/s12859-020-03876-w"}
{"title": "Automatic COVID-19 CT segmentation using U-Net integrated spatial and channel attention mechanism.", "abstract": "The coronavirus disease (COVID-19) pandemic has led to a devastating effect on the global public health. Computed Tomography (CT) is an effective tool in the screening of COVID-19. It is of great importance to rapidly and accurately segment COVID-19 from CT to help diagnostic and patient monitoring. In this paper, we propose a U-Net based segmentation network using attention mechanism. As not all the features extracted from the encoders are useful for segmentation, we propose to incorporate an attention mechanism including a spatial attention module and a channel attention module, to a U-Net architecture to re-weight the feature representation spatially and channel-wise to capture rich contextual relationships for better feature representation. In addition, the focal Tversky loss is introduced to deal with small lesion segmentation. The experiment results, evaluated on a COVID-19 CT segmentation dataset where 473 CT slices are available, demonstrate the proposed method can achieve an accurate and rapid segmentation result on COVID-19. The method takes only 0.29\u2009second to segment a single CT slice. The obtained Dice Score and Hausdorff Distance are 83.1% and 18.8, respectively.", "journal": "International journal of imaging systems and technology", "date": "2020-12-29", "authors": ["TongxueZhou", "St\u00e9phaneCanu", "SuRuan"], "doi": "10.1002/ima.22527"}
{"title": "DeepTracer for fast de novo cryo-EM protein structure modeling and special studies on CoV-related complexes.", "abstract": "Information about macromolecular structure of protein complexes and related cellular and molecular mechanisms can assist the search for vaccines and drug development processes. To obtain such structural information, we present DeepTracer, a fully automated deep learning-based method for fast de novo multichain protein complex structure determination from high-resolution cryoelectron microscopy (cryo-EM) maps. We applied DeepTracer on a previously published set of 476 raw experimental cryo-EM maps and compared the results with a current state of the art method. The residue coverage increased by over 30% using DeepTracer, and the rmsd value improved from 1.29 \u00c5 to 1.18 \u00c5. Additionally, we applied DeepTracer on a set of 62 coronavirus-related cryo-EM maps, among them 10 with no deposited structure available in EMDataResource. We observed an average residue match of 84% with the deposited structures and an average rmsd of 0.93 \u00c5. Additional tests with related methods further exemplify DeepTracer's competitive accuracy and efficiency of structure modeling. DeepTracer allows for exceptionally fast computations, making it possible to trace around 60,000 residues in 350 chains within only 2 h. The web service is globally accessible at https://deeptracer.uw.edu.", "journal": "Proceedings of the National Academy of Sciences of the United States of America", "date": "2020-12-29", "authors": ["JonasPfab", "Nhut MinhPhan", "DongSi"], "doi": "10.1073/pnas.2017525118"}
{"title": "Lightweight deep learning models for detecting COVID-19 from chest X-ray images.", "abstract": "Deep learning methods have already enjoyed an unprecedented success in medical imaging problems. Similar success has been evidenced when it comes to the detection of COVID-19 from medical images, therefore deep learning approaches are considered good candidates for detecting this disease, in collaboration with radiologists and/or physicians. In this paper, we propose a new approach to detect COVID-19 via exploiting a conditional generative adversarial network to generate synthetic images for augmenting the limited amount of data available. Additionally, we propose two deep learning models following a lightweight architecture, commensurating with the overall amount of data available. Our experiments focused on both binary classification for COVID-19 vs Normal cases and multi-classification that includes a third class for bacterial pneumonia. Our models achieved a competitive performance compared to other studies in literature and also a ResNet8 model. Our best performing binary model achieved 98.7% accuracy, 100% sensitivity and 98.3% specificity, while our three-class model achieved 98.3% accuracy, 99.3% sensitivity and 98.1% specificity. Moreover, via adopting a testing protocol proposed in literature, our models proved to be more robust and reliable in COVID-19 detection than a baseline ResNet8, making them good candidates for detecting COVID-19 from posteroanterior chest X-ray images.", "journal": "Computers in biology and medicine", "date": "2020-12-29", "authors": ["StefanosKarakanis", "GeorgiosLeontidis"], "doi": "10.1016/j.compbiomed.2020.104181"}
{"title": "Clinical longitudinal evaluation of COVID-19 patients and prediction of organ-specific recovery using artificial intelligence.", "abstract": "Within COVID-19 there is an urgent unmet need to predict at the time of hospital admission which COVID-19 patients will recover from the disease, and how fast they recover to deliver personalized treatments and to properly allocate hospital resources so that healthcare systems do not become overwhelmed. To this end, we have combined clinically salient CT imaging data synergistically with laboratory testing data in an integrative machine learning model to predict organ-specific recovery of patients from COVID-19. We trained and validated our model in 285 patients on each separate major organ system impacted by COVID-19 including the renal, pulmonary, immune, cardiac, and hepatic systems. To greatly enhance the speed and utility of our model, we applied an artificial intelligence method to segment and classify regions on CT imaging, from which interpretable data could be directly fed into the predictive machine learning model for overall recovery. Across all organ systems we achieved validation set area under the receiver operator characteristic curve (AUC) values for organ-specific recovery ranging from 0.80 to 0.89, and significant overall recovery prediction in Kaplan-Meier analyses. This demonstrates that the synergistic use of an artificial intelligence (AI) framework applied to CT lung imaging and a machine learning model that integrates laboratory test data with imaging data can accurately predict the overall recovery of COVID-19 patients from baseline characteristics.", "journal": "Precision clinical medicine", "date": "2020-12-28", "authors": ["Winston TWang", "Charlotte LZhang", "KangWei", "YeSang", "JunShen", "GuangyuWang", "Alexander XLozano"], "doi": "10.1093/pcmedi/pbaa040\n10.1056/NEJMp2000929\n10.1016/S0140-6736(20)30185-9\n10.1056/NEJMc2001272\n10.1016/S0140-6736(20)30154-9\n10.1056/NEJMoa2001017\n10.1007/s00405-020-05965-1\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/S2213-2600(20)30079-5\n10.1148/rg.2020190099\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2020200330\n10.1021/acscentsci.0c00501\n10.1148/radiol.2020200343\n10.1001/jamacardio.2020.3557\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.12603\n10.1001/jama.2020.19719\n10.1016/j.cell.2020.04.045\n10.1016/S1473-3099(20)30086-4\n10.1515/cclm-2020-0620\n10.1177/1753466620937175\n10.1016/j.ajem.2020.05.073\n10.1016/j.ijantimicag.2020.106110\n10.1016/j.virusres.2020.198034\n10.1016/j.transci.2020.102926\n10.1007/s11427-020-1733-4\n10.1155/2019/3761203\n10.1145/3292500.3330701\n10.1016/S0002-9610(00)00564-X\n10.5754/hge11216.\n10.1186/s40425-015-0081-1\n10.1097/00003246-199510000-00007\n10.1001/jama.2016.0287\n10.1053/gast.2003.50016"}
{"title": "Toward data-efficient learning: A benchmark for COVID-19 CT lung and infection segmentation.", "abstract": "Accurate segmentation of lung and infection in COVID-19 computed tomography (CT) scans plays an important role in the quantitative management of patients. Most of the existing studies are based on large and private annotated datasets that are impractical to obtain from a single institution, especially when radiologists are busy fighting the coronavirus disease. Furthermore, it is hard to compare current COVID-19 CT segmentation methods as they are developed on different datasets, trained in different settings, and evaluated with different metrics.\nTo promote the development of data-efficient deep learning methods, in this paper, we built three benchmarks for lung and infection segmentation based on 70 annotated COVID-19 cases, which contain current active research areas, for example, few-shot learning, domain generalization, and knowledge transfer. For a fair comparison among different segmentation methods, we also provide standard training, validation and testing splits, evaluation metrics and, the corresponding code.\nBased on the state-of-the-art network, we provide more than 40 pretrained baseline models, which not only serve as out-of-the-box segmentation tools but also save computational time for researchers who are interested in COVID-19 lung and infection segmentation. We achieve average dice similarity coefficient (DSC) scores of 97.3%, 97.7%, and 67.3% and average normalized surface dice (NSD) scores of 90.6%, 91.4%, and 70.0% for left lung, right lung, and infection, respectively.\nTo the best of our knowledge, this work presents the first data-efficient learning benchmark for medical image segmentation, and the largest number of pretrained models up to now. All these resources are publicly available, and our work lays the foundation for promoting the development of deep learning methods for efficient COVID-19 CT segmentation with limited data.", "journal": "Medical physics", "date": "2020-12-24", "authors": ["JunMa", "YixinWang", "XingleAn", "ChengGe", "ZiqiYu", "JiananChen", "QiongjieZhu", "GuoqiangDong", "JianHe", "ZhiqiangHe", "TianjiaCao", "YuntaoZhu", "ZiweiNie", "XiaopingYang"], "doi": "10.1002/mp.14676"}
{"title": "A Lightweight Internet Sharing Scheme for Sectional Medical Images according to Existing Hospital Network Facilities and Basic Information Security Rules.", "abstract": "With the outbreak of COVID-19, large-scale telemedicine applications can play an important role in the epidemic areas or less developed areas. However, the transmission of hundreds of megabytes of Sectional Medical Images (SMIs) from hospital's Intranet to the Internet has the problems of efficiency, cost, and security. This article proposes a novel lightweight sharing scheme for permitting Internet users to quickly and safely access the SMIs from a hospital using an Internet computer anywhere but without relying on a virtual private network or another complex deployment.\nA four-level endpoint network penetration scheme based on the existing hospital network facilities and information security rules was proposed to realize the secure and lightweight sharing of SMIs over the Internet. A \"Master-Slave\" interaction to the interactive characteristics of multiplanar reconstruction and maximum/minimum/average intensity projection was designed to enhance the user experience. Finally, a prototype system was established.\nWhen accessing SMIs with a data size ranging from 251.6 to 307.04\u2009MB with 200 kBps client bandwidth (extreme test), the network response time to each interactive request remained at approximately 1\u2009s, the original SMIs were kept in the hospital, and the deployment did not require a complex process; the imaging quality and interactive experience were recognized by radiologists.\nThis solution could serve Internet medicine at a low cost and may promote the diversified development of mobile medical technology. Under the current COVID-19 epidemic situation, we expect that it could play a low-cost and high-efficiency role in remote emergency support.", "journal": "Journal of healthcare engineering", "date": "2020-12-24", "authors": ["LiangQiao", "HaoWu", "YiWu", "WenjingWu", "JingyiYang", "YongjianNian", "MingshengChen", "SenBai", "HaoHuang", "MingguoQiu"], "doi": "10.1155/2020/8838390\n10.1089/tmj.2020.0084\n10.1155/2012/713739\n10.4103/0971-3026.63043\n10.1007/s00330-006-0153-1\n10.1155/2017/4074137\n10.1016/j.cag.2014.02.002\n10.1016/j.procs.2014.11.041\n10.1016/j.micpro.2014.06.005\n10.1016/j.ijmedinf.2018.09.004\n10.1155/2019/7516035\n10.1007/s11042-015-2481-0\n10.1016/j.cmpb.2007.11.012\n10.1016/j.ijinfomgt.2013.11.005\n10.1016/j.jbi.2019.103188\n10.1016/j.datak.2012.09.004\n10.1007/s10916-014-0044-y"}
{"title": "Lung ultrasound education: simulation and hands-on.", "abstract": "COVID-19 can cause damage to the lung, which can result in progressive respiratory failure and potential death. Chest radiography and CT are the imaging tools used to diagnose and monitor patients with COVID-19. Lung ultrasound (LUS) during COVID-19 is being used in some areas to aid decision-making and improve patient care. However, its increased use could help improve existing practice for patients with suspected COVID-19, or other lung disease. A limitation of LUS is that it requires practitioners with sufficient competence to ensure timely, safe, and diagnostic clinical/imaging assessments. This commentary discusses the role and governance of LUS during and beyond the COVID-19 pandemic, and how increased education and training in this discipline can be undertaken given the restrictions in imaging highly infectious patients. The use of simulation, although numerical methods or dedicated scan trainers, and machine learning algorithms could further improve the accuracy of LUS, whilst helping to reduce its learning curve for greater uptake in clinical practice.", "journal": "The British journal of radiology", "date": "2020-12-24", "authors": ["StephenWolstenhulme", "James RossMcLaughlan"], "doi": "10.1259/bjr.20200755\n10.1186/s13054-020-2828-4\n10.1007/s00134-012-2513-4\n10.1093/ehjci/jeaa163\n10.1111/anae.15082\n10.1093/ndt/gfw329\n10.1177/1751143720914216\n10.1001/jama.2011.1234\n10.1016/j.chest.2016.09.033\n10.1371/journal.pone.0057687\n10.1121/1.4712021\n10.1109/JBHI.2019.2936151"}
{"title": "Artificial Intelligence of COVID-19 Imaging: A Hammer in Search of a Nail.", "abstract": null, "journal": "Radiology", "date": "2020-12-23", "authors": ["Ronald MSummers"], "doi": "10.1148/radiol.2020204226"}
{"title": "IoMT-Based Automated Detection and Classification of Leukemia Using Deep Learning.", "abstract": "For the last few years, computer-aided diagnosis (CAD) has been increasing rapidly. Numerous machine learning algorithms have been developed to identify different diseases, e.g., leukemia. Leukemia is a white blood cells- (WBC-) related illness affecting the bone marrow and/or blood. A quick, safe, and accurate early-stage diagnosis of leukemia plays a key role in curing and saving patients' lives. Based on developments, leukemia consists of two primary forms, i.e., acute and chronic leukemia. Each form can be subcategorized as myeloid and lymphoid. There are, therefore, four leukemia subtypes. Various approaches have been developed to identify leukemia with respect to its subtypes. However, in terms of effectiveness, learning process, and performance, these methods require improvements. This study provides an Internet of Medical Things- (IoMT-) based framework to enhance and provide a quick and safe identification of leukemia. In the proposed IoMT system, with the help of cloud computing, clinical gadgets are linked to network resources. The system allows real-time coordination for testing, diagnosis, and treatment of leukemia among patients and healthcare professionals, which may save both time and efforts of patients and clinicians. Moreover, the presented framework is also helpful for resolving the problems of patients with critical condition in pandemics such as COVID-19. The methods used for the identification of leukemia subtypes in the suggested framework are Dense Convolutional Neural Network (DenseNet-121) and Residual Convolutional Neural Network (ResNet-34). Two publicly available datasets for leukemia, i.e., ALL-IDB and ASH image bank, are used in this study. The results demonstrated that the suggested models supersede the other well-known machine learning algorithms used for healthy-versus-leukemia-subtypes identification.", "journal": "Journal of healthcare engineering", "date": "2020-12-22", "authors": ["NighatBibi", "MisbaSikandar", "IkramUd Din", "AhmadAlmogren", "SikandarAli"], "doi": "10.1155/2020/6648574\n10.1109/access.2020.3006040\n10.1109/access.2020.2968948\n10.1016/j.jisa.2020.102615\n10.1109/access.2020.3030192\n10.3390/s20092468\n10.3390/su12083088\n10.1109/access.2020.2985851\n10.3390/s20072081\n10.1109/access.2017.2757844\n10.1016/j.future.2018.07.050\n10.1016/j.future.2019.04.017\n10.1016/j.future.2020.02.054\n10.1016/j.future.2020.03.054\n10.1016/j.future.2019.05.059\n10.3390/electronics9071172\n10.1016/j.future.2019.01.033\n10.1109/access.2019.2960633\n10.1016/j.patrec.2019.03.022\n10.1109/access.2019.2963797\n10.1038/nature14539\n10.1007/978-981-10-3773-3_64\n10.1016/j.bspc.2018.08.012\n10.1016/j.engappai.2018.04.024\n10.3390/diagnostics9030104\n10.1016/j.patrec.2019.03.024\n10.1016/j.cmpb.2019.104987\n10.7763/ijcte.2018.v10.1198\n10.1007/s10278-018-0074-y\n10.20532/cit.2018.1004123\n10.1080/21681163.2016.1234948\n10.1016/j.bbe.2017.07.003\n10.1016/j.aca.2011.12.069"}
{"title": "Diagnosis of COVID-19 for controlling the pandemic: A review of the state-of-the-art.", "abstract": "To date, health organizations and countries around the world are struggling to completely control the spread of the coronavirus disease 2019 (COVID-19). Scientists and researchers are developing tests for the rapid detection of individuals who may carry the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), while striving to find a suitable vaccine to immunize healthy individuals. As there are clinically reported cases of asymptomatic carriers of SARS-CoV-2, fast and accurate diagnosis plays an important role in the control and further prevention of this disease. Herein, we present recent technologies and techniques that have been implemented for the diagnosis of COVID-19. We summarize the methods created by different research institutes as well as the commercial devices and kits developed by companies for the detection of SARS-CoV-2. The description of the existing methods is followed by highlighting their advantages and challenges. Finally, we propose some promising techniques that could potentially be applied to the detection of SARS-CoV-2, and tracing the asymptomatic carriers of COVID-19 rapidly and accurately in the early stages of infection, based on reviewing the research studies on the detection of similar infectious viruses, especially severe acute respiratory syndrome (SARS) coronavirus, and Middle East respiratory syndrome (MERS) coronavirus.", "journal": "Biosensors & bioelectronics", "date": "2020-12-20", "authors": ["NastaranTaleghani", "FariborzTaghipour"], "doi": "10.1016/j.bios.2020.112830\n10.1148/radiol.2020200642\n10.1038/s41591-020-0820-9\n10.1038/nmat2162\n10.1002/rmv.2154\n10.2807/ese.17.40.20290-en\n10.1148/radiol.2020200463\n10.1016/j.aca.2010.09.038\n10.1080/22221751.2020.1719902\n10.1016/S0140-6736(20)30154-9\n10.1002/adfm.201702741\n10.1371/journal.pone.0113234\n10.1148/radiol.2020200230\n10.1016/j.bios.2020.112349\n10.1038/nrmicro.2016.81\n10.1098/rsos.190255\n10.1177/0300060520949077\n10.1148/radiol.2020200432\n10.1146/annurev-med-051215-031152\n10.1039/c2lc40630f\n10.1101/2020.04.24.057323\n10.2144/99261rv01\n10.1021/ac201950g\n10.1038/nbt1269\n10.1126/science.aam9321\n10.1101/2020.02.07.937862\n10.1038/s41586-020-2286-9\n10.1126/science.1087139\n10.1056/NEJMoa2002032\n10.1038/s41551-019-0371-x\n10.26719/2013.19.supp1.s12\n10.1056/NEJMoa2001191\n10.3389/fmicb.2018.01101\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200330\n10.1038/nature03712\n10.1021/acs.chemrev.8b00738\n10.1038/s41467-020-18685-1\n10.1101/2020.05.07.20055947\n10.1128/JCM.41.4.1548-1557.2003\n10.1021/jm9804836\n10.1038/s41596-019-0210-2\n10.1002/advs.201900471\n10.1101/2020.09.01.20182220\n10.1038/nm1267\n10.20944/preprints202003.0440.v1\n10.1016/j.bios.2006.06.030\n10.1016/j.rbmo.2020.06.001\n10.1073/pnas.0506735102\n10.1038/s41564-020-0688-y\n10.1146/annurev-virology-110615-042301\n10.1111/tbed.13620\n10.1126/science.1116480\n10.1111/j.1574-695X.2006.00045.x\n10.1016/S0140-6736(20)30251-8\n10.1016/j.celrep.2020.107725\n10.1021/acs.est.0c01174\n10.1016/j.jiph.2020.06.005\n10.3201/eid1911.131172\n10.1016/j.bios.2020.112435\n10.1186/s12985-016-0544-0\n10.1016/j.cell.2020.04.004\n10.1006/bbrc.2001.5921\n10.1109/NEMS.2015.7147439\n10.1016/j.bios.2018.05.050\n10.7759/cureus.9942\n10.1016/S1473-3099(20)30113-4\n10.1128/JCM.42.1.257-263.2004\n10.1016/S1473-3099(20)30517-X\n10.1056/NEJMe2001126\n10.20964/2017.01.44\n10.1021/acsnano.0c02439\n10.1021/acsphotonics.0c01245\n10.1073/pnas.1812296116\n10.1016/S1473-3099(13)70164-6\n10.1016/j.bios.2020.112765\n10.1097/RTI.0000000000000533\n10.1021/acsnano.0c02823\n10.1073/pnas.2003138117\n10.1016/j.bios.2015.10.041\n10.1016/j.bios.2016.07.110\n10.1021/acsnano.0c02624\n10.1109/MEMSYS.2012.6170346\n10.1093/bjaceaccp/mku063\n10.1101/2020.01.23.20018549V2\n10.1101/2020.03.19.20034447\n10.1101/2020.02.27.20028787\n10.1148/radiol.2020200343\n10.1002/ctm2.158\n10.1038/nature05064\n10.1021/acs.analchem.7b02257\n10.1101/2020.02.11.20021493\n10.1056/NEJMoa1211721\n10.1080/22221751.2020.1729071\n10.1002/elps.200305966\n10.1038/s41586-020-2012-7\n10.1016/j.bios.2020.112437\n10.1021/ac035367b"}
{"title": "Diagnostic accuracy of infrared thermal imaging for detecting COVID-19 infection in minimally symptomatic patients.", "abstract": "Despite being widely used as a screening tool, a rigorous scientific evaluation of infrared thermography for the diagnosis of minimally symptomatic patients suspected of having COVID-19 infection has not been performed.\nA consecutive sample of 60 adult individuals with a history of close contact with COVID-19 infected individuals and mild respiratory symptoms for less than 7\u00a0days and 20 confirmed COVID-19 negative healthy volunteers were enrolled in the study. Infrared thermograms of the face were obtained with a mobile camera, and RT-PCR was used as the reference standard test to diagnose COVID-19 infection. Temperature values and distribution of the face of healthy volunteers and patients with and without COVID-19 infection were then compared.\nThirty-four patients had an RT-PCR confirmed diagnosis of COVID-19 and 26 had negative test results. The temperature asymmetry between the lacrimal caruncles and the forehead was significantly higher in COVID-19 positive individuals. Through a random forest analysis, a cut-off value of 0.55\u00b0C was found to discriminate with an 82% accuracy between patients with and without COVID-19 confirmed infection.\nAmong adults with a history of COVID-19 exposure and mild respiratory symptoms, a temperature asymmetry of\u00a0\u2265\u00a00.55\u00b0C between the lacrimal caruncle and the forehead is highly suggestive of COVID-19 infection. This finding questions the widespread use of the measurement of absolute temperature values of the forehead as a COVID-19 screening tool.", "journal": "European journal of clinical investigation", "date": "2020-12-19", "authors": ["Mario AMartinez-Jimenez", "Victor MLoza-Gonzalez", "E SamuelKolosovas-Machuca", "Mercedes EYanes-Lane", "Ana SofiaRamirez-GarciaLuna", "Jose LRamirez-GarciaLuna"], "doi": "10.1111/eci.13474\n10.1001/jama.2020.2648\n10.1016/S2213-2600(20)30076-X\n10.1201/9780203008997-25\n10.1088/1361-6579/aa4eaf\n10.1155/2016/4372617\n10.1371/journal.pone.0014490\n10.4178/epih/e2014004\n10.1021/acsnano.0c02624\n10.1016/j.infrared.2019.103165\n10.1111/j.1365-2362.2009.02234.x\n10.1016/j.jtherbio.2017.07.006\n10.1111/his.14134\n10.1259/dmfr.20150264\n10.1186/1746-160X-3-17\n10.1117/1.JBO.25.9.097002\n10.3390/ijerph17082906\n10.1016/S0140-6736(20)30374-3\n10.1111/j.1600-0846.2011.00501.x\n10.1259/dmfr/55922484\n10.1159/000495982\n10.17179/excli2018-1735\n10.1186/ar2360\n10.1177/2050313X14561779\n10.1371/journal.pone.0206477\n10.1097/PRS.0b013e3182a806f0\n10.1177/2292550317740688\n10.1088/0967-3334/33/3/R33\n10.1128/MMBR.00058-12\n10.1136/bjo.2003.035931\n10.3201/eid1108.050110\n10.1080/09273948.2020.1738501\n10.2174/1874364101711010122\n10.1136/bmjophth-2020-000563"}
{"title": "Analysis of Stroke Detection during the COVID-19 Pandemic Using Natural Language Processing of Radiology Reports.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic has led to decreases in neuroimaging volume. Our aim was to quantify the change in acute or subacute ischemic strokes detected on CT or MR imaging during the pandemic using natural language processing of radiology reports.\nWe retrospectively analyzed 32,555 radiology reports from brain CTs and MRIs from a comprehensive stroke center, performed from March 1 to April 30 each year from 2017 to 2020, involving 20,414 unique patients. To detect acute or subacute ischemic stroke in free-text reports, we trained a random forest natural language processing classifier using 1987 randomly sampled radiology reports with manual annotation. Natural language processing classifier generalizability was evaluated using 1974 imaging reports from an external dataset.\nThe natural language processing classifier achieved a 5-fold cross-validation classification accuracy of 0.97 and an F1 score of 0.74, with a slight underestimation (-5%) of actual numbers of acute or subacute ischemic strokes in cross-validation. Importantly, cross-validation performance stratified by year was similar. Applying the classifier to the complete study cohort, we found an estimated 24% decrease in patients with acute or subacute ischemic strokes reported on CT or MR imaging from March to April 2020 compared with the average from those months in 2017-2019. Among patients with stroke-related order indications, the estimated proportion who underwent neuroimaging with acute or subacute ischemic stroke detection significantly increased from 16% during 2017-2019 to 21% in 2020 (\nAcute or subacute ischemic stroke cases detected by neuroimaging decreased during the COVID-19 pandemic, though a higher proportion of studies ordered for stroke were positive for acute or subacute ischemic strokes. Natural language processing approaches can help automatically track acute or subacute ischemic stroke numbers for epidemiologic studies, though local classifier training is important due to radiologist reporting style differences.", "journal": "AJNR. American journal of neuroradiology", "date": "2020-12-19", "authors": ["M DLi", "MLang", "FDeng", "KChang", "KBuch", "SRincon", "W AMehan", "T MLeslie-Mazwi", "JKalpathy-Cramer"], "doi": "10.3174/ajnr.A6961\n10.1212/WNL.0000000000009713\n10.1177/1747493020923472\n10.3174/ajnr.A6565\n10.1056/NEJMc2014816\n10.1148/radiol.2020201933\n10.1016/j.jns.2020.116923\n10.1148/radiol.16142770\n10.1186/s12911-019-0908-7\n10.1016/j.jstrokecerebrovasdis.2019.02.004\n10.2214/AJR.16.16128\n10.1056/NEJMc2009787\n10.1161/STROKEAHA.120.029701\n10.1016/S2589-7500(20)30186-2\n10.1371/journal.pone.0135834"}
{"title": "Vulnerability of deep neural networks for detecting COVID-19 cases from chest X-ray images to universal adversarial attacks.", "abstract": "Owing the epidemic of the novel coronavirus disease 2019 (COVID-19), chest X-ray computed tomography imaging is being used for effectively screening COVID-19 patients. The development of computer-aided systems based on deep neural networks (DNNs) has become an advanced open source to rapidly and accurately detect COVID-19 cases because the need for expert radiologists, who are limited in number, forms a bottleneck for screening. However, thus far, the vulnerability of DNN-based systems has been poorly evaluated, although realistic and high-risk attacks using universal adversarial perturbation (UAP), a single (input image agnostic) perturbation that can induce DNN failure in most classification tasks, are available. Thus, we focus on representative DNN models for detecting COVID-19 cases from chest X-ray images and evaluate their vulnerability to UAPs. We consider non-targeted UAPs, which cause a task failure, resulting in an input being assigned an incorrect label, and targeted UAPs, which cause the DNN to classify an input into a specific class. The results demonstrate that the models are vulnerable to non-targeted and targeted UAPs, even in the case of small UAPs. In particular, the 2% norm of the UAPs to the average norm of an image in the image dataset achieves >85% and >90% success rates for the non-targeted and targeted attacks, respectively. Owing to the non-targeted UAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The targeted UAPs allow the DNN models to classify most chest X-ray images into a specified target class. The results indicate that careful consideration is required in practical applications of DNNs to COVID-19 diagnosis; in particular, they emphasize the need for strategies to address security concerns. As an example, we show that iterative fine-tuning of DNN models using UAPs improves the robustness of DNN models against UAPs.", "journal": "PloS one", "date": "2020-12-18", "authors": ["HokutoHirano", "KazukiKoga", "KazuhiroTakemoto"], "doi": "10.1371/journal.pone.0243963\n10.1016/S1473-3099(20)30120-1\n10.1016/S0140-6736(20)30183-5\n10.1016/S2468-2667(20)30085-2\n10.1001/jama.2020.1585\n10.1148/radiol.2020200432\n10.1148/ryct.2020200034\n10.1016/j.cell.2020.04.045\n10.1016/j.media.2017.07.005\n10.1016/S2589-7500(19)30123-2\n10.1016/j.cell.2018.02.010\n10.1038/s41598-020-76550-z\n10.1109/TNNLS.2018.2886017\n10.1126/science.aaw4399\n10.3390/a13110268\n10.1038/s42256-019-0048-x\n10.1007/s11263-015-0816-y\n10.1016/j.cose.2019.04.014\n10.1109/ACCESS.2019.2939352"}
{"title": "Analysis of COVID-19 Infections on a CT Image Using DeepSense Model.", "abstract": "In this paper, a data mining model on a hybrid deep learning framework is designed to diagnose the medical conditions of patients infected with the coronavirus disease 2019 (COVID-19) virus. The hybrid deep learning model is designed as a combination of convolutional neural network (CNN) and recurrent neural network (RNN) and named as DeepSense method. It is designed as a series of layers to extract and classify the related features of COVID-19 infections from the lungs. The computerized tomography image is used as an input data, and hence, the classifier is designed to ease the process of classification on learning the multidimensional input data using the Expert Hidden layers. The validation of the model is conducted against the medical image datasets to predict the infections using deep learning classifiers. The results show that the DeepSense classifier offers accuracy in an improved manner than the conventional deep and machine learning classifiers. The proposed method is validated against three different datasets, where the training data are compared with 70%, 80%, and 90% training data. It specifically provides the quality of the diagnostic method adopted for the prediction of COVID-19 infections in a patient.", "journal": "Frontiers in public health", "date": "2020-12-18", "authors": ["AdilKhadidos", "Alaa OKhadidos", "SrihariKannan", "YuvarajNatarajan", "Sachi NandanMohanty", "GeorgiosTsaramirsis"], "doi": "10.3389/fpubh.2020.599550\n10.1016/j.irbm.2020.05.003\n10.1007/s10096-020-03901-z\n10.1016/j.neucom.2018.12.086\n10.1109/TMI.2018.2858202\n10.1109/JBHI.2019.2891049\n10.1109/TMI.2018.2833385\n10.1002/clen.201700162\n10.1109/ACCESS.2019.2929270\n10.1109/TMI.2018.2876510\n10.1109/TMI.2018.2883807\n10.12688/wellcomeopenres.15819.2\n10.1016/j.cmpb.2020.105532\n10.1109/TMI.2020.2995965\n10.1007/s00521-018-3688-6\n10.1016/j.procs.2016.02.042\n10.1007/s00034-019-01041-0\n10.1007/s00521-018-3896-0\n10.1016/j.jclepro.2018.12.096\n10.3390/app9112212\n10.1109/ACCESS.2020.2981337\n10.3390/ijerph17010267\n10.1109/ACCESS.2020.3000322\n10.18201/ijisae.2019252786\n10.1109/ICSEngT.2019.8906408\n10.3390/app9142921\n10.1007/s12553-018-00284-2\n10.1016/j.knosys.2018.08.036"}
{"title": "Prediction of disease progression in patients with COVID-19 by artificial intelligence assisted lesion quantification.", "abstract": "To investigate the value of artificial intelligence (AI) assisted quantification on initial chest CT for prediction of disease progression and clinical outcome in patients with coronavirus disease 2019 (COVID-19). Patients with confirmed COVID-19 infection and initially of non-severe type were retrospectively included. The initial CT scan on admission was used for imaging analysis. The presence of ground glass opacity (GGO), consolidation and other findings were visually evaluated. CT severity score was calculated according to the extent of lesion involvement. In addition, AI based quantification of GGO and consolidation volume were also performed. 123 patients (mean age: 64.43\u2009\u00b1\u200914.02; 62 males) were included. GGO\u2009+\u2009consolidation was more frequently revealed in progress-to-severe group whereas pure GGO was more likely to be found in non-severe group. Compared to non-severe group, patients in progress-to-severe group had larger GGO volume (167.33\u2009\u00b1\u2009167.88\u00a0cm", "journal": "Scientific reports", "date": "2020-12-18", "authors": ["YuehuaLi", "KaiShang", "WeiBian", "LiHe", "YingFan", "TaoRen", "JiayinZhang"], "doi": "10.1038/s41598-020-79097-1\n10.1056/NEJMoa2001316\n10.1056/NEJMoa2002032\n10.1001/jama.2020.6775\n10.1148/radiol.2020200490\n10.1148/radiol.2020200642\n10.2214/AJR.19.21572\n10.1148/radiol.2020201491\n10.1007/s00330-020-07087-y\n10.1016/j.compbiomed.2020.103949\n10.1148/radiol.2462070712\n10.1148/radiol.2303030853\n10.2214/ajr.175.5.1751329\n10.1148/radiology.143.1.7063747\n10.1016/S1473-3099(20)30086-4\n10.1097/RLI.0000000000000672\n10.1093/ajcp/aqaa062\n10.1148/rg.2018170048\n10.7150/thno.45985\n10.1016/j.ejrad.2020.109256\n10.1007/s00330-019-06024-y\n10.1016/j.lungcan.2017.10.015"}
{"title": "MAMA Net: Multi-Scale Attention Memory Autoencoder Network for Anomaly Detection.", "abstract": "Anomaly detection refers to the identification of cases that do not conform to the expected pattern, which takes a key role in diverse research areas and application domains. Most of existing methods can be summarized as anomaly object detection-based and reconstruction error-based techniques. However, due to the bottleneck of defining encompasses of real-world high-diversity outliers and inaccessible inference process, individually, most of them have not derived groundbreaking progress. To deal with those imperfectness, and motivated by memory-based decision-making and visual attention mechanism as a filter to select environmental information in human vision perceptual system, in this paper, we propose a Multi-scale Attention Memory with hash addressing Autoencoder network (MAMA Net) for anomaly detection. First, to overcome a battery of problems result from the restricted stationary receptive field of convolution operator, we coin the multi-scale global spatial attention block which can be straightforwardly plugged into any networks as sampling, upsampling and downsampling function. On account of its efficient features representation ability, networks can achieve competitive results with only several level blocks. Second, it's observed that traditional autoencoder can only learn an ambiguous model that also reconstructs anomalies \"well\" due to lack of constraints in training and inference process. To mitigate this challenge, we design a hash addressing memory module that proves abnormalities to produce higher reconstruction error for classification. In addition, we couple the mean square error (MSE) with Wasserstein loss to improve the encoding data distribution. Experiments on various datasets, including two different COVID-19 datasets and one brain MRI (RIDER) dataset prove the robustness and excellent generalization of the proposed MAMA Net.", "journal": "IEEE transactions on medical imaging", "date": "2020-12-17", "authors": ["YurongChen", "HuiZhang", "YaonanWang", "YiminYang", "XianenZhou", "Q M JonathanWu"], "doi": "10.1109/TMI.2020.3045295\n10.1109/TMI.2020.2992546\n10.1109/TIP.2019.2917862\n10.1109/TMI.2020.2995965\n10.1101/2020.02.14.20023028\n10.1101/2020.03.12.20027185\n10.1109/TIP.2017.2713048"}
{"title": "Novel Deep Learning Technique Used in Management and Discharge of Hospitalized Patients with COVID-19 in China.", "abstract": "The low sensitivity and false-negative results of nucleic acid testing greatly affect its performance in diagnosing and discharging patients with coronavirus disease (COVID-19). Chest computed tomography (CT)-based evaluation of pneumonia may indicate a need for isolation. Therefore, this radiologic modality plays an important role in managing patients with suspected COVID-19. Meanwhile, deep learning (DL) technology has been successful in detecting various imaging features of chest CT. This study applied a novel DL technique to standardize the discharge criteria of COVID-19 patients with consecutive negative respiratory pathogen nucleic acid test results at a \"square cabin\" hospital.\nDL was used to evaluate the chest CT scans of 270 hospitalized COVID-19 patients who had two consecutive negative nucleic acid tests (sampling interval >1 day). The CT scans evaluated were obtained after the patients' second negative test result. The standard criterion determined by DL for patient discharge was a total volume ratio of lesion to lung <50%.\nThe mean number of days between hospitalization and DL was 14.3 (\u00b1 2.4). The average intersection over union was 0.7894. Two hundred and thirteen (78.9%) patients exhibited pneumonia, of whom 54.0% (115/213) had mild interstitial fibrosis. Twenty-one, 33, and 4 cases exhibited vascular enlargement, pleural thickening, and mediastinal lymphadenopathy, respectively. Of the latter, 18.8% (40/213) had a total volume ratio of lesions to lung \u226550% according to our severity scale and were monitored continuously in the hospital. Three cases had a positive follow-up nucleic acid test during hospitalization. None of the 230 discharged cases later tested positive or exhibited pneumonia progression.\nThe novel DL enables the accurate management of hospitalized patients with COVID-19 and can help avoid cluster transmission or exacerbation in patients with false-negative acid test.", "journal": "Therapeutics and clinical risk management", "date": "2020-12-17", "authors": ["QingchengMeng", "WentaoLiu", "PengruiGao", "JiaqiZhang", "AnlanSun", "JiaDing", "HaoLiu", "ZiqiaoLei"], "doi": "10.2147/TCRM.S280726\n10.1056/NEJMe2001126\n10.1056/NEJMoa2001017\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.2214/AJR.20.22954\n10.1016/j.compmedimag.2019.101688\n10.1007/s00330-019-06163-2\n10.1007/s00330-020-07042-x\n10.1109/TPAMI.2016.2577031\n10.1016/j.jtho.2020.02.010\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2462070712\n10.1109/EMBC.2018.8512337\n10.1016/S1473-3099(20)30086-4"}
{"title": "Optimised genetic algorithm-extreme learning machine approach for automatic COVID-19 detection.", "abstract": "The coronavirus disease (COVID-19), is an ongoing global pandemic caused by severe acute respiratory syndrome. Chest Computed Tomography (CT) is an effective method for detecting lung illnesses, including COVID-19. However, the CT scan is expensive and time-consuming. Therefore, this work focus on detecting COVID-19 using chest X-ray images because it is widely available, faster, and cheaper than CT scan. Many machine learning approaches such as Deep Learning, Neural Network, and Support Vector Machine; have used X-ray for detecting the COVID-19. Although the performance of those approaches is acceptable in terms of accuracy, however, they require high computational time and more memory space. Therefore, this work employs an Optimised Genetic Algorithm-Extreme Learning Machine (OGA-ELM) with three selection criteria (i.e., random, K-tournament, and roulette wheel) to detect COVID-19 using X-ray images. The most crucial strength factors of the Extreme Learning Machine (ELM) are: (i) high capability of the ELM in avoiding overfitting; (ii) its usability on binary and multi-type classifiers; and (iii) ELM could work as a kernel-based support vector machine with a structure of a neural network. These advantages make the ELM efficient in achieving an excellent learning performance. ELMs have successfully been applied in many domains, including medical domains such as breast cancer detection, pathological brain detection, and ductal carcinoma in situ detection, but not yet tested on detecting COVID-19. Hence, this work aims to identify the effectiveness of employing OGA-ELM in detecting COVID-19 using chest X-ray images. In order to reduce the dimensionality of a histogram oriented gradient features, we use principal component analysis. The performance of OGA-ELM is evaluated on a benchmark dataset containing 188 chest X-ray images with two classes: a healthy and a COVID-19 infected. The experimental result shows that the OGA-ELM achieves 100.00% accuracy with fast computation time. This demonstrates that OGA-ELM is an efficient method for COVID-19 detecting using chest X-ray images.", "journal": "PloS one", "date": "2020-12-16", "authors": ["Musatafa Abbas AbboodAlbadr", "SabrinaTiun", "MasriAyob", "Fahad TahaAl-Dhief", "KhairuddinOmar", "Faizal AmriHamzah"], "doi": "10.1371/journal.pone.0242899\n10.1001/jama.2020.2565\n10.1016/S0140-6736(20)30360-3\n10.1148/radiol.2020200432\n10.1183/09031936.01.00213501\n10.1007/s13246-020-00865-4\n10.1109/RBME.2020.2987975\n10.1109/TSMCB.2011.2168604\n10.1371/journal.pone.0194770\n10.1109/TNN.2006.875977\n10.1109/TIP.2018.2847035\n10.1364/OL.43.001107\n10.1016/j.neunet.2009.11.009\n10.1016/j.cmpb.2020.105581\n10.1016/j.compbiomed.2020.103792\n10.1109/72.788640\n10.1016/j.asoc.2020.106580\n10.1109/TCYB.2020.2983860"}
{"title": "Chest CT imaging features and severity scores as biomarkers for prognostic prediction in patients with COVID-19.", "abstract": "Coronavirus disease 2019 (COVID-19) has become a pandemic. Few studies have explored the role of chest computed tomography (CT) features and severity scores for prognostic prediction. In this study, we aimed to investigate the role of chest CT severity score and imaging features in the prediction of the prognosis of COVID-19 patients.\nA total of 134 patients (62 recovered and 72 deceased patients) with confirmed COVID-19 were enrolled. The clinical, laboratory, and chest CT (316 scans) data were retrospectively reviewed. Demographics, symptoms, comorbidities, and temporal changes of laboratory results, CT features, and severity scores were compared between recovered and deceased groups using the Mann-Whitney U test and logistic regression to identify the risk factors for poor prognosis.\nMedian age was 48 and 58 years for recovered and deceased patients, respectively. More patients had at least one comorbidity in the deceased group than the recovered group (60% \nSex (male), older age (>60 years), elevated neutrophil, IL-2, IL-6 level, and total CT scores (\u226516) were independent risk factors for poor prognosis in patients with COVID-19. Temporal changes of chest CT features and severity scores could be valuable for early identification of severe cases and eventually reducing the mortality rate of COVID-19.", "journal": "Annals of translational medicine", "date": "2020-12-15", "authors": ["ShuchangZhou", "ChengyangChen", "YiqiHu", "WenzhiLv", "TaoAi", "LimingXia"], "doi": "10.21037/atm-20-3421\n10.4103/jfmpc.jfmpc_424_20\n10.1016/S0140-6736(20)30566-3\n10.1002/rmv.2103\n10.3201/eid2606.200233\n10.1007/s00134-020-05991-x\n10.1097/CM9.0000000000000775\n10.1007/s12032-020-01382-w\n10.1007/s00404-020-05573-8\n10.1371/journal.pone.0230548\n10.1148/radiol.2020200370\n10.1038/s41422-020-0282-0\n10.1001/jama.2020.4940\n10.1016/S2213-2600(20)30079-5\n10.1001/jamainternmed.2020.0994\n10.1016/j.chom.2016.01.012\n10.1136/bmj.m1091\n10.1016/S2213-2600(20)30076-X\n10.1016/j.phrs.2016.03.018\n10.1016/j.jfma.2020.03.010\n10.1186/s12931-020-01440-x\n10.1007/s00330-020-07022-1\n10.1016/S1473-3099(20)30086-4\n10.2214/AJR.20.22975\n10.1148/radiol.2020200463\n10.1097/RLI.0000000000000672\n10.1007/s00330-020-06817-6"}
{"title": "Estimating myelin-water content from anatomical and diffusion images using spatially undersampled myelin-water imaging through machine learning.", "abstract": "Myelin is vital for healthy neuronal development, and can therefore provide valuable information regarding neuronal maturation. Anatomical and diffusion weighted images (DWI) possess information related to the myelin content and the current study investigates whether quantitative myelin markers can be extracted from anatomical and DWI using neural networks. Thirteen volunteers (mean age 29y) are included, and for each subject, a residual neural network was trained using spatially undersampled reference myelin-water markers. The network is trained on a voxel-by-voxel basis, resulting in a large amount of training data for each volunteer. The inputs used are the anatomical contrasts (cT1w, cT2w), the standardized T1w/T2w ratio, estimates of the relaxation times (T1, T2) and their ratio (T1/T2), and common DWI metrics (FA, RD, MD, \u03bb", "journal": "NeuroImage", "date": "2020-12-11", "authors": ["Gerhard SDrenthen", "Walter HBackes", "Jacobus F AJansen"], "doi": "10.1016/j.neuroimage.2020.117626"}
{"title": "Computing SARS-CoV-2 Infection Risk From Symptoms, Imaging, and Test Data: Diagnostic Model Development.", "abstract": "Assigning meaningful probabilities of SARS-CoV-2 infection risk presents a diagnostic challenge across the continuum of care.\nThe aim of this study was to develop and clinically validate an adaptable, personalized diagnostic model to assist clinicians in ruling in and ruling out COVID-19 in potential patients. We compared the diagnostic performance of probabilistic, graphical, and machine learning models against a previously published benchmark model.\nWe integrated patient symptoms and test data using machine learning and Bayesian inference to quantify individual patient risk of SARS-CoV-2 infection. We trained models with 100,000 simulated patient profiles based on 13 symptoms and estimated local prevalence, imaging, and molecular diagnostic performance from published reports. We tested these models with consecutive patients who presented with a COVID-19-compatible illness at the University of California San Diego Medical Center over the course of 14 days starting in March 2020.\nWe included 55 consecutive patients with fever (n=43, 78%) or cough (n=42, 77%) presenting for ambulatory (n=11, 20%) or hospital care (n=44, 80%). In total, 51% (n=28) were female and 49% (n=27) were aged <60 years. Common comorbidities included diabetes (n=12, 22%), hypertension (n=15, 27%), cancer (n=9, 16%), and cardiovascular disease (n=7, 13%). Of these, 69% (n=38) were confirmed via reverse transcription-polymerase chain reaction (RT-PCR) to be positive for SARS-CoV-2 infection, and 20% (n=11) had repeated negative nucleic acid testing and an alternate diagnosis. Bayesian inference network, distance metric learning, and ensemble models discriminated between patients with SARS-CoV-2 infection and alternate diagnoses with sensitivities of 81.6%-84.2%, specificities of 58.8%-70.6%, and accuracies of 61.4%-71.8%. After integrating imaging and laboratory test statistics with the predictions of the Bayesian inference network, changes in diagnostic uncertainty at each step in the simulated clinical evaluation process were highly sensitive to location, symptom, and diagnostic test choices.\nDecision support models that incorporate symptoms and available test results can help providers diagnose SARS-CoV-2 infection in real-world settings.", "journal": "Journal of medical Internet research", "date": "2020-12-11", "authors": ["ChristopherD'Ambrosia", "HenrikChristensen", "EliahAronoff-Spencer"], "doi": "10.2196/24478\n10.1056/nejmc2001801\n10.2807/1560-7917.ES.2020.25.8.2000170\n10.2807/1560-7917.ES.2020.25.8.2000170\n10.1056/nejmp2015897\n10.1148/radiol.2020201160\n10.1148/radiol.2020200642\n10.1056/nejmoa1716614\n10.1016/s0140-6736(15)60263-x\n10.1056/nejmoa1917130\n10.1038/nature21056\n10.1001/jama.2019.16489\n10.1056/NEJMp1205634\n10.1038/s41591-020-0916-2\n10.3390/sym12091526\n10.2196/17550\n10.1016/s1473-3099(20)30120-1\n10.1101/2020.06.25.20140384\n10.1101/2020.03.21.20040303\n10.1056/nejmc2010419\n10.1002/jmv.25884\n10.1101/2020.03.26.20044057\n10.1093/cid/ciaa345\n10.1002/sim.2677"}
{"title": "Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images.", "abstract": "The novel Coronavirus disease (COVID-19), which first appeared at the end of December 2019, continues to spread rapidly in most countries of the world. Respiratory infections occur primarily in the majority of patients treated with COVID-19. In light of the growing number of COVID-19 cases, the need for diagnostic tools to identify COVID-19 infection at early stages is of vital importance. For decades, chest X-ray (CXR) technologies have proven their ability to accurately detect respiratory diseases. More recently, with the availability of COVID-19 CXR scans, deep learning algorithms have played a critical role in the healthcare arena by allowing radiologists to recognize COVID-19 patients from their CXR images. However, the majority of screening methods for COVID-19 reported in recent studies are based on 2D convolutional neural networks (CNNs). Although 3D CNNs are capable of capturing contextual information compared to their 2D counterparts, their use is limited due to their increased computational cost (i.e. requires much extra memory and much more computing power). In this study, a transfer learning-based hybrid 2D/3D CNN architecture for COVID-19 screening using CXRs has been developed. The proposed architecture consists of the incorporation of a pre-trained deep model (VGG16) and a shallow 3D CNN, combined with a depth-wise separable convolution layer and a spatial pyramid pooling module (SPP). Specifically, the depth-wise separable convolution helps to preserve the useful features while reducing the computational burden of the model. The SPP module is designed to extract multi-level representations from intermediate ones. Experimental results show that the proposed framework can achieve reasonable performances when evaluated on a collected dataset (3 classes to be predicted: COVID-19, Pneumonia, and Normal). Notably, it achieved a sensitivity of 98.33%, a specificity of 98.68% and an overall accuracy of 96.91.", "journal": "Physical and engineering sciences in medicine", "date": "2020-12-11", "authors": ["KhaledBayoudh", "Fay\u00e7alHamdaoui", "AbdellatifMtibaa"], "doi": "10.1007/s13246-020-00957-1\n10.1001/jama.2020.2565\n10.1016/j.earlhumdev.2020.105026\n10.1016/j.bios.2020.112455\n10.1148/radiol.2020200463\n10.1148/radiol.2020200642\n10.1148/ryct.2020200196\n10.1148/radiol.2020200432\n10.1148/radiol.2020200823\n10.1007/s00330-018-5810-7\n10.1016/j.clinimag.2020.04.001\n10.1007/s13246-020-00899-8\n10.1148/ryct.2020200034\n10.1093/cid/ciaa247\n10.1007/s42058-020-00031-5\n10.1007/s10489-020-01714-3\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.compbiomed.2020.103805\n10.1007/s13246-020-00888-x\n10.1007/s11042-018-6912-6\n10.1109/TPAMI.2012.59\n10.3390/s20185097\n10.1007/s10489-020-01801-5\n10.1007/s10462-020-09825-6\n10.1038/nature14539azz\n10.1007/s11263-019-01228-7\n10.1186/s40537-019-0197-0\n10.1016/j.cmpb.2020.105581\n10.1016/j.chaos.2020.110122\n10.1016/j.jksuci.2019.09.014"}
{"title": "Exploiting Multiple Optimizers with Transfer Learning Techniques for the Identification of COVID-19 Patients.", "abstract": "Due to the rapid spread of COVID-19 and its induced death worldwide, it is imperative to develop a reliable tool for the early detection of this disease. Chest X-ray is currently accepted to be one of the reliable means for such a detection purpose. However, most of the available methods utilize large training data, and there is a need for improvement in the detection accuracy due to the limited boundary segment of the acquired images for symptom identifications. In this study, a robust and efficient method based on transfer learning techniques is proposed to identify normal and COVID-19 patients by employing small training data. Transfer learning builds accurate models in a timesaving way. First, data augmentation was performed to help the network for memorization of image details. Next, five state-of-the-art transfer learning models, AlexNet, MobileNetv2, ShuffleNet, SqueezeNet, and Xception, with three optimizers, Adam, SGDM, and RMSProp, were implemented at various learning rates, 1e-4, 2e-4, 3e-4, and 4e-4, to reduce the probability of overfitting. All the experiments were performed on publicly available datasets with several analytical measurements attained after execution with a 10-fold cross-validation method. The results suggest that MobileNetv2 with Adam optimizer at a learning rate of 3e-4 provides an average accuracy, recall, precision, and ", "journal": "Journal of healthcare engineering", "date": "2020-12-11", "authors": ["ZemingFan", "MudasirJamil", "Muhammad TariqSadiq", "XiweiHuang", "XiaojunYu"], "doi": "10.1155/2020/8889412\n10.1016/j.idm.2020.02.002\n10.1101/2020.02.27.20028027\n10.2807/1560-7917.es.2020.25.6.2000094\n10.1016/j.eswa.2020.114031\n10.1101/2020.03.12.20027185\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.3390/s20185283\n10.1148/radiol.2020200642\n10.1109/access.2019.2956018\n10.1101/2020.02.14.20023028\n10.1109/tkde.2009.191\n10.1109/icdar.1999.791887"}
{"title": "Hypergraph learning for identification of COVID-19 with CT imaging.", "abstract": "The coronavirus disease, named COVID-19, has become the largest global public health crisis since it started in early 2020. CT imaging has been used as a complementary tool to assist early screening, especially for the rapid identification of COVID-19 cases from community acquired pneumonia (CAP) cases. The main challenge in early screening is how to model the confusing cases in the COVID-19 and CAP groups, with very similar clinical manifestations and imaging features. To tackle this challenge, we propose an Uncertainty Vertex-weighted Hypergraph Learning (UVHL) method to identify COVID-19 from CAP using CT images. In particular, multiple types of features (including regional features and radiomics features) are first extracted from CT image for each case. Then, the relationship among different cases is formulated by a hypergraph structure, with each case represented as a vertex in the hypergraph. The uncertainty of each vertex is further computed with an uncertainty score measurement and used as a weight in the hypergraph. Finally, a learning process of the vertex-weighted hypergraph is used to predict whether a new testing case belongs to COVID-19 or not. Experiments on a large multi-center pneumonia dataset, consisting of 2148 COVID-19 cases and 1182 CAP cases from five hospitals, are conducted to evaluate the prediction accuracy of the proposed method. Results demonstrate the effectiveness and robustness of our proposed method on the identification of COVID-19 in comparison to state-of-the-art methods.", "journal": "Medical image analysis", "date": "2020-12-08", "authors": ["DonglinDi", "FengShi", "FuhuaYan", "LimingXia", "ZhanhaoMo", "ZhongxiangDing", "FeiShan", "BinSong", "ShengruiLi", "YingWei", "YingShao", "MiaofeiHan", "YaozongGao", "HeSui", "YueGao", "DinggangShen"], "doi": "10.1016/j.media.2020.101910"}
{"title": "COVID-AL: The diagnosis of COVID-19 with deep active learning.", "abstract": "The efficient diagnosis of COVID-19 plays a key role in preventing the spread of this disease. The computer-aided diagnosis with deep learning methods can perform automatic detection of COVID-19 using CT scans. However, large scale annotation of CT scans is impossible because of limited time and heavy burden on the healthcare system. To meet the challenge, we propose a weakly-supervised deep active learning framework called COVID-AL to diagnose COVID-19 with CT scans and patient-level labels. The COVID-AL consists of the lung region segmentation with a 2D U-Net and the diagnosis of COVID-19 with a novel hybrid active learning strategy, which simultaneously considers sample diversity and predicted loss. With a tailor-designed 3D residual network, the proposed COVID-AL can diagnose COVID-19 efficiently and it is validated on a large CT scan dataset collected from the CC-CCII. The experimental results demonstrate that the proposed COVID-AL outperforms the state-of-the-art active learning approaches in the diagnosis of COVID-19. With only 30% of the labeled data, the COVID-AL achieves over 95% accuracy of the deep learning method using the whole dataset. The qualitative and quantitative analysis proves the effectiveness and efficiency of the proposed COVID-AL framework.", "journal": "Medical image analysis", "date": "2020-12-08", "authors": ["XingWu", "ChengChen", "MingyuZhong", "JianjiaWang", "JunShi"], "doi": "10.1016/j.media.2020.101913"}
{"title": "Three-Dimensional Analysis of Particle Distribution on Filter Layers inside N95 Respirators by Deep Learning.", "abstract": "The global COVID-19 pandemic has changed many aspects of daily lives. Wearing personal protective equipment, especially respirators (face masks), has become common for both the public and medical professionals, proving to be effective in preventing spread of the virus. Nevertheless, a detailed understanding of respirator filtration-layer internal structures and their physical configurations is lacking. Here, we report three-dimensional (3D) internal analysis of N95 filtration layers via X-ray tomography. Using deep learning methods, we uncover how the distribution and diameters of fibers within these layers directly affect contaminant particle filtration. The average porosity of the filter layers is found to be 89.1%. Contaminants are more efficiently captured by denser fiber regions, with fibers <1.8 \u03bcm in diameter being particularly effective, presumably because of the stronger electric field gradient on smaller diameter fibers. This study provides critical information for further development of N95-type respirators that combine high efficiency with good breathability.", "journal": "Nano letters", "date": "2020-12-08", "authors": ["Hye RyoungLee", "LeiLiao", "WangXiao", "ArturasVailionis", "Antonio JRicco", "RobinWhite", "YoshioNishi", "WahChiu", "StevenChu", "YiCui"], "doi": "10.1021/acs.nanolett.0c04230\n10.1146/annurev-micro-020518-115759\n10.1038/s41586-020-2008-3\n10.1038/s41586-020-2012-7\n10.1056/NEJMc2007800\n10.3390/ijerph17082932\n10.1136/bmj.m3223\n10.1021/acsnano.0c03597\n10.1021/acs.nanolett.0c02211\n10.1021/acsnano.0c03972\n10.1021/acsnano.0c03252\n10.1109/ACCESS.2019.2912200\n10.1038/nature14539\n10.1109/ISBI.2011.5872394\n10.1007/978-3-319-24574-4_28\n10.1038/s41592-018-0261-2\n10.1016/j.media.2014.10.012\n10.1002/pen.760302202\n10.1002/1097-4628(20000829)77:9<1921::AID-APP8>3.0.CO;2-1\n10.1007/s10853-012-6742-2\n10.1109/27.125038\n10.1016/0169-4332(93)90025-7\n10.1002/1098-2329(200024)19:4<312::AID-ADV7>3.0.CO;2-X\n10.1016/j.elstat.2007.05.002\n10.1016/j.compositesa.2011.12.025\n10.1038/nmeth.2089\n10.1017/S143192761800315X\n10.1007/s10853-020-05148-7"}
{"title": "Radiology indispensable for tracking COVID-19.", "abstract": "With the rapid spread of COVID-19 worldwide, early detection and efficient isolation of suspected patients are especially important to prevent the transmission. Although nucleic acid testing of SARS-CoV-2 is still the gold standard for diagnosis, there are well-recognized early-detection problems including time-consuming in the diagnosis process, noticeable false-negative rate in the early stage and lacking nucleic acid testing kits in some areas. Therefore, effective and rational applications of imaging technologies are critical in aiding the screen and helping the diagnosis of suspected patients. Currently, chest computed tomography is recommended as the first-line imaging test for detecting COVID-19 pneumonia, which could allow not only early detection of the typical chest manifestations, but also timely estimation of the disease severity and therapeutic effects. In addition, other radiological methods including chest X-ray, magnetic resonance imaging, and positron emission computed tomography also show significant advantages in the detection of COVID-19 pneumonia. This review summarizes the applications of radiology and nuclear medicine in detecting and diagnosing COVID-19. It highlights the importance for these technologies to curb the rapid transmission during the pandemic, considering findings from special groups such as children and pregnant women.", "journal": "Diagnostic and interventional imaging", "date": "2020-12-08", "authors": ["JingwenLi", "XiLong", "XinyiWang", "FangFang", "XuefeiLv", "DandanZhang", "YuSun", "ShaopingHu", "ZhichengLin", "NianXiong"], "doi": "10.1016/j.diii.2020.11.008\n10.1148/radiol.2020202568\n10.1016/j.acra.2020.10.002\n10.1016/j.jinf.2020.09.027\n10.1007/s00330-020-07226-5\n10.1016/j.jrid.2020.04.003"}
{"title": "COVID-19 CT Image Synthesis With a Conditional Generative Adversarial Network.", "abstract": "Coronavirus disease 2019 (COVID-19) is an ongoing global pandemic that has spread rapidly since December 2019. Real-time reverse transcription polymerase chain reaction (rRT-PCR) and chest computed tomography (CT) imaging both play an important role in COVID-19 diagnosis. Chest CT imaging offers the benefits of quick reporting, a low cost, and high sensitivity for the detection of pulmonary infection. Recently, deep-learning-based computer vision methods have demonstrated great promise for use in medical imaging applications, including X-rays, magnetic resonance imaging, and CT imaging. However, training a deep-learning model requires large volumes of data, and medical staff faces a high risk when collecting COVID-19 CT data due to the high infectivity of the disease. Another issue is the lack of experts available for data labeling. In order to meet the data requirements for COVID-19 CT imaging, we propose a CT image synthesis approach based on a conditional generative adversarial network that can effectively generate high-quality and realistic COVID-19 CT images for use in deep-learning-based medical imaging tasks. Experimental results show that the proposed method outperforms other state-of-the-art image synthesis methods with the generated COVID-19 CT images and indicates promising for various machine learning applications including semantic segmentation and classification.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-12-05", "authors": ["YifanJiang", "HanChen", "MurrayLoew", "HanseokKo"], "doi": "10.1109/JBHI.2020.3042523"}
{"title": "StackNet-DenVIS: a multi-layer perceptron stacked ensembling approach for COVID-19 detection using X-ray images.", "abstract": "The highly contagious nature of Coronavirus disease 2019 (Covid-19) resulted in a global pandemic. Due to the relatively slow and taxing nature of conventional testing for Covid-19, a faster method needs to be in place. The current researches have suggested that visible irregularities found in the chest X-ray of Covid-19 positive patients are indicative of the presence of the disease. Hence, Deep Learning and Image Classification techniques can be employed to learn from these irregularities, and classify accordingly with high accuracy. This research presents an approach to create a classifier model named StackNet-DenVIS which is designed to act as a screening process before conducting the existing swab tests. Using a novel approach, which incorporates Transfer Learning and Stacked Generalization, the model aims to lower the False Negative rate of classification compensating for the 30% False Negative rate of the swab tests. A dataset gathered from multiple reliable sources consisting of 9953 Chest X-rays (868 Covid and 9085 Non-Covid) was used. Also, this research demonstrates handling data imbalance using various techniques involving Generative Adversarial Networks and sampling techniques. The accuracy, sensitivity, and specificity obtained on our proposed model were 95.07%, 99.40% and 94.61% respectively. To the best of our knowledge, the combination of accuracy and false negative rate obtained by this paper outperforms the current implementations. We must also highlight that our proposed architecture also considers other types of viral pneumonia. Given the unprecedented sensitivity of our model we are optimistic it contributes to a better Covid-19 detection.", "journal": "Physical and engineering sciences in medicine", "date": "2020-12-05", "authors": ["PratikAutee", "SagarBagwe", "VimalShah", "KritiSrivastava"], "doi": "10.1007/s13246-020-00952-6\n10.1109/TMI.2016.2553401\n10.1109/ACCESS.2020.2994762\n10.1109/TMI.2016.2528162\n10.33889/IJMEMS.2020.5.4.052\n10.1145/1007730.1007735\n10.1109/TMI.2013.2290491\n10.1109/42.929615\n10.1109/TMI.2014.2337057\n10.3978/j.issn.2223-4292.2014.11.20\n10.1109/34.58871\n10.1016/S0893-6080(05)80023-1\n10.1016/j.neunet.2018.07.011"}
{"title": "A Deep-Learning Diagnostic Support System for the Detection of COVID-19 Using Chest Radiographs: A Multireader Validation Study.", "abstract": "Five publicly available databases comprising normal CXR, confirmed COVID-19 pneumonia cases, and other pneumonias were used. After the harmonization of the data, the training set included 7966 normal cases, 5451 with other pneumonia, and 258 CXRs with COVID-19 pneumonia, whereas in the testing data set, each category was represented by 100 cases. Eleven blinded radiologists with various levels of expertise independently read the testing data set. The data were analyzed separately with the newly proposed artificial intelligence-based system and by consultant radiologists and residents, with respect to positive predictive value (PPV), sensitivity, and F-score (harmonic mean for PPV and sensitivity). The \u03c72 test was used to compare the sensitivity, specificity, accuracy, PPV, and F-scores of the readers and the system.\nThe proposed system achieved higher overall diagnostic accuracy (94.3%) than the radiologists (61.4% \u00b1 5.3%). The radiologists reached average sensitivities for normal CXR, other type of pneumonia, and COVID-19 pneumonia of 85.0% \u00b1 12.8%, 60.1% \u00b1 12.2%, and 53.2% \u00b1 11.2%, respectively, which were significantly lower than the results achieved by the algorithm (98.0%, 88.0%, and 97.0%; P < 0.00032). The mean PPVs for all 11 radiologists for the 3 categories were 82.4%, 59.0%, and 59.0% for the healthy, other pneumonia, and COVID-19 pneumonia, respectively, resulting in an F-score of 65.5% \u00b1 12.4%, which was significantly lower than the F-score of the algorithm (94.3% \u00b1 2.0%, P < 0.00001). When other pneumonia and COVID-19 pneumonia cases were pooled, the proposed system reached an accuracy of 95.7% for any pathology and the radiologists, 88.8%. The overall accuracy of consultants did not vary significantly compared with residents (65.0% \u00b1 5.8% vs 67.4% \u00b1 4.2%); however, consultants detected significantly more COVID-19 pneumonia cases (P = 0.008) and less healthy cases (P < 0.00001).\nThe system showed robust accuracy for COVID-19 pneumonia detection on CXR and surpassed radiologists at various training levels.", "journal": "Investigative radiology", "date": "2020-12-02", "authors": ["MatthiasFontanellaz", "LukasEbner", "AdrianHuber", "AlanPeters", "LauraL\u00f6belenz", "CynthiaHourscht", "JeremiasKlaus", "JaroMunz", "ThomasRuder", "DionysiosDrakopoulos", "DominikSieron", "EliasPrimetis", "Johannes THeverhagen", "StavroulaMougiakakou", "AndreasChriste"], "doi": "10.1097/RLI.0000000000000748\n10.1097/RLI.0000000000000716\n10.1016/j.eng.2020.04.010.\n10.1101/2020.02.25.20021568."}
{"title": "An Analysis Review of Detection Coronavirus Disease 2019 (COVID-19) Based on Biosensor Application.", "abstract": "Timely detection and diagnosis are essentially needed to guide outbreak measures and infection control. It is vital to improve healthcare quality in public places, markets, schools and airports and provide useful insights into the technological environment and help researchers acknowledge the choices and gaps available in this field. In this narrative review, the detection of coronavirus disease 2019 (COVID-19) technologies is summarized and discussed with a comparison between them from several aspects to arrive at an accurate decision on the feasibility of applying the best of these techniques in the biosensors that operate using laser detection technology. The collection of data in this analysis was done by using six reliable academic databases, namely, Science Direct, IEEE Xplore, Scopus, Web of Science, Google Scholar and PubMed. This review includes an analysis review of three highlights: evaluating the hazard of pandemic COVID-19 transmission styles and comparing them with Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome (MERS) to identify the main causes of the virus spreading, a critical analysis to diagnose coronavirus disease 2019 (COVID-19) based on artificial intelligence using CT scans and CXR images and types of biosensors. Finally, we select the best methods that can potentially stop the propagation of the coronavirus pandemic.", "journal": "Sensors (Basel, Switzerland)", "date": "2020-12-02", "authors": ["Bakr AhmedTaha", "YousifAl Mashhadany", "Mohd HadriHafiz Mokhtar", "Mohd SaifulDzulkefly Bin Zan", "NorhanaArsad"], "doi": "10.3390/s20236764\n10.1016/S1473-3099(20)30120-1\n10.1016/j.ijid.2020.03.004\n10.1126/science.1085952\n10.1016/j.cell.2020.02.058\n10.1016/j.cmi.2020.03.026\n10.1038/s41586-020-2179-y\n10.1021/nn901889v\n10.1021/nn4037706\n10.1007/s00216-014-8411-6\n10.1016/j.ab.2015.06.022\n10.1109/JSEN.2015.2429738\n10.4236/opj.2016.65011\n10.1007/s12010-015-1902-x\n10.1016/j.jviromet.2016.03.007\n10.1088/1751-8113/44/8/085201\n10.1016/j.ijleo.2017.05.001\n10.1016/j.bios.2016.09.021\n10.1016/j.bios.2016.10.045\n10.5220/0006596400360047\n10.1109/JSEN.2018.2866169\n10.1038/s41565-018-0320-y\n10.1364/OE.27.009536\n10.1109/nems.2019.8915614\n10.1021/acsami.9b03920\n10.1109/RBME.2020.2987975\n10.33889/IJMEMS.2020.5.4.052\n10.1021/acs.chemrev.9b00692\n10.1007/s00330-020-06934-2\n10.1109/TMI.2020.2993291\n10.1101/2020.05.12.20098954\n10.1016/j.trac.2019.115796\n10.1021/acsnano.0c02823\n10.3390/sym12040651\n10.1101/2020.05.01.20088211\n10.1016/j.bios.2020.112291\n10.1007/s41403-020-00128-4\n10.1021/acsnano.0c04421\n10.1007/s41403-020-00122-w\n10.1101/2020.02.14.20023028\n10.1021/acsnano.0c02439\n10.1016/j.compbiomed.2020.103792\n10.1038/s41591-020-0931-3\n10.3390/s20030813\n10.1038/s41370-019-0157-y\n10.1056/NEJMc2004973\n10.1101/2020.03.29.20046557\n10.1016/j.scitotenv.2020.140540\n10.1016/j.jhin.2015.08.027\n10.1016/j.scitotenv.2020.139211\n10.1128/mSystems.00245-20\n10.1016/j.watres.2020.115899\n10.1016/j.jmst.2019.11.010\n10.1101/2020.04.03.20052175\n10.1080/22221751.2020.1745095\n10.1016/S0140-6736(20)30154-9\n10.1016/j.ijsu.2020.05.066\n10.1111/cxo.13088\n10.1080/09273948.2020.1772313\n10.1056/NEJMoa2001316\n10.1038/s41368-020-0075-9\n10.1016/j.visj.2020.100740\n10.1002/jmv.25682\n10.1038/s41586-020-2008-3\n10.1021/acssensors.9b02585\n10.1016/j.ijid.2020.01.009\n10.1126/science.abb0611\n10.1038/s41579-020-0394-z\n10.1186/s12985-015-0422-1\n10.1136/bmj.m1443\n10.1016/j.puhe.2020.05.045\n10.1016/j.jhin.2020.01.022\n10.1016/j.scitotenv.2020.138764\n10.1021/acs.estlett.0c00313\n10.1038/s41467-020-16670-2\n10.1016/j.autrev.2020.102570\n10.1016/j.autrev.2020.102571\n10.1016/j.autrev.2020.102558\n10.1001/jama.2020.3227\n10.1016/j.scitotenv.2020.141560\n10.1016/j.onehlt.2020.100124\n10.1016/j.ipm.2009.03.002\n10.1016/j.jiph.2020.06.028\n10.3348/kjr.2020.0146\n10.1016/j.chemolab.2020.104054\n10.1016/j.mehy.2020.109761\n10.1016/j.afjem.2020.03.002\n10.1016/j.compbiomed.2020.103805\n10.1016/j.cmpb.2020.105608\n10.1016/S2589-7500(20)30079-0\n10.1016/S2589-7500(20)30054-6\n10.1016/j.cmpb.2020.105532\n10.1109/ACCESS.2020.2990893\n10.1038/s41551-018-0305-z\n10.1021/la503533g\n10.1016/j.trac.2010.07.008\n10.1038/lsa.2014.3\n10.1126/science.aas9315\n10.1016/j.talanta.2010.08.033\n10.1038/s41587-019-0045-y\n10.1002/ange.201913804\n10.1101/2020.04.24.059204\n10.1007/s00604-019-3345-5\n10.1016/j.bios.2020.112431\n10.1021/acsomega.8b02458\n10.1016/j.ab.2018.01.007\n10.1021/acs.analchem.8b01217\n10.1016/j.bios.2017.07.010\n10.1021/ac035367b\n10.1101/2020.06.02.131102\n10.1016/j.jbiotec.2017.12.011\n10.1016/j.aca.2010.06.020\n10.1007/978-1-60327-567-5_22\n10.1016/j.biosx.2019.100015\n10.1016/j.bios.2009.07.012\n10.1038/nature12328\n10.1002/jctb.2721\n10.1109/JSEN.2018.2829084\n10.1016/j.aca.2010.09.038\n10.3390/bios9020065\n10.1107/S2059798315024328\n10.3390/bios7020023\n10.1021/acs.analchem.9b01554\n10.1021/acssensors.0c00339\n10.1073/pnas.2013169117\n10.3390/app10010394\n10.1038/s41467-019-11604-z\n10.1371/journal.pone.0216247\n10.1021/acsnano.0c02624\n10.1016/j.diagmicrobio.2020.115141\n10.1016/j.diagmicrobio.2020.115109\n10.1056/NEJMc2001737\n10.1515/cclm-2020-0187\n10.1007/s00146-020-00978-0\n10.7547/0960272\n10.1148/radiol.2020200343\n10.1016/j.celrep.2020.107725\n10.1016/b978-0-12-801238-3.66161-5"}
{"title": "COVID-CheXNet: hybrid deep learning framework for identifying COVID-19 virus in chest X-rays images.", "abstract": "The outbreaks of Coronavirus (COVID-19) epidemic have increased the pressure on healthcare and medical systems worldwide. The timely diagnosis of infected patients is a critical step to limit the spread of the COVID-19 epidemic. The chest radiography imaging has shown to be an effective screening technique in diagnosing the COVID-19 epidemic. To reduce the pressure on radiologists and control of the epidemic, fast and accurate a hybrid deep learning framework for diagnosing COVID-19 virus in chest X-ray images is developed and termed as the COVID-CheXNet system. First, the contrast of the X-ray image was enhanced and the noise level was reduced using the contrast-limited adaptive histogram equalization and Butterworth bandpass filter, respectively. This was followed by fusing the results obtained from two different pre-trained deep learning models based on the incorporation of a ResNet34 and high-resolution network model trained using a large-scale dataset. Herein, the parallel architecture was considered, which provides radiologists with a high degree of confidence to discriminate between the healthy and COVID-19 infected people. The proposed COVID-CheXNet system has managed to correctly and accurately diagnose the COVID-19 patients with a detection accuracy rate of 99.99%, sensitivity of 99.98%, specificity of 100%, precision of 100%, F1-score of 99.99%, MSE of 0.011%, and RMSE of 0.012% using the weighted sum rule at the score-level. The efficiency and usefulness of the proposed COVID-CheXNet system are established along with the possibility of using it in real clinical centers for fast diagnosis and treatment supplement, with less than 2\u00a0s per image to get the prediction result.", "journal": "Soft computing", "date": "2020-12-01", "authors": ["Alaa SAl-Waisy", "ShumoosAl-Fahdawi", "Mazin AbedMohammed", "Karrar HameedAbdulkareem", "Salama AMostafa", "Mashael SMaashi", "MuhammadArif", "BegonyaGarcia-Zapirain"], "doi": "10.1007/s00500-020-05424-3\n10.1007/s00138-017-0870-2\n10.1007/s10044-017-0656-1\n10.1007/s13244-016-0534-1\n10.1016/j.chaos.2020.110242\n10.1016/S0140-6736(20)30211-7\n10.1007/s00500-020-05275-y\n10.1038/s41591-018-0107-6\n10.1016/j.jds.2020.02.002\n10.1016/S0140-6736(20)30183-5\n10.1016/j.jocs.2018.11.008\n10.1109/ACCESS.2020.2995597\n10.14358/PERS.80.2.000\n10.1016/j.compbiomed.2020.103792\n10.1016/j.icte.2018.10.007\n10.1109/tpami.2020.2983686\n10.1001/jama.2020.3786\n10.1023/B:VLSI.0000028532.53893.82"}
{"title": "Viral Pneumonia Screening on Chest X-Rays Using Confidence-Aware Anomaly Detection.", "abstract": "Clusters of viral pneumonia occurrences over a short period may be a harbinger of an outbreak or pandemic. Rapid and accurate detection of viral pneumonia using chest X-rays can be of significant value for large-scale screening and epidemic prevention, particularly when other more sophisticated imaging modalities are not readily accessible. However, the emergence of novel mutated viruses causes a substantial dataset shift, which can greatly limit the performance of classification-based approaches. In this paper, we formulate the task of differentiating viral pneumonia from non-viral pneumonia and healthy controls into a one-class classification-based anomaly detection problem. We therefore propose the confidence-aware anomaly detection (CAAD) model, which consists of a shared feature extractor, an anomaly detection module, and a confidence prediction module. If the anomaly score produced by the anomaly detection module is large enough, or the confidence score estimated by the confidence prediction module is small enough, the input will be accepted as an anomaly case (i.e., viral pneumonia). The major advantage of our approach over binary classification is that we avoid modeling individual viral pneumonia classes explicitly and treat all known viral pneumonia cases as anomalies to improve the one-class model. The proposed model outperforms binary classification models on the clinical X-VIRAL dataset that contains 5,977 viral pneumonia (no COVID-19) cases, 37,393 non-viral pneumonia or healthy cases. Moreover, when directly testing on the X-COVID dataset that contains 106 COVID-19 cases and 107 normal controls without any fine-tuning, our model achieves an AUC of 83.61% and sensitivity of 71.70%, which is comparable to the performance of radiologists reported in the literature.", "journal": "IEEE transactions on medical imaging", "date": "2020-11-28", "authors": ["JianpengZhang", "YutongXie", "GuansongPang", "ZhibinLiao", "JohanVerjans", "WenxingLi", "ZongjiSun", "JianHe", "YiLi", "ChunhuaShen", "YongXia"], "doi": "10.1109/TMI.2020.3040950"}
{"title": "Machine-learning classification of texture features of portable chest X-ray accurately classifies COVID-19 lung infection.", "abstract": "The large volume and suboptimal image quality of portable chest X-rays (CXRs) as a result of the COVID-19 pandemic could post significant challenges for radiologists and frontline physicians. Deep-learning artificial intelligent (AI) methods have the potential to help improve diagnostic efficiency and accuracy for reading portable CXRs.\nThe study aimed at developing an AI imaging analysis tool to classify COVID-19 lung infection based on portable CXRs.\nPublic datasets of COVID-19 (N\u2009=\u2009130), bacterial pneumonia (N\u2009=\u2009145), non-COVID-19 viral pneumonia (N\u2009=\u2009145), and normal (N\u2009=\u2009138) CXRs were analyzed. Texture and morphological features were extracted. Five supervised machine-learning AI algorithms were used to classify COVID-19 from other conditions. Two-class and multi-class classification were performed. Statistical analysis was done using unpaired two-tailed t tests with unequal variance between groups. Performance of classification models used the receiver-operating characteristic (ROC) curve analysis.\nFor the two-class classification, the accuracy, sensitivity and specificity were, respectively, 100%, 100%, and 100% for COVID-19 vs normal; 96.34%, 95.35% and 97.44% for COVID-19 vs bacterial pneumonia; and 97.56%, 97.44% and 97.67% for COVID-19 vs non-COVID-19 viral pneumonia. For the multi-class classification, the combined accuracy and AUC were 79.52% and 0.87, respectively.\nAI classification of texture and morphological features of portable CXRs accurately distinguishes COVID-19 lung infection in patients in multi-class datasets. Deep-learning methods have the potential to improve diagnostic efficiency and accuracy for portable CXRs.", "journal": "Biomedical engineering online", "date": "2020-11-27", "authors": ["LalHussain", "TonyNguyen", "HaifangLi", "Adeel AAbbasi", "Kashif JLone", "ZirunZhao", "MahnoorZaib", "AnneChen", "Tim QDuong"], "doi": "10.1186/s12938-020-00831-x\n10.1002/jmv.25678\n10.1038/nrmicro3143\n10.1016/S0140-6736(20)30211-7\n10.1016/j.tmaid.2020.101567\n10.1164/rccm.2014P7\n10.1056/NEJMp2000929\n10.1148/radiol.2020200230\n10.1148/radiol.2020200280\n10.1109/TMI.2005.862753\n10.1016/S0140-6736(20)30154-9\n10.1073/pnas.1505935112\n10.1016/j.neubiorev.2012.01.004\n10.1038/srep44196\n10.1038/s41598-017-01931-w\n10.1177/117693510600200030\n10.1109/TMI.2017.2655486\n10.1146/annurev-bioeng-071516-044442\n10.1007/s10278-017-9955-8\n10.1016/j.cmpb.2016.10.007\n10.1007/s10278-017-9945-x\n10.1016/j.ultras.2016.08.004\n10.1142/S0129065716500258\n10.1016/j.jalz.2015.01.010\n10.1109/TMI.2016.2535865\n10.1109/TMI.2020.2996645\n10.1118/1.4944498\n10.1016/j.nicl.2017.01.033\n10.1109/JBHI.2016.2631401\n10.1007/s10278-016-9914-9\n10.1118/1.4967345\n10.1016/S0031-3203(96)00142-2\n10.1186/s12880-015-0069-9\n10.1186/s40644-017-0106-8\n10.1007/s00261-017-1144-1\n10.1016/j.crad.2004.07.008\n10.1007/978-3-540-69139-6_157\n10.1109/4233.992163\n10.1016/0169-2607(93)90068-V\n10.1016/j.cmpb.2006.07.010\n10.1214/aos/1013203451\n10.1142/S0218339007002076\n10.1109/TIFS.2012.2223675\n10.1016/j.aap.2016.02.002\n10.1016/j.knosys.2013.10.016\n10.1016/j.patrec.2013.10.017\n10.3233/CBM-170643\n10.1016/j.compbiomed.2015.03.004"}
{"title": "The Potential of Artificial Intelligence to Analyze Chest Radiographs for Signs of COVID-19 Pneumonia.", "abstract": null, "journal": "Radiology", "date": "2020-11-26", "authors": ["Bramvan Ginneken"], "doi": "10.1148/radiol.2020204238\n10.1148/ryai.2020200079"}
{"title": "Deep-learning algorithms for the interpretation of chest radiographs to aid in the triage of COVID-19 patients: A multicenter retrospective study.", "abstract": "The recent medical applications of deep-learning (DL) algorithms have demonstrated their clinical efficacy in improving speed and accuracy of image interpretation. If the DL algorithm achieves a performance equivalent to that achieved by physicians in chest radiography (CR) diagnoses with Coronavirus disease 2019 (COVID-19) pneumonia, the automatic interpretation of the CR with DL algorithms can significantly reduce the burden on clinicians and radiologists in sudden surges of suspected COVID-19 patients. The aim of this study was to evaluate the efficacy of the DL algorithm for detecting COVID-19 pneumonia on CR compared with formal radiology reports. This is a retrospective study of adult patients that were diagnosed as positive COVID-19 cases based on the reverse transcription polymerase chain reaction among all the patients who were admitted to five emergency departments and one community treatment center in Korea from February 18, 2020 to May 1, 2020. The CR images were evaluated with a publicly available DL algorithm. For reference, CR images without chest computed tomography (CT) scans classified as positive for COVID-19 pneumonia were used given that the radiologist identified ground-glass opacity, consolidation, or other infiltration in retrospectively reviewed CR images. Patients with evidence of pneumonia on chest CT scans were also classified as COVID-19 pneumonia positive outcomes. The overall sensitivity and specificity of the DL algorithm for detecting COVID-19 pneumonia on CR were 95.6%, and 88.7%, respectively. The area under the curve value of the DL algorithm for the detection of COVID-19 with pneumonia was 0.921. The DL algorithm demonstrated a satisfactory diagnostic performance comparable with that of formal radiology reports in the CR-based diagnosis of pneumonia in COVID-19 patients. The DL algorithm may offer fast and reliable examinations that can facilitate patient screening and isolation decisions, which can reduce the medical staff workload during COVID-19 pandemic situations.", "journal": "PloS one", "date": "2020-11-25", "authors": ["Se BumJang", "Suk HeeLee", "Dong EunLee", "Sin-YoulPark", "Jong KunKim", "Jae WanCho", "JaekyungCho", "Ki BeomKim", "ByunggeonPark", "JongminPark", "Jae-KwangLim"], "doi": "10.1371/journal.pone.0242759\n10.1056/NEJMoa2001017\n10.3346/jkms.2020.35.e189\n10.1148/radiol.2019191225\n10.1001/jamanetworkopen.2019.1095\n10.1148/radiol.2017162326\n10.1148/radiol.2018180237\n10.3390/jcm9061981\n10.3346/jkms.2020.35.e140\n10.2214/AJR.18.20490\n10.3348/kjr.2020.0536\n10.1097/RTI.0000000000000512\n10.1148/radiol.2020200642\n10.2214/AJR.20.22954\n10.1148/radiol.2020203173\n10.1148/radiol.2020201365\n10.1148/radiol.2020201160\n10.1007/s00330-020-06827-4"}
{"title": "DeepCOVID-XR: An Artificial Intelligence Algorithm to Detect COVID-19 on Chest Radiographs Trained and Tested on a Large U.S. Clinical Data Set.", "abstract": "Background There are characteristic findings of coronavirus disease 2019 (COVID-19) on chest images. An artificial intelligence (AI) algorithm to detect COVID-19 on chest radiographs might be useful for triage or infection control within a hospital setting, but prior reports have been limited by small data sets, poor data quality, or both. Purpose To present DeepCOVID-XR, a deep learning AI algorithm to detect COVID-19 on chest radiographs, that was trained and tested on a large clinical data set. Materials and Methods DeepCOVID-XR is an ensemble of convolutional neural networks developed to detect COVID-19 on frontal chest radiographs, with reverse-transcription polymerase chain reaction test results as the reference standard. The algorithm was trained and validated on 14\u2009788 images (4253 positive for COVID-19) from sites across the Northwestern Memorial Health Care System from February 2020 to April 2020 and was then tested on 2214 images (1192 positive for COVID-19) from a single hold-out institution. Performance of the algorithm was compared with interpretations from five experienced thoracic radiologists on 300 random test images using the McNemar test for sensitivity and specificity and the DeLong test for the area under the receiver operating characteristic curve (AUC). Results A total of 5853 patients (mean age, 58 years \u00b1 19 [standard deviation]; 3101 women) were evaluated across data sets. For the entire test set, accuracy of DeepCOVID-XR was 83%, with an AUC of 0.90. For 300 random test images (134 positive for COVID-19), accuracy of DeepCOVID-XR was 82%, compared with that of individual radiologists (range, 76%-81%) and the consensus of all five radiologists (81%). DeepCOVID-XR had a significantly higher sensitivity (71%) than one radiologist (60%, ", "journal": "Radiology", "date": "2020-11-25", "authors": ["Ramsey MWehbe", "JiayueSheng", "ShinjanDutta", "SiyuanChai", "AmilDravid", "SemihBarutcu", "YunanWu", "Donald RCantrell", "NicholasXiao", "Bradley DAllen", "Gregory AMacNealy", "HaticeSavas", "RishiAgrawal", "NishantParekh", "Aggelos KKatsaggelos"], "doi": "10.1148/radiol.2020203511\n10.1109/CVPR.2016.90\n10.1109/CVPR.2016.308\n10.1109/CVPR.2017.369\n10.1109/ICCV.2017.74\n10.1101/2020.09.13.20193565"}
{"title": "Deep Transfer Learning for COVID-19 Prediction: Case Study for Limited Data Problems.", "abstract": "Automatic prediction of COVID-19 using deep convolution neural networks based pre-trained transfer models and Chest X-ray images.\nThis research employs the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of the disease. Using Deep Learning models, the research aims at evaluating the effectiveness and accuracy of different convolutional neural networks models in the automatic diagnosis of COVID-19 from X-ray images as compared to diagnosis performed by experts in the medical community.\nDue to the fact that the dataset available for COVID-19 is still limited, the best model to use is the InceptionNetV3. Performance results show that the InceptionNetV3 model yielded the highest accuracy of 98.63% (with data augmentation) and 98.90% (without data augmentation) among the three models designed. However, as the dataset gets bigger, the Inception ResNetV2 and NASNetlarge will do a better job of classification. All the performed networks tend to over-fit when data augmentation is not used, this is due to the small amount of data used for training and validation.\nA deep transfer learning is proposed to detecting the COVID-19 automatically from chest X-ray by training it with X-ray images gotten from both COVID-19 patients and people with normal chest X-rays. The study is aimed at helping doctors in making decisions in their clinical practice due its high performance and effectiveness, the study also gives an insight to how transfer learning was used to automatically detect the COVID-19.", "journal": "Current medical imaging", "date": "2020-11-25", "authors": ["SalehAlbahli", "WaleedAlbattah"], "doi": "10.2174/1573405616666201123120417\n10.1109/ACCESS.2020.3031614\n10.1148/radiol.2020200642\n10.1109/TMI.2016.2553401\n10.1039/C8SC00148K\n10.1016/j.catena.2019.104426\n10.1021/acs.molpharmaceut.7b00578\n10.1023/A:1007379606734\n10.1109/TPAMI.2013.50\n10.1016/j.cell.2018.02.010\n10.1016/j.irbm.2020.05.003\n10.1109/IIPHDW.2018.8388338\n10.1118/1.1487426\n10.1007/s10278-003-1655-x\n10.1016/j.acra.2006.01.009\n10.1007/s13089-009-0003-x\n10.1007/BF03167768\n10.1007/BF03167769\n10.1088/0031-9155/56/24/004\n10.1109/TITB.2005.859872\n10.1109/CHASE.2017.59\n10.1016/j.neucom.2018.12.086\n10.1016/j.compbiomed.2020.103795\n10.1109/BigComp48618.2020.00-25"}
{"title": "COVID-19 pneumonia accurately detected on chest radiographs with artificial intelligence.", "abstract": "To investigate the diagnostic performance of an Artificial Intelligence (AI) system for detection of COVID-19 in chest radiographs (CXR), and compare results to those of physicians working alone, or with AI support.\nAn AI system was fine-tuned to discriminate confirmed COVID-19 pneumonia, from other viral and bacterial pneumonia and non-pneumonia patients and used to review 302 CXR images from adult patients retrospectively sourced from nine different databases. Fifty-four physicians blind to diagnosis, were invited to interpret images under identical conditions in a test set, and randomly assigned either to receive or not receive support from the AI system. Comparisons were then made between diagnostic performance of physicians working with and without AI support. AI system performance was evaluated using the area under the receiver operating characteristic (AUROC), and sensitivity and specificity of physician performance compared to that of the AI system.\nDiscrimination by the AI system of COVID-19 pneumonia showed an AUROC curve of 0.96 in the validation and 0.83 in the external test set, respectively. The AI system outperformed physicians in the AUROC overall (70% increase in sensitivity and 1% increase in specificity, p\u202f<\u202f0.0001). When working with AI support, physicians increased their diagnostic sensitivity from 47% to 61% (p\u202f<\u202f0.001), although specificity decreased from 79% to 75% (p\u202f=\u202f0.007).\nOur results suggest interpreting chest radiographs (CXR) supported by AI, increases physician diagnostic sensitivity for COVID-19 detection. This approach involving a human-machine partnership may help expedite triaging efforts and improve resource allocation in the current crisis.", "journal": "Intelligence-based medicine", "date": "2020-11-25", "authors": ["FranciscoDorr", "Hern\u00e1nChaves", "Mar\u00eda MercedesSerra", "Andr\u00e9sRamirez", "Mart\u00edn El\u00edasCosta", "Joaqu\u00ednSeia", "ClaudiaCejas", "MarceloCastro", "EduardoEyheremendy", "DiegoFern\u00e1ndez Slezak", "Mauricio FFarez", "NoneNone"], "doi": "10.1016/j.ibmed.2020.100014\n10.1016/j.ijid.2020.01.009\n10.1002/jmv.25678\n10.1016/S0140-6736(20)30183-5\n10.1101/2020.02.07.937862\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2002032\n10.1016/j.chest.2020.04.003\n10.1101/2020.02.11.20021493\n10.1371/journal.pone.0204155\n10.1371/journal.pmed.1002686\n10.1097/RTI.0000000000000387\n10.1016/j.cell.2018.02.010\n10.1016/j.crad.2018.12.015\n10.2214/AJR.20.23034\n10.1016/j.jclinepi.2009.11.009\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1038/s41591-020-0931-3\n10.1148/radiol.2020201326\n10.1136/pmj.79.930.214\n10.1148/radiol.2020201160\n10.1016/j.ejrad.2020.109272\n10.1148/radiol.2020201874\n10.1016/j.dsx.2020.04.012\n10.1038/s41746-019-0189-7"}
{"title": "Abnormal lung quantification in chest CT images of COVID-19 patients with deep learning and its application to severity prediction.", "abstract": "Computed tomography (CT) provides rich diagnosis and severity information of COVID-19 in clinical practice. However, there is no computerized tool to automatically delineate COVID-19 infection regions in chest CT scans for quantitative assessment in advanced applications such as severity prediction. The aim of this study was to develop a deep learning (DL)-based method for automatic segmentation and quantification of infection regions as well as the entire lungs from chest CT scans.\nThe DL-based segmentation method employs the \"VB-Net\" neural network to segment COVID-19 infection regions in CT scans. The developed DL-based segmentation system is trained by CT scans from 249 COVID-19 patients, and further validated by CT scans from other 300 COVID-19 patients. To accelerate the manual delineation of CT scans for training, a human-involved-model-iterations (HIMI) strategy is also adopted to assist radiologists to refine automatic annotation of each training case. To evaluate the performance of the DL-based segmentation system, three metrics, that is, Dice similarity coefficient, the differences of volume, and percentage of infection (POI), are calculated between automatic and manual segmentations on the validation set. Then, a clinical study on severity prediction is reported based on the quantitative infection assessment.\nThe proposed DL-based segmentation system yielded Dice similarity coefficients of 91.6%\u00a0\u00b1\u00a010.0% between automatic and manual segmentations, and a mean POI estimation error of 0.3% for the whole lung on the validation dataset. Moreover, compared with the cases with fully manual delineation that often takes hours, the proposed HIMI training strategy can dramatically reduce the delineation time to 4\u00a0min after three iterations of model updating. Besides, the best accuracy of severity prediction was 73.4%\u00a0\u00b1\u00a01.3% when the mass of infection (MOI) of multiple lung lobes and bronchopulmonary segments were used as features for severity prediction, indicating the potential clinical application of our quantification technique on severity prediction.\nA DL-based segmentation system has been developed to automatically segment and quantify infection regions in CT scans of COVID-19 patients. Quantitative evaluation indicated high accuracy in automatic infection delineation and severity prediction.", "journal": "Medical physics", "date": "2020-11-24", "authors": ["FeiShan", "YaozongGao", "JunWang", "WeiyaShi", "NannanShi", "MiaofeiHan", "ZhongXue", "DinggangShen", "YuxinShi"], "doi": "10.1002/mp.14609\n10.1109/rbme.2020.2990959\n10.2139/ssrn.3546089\n10.2214/AJR.20.23202"}
{"title": "Designing Futuristic Telemedicine Using Artificial Intelligence and Robotics in the COVID-19 Era.", "abstract": "Technological innovations such as artificial intelligence and robotics may be of potential use in telemedicine and in building capacity to respond to future pandemics beyond the current COVID-19 era. Our international consortium of interdisciplinary experts in clinical medicine, health policy, and telemedicine have identified gaps in uptake and implementation of telemedicine or telehealth across geographics and medical specialties. This paper discusses various artificial intelligence and robotics-assisted telemedicine or telehealth applications during COVID-19 and presents an alternative artificial intelligence assisted telemedicine framework to accelerate the rapid deployment of telemedicine and improve access to quality and cost-effective healthcare. We postulate that the artificial intelligence assisted telemedicine framework would be indispensable in creating futuristic and resilient health systems that can support communities amidst pandemics.", "journal": "Frontiers in public health", "date": "2020-11-24", "authors": ["SonuBhaskar", "SianBradley", "SateeshSakhamuri", "SebastianMoguilner", "Vijay KumarChattu", "ShawnaPandya", "StarrSchroeder", "DanielRay", "MaciejBanach"], "doi": "10.3389/fpubh.2020.556789\n10.1056/NEJMsr1503323\n10.2196/19866\n10.3390/ijerph17155330\n10.2196/18810\n10.2196/18980\n10.1089/pop.2020.0187\n10.2196/20049\n10.1056/NEJMp2003539\n10.3389/fpubh.2020.00410\n10.3389/fpubh.2020.556720\n10.1038/nn.4238\n10.1055/s-0039-1677897\n10.1684/ejd.2019.3538\n10.1136/bmjinnov-2020-000492\n10.7717/peerj.7702\n10.1063/1.5023979\n10.1136/bmjgh-2018-000798\n10.1016/j.soin.2020.01.009\n10.1177/1094670517752459\n10.1002/hbm.24886\n10.1136/medethics-2019-105586\n10.1002/hast.973\n10.1136/medethics-2018-105118\n10.1371/journal.pone.0235502\n10.1197/jamia.M1471\n10.7861/futurehosp.6-2-94\n10.1016/j.bushor.2018.03.007\n10.3389/fneur.2020.00664\n10.3389/fcvm.2020.00112\n10.3389/fneur.2020.00579\n10.3389/fpubh.2020.00514\n10.1111/jnu.12436\n10.1001/jama.2020.14136\n10.1097/ACM.0000000000003711\n10.1097/ACM.0000000000003572\n10.7326/M19-0283\n10.1002/aisy.202000071\n10.1126/scirobotics.abb5589\n10.1038/519014a\n10.1186/s12879-017-2275-2\n10.1186/s10033-020-00464-0\n10.3802/jgo.2020.31.e59\n10.1186/s12938-016-0217-7\n10.1016/j.ejogrb.2020.06.014\n10.1016/j.jstrokecerebrovasdis.2018.06.024\n10.1097/CCM.0000000000004190\n10.1186/s12913-017-2341-x\n10.1136/postgradmedj-2019-136413\n10.1111/ane.13343\n10.1016/j.cor.2008.10.004\n10.1108/09593841011052138\n10.2196/19264\n10.2478/emj-2019-0004"}
{"title": "Rapid COVID-19 diagnosis using ensemble deep transfer learning models from chest radiographic images.", "abstract": "The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) causes novel coronavirus disease (COVID-19) outbreak in more than 200 countries around the world. The early diagnosis of infected patients is needed to discontinue this outbreak. The diagnosis of coronavirus infection from radiography images is the fastest method. In this paper, two different ensemble deep transfer learning models have been designed for COVID-19 diagnosis utilizing the chest X-rays. Both models have utilized pre-trained models for better performance. They are able to differentiate COVID-19, viral pneumonia, and bacterial pneumonia. Both models have been developed to improve the generalization capability of the classifier for binary and multi-class problems. The proposed models have been tested on two well-known datasets. Experimental results reveal that the proposed framework outperforms the existing techniques in terms of sensitivity, specificity, and accuracy.", "journal": "Journal of ambient intelligence and humanized computing", "date": "2020-11-24", "authors": ["NehaGianchandani", "AayushJaiswal", "DilbagSingh", "VijayKumar", "ManjitKaur"], "doi": "10.1007/s12652-020-02669-6\n10.3390/s19194139\n10.1049/trit.2019.0028\n10.1016/j.radi.2020.04.017\n10.3390/app10020559\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1007/s13246-020-00888-x\n10.1016/j.irbm.2020.07.001\n10.1049/trit.2019.0051\n10.1049/trit.2018.1006\n10.1186/s40708-018-0080-3\n10.1016/j.imu.2020.100412\n10.1016/j.cmpb.2020.105581\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103869\n10.1504/IJHM.2019.098951\n10.1016/j.compbiomed.2020.103792\n10.1016/j.imu.2020.100360\n10.1101/2020.06.21.20136598\n10.1146/annurev-bioeng-071516-044442\n10.2478/v10006-009-0026-2\n10.1016/j.compbiomed.2020.103805\n10.1016/j.chemolab.2020.104054\n10.1109/ACCESS.2020.2994762\n10.1007/s10916-018-0932-7\n10.1504/IJHM.2019.102893\n10.1504/IJHM.2019.098949"}
{"title": "Automatic detection of COVID-19 from chest radiographs using deep learning.", "abstract": "The breakdown of a deadly infectious disease caused by a newly discovered coronavirus (named SARS n-CoV2) back in December 2019 has shown no respite to slow or stop in general. This contagious disease has spread across different lengths and breadths of the globe, taking a death toll to nearly 700\u00a0k by the start of August 2020. The number is well expected to rise even more significantly. In the absence of a thoroughly tested and approved vaccine, the onus primarily lies on obliging to standard operating procedures and timely detection and isolation of the infected persons. The detection of SARS n-CoV2 has been one of the core concerns during the fight against this pandemic. To keep up with the scale of the outbreak, testing needs to be scaled at par with it. With the conventional PCR testing, most of the countries have struggled to minimize the gap between the scale of outbreak and scale of testing.\nOne way of expediting the scale of testing is to shift to a rigorous computational model driven by deep neural networks, as proposed here in this paper. The proposed model is a non-contact process of determining whether a subject is infected or not and is achieved by using chest radiographs; one of the most widely used imaging technique for clinical diagnosis due to fast imaging and low cost. The dataset used in this work contains 1428 chest radiographs with confirmed COVID-19 positive, common bacterial pneumonia, and healthy cases (no infection). We explored the pre-trained VGG-16 model for classification tasks in this. Transfer learning with fine-tuning was used in this study to train the network on relatively small chest radiographs effectively.\nInitial experiments showed that the model achieved promising results and can be significantly used to expedite COVID-19 detection. The experimentation showed an accuracy of 96% and 92.5% in two and three output class cases, respectively.\nWe believe that this study could be used as an initial screening, which can help healthcare professionals to treat the COVID patients by timely detecting better and screening the presence of disease.\nIts simplicity drives the proposed deep neural network model, the capability to work on small image dataset, the non-contact method with acceptable accuracy is a potential alternative for rapid COVID-19 testing that can be adapted by the medical fraternity considering the criticality of the time along with the magnitudes of the outbreak.", "journal": "Radiography (London, England : 1995)", "date": "2020-11-24", "authors": ["M KPandit", "S ABanday", "RNaaz", "M AChishti"], "doi": "10.1016/j.radi.2020.10.018"}
{"title": "Epicardial adipose tissue is associated with extent of pneumonia and adverse outcomes in patients with COVID-19.", "abstract": "We sought to examine the association of epicardial adipose tissue (EAT) quantified on chest computed tomography (CT) with the extent of pneumonia and adverse outcomes in patients with coronavirus disease 2019 (COVID-19).\nWe performed a post-hoc analysis of a prospective international registry comprising 109 consecutive patients (age 64\u202f\u00b1\u202f16\u202fyears; 62% male) with laboratory-confirmed COVID-19 and noncontrast chest CT imaging. Using semi-automated software, we quantified the burden (%) of lung abnormalities associated with COVID-19 pneumonia. EAT volume (mL) and attenuation (Hounsfield units) were measured using deep learning software. The primary outcome was clinical deterioration (intensive care unit admission, invasive mechanical ventilation, or vasopressor therapy) or in-hospital death.\nIn multivariable linear regression analysis adjusted for patient comorbidities, the total burden of COVID-19 pneumonia was associated with EAT volume (\u03b2\u202f=\u202f10.6, p\u202f=\u202f0.005) and EAT attenuation (\u03b2\u202f=\u202f5.2, p\u202f=\u202f0.004). EAT volume correlated with serum levels of lactate dehydrogenase (r\u202f=\u202f0.361, p\u202f=\u202f0.001) and C-reactive protein (r\u202f=\u202f0.450, p\u202f<\u202f0.001). Clinical deterioration or death occurred in 23 (21.1%) patients at a median of 3\u202fdays (IQR 1-13\u202fdays) following the chest CT. In multivariable logistic regression analysis, EAT volume (OR 5.1 [95% CI 1.8-14.1] per doubling p\u202f=\u202f0.011) and EAT attenuation (OR 3.4 [95% CI 1.5-7.5] per 5 Hounsfield unit increase, p\u202f=\u202f0.003) were independent predictors of clinical deterioration or death, as was total pneumonia burden (OR 2.5, 95% CI 1.4-4.6, p\u202f=\u202f0.002), chronic lung disease (OR 1.3 [95% CI 1.1-1.7], p\u202f=\u202f0.011), and history of heart failure (OR 3.5 [95% 1.1-8.2], p\u202f=\u202f0.037).\nEAT measures quantified from chest CT are independently associated with extent of pneumonia and adverse outcomes in patients with COVID-19, lending support to their use in clinical risk stratification.", "journal": "Metabolism: clinical and experimental", "date": "2020-11-23", "authors": ["KajetanGrodecki", "AndrewLin", "AryabodRazipour", "SebastienCadet", "Priscilla AMcElhinney", "CatoChan", "Barry DPressman", "PeterJulien", "PalMaurovich-Horvat", "NicolaGaibazzi", "UditThakur", "ElisabettaMancini", "CeciliaAgalbato", "RobertMen\u00e8", "GianfrancoParati", "FrancoCernigliaro", "NiteshNerlekar", "CamillaTorlasco", "GianlucaPontone", "Piotr JSlomka", "DaminiDey"], "doi": "10.1016/j.metabol.2020.154436\n10.1111/dom.14125\n10.1002/dmrr.3325\n10.1002/oby.23019\n10.1056/NEJMoa2021436"}
{"title": "Advances in Technology to Address COVID-19.", "abstract": null, "journal": "SLAS technology", "date": "2020-11-21", "authors": ["Edward Kai-HuaChow", "Pak KinWong", "XiantingDing"], "doi": "10.1177/2472630320969634"}
{"title": "Deep learning applications to combat the dissemination of COVID-19 disease: a review.", "abstract": "Recent Coronavirus (COVID-19) is one of the respiratory diseases, and it is known as fast infectious ability. This dissemination can be decelerated by diagnosing and quarantining patients with COVID-19 at early stages, thereby saving numerous lives. Reverse transcription-polymerase chain reaction (RT-PCR) is known as one of the primary diagnostic tools. However, RT-PCR tests are costly and time-consuming; it also requires specific materials, equipment, and instruments. Moreover, most countries are suffering from a lack of testing kits because of limitations on budget and techniques. Thus, this standard method is not suitable to meet the requirements of fast detection and tracking during the COVID-19 pandemic, which motived to employ deep learning (DL)/convolutional neural networks (CNNs) technology with X-ray and CT scans for efficient analysis and diagnostic. This study provides insight about the literature that discussed the deep learning technology and its various techniques that are recently developed to combat the dissemination of COVID-19 disease.", "journal": "European review for medical and pharmacological sciences", "date": "2020-11-21", "authors": ["M HAlsharif", "Y HAlsharif", "KYahya", "O AAlomari", "M AAlbreem", "AJahid"], "doi": "10.26355/eurrev_202011_23640"}
{"title": "[Chest radiological lesions in COVID-19 : from classical imaging to artificial intelligence].", "abstract": "In the course of the pandemic induced by the appearance of a new coronavirus (SARS-CoV-2; COVID-19) causing acute respiratory distress syndrome (ARDS), we had to rethink the diagnostic approach for patients suffering from respiratory symptoms. Indeed, although the use of RT-PCR remains the keystone of the diagnosis, the delay in diagnosis as well as the overload of the microbiological platforms have led us to make almost systematic the use of thoracic imaging for taking in charge of patients. In this context, thoracic imaging has shown a major interest in diagnostic aid in order to better guide the management of patients admitted to hospital. The most common signs encountered are particularly well described in thoracic computed tomography. Typical imaging combines bilateral, predominantly peripheral and posterior, multi-lobar, ground glass opacities. Of note, it is common to identify significant lesions in asymptomatic patients, with imaging sometimes preceding the onset of symptoms. Beyond conventional chest imaging, many teams have developed new artificial intelligence tools to better help clinicians in decision-making.\nDans le d\u00e9cours de la pand\u00e9mie induite par l\u2019apparition d\u2019un nouveau coronavirus (SARS-CoV-2; COVID-19) \u00e0 l\u2019origine d\u2019un syndrome de d\u00e9tresse respiratoire aigu (SDRA), nous avons d\u00fb repenser l\u2019approche diagnostique des patients souffrant de sympt\u00f4mes respiratoires. En effet, bien que l\u2019usage de la RT-PCR reste la cl\u00e9 de vo\u00fbte du diagnostic, le retard de diagnostic ainsi que la surcharge des plateformes microbiologiques nous ont men\u00e9s \u00e0 rendre quasi syst\u00e9matique l\u2019usage de l\u2019imagerie thoracique pour la prise en charge des patients. L\u2019imagerie thoracique a d\u00e9montr\u00e9, dans ce contexte, un int\u00e9r\u00eat majeur dans l\u2019aide au diagnostic afin d\u2019orienter, au mieux, la prise en charge des patients admis \u00e0 l\u2019h\u00f4pital. Les signes les plus couramment rencontr\u00e9s sont particuli\u00e8rement bien d\u00e9crits en tomodensitom\u00e9trie thoracique. L\u2019imagerie typique associe des l\u00e9sions en verre d\u00e9poli bilat\u00e9rales, multi-lobaires, \u00e0 pr\u00e9dominance p\u00e9riph\u00e9rique et post\u00e9rieure. Il est classique d\u2019identifier des l\u00e9sions significatives chez des patients asymptomatiques, l\u2019imagerie pr\u00e9c\u00e9dant parfois l\u2019apparition de sympt\u00f4mes. Au-del\u00e0 de l\u2019imagerie thoracique conventionnelle, de nombreuses \u00e9quipes ont d\u00e9velopp\u00e9 de nouveaux outils d\u2019intelligence artificielle afin d\u2019aider, au mieux, les cliniciens dans la prise de d\u00e9cisions.", "journal": "Revue medicale de Liege", "date": "2020-11-20", "authors": ["JGuiot", "DDanthine", "LDeprez", "RLouis", "PLovinfosse", "PMeunier"], "doi": null}
{"title": "Routine laboratory testing to determine if a patient has COVID-19.", "abstract": "Specific diagnostic tests to detect severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and resulting COVID-19 disease are not always available and take time to obtain results. Routine laboratory markers such as white blood cell count, measures of anticoagulation, C-reactive protein (CRP) and procalcitonin, are used to assess the clinical status of a patient. These laboratory tests may be useful for the triage of people with potential COVID-19 to prioritize them for different levels of treatment, especially in situations where time and resources are limited.\nTo assess the diagnostic accuracy of routine laboratory testing as a triage test to determine if a person has COVID-19.\nOn 4 May 2020 we undertook electronic searches in the Cochrane COVID-19 Study Register and the COVID-19 Living Evidence Database from the University of Bern, which is updated daily with published articles from PubMed and Embase and with preprints from medRxiv and bioRxiv. In addition, we checked repositories of COVID-19 publications. We did not apply any language restrictions.\nWe included both case-control designs and consecutive series of patients that assessed the diagnostic accuracy of routine laboratory testing as a triage test to determine if a person has COVID-19. The reference standard could be reverse transcriptase polymerase chain reaction (RT-PCR) alone; RT-PCR plus clinical expertise or and imaging; repeated RT-PCR several days apart or from different samples; WHO and other case definitions; and any other reference standard used by the study authors.\nTwo review authors independently extracted data from each included study. They also assessed the methodological quality of the studies, using QUADAS-2. We used the 'NLMIXED' procedure in SAS 9.4 for the hierarchical summary receiver operating characteristic (HSROC) meta-analyses of tests for which we included four or more studies. To facilitate interpretation of results, for each meta-analysis we estimated summary sensitivity at the points on the SROC curve that corresponded to the median and interquartile range boundaries of specificities in the included studies.\nWe included 21 studies in this review, including 14,126 COVID-19 patients and 56,585 non-COVID-19 patients in total. Studies evaluated a total of 67 different laboratory tests. Although we were interested in the diagnotic accuracy of routine tests for COVID-19, the included studies used detection of SARS-CoV-2 infection through RT-PCR as reference standard. There was considerable heterogeneity between tests, threshold values and the settings in which they were applied. For some tests a positive result was defined as a decrease compared to normal vaues, for other tests a positive result was defined as an increase, and for some tests both increase and decrease may have indicated test positivity. None of the studies had either low risk of bias on all domains or low concerns for applicability for all domains. Only three of the tests evaluated had a summary sensitivity and specificity over 50%. These were: increase in interleukin-6, increase in C-reactive protein and lymphocyte count decrease. Blood count Eleven studies evaluated a decrease in white blood cell count, with a median specificity of 93% and a summary sensitivity of 25% (95% CI 8.0% to 27%; very low-certainty evidence). The 15 studies that evaluated an increase in white blood cell count had a lower median specificity and a lower corresponding sensitivity. Four studies evaluated a decrease in neutrophil count. Their median specificity was 93%, corresponding to a summary sensitivity of 10% (95% CI 1.0% to 56%; low-certainty evidence). The 11 studies that evaluated an increase in neutrophil count had a lower median specificity and a lower corresponding sensitivity. The summary sensitivity of an increase in neutrophil percentage (4 studies) was 59% (95% CI 1.0% to 100%) at median specificity (38%; very low-certainty evidence). The summary sensitivity of an increase in monocyte count (4 studies) was 13% (95% CI 6.0% to 26%) at median specificity (73%; very low-certainty evidence). The summary sensitivity of a decrease in lymphocyte count (13 studies) was 64% (95% CI 28% to 89%) at median specificity (53%; low-certainty evidence). Four studies that evaluated a decrease in lymphocyte percentage showed a lower median specificity and lower corresponding sensitivity. The summary sensitivity of a decrease in platelets (4 studies) was 19% (95% CI 10% to 32%) at median specificity (88%; low-certainty evidence). Liver function tests The summary sensitivity of an increase in alanine aminotransferase (9 studies) was 12% (95% CI 3% to 34%) at median specificity (92%; low-certainty evidence). The summary sensitivity of an increase in aspartate aminotransferase (7 studies) was 29% (95% CI 17% to 45%) at median specificity (81%) (low-certainty evidence). The summary sensitivity of a decrease in albumin (4 studies) was 21% (95% CI 3% to 67%) at median specificity (66%; low-certainty evidence). The summary sensitivity of an increase in total bilirubin (4 studies) was 12% (95% CI 3.0% to 34%) at median specificity (92%; very low-certainty evidence). Markers of inflammation The summary sensitivity of an increase in CRP (14 studies) was 66% (95% CI 55% to 75%) at median specificity (44%; very low-certainty evidence). The summary sensitivity of an increase in procalcitonin (6 studies) was 3% (95% CI 1% to 19%) at median specificity (86%; very low-certainty evidence). The summary sensitivity of an increase in IL-6 (four studies) was 73% (95% CI 36% to 93%) at median specificity (58%) (very low-certainty evidence). Other biomarkers The summary sensitivity of an increase in creatine kinase (5 studies) was 11% (95% CI 6% to 19%) at median specificity (94%) (low-certainty evidence). The summary sensitivity of an increase in serum creatinine (four studies) was 7% (95% CI 1% to 37%) at median specificity (91%; low-certainty evidence). The summary sensitivity of an increase in lactate dehydrogenase (4 studies) was 25% (95% CI 15% to 38%) at median specificity (72%; very low-certainty evidence).\nAlthough these tests give an indication about the general health status of patients and some tests may be specific indicators for inflammatory processes, none of the tests we investigated are useful for accurately ruling in or ruling out COVID-19 on their own. Studies were done in specific hospitalized populations, and future studies should consider non-hospital settings to evaluate how these tests would perform in people with milder symptoms.", "journal": "The Cochrane database of systematic reviews", "date": "2020-11-20", "authors": ["IngeStegeman", "Eleanor AOchodo", "FatumaGuleid", "Gea AHoltman", "BadaYang", "ClareDavenport", "Jonathan JDeeks", "JacquelineDinnes", "SabineDittrich", "DevyEmperador", "LottyHooft", "Ren\u00e9Spijker", "YemisiTakwoingi", "AnnVan den Bruel", "JunfengWang", "MirandaLangendam", "Jan YVerbakel", "Mariska MgLeeflang", "NoneNone"], "doi": "10.1002/14651858.CD013787\n10.1002/jmv.25810\n10.1101/2020.04.09.20059964\n10.1093/cid/ciaa247\n10.1097/CM9.0000000000000774\n10.34172/aim.2020.10\n10.1055/s-0040-1710018\n10.1007/s11239-020-02105-8\n10.1002/14651858.CD013652\n10.1002/14651858.CD013596\n10.1002/14651858.CD013705\n10.1002/14651858.CD013639\n10.1002/14651858.CD013665"}
{"title": "Preliminary investigation of relationship between clinical indicators and CT manifestation patterns of COVID-19 pneumonia improvement.", "abstract": "To retrospectively evaluate several clinical indicators related to the improvement of COVID-19 pneumonia on CT.\nA total of 62 patients with COVID-19 pneumonia were included. The CT scores based on lesion patterns and distributions in serial CT were investigated. The improvement and deterioration of pneumonia was assessed based on the changes of CT scores. Grouped by using the temperature, serum lymphocytes and high sensitivity CRP (hs-CRP) on admission respectively, the CT scores on admission, at peak time and at discharge were evaluated. Correlation analysis was carried out between the time to onset of pneumonia resolution on CT images and the recovery time of temperature, negative conversion of viral nucleic acid, serum lymphocytes and hs-CRP.\nThe CT scores of the fever group and lymphopenia group were significantly higher than those of normal group on admission, at peak time and at discharge; and the CT scores of normal hs-CRP group were significantly lower than those of the elevated hs-CRP group at peak time and at discharge (P all<0.05). The time to onset of pneumonia resolution on CT image was moderately correlated with negative conversion duration of viral nucleic acid (r =0.501, P<0.05) and the recovery time of hs-CPR (r =0.496, P<0.05).\nCOVID-19 pneumonia patients with no fever, normal lymphocytes and hs-CRP had mild lesions on admission, and presented with more absorption and fewer pulmonary lesions on discharge. The negative conversion duration of viral nucleic acid and the recovery time of hs-CPR may be the indicator of the pneumonia resolution.", "journal": "Journal of thoracic disease", "date": "2020-11-20", "authors": ["NannanShi", "FengxiangSong", "FengjunLiu", "PengruiSong", "YangLu", "QinguoHou", "XinyanHua", "YunLing", "JiulongZhang", "ChaoHuang", "LeiShi", "ZhiyongZhang", "FeiShan", "QiZhang", "YuxinShi"], "doi": "10.21037/jtd-20-1420\n10.1111/1751-2980.12851\n10.1002/jmv.25726\n10.1111/resp.13196\n10.1016/j.meegid.2020.104211\n10.1371/journal.pone.0093885\n10.1016/j.ejrad.2010.12.085\n10.1148/radiol.2020200241\n10.1148/radiol.2020200274\n10.1148/radiol.2020200370\n10.1097/CM9.0000000000000774\n10.1186/1465-9921-6-42"}
{"title": "CT imaging features of different clinical types of COVID-19 calculated by AI system: a Chinese multicenter study.", "abstract": "The study is designed to explore the chest CT features of different clinical types of coronavirus disease 2019 (COVID-19) pneumonia based on a Chinese multicenter dataset using an artificial intelligence (AI) system.\nA total of 164 patients confirmed COVID-19 were retrospectively enrolled from 6 hospitals. All patients were divided into the mild type (136 cases) and the severe type (28 cases) according to their clinical manifestations. The total CT severity score and quantitative CT features were calculated by AI pneumonia detection and evaluation system with correction by radiologists. The clinical and CT imaging features of different types were analyzed.\nIt was observed that patients in the severe type group were older than the mild type group. Round lesions, Fan-shaped lesions, crazy-paving pattern, fibrosis, \"white lung\", pleural thickening, pleural indentation, mediastinal lymphadenectasis were more common in the CT images of severe patients than in the mild ones. A higher total lung severity score and scores of each lobe were observed in the severe group, with higher scores in bilateral lower lobes of both groups. Further analysis showed that the volume and number of pneumonia lesions and consolidation lesions in overall lung were higher in the severe group, and showed a wider distribution in the lower lobes of bilateral lung in both groups.\nChest CT of patients with severe COVID-19 pneumonia showed more consolidative and progressive lesions. With the assistance of AI, CT could evaluate the clinical severity of COVID-19 pneumonia more precisely and help the early diagnosis and surveillance of the patients.", "journal": "Journal of thoracic disease", "date": "2020-11-20", "authors": ["XiaofeiHu", "WenbingZeng", "YuhanZhang", "ZhimingZhen", "YalanZheng", "LinCheng", "XianqiWang", "HaoranLuo", "ShuZhang", "ZifengWu", "ZeyuSun", "XiuliLi", "YangCao", "MingXu", "JianWang", "WeiChen"], "doi": "10.21037/jtd-20-1584\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2002032\n10.1001/jama.2020.2648\n10.1148/radiol.2020200230\n10.1148/ryct.2020200034\n10.1007/s00330-020-06731-x\n10.1001/jama.2013.281053\n10.1148/radiol.2462070712\n10.1148/rg.2018170048\n10.1148/radiol.2020200463\n10.1148/ryct.2020200028\n10.1148/radiol.2020200370\n10.1016/j.jtho.2020.02.010\n10.1016/S2213-2600(20)30076-X\n10.1097/RLI.0000000000000670\n10.1101/2020.02.10.20021675v2"}
{"title": "Open resource of clinical data from patients with pneumonia for the prediction of COVID-19 outcomes via deep learning.", "abstract": "Data from patients with coronavirus disease 2019 (COVID-19) are essential for guiding clinical decision making, for furthering the understanding of this viral disease, and for diagnostic modelling. Here, we describe an open resource containing data from 1,521 patients with pneumonia (including COVID-19 pneumonia) consisting of chest computed tomography (CT) images, 130 clinical features (from a range of biochemical and cellular analyses of blood and urine samples) and laboratory-confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) clinical status. We show the utility of the database for prediction of COVID-19 morbidity and mortality outcomes using a deep learning algorithm trained with data from 1,170 patients and 19,685 manually labelled CT slices. In an independent validation cohort of 351 patients, the algorithm discriminated between negative, mild and severe cases with areas under the receiver operating characteristic curve of 0.944, 0.860 and 0.884, respectively. The open database may have further uses in the diagnosis and management of patients with COVID-19.", "journal": "Nature biomedical engineering", "date": "2020-11-20", "authors": ["WanshanNing", "ShijunLei", "JingjingYang", "YukunCao", "PeiranJiang", "QianqianYang", "JiaoZhang", "XiaobeiWang", "FenghuaChen", "ZhiGeng", "LiangXiong", "HongmeiZhou", "YapingGuo", "YulanZeng", "HeshuiShi", "LinWang", "YuXue", "ZhengWang"], "doi": "10.1038/s41551-020-00633-5\n10.1056/NEJMoa2001017\n10.1038/s41586-020-2008-3\n10.1016/S0140-6736(20)30627-9\n10.1148/radiol.2020200490\n10.1038/s41586-020-2012-7\n10.1097/RLI.0000000000000670\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30251-8\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30154-9\n10.1016/j.cca.2020.03.009\n10.1016/j.tmaid.2020.101623\n10.1016/S1473-3099(20)30086-4\n10.1136/bmj.m1443\n10.1136/bmj.m1091\n10.1016/S0140-6736(20)30566-3\n10.1016/j.ejrad.2020.108941\n10.1056/NEJMoa2002032\n10.1016/S2213-2600(20)30079-5\n10.1001/jamainternmed.2020.0994\n10.1016/j.cell.2020.04.045\n10.1007/s10916-020-01562-1\n10.3348/kjr.2020.0146\n10.1148/radiol.2020200905\n10.21037/atm-20-3026\n10.1109/TMI.2020.2995965\n10.1007/s13246-020-00888-x\n10.7717/peerj.453"}
{"title": "An efficient mixture of deep and machine learning models for COVID-19 diagnosis in chest X-ray images.", "abstract": "A newly emerged coronavirus (COVID-19) seriously threatens human life and health worldwide. In coping and fighting against COVID-19, the most critical step is to effectively screen and diagnose infected patients. Among them, chest X-ray imaging technology is a valuable imaging diagnosis method. The use of computer-aided diagnosis to screen X-ray images of COVID-19 cases can provide experts with auxiliary diagnosis suggestions, which can reduce the burden of experts to a certain extent. In this study, we first used conventional transfer learning methods, using five pre-trained deep learning models, which the Xception model showed a relatively ideal effect, and the diagnostic accuracy reached 96.75%. In order to further improve the diagnostic accuracy, we propose an efficient diagnostic method that uses a combination of deep features and machine learning classification. It implements an end-to-end diagnostic model. The proposed method was tested on two datasets and performed exceptionally well on both of them. We first evaluated the model on 1102 chest X-ray images. The experimental results show that the diagnostic accuracy of Xception + SVM is as high as 99.33%. Compared with the baseline Xception model, the diagnostic accuracy is improved by 2.58%. The sensitivity, specificity and AUC of this model reached 99.27%, 99.38% and 99.32%, respectively. To further illustrate the robustness of our method, we also tested our proposed model on another dataset. Finally also achieved good results. Compared with related research, our proposed method has higher classification accuracy and efficient diagnostic performance. Overall, the proposed method substantially advances the current radiology based methodology, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis and follow-up of COVID-19 cases.", "journal": "PloS one", "date": "2020-11-18", "authors": ["DingdingWang", "JiaqingMo", "GangZhou", "LiangXu", "YajunLiu"], "doi": "10.1371/journal.pone.0242535\n10.1016/j.ijsu.2020.02.034\n10.1148/radiol.2020200330\n10.1148/ryct.2020200034\n10.1016/j.jinf.2020.03.007\n10.1038/nature14539\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.chaos.2020.109944\n10.1016/j.cmpb.2020.105581\n10.1007/BF00116251\n10.1007/BF00994018\n10.1101/2020.02.14.20023028%JmedRxiv\n10.1101/2020.03.12.20027185%JmedRxiv"}
{"title": "Validation of Chest Computed Tomography Artificial Intelligence to Determine the Requirement for Mechanical Ventilation and Risk of Mortality in Hospitalized Coronavirus Disease-19 Patients in a Tertiary Care Center In Mexico City.", "abstract": "Artificial intelligence (AI) in radiology has improved diagnostic performance and shortened reading times of coronavirus disease 2019 (COVID-19) patients' studies.\nThe objectives pf the study were to analyze the performance of a chest computed tomography (CT) AI quantitative algorithm for determining the risk of mortality/mechanical ventilation (MV) in hospitalized COVID-19 patients and explore a prognostic multivariate model in a tertiary-care center in Mexico City.\nChest CT images of 166 COVID-19 patients hospitalized from April 1 to 20, 2020, were retrospectively analyzed using AI algorithm software. Data were collected from their medical records. We analyzed the diagnostic yield of the relevant CT variables using the area under the ROC curve (area under the curve [AUC]). Optimal thresholds were obtained using the Youden index. We proposed a predictive logistic model for each outcome based on CT AI measures and predetermined laboratory and clinical characteristics.\nThe highest diagnostic yield of the assessed CT variables for mortality was the percentage of total opacity (threshold >51%; AUC = 0.88, sensitivity = 74%, and specificity = 91%). The AUC of the CT severity score (threshold > 12.5) was 0.88 for MV (sensitivity = 65% and specificity = 92%). The proposed prognostic models include the percentage of opacity and lactate dehydrogenase level for mortality and troponin I and CT severity score for MV requirement.\nThe AI-calculated CT severity score and total opacity percentage showed good diagnostic accuracy for mortality and met MV criteria. The proposed prognostic models using biochemical variables and imaging data measured by AI on chest CT showed good risk classification in our population of hospitalized COVID-19 patients.", "journal": "Revista de investigacion clinica; organo del Hospital de Enfermedades de la Nutricion", "date": "2020-11-18", "authors": ["YukiyoshiKimura-Sandoval", "Mary EAr\u00e9valo-Molina", "C\u00e9sar NCristancho-Rojas", "YumiKimura-Sandoval", "VictoriaRebollo-Hurtado", "MarianaLicano-Zubiate", "M\u00f3nicaChapa-Ibarg\u00fcengoitia", "GiselaMu\u00f1oz-L\u00f3pez"], "doi": "10.24875/RIC.20000451"}
{"title": "AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system.", "abstract": "The sudden outbreak of novel coronavirus 2019 (COVID-19) increased the diagnostic burden of radiologists. In the time of an epidemic crisis, we hope artificial intelligence (AI) to reduce physician workload in regions with the outbreak, and improve the diagnosis accuracy for physicians before they could acquire enough experience with the new disease. In this paper, we present our experience in building and deploying an AI system that automatically analyzes CT images and provides the probability of infection to rapidly detect COVID-19 pneumonia. The proposed system which consists of classification and segmentation will save about 30%-40% of the detection time for physicians and promote the performance of COVID-19 detection. Specifically, working in an interdisciplinary team of over 30 people with medical and/or AI background, geographically distributed in Beijing and Wuhan, we are able to overcome a series of challenges (", "journal": "Applied soft computing", "date": "2020-11-18", "authors": ["BoWang", "ShuoJin", "QingsenYan", "HaiboXu", "ChuanLuo", "LaiWei", "WeiZhao", "XuexueHou", "WenshuoMa", "ZhengqingXu", "ZhuozhaoZheng", "WenboSun", "LanLan", "WeiZhang", "XiangdongMu", "ChenxiShi", "ZhongxiaoWang", "JihaeLee", "ZijianJin", "MingguiLin", "HongboJin", "LiangZhang", "JunGuo", "BenqiZhao", "ZhizhongRen", "ShuhaoWang", "WeiXu", "XinghuanWang", "JianmingWang", "ZhengYou", "JiahongDong"], "doi": "10.1016/j.asoc.2020.106897\n10.1016/j.cviu.2020.103079"}
{"title": "The ensemble deep learning model for novel COVID-19 on CT images.", "abstract": "The rapid detection of the novel coronavirus disease, COVID-19, has a positive effect on preventing propagation and enhancing therapeutic outcomes. This article focuses on the rapid detection of COVID-19. We propose an ensemble deep learning model for novel COVID-19 detection from CT images. 2933 lung CT images from COVID-19 patients were obtained from previous publications, authoritative media reports, and public databases. The images were preprocessed to obtain 2500 high-quality images. 2500 CT images of lung tumor and 2500 from normal lung were obtained from a hospital. Transfer learning was used to initialize model parameters and pretrain three deep convolutional neural network models: AlexNet, GoogleNet, and ResNet. These models were used for feature extraction on all images. Softmax was used as the classification algorithm of the fully connected layer. The ensemble classifier EDL-COVID was obtained via relative majority voting. Finally, the ensemble classifier was compared with three component classifiers to evaluate accuracy, sensitivity, specificity, F value, and Matthews correlation coefficient. The results showed that the overall classification performance of the ensemble model was better than that of the component classifier. The evaluation indexes were also higher. This algorithm can better meet the rapid detection requirements of the novel coronavirus disease COVID-19.", "journal": "Applied soft computing", "date": "2020-11-17", "authors": ["TaoZhou", "HuilingLu", "ZaoliYang", "ShiQiu", "BingqiangHuo", "YaliDong"], "doi": "10.1016/j.asoc.2020.106885\n10.1001/jama.2020.1585\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020200343\n10.1007/s00330-020-06801-0\n10.2214/AJR.20.22954\n10.1148/radiol.2020200370\n10.1007/s11604-020-00956-y\n10.1016/j.jinf.2020.03.007\n10.1007/s11547-020-01179-x\n10.1148/radiol.2020200642\n10.1007/s11604-020-00958-w\n10.1148/radiol.2020200905\n10.1101/2020.03.24.20042317\n10.1155/2018/5264526\n10.1016/j.asoc.2018.11.001\n10.1155/2020/7602384\n10.1155/2020/7653946"}
{"title": "Automated detection of COVID-19 using ensemble of transfer learning with deep convolutional neural network based on CT scans.", "abstract": "COVID-19 has infected millions of people worldwide. One of the most important hurdles in controlling the spread of this disease is the inefficiency and lack of medical tests. Computed tomography (CT) scans are promising in providing accurate and fast detection of COVID-19. However, determining COVID-19 requires highly trained radiologists and suffers from inter-observer variability. To remedy these limitations, this paper introduces an automatic methodology based on an ensemble of deep transfer learning for the detection of COVID-19.\nA total of 15 pre-trained convolutional neural networks (CNNs) architectures: EfficientNets(B0-B5), NasNetLarge, NasNetMobile, InceptionV3, ResNet-50, SeResnet 50, Xception, DenseNet121, ResNext50 and Inception_resnet_v2 are used and then fine-tuned on the target task. After that, we built an ensemble method based on majority voting of the best combination of deep transfer learning outputs to further improve the recognition performance. We have used a publicly available dataset of CT scans, which consists of 349 CT scans labeled as being positive for COVID-19 and 397 negative COVID-19 CT scans that are normal or contain other types of lung diseases.\nThe experimental results indicate that the majority voting of 5 deep transfer learning architecture with EfficientNetB0, EfficientNetB3, EfficientNetB5, Inception_resnet_v2, and Xception has the higher results than the individual transfer learning structure and among the other models based on precision (0.857), recall (0.854) and accuracy (0.85) metrics in diagnosing COVID-19 from CT scans.\nOur study based on an ensemble deep transfer learning system with different pre-trained CNNs architectures can work well on a publicly available dataset of CT images for the diagnosis of COVID-19 based on CT scans.", "journal": "International journal of computer assisted radiology and surgery", "date": "2020-11-17", "authors": ["ParisaGifani", "AhmadShalbaf", "MajidVafaeezadeh"], "doi": "10.1007/s11548-020-02286-w\n10.1148/radiol.2020200463\n10.1016/j.media.2017.07.005\n10.4103/2153-3539.186902\n10.1109/TMI.2016.2553401\n10.1038/s41551-018-0195-0\n10.1038/nature21056\n10.3390/app10020559\n10.1117/1.JMI.3.3.034501\n10.1016/j.jcmg.2019.06.009\n10.1038/s41591-018-0107-6\n10.1038/srep26286\n10.1016/j.eswa.2019.01.060\n10.1016/j.eswa.2020.113514\n10.1016/j.measurement.2019.02.042\n10.1038/s41598-019-56989-5\n10.1145/3065386\n10.1109/TMI.2016.2535302"}
{"title": "Deep learning analysis provides accurate COVID-19 diagnosis on chest computed tomography.", "abstract": "Computed Tomography is an essential diagnostic tool in the management of COVID-19. Considering the large amount of examinations in high case-load scenarios, an automated tool could facilitate and save critical time in the diagnosis and risk stratification of the disease.\nA novel deep learning derived machine learning (ML) classifier was developed using a simplified programming approach and an open source dataset consisting of 6868 chest CT images from 418 patients which was split into training and validation subsets. The diagnostic performance was then evaluated and compared to experienced radiologists on an independent testing dataset. Diagnostic performance metrics were calculated using Receiver Operating Characteristics (ROC) analysis. Operating points with high positive (>10) and low negative (<0.01) likelihood ratios to stratify the risk of COVID-19 being present were identified and validated.\nThe model achieved an overall accuracy of 0.956 (AUC) on an independent testing dataset of 90 patients. Both rule-in and rule out thresholds were identified and tested. At the rule-in operating point, sensitivity and specificity were 84.4 % and 93.3 % and did not differ from both radiologists (p\u202f>\u202f0.05). At the rule-out threshold, sensitivity (100 %) and specificity (60 %) differed significantly from the radiologists (p\u202f<\u202f0.05). Likelihood ratios and a Fagan nomogram provide prevalence independent test performance estimates.\nAccurate diagnosis of COVID-19 using a basic deep learning approach is feasible using open-source CT image data. In addition, the machine learning classifier provided validated rule-in and rule-out criteria could be used to stratify the risk of COVID-19 being present.", "journal": "European journal of radiology", "date": "2020-11-16", "authors": ["DJavor", "HKaplan", "AKaplan", "S BPuchner", "CKrestan", "PBaltzer"], "doi": "10.1016/j.ejrad.2020.109402\n10.1186/s40537-019-0192-5\n10.3390/info11020108\n10.1016/j.ejrad.2019.108774"}
{"title": "Outcomes for Out-of-Hospital Cardiac Arrest in the United States During the Coronavirus Disease 2019 Pandemic.", "abstract": "Recent reports from communities severely affected by the coronavirus disease 2019 (COVID-19) pandemic found lower rates of sustained return of spontaneous circulation (ROSC) for out-of-hospital cardiac arrest (OHCA). Whether the pandemic has affected OHCA outcomes more broadly is unknown.\nTo assess the association between the COVID-19 pandemic and OHCA outcomes, including in areas with low and moderate COVID-19 disease burden.\nThis study used a large US registry of OHCAs to compare outcomes during the pandemic period of March 16 through April 30, 2020, with those from March 16 through April 30, 2019. Cases were geocoded to US counties, and the COVID-19 mortality rate in each county was categorized as very low (0-25 per million residents), low (26-100 per million residents), moderate (101-250 per million residents), high (251-500 per million residents), or very high (>500 per million residents). As additional controls, the study compared OHCA outcomes during the prepandemic period (January through February) and peripandemic period (March 1 through 15).\nThe COVID-19 pandemic.\nSustained ROSC (\u226520 minutes), survival to discharge, and OHCA incidence.\nA total of 19\u202f303 OHCAs occurred from March 16 through April 30 in both years, with 9863 cases in 2020 (mean [SD] age, 62.6 [19.3] years; 6040 men [61.3%]) and 9440 in 2019 (mean [SD] age, 62.2 [19.2] years; 5922 men [62.7%]). During the pandemic, rates of sustained ROSC were lower than in 2019 (23.0% vs 29.8%; adjusted rate ratio, 0.82 [95% CI, 0.78-0.87]; P\u2009<\u2009.001). Sustained ROSC rates were lower by between 21% (286 of 1429 [20.0%] in 2020 vs 305 of 1130 [27.0%] in 2019; adjusted RR, 0.79 [95% CI, 0.65-0.97]) and 33% (149 of 863 [17.3%] in 2020 vs 192 of 667 [28.8%] in 2019; adjusted RR, 0.67 [95% CI, 0.56-0.80]) in communities with high or very high COVID-19 mortality, respectively; however, rates of sustained ROSC were also lower by 11% (583 of 2317 [25.2%] in 2020 vs 740 of 2549 [29.0%] in 2019; adjusted RR, 0.89 [95% CI, 0.81-0.98]) to 15% (889 of 3495 [25.4%] in 2020 vs 1109 of 3532 [31.4%] in 2019; adjusted RR, 0.85 [95% CI, 0.78-0.93]) in communities with very low and low COVID-19 mortality. Among emergency medical services agencies with complete data on hospital survival (7085 total patients), survival to discharge was lower during the pandemic compared with 2019 (6.6% vs 9.8%; adjusted RR, 0.83 [95% CI, 0.69-1.00]; P\u2009=\u2009.048), primarily in communities with moderate to very high COVID-19 mortality (interaction P\u2009=\u2009.049). Incidence of OHCA was higher than in 2019, but the increase was largely observed in communities with high COVID-19 mortality (adjusted mean difference, 38.6 [95% CI, 37.1-40.1] per million residents) and very high COVID-19 mortality (adjusted mean difference, 28.7 [95% CI, 26.7-30.6] per million residents). In contrast, there was no difference in rates of sustained ROSC or survival to discharge during the prepandemic and peripandemic periods in 2020 vs 2019.\nEarly during the pandemic, rates of sustained ROSC for OHCA were lower throughout the US, even in communities with low COVID-19 mortality rates. Overall survival was lower, primarily in communities with moderate or high COVID-19 mortality.", "journal": "JAMA cardiology", "date": "2020-11-15", "authors": ["Paul SChan", "SaketGirotra", "YuanyuanTang", "RababAl-Araji", "Brahmajee KNallamothu", "BryanMcNally"], "doi": "10.1001/jamacardio.2020.6210\n10.1056/NEJMc2010418\n10.1016/S2468-2667(20)30117-1\n10.1016/j.annemergmed.2009.03.018\n10.1161/01.CIR.0000147236.85306.15\n10.1080/03610910902859574\n10.1093/aje/kwh221\n10.1093/aje/kwh090\n10.1056/NEJMc2009166\n10.1016/j.cardfail.2020.05.005\n10.1161/STR.0000000000000347\n10.1161/CIRCULATIONAHA.120.047463"}
{"title": "Analyzing inter-reader variability affecting deep ensemble learning for COVID-19 detection in chest radiographs.", "abstract": "Data-driven deep learning (DL) methods using convolutional neural networks (CNNs) demonstrate promising performance in natural image computer vision tasks. However, their use in medical computer vision tasks faces several limitations, viz., (i) adapting to visual characteristics that are unlike natural images; (ii) modeling random noise during training due to stochastic optimization and backpropagation-based learning strategy; (iii) challenges in explaining DL black-box behavior to support clinical decision-making; and (iv) inter-reader variability in the ground truth (GT) annotations affecting learning and evaluation. This study proposes a systematic approach to address these limitations through application to the pandemic-caused need for Coronavirus disease 2019 (COVID-19) detection using chest X-rays (CXRs). Specifically, our contribution highlights significant benefits obtained through (i) pretraining specific to CXRs in transferring and fine-tuning the learned knowledge toward improving COVID-19 detection performance; (ii) using ensembles of the fine-tuned models to further improve performance over individual constituent models; (iii) performing statistical analyses at various learning stages for validating results; (iv) interpreting learned individual and ensemble model behavior through class-selective relevance mapping (CRM)-based region of interest (ROI) localization; and, (v) analyzing inter-reader variability and ensemble localization performance using Simultaneous Truth and Performance Level Estimation (STAPLE) methods. We find that ensemble approaches markedly improved classification and localization performance, and that inter-reader variability and performance level assessment helps guide algorithm design and parameter optimization. To the best of our knowledge, this is the first study to construct ensembles, perform ensemble-based disease ROI localization, and analyze inter-reader variability and algorithm performance for COVID-19 detection in CXRs.", "journal": "PloS one", "date": "2020-11-13", "authors": ["SivaramakrishnanRajaraman", "SudhirSornapudi", "Philip OAlderson", "Les RFolio", "Sameer KAntani"], "doi": "10.1371/journal.pone.0242301\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020200823\n10.1109/access.2020.3003810\n10.3390/diagnostics10060358\n10.1148/radiol.2020200905\n10.1146/annurev-bioeng-071516-044442\n10.1249/MSS.0000000000001291\n10.1016/j.ejrad.2013.02.018\n10.1109/TMI.2004.828354\n10.1371/journal.pone.0202121\n10.3390/diagnostics9020038\n10.1109/access.2020.2971257\n10.1148/radiol.2017162326\n10.1109/EMBC.2019.8856715\n10.1136/bmj.331.7513.379\n10.4103/0256-4947.60518\n10.1016/j.jrid.2020.05.001\n10.1016/j.cell.2018.02.010\n10.1007/s10278-019-00227-x\n10.1016/j.artint.2014.02.004\n10.1007/s11548-019-01917-1\n10.1017/thg.2017.28\n10.1016/j.jss.2007.02.053\n10.1515/jib-2017-0063\n10.4049/jimmunol.1602077"}
{"title": "COVID-19, AI enthusiasts, and toy datasets: radiology without radiologists.", "abstract": null, "journal": "European radiology", "date": "2020-11-13", "authors": ["H RTizhoosh", "JenniferFratesi"], "doi": "10.1007/s00330-020-07453-w"}
{"title": "COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images.", "abstract": "The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.", "journal": "Scientific reports", "date": "2020-11-13", "authors": ["LindaWang", "Zhong QiuLin", "AlexanderWong"], "doi": "10.1038/s41598-020-76550-z\n10.1148/ryct.2020200034\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.04.003\n10.1016/j.crad.2020.03.008\n10.1177/0846537120924606\n10.1016/j.clinimag.2020.04.001"}
{"title": "Accuracy of Conventional and Machine Learning Enhanced Chest Radiography for the Assessment of COVID-19 Pneumonia: Intra-Individual Comparison with CT.", "abstract": "To evaluate diagnostic accuracy of conventional radiography (CXR) and machine learning enhanced CXR (mlCXR) for the detection and quantification of disease-extent in COVID-19 patients compared to chest-CT.\nReal-time polymerase chain reaction (rt-PCR)-confirmed COVID-19-patients undergoing CXR from March to April 2020 together with COVID-19 negative patients as control group were retrospectively included. Two independent readers assessed CXR and mlCXR images for presence, disease extent and type (consolidation vs. ground-glass opacities (GGOs) of COVID-19-pneumonia. Further, readers had to assign confidence levels to their diagnosis. CT obtained \u2264 36 h from acquisition of CXR served as standard of reference. Inter-reader agreement, sensitivity for detection and disease extent of COVID-19-pneumonia compared to CT was calculated. McNemar test was used to test for significant differences.\nSixty patients (21 females; median age 61 years, range 38-81 years) were included. Inter-reader agreement improved from good to excellent when mlCXR instead of CXR was used (k = 0.831 vs. k = 0.742). Sensitivity for pneumonia detection improved from 79.5% to 92.3%, however, on the cost of specificity 100% vs. 71.4% (\nIn line with the current literature, the sensitivity for detection and quantification of COVID-19-pneumonia was moderate with CXR and could be improved when mlCXR was used for image interpretation.", "journal": "Journal of clinical medicine", "date": "2020-11-12", "authors": ["KatharinaMartini", "ChristianBl\u00fcthgen", "Joan EWalter", "MichaelMesserli", "Thi Dan LinhNguyen-Kim", "ThomasFrauenfelder"], "doi": "10.3390/jcm9113576\n10.1007/s00259-020-04735-9\n10.1148/radiol.2020200843\n10.1016/j.ijsu.2020.05.018\n10.1186/1471-2342-6-8\n10.1371/journal.pone.0174285\n10.4103/1817-1737.49416\n10.2307/2529310\n10.1016/j.ccm.2007.11.005\n10.1183/09031936.01.00213501\n10.1148/radiol.10100791\n10.1148/rg.261055034\n10.1016/j.acra.2009.08.006\n10.1007/s11547-020-01200-3"}
{"title": "AI-driven quantification, staging and outcome prediction of COVID-19 pneumonia.", "abstract": "Coronavirus disease 2019 (COVID-19) emerged in 2019 and disseminated around the world rapidly. Computed tomography (CT) imaging has been proven to be an important tool for screening, disease quantification and staging. The latter is of extreme importance for organizational anticipation (availability of intensive care unit beds, patient management planning) as well as to accelerate drug development through rapid, reproducible and quantified assessment of treatment response. Even if currently there are no specific guidelines for the staging of the patients, CT together with some clinical and biological biomarkers are used. In this study, we collected a multi-center cohort and we investigated the use of medical imaging and artificial intelligence for disease quantification, staging and outcome prediction. Our approach relies on automatic deep learning-based disease quantification using an ensemble of architectures, and a data-driven consensus for the staging and outcome prediction of the patients fusing imaging biomarkers with clinical and biological attributes. Highly promising results on multiple external/independent evaluation cohorts as well as comparisons with expert human readers demonstrate the potentials of our approach.", "journal": "Medical image analysis", "date": "2020-11-11", "authors": ["GuillaumeChassagnon", "MariaVakalopoulou", "EnzoBattistella", "StergiosChristodoulidis", "Trieu-NghiHoang-Thi", "SeverineDangeard", "EricDeutsch", "FabriceAndre", "EnoraGuillo", "NaraHalm", "StefanyEl Hajj", "FlorianBompard", "SophieNeveu", "ChahinezHani", "InesSaab", "Ali\u00e9norCampredon", "HasmikKoulakian", "SouhailBennani", "GaelFreche", "MaximeBarat", "AurelienLombard", "LaureFournier", "HippolyteMonnier", "T\u00e9odorGrand", "JulesGregory", "YannNguyen", "AntoineKhalil", "ElyasMahdjoub", "Pierre-YvesBrillet", "St\u00e9phaneTran Ba", "Val\u00e9rieBousson", "AhmedMekki", "Robert-YvesCarlier", "Marie-PierreRevel", "NikosParagios"], "doi": "10.1016/j.media.2020.101860\n10.1007/s00330-019-06564-3\n10.1007/s00330-020-06817-6\n10.1148/radiol.2020200905\n10.1038/s41591-020-0931-3\n10.1038/s41574-020-0364-6"}
{"title": "COVIDGR Dataset and COVID-SDNet Methodology for Predicting COVID-19 Based on Chest X-Ray Images.", "abstract": "Currently, Coronavirus disease (COVID-19), one of the most infectious diseases in the 21st century, is diagnosed using RT-PCR testing, CT scans and/or Chest X-Ray (CXR) images. CT (Computed Tomography) scanners and RT-PCR testing are not available in most medical centers and hence in many cases CXR images become the most time/cost effective tool for assisting clinicians in making decisions. Deep learning neural networks have a great potential for building COVID-19 triage systems and detecting COVID-19 patients, especially patients with low severity. Unfortunately, current databases do not allow building such systems as they are highly heterogeneous and biased towards severe cases. This article is three-fold: (i) we demystify the high sensitivities achieved by most recent COVID-19 classification models, (ii) under a close collaboration with Hospital Universitario Cl\u00ednico San Cecilio, Granada, Spain, we built COVIDGR-1.0, a homogeneous and balanced database that includes all levels of severity, from normal with Positive RT-PCR, Mild, Moderate to Severe. COVIDGR-1.0 contains 426 positive and 426 negative PA (PosteroAnterior) CXR views and (iii) we propose COVID Smart Data based Network (COVID-SDNet) methodology for improving the generalization capacity of COVID-classification models. Our approach reaches good and stable results with an accuracy of [Formula: see text], [Formula: see text], [Formula: see text] in severe, moderate and mild COVID-19 severity levels. Our approach could help in the early detection of COVID-19. COVIDGR-1.0 along with the severity level labels are available to the scientific community through this link https://dasci.es/es/transferencia/open-data/covidgr/.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-11-11", "authors": ["STabik", "AGomez-Rios", "J LMartin-Rodriguez", "ISevillano-Garcia", "MRey-Area", "DCharte", "EGuirado", "J LSuarez", "JLuengo", "M AValero-Gonzalez", "PGarcia-Villanova", "EOlmedo-Sanchez", "FHerrera"], "doi": "10.1109/JBHI.2020.3037127"}
{"title": "Deep learning and its role in COVID-19 medical imaging.", "abstract": "COVID-19 is one of the greatest global public health challenges in history. COVID-19 is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and is estimated to have an cumulative global case-fatality rate as high as 7.2% (Onder et\u00a0al., 2020) [1]. As the SARS-CoV-2 spread across the globe it catalyzed new urgency in building systems to allow rapid sharing and dissemination of data between international healthcare infrastructures and governments in a worldwide effort focused on case tracking/tracing, identifying effective therapeutic protocols, securing healthcare resources, and in drug and vaccine research. In addition to the worldwide efforts to share clinical and routine population health data, there are many large-scale efforts to collect and disseminate medical imaging data, owing to the critical role that imaging has played in diagnosis and management around the world. Given reported false negative rates of the reverse transcriptase polymerase chain reaction (RT-PCR) of up to 61% (Centers for Disease Control and Prevention, Division of Viral Diseases, 2020; Kucirka et\u00a0al., 2020) [2,3], imaging can be used as an important adjunct or alternative. Furthermore, there has been a shortage of test-kits worldwide and laboratories in many testing sites have struggled to process the available tests within a reasonable time frame. Given these issues surrounding COVID-19, many groups began to explore the benefits of 'big data' processing and algorithms to assist with the diagnosis and therapeutic development of COVID-19.", "journal": "Intelligence-based medicine", "date": "2020-11-11", "authors": ["Sudhen BDesai", "AnujPareek", "Matthew PLungren"], "doi": "10.1016/j.ibmed.2020.100013\n10.1038/nature14539\n10.1016/j.neunet.2014.09.003\n10.1016/j.acra.2018.02.018\n10.1371/journal.pmed.1002686\n10.21105/joss.00747\n10.3389/fmed.2020.00550\n10.1148/radiol.2020201491\n10.1007/s13246-020-00865-4\n10.1109/ACCESS.2020.3016780\n10.1038/s41591-020-0931-3\n10.1148/radiol.2020200463\n10.1016/j.cell.2020.04.045\n10.1109/TMI.2020.3000314\n10.1109/TMI.2020.2996645\n10.1111/anae.15082\n10.1002/jum.15284\n10.1109/TMI.2020.2994459\n10.1109/ACCESS.2020.3010287"}
{"title": "Classification of Severe and Critical Covid-19 Using Deep Learning and Radiomics.", "abstract": "The coronavirus disease 2019 (COVID-19) is rapidly spreading inside China and internationally. We aimed to construct a model integrating information from radiomics and deep learning (DL) features to discriminate critical cases from severe cases of COVID-19 using computed tomography (CT) images.\nWe retrospectively enrolled 217 patients from three centers in China, including 82 patients with severe disease and 135 with critical disease. Patients were randomly divided into a training cohort (n = 174) and a test cohort (n = 43). We extracted 102 3-dimensional radiomic features from automatically segmented lung volume and selected the significant features. We also developed a 3-dimensional DL network based on center-cropped slices. Using multivariable logistic regression, we then created a merged model based on significant radiomic features and DL scores. We employed the area under the receiver operating characteristic curve (AUC) to evaluate the model's performance. We then conducted cross validation, stratified analysis, survival analysis, and decision curve analysis to evaluate the robustness of our method.\nThe merged model can distinguish critical patients with AUCs of 0.909 (95% confidence interval [CI]: 0.859-0.952) and 0.861 (95% CI: 0.753-0.968) in the training and test cohorts, respectively. Stratified analysis indicated that our model was not affected by sex, age, or chronic disease. Moreover, the results of the merged model showed a strong correlation with patient outcomes.\nA model combining radiomic and DL features of the lung could help distinguish critical cases from severe cases of COVID-19.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-11-10", "authors": ["CongLi", "DiDong", "LiangLi", "WeiGong", "XiaohuLi", "YanBai", "MeiyunWang", "ZhenhuaHu", "YunfeiZha", "JieTian"], "doi": "10.1109/JBHI.2020.3036722"}
{"title": "Breakthrough healthcare technologies in the COVID-19 era: a unique opportunity for cardiovascular practitioners and patients.", "abstract": "The Coronavirus disease 2019 (COVID-19) pandemic, caused by symptomatic severe acute respiratory syndrome-Coronavirus-2 (SARS-CoV-2) infection, has wreaked havoc globally, challenging the healthcare, economical, technological and social status quo of developing but also developed countries. For instance, the COVID-19 scare has reduced timely hospital admissions for ST-elevation myocardial infarction in Europe and the USA, causing unnecessary deaths and disabilities. While the emergency is still ongoing, enough efforts have been put to study and tackle this condition such that a comprehensive perspective and synthesis on the potential role of breakthrough healthcare technologies is possible. Indeed, current state-of-the-art information technologies can provide a unique opportunity to adapt and adjust to the current healthcare needs associated with COVID-19, either directly or indirectly, and in particular those of cardiovascular patients and practitioners.\nWe searched several biomedical databases, websites and social media, including PubMed, Medscape, and Twitter, for smartcare approaches suitable for application in the COVID-19 pandemic.\nWe retrieved details on several promising avenues for present and future healthcare technologies, capable of substantially reduce the mortality, morbidity, and resource use burden of COVID-19 as well as that of cardiovascular disease. In particular, we have found data supporting the importance of data sharing, model sharing, preprint archiving, social media, medical case sharing, distance learning and continuous medical education, smartphone apps, telemedicine, robotics, big data analysis, machine learning, and deep learning, with the ultimate goal of optimization of individual prevention, diagnosis, tracing, risk-stratification, treatment and rehabilitation.\nWe are confident that refinement and command of smartcare technologies will prove extremely beneficial in the short-term, but also dramatically reshape cardiovascular practice and healthcare delivery in the long-term future, for COVID-19 as well as other diseases.", "journal": "Panminerva medica", "date": "2020-11-10", "authors": ["RaffaeleNudi", "MarcoCampagna", "AlessioParma", "AndreaNudi", "GiuseppeBiondi Zoccai"], "doi": "10.23736/S0031-0808.20.04188-9"}
{"title": "Using artificial intelligence to assist radiologists in distinguishing COVID-19 from other pulmonary infections.", "abstract": "Accurate and rapid diagnosis of coronavirus disease (COVID-19) is crucial for timely quarantine and treatment.\nIn this study, a deep learning algorithm-based AI model using ResUNet network was developed to evaluate the performance of radiologists with and without AI assistance in distinguishing COVID-19 infected pneumonia patients from other pulmonary infections on CT scans.\nFor model development and validation, a total number of 694 cases with 111,066 CT slides were retrospectively collected as training data and independent test data in the study. Among them, 118 are confirmed COVID-19 infected pneumonia cases and 576 are other pulmonary infection cases (e.g. tuberculosis cases, common pneumonia cases and non-COVID-19 viral pneumonia cases). The cases were divided into training and testing datasets. The independent test was performed by evaluating and comparing the performance of three radiologists with different years of practice experience in distinguishing COVID-19 infected pneumonia cases with and without the AI assistance.\nOur final model achieved an overall test accuracy of 0.914 with an area of the receiver operating characteristic (ROC) curve (AUC) of 0.903 in which the sensitivity and specificity are 0.918 and 0.909, respectively. The deep learning-based model then achieved a comparable performance by improving the radiologists' performance in distinguish COVOD-19 from other pulmonary infections, yielding better average accuracy and sensitivity, from 0.941 to 0.951 and from 0.895 to 0.942, respectively, when compared to radiologists without using AI assistance.\nA deep learning algorithm-based AI model developed in this study successfully improved radiologists' performance in distinguishing COVID-19 from other pulmonary infections using chest CT images.", "journal": "Journal of X-ray science and technology", "date": "2020-11-10", "authors": ["YanhongYang", "Fleming Y MLure", "HengyuanMiao", "ZiqiZhang", "StefanJaeger", "JinxinLiu", "LinGuo"], "doi": "10.3233/XST-200735\n10.1016/j.genhosppsych.2020.03.011"}
{"title": "Deep Learning Applications to Combat Novel Coronavirus (COVID-19) Pandemic.", "abstract": "During this global pandemic, researchers around the world are trying to find out innovative technology for a smart healthcare system to combat coronavirus. The evidence of deep learning applications on the past epidemic inspires the experts by giving a new direction to control this outbreak. The aim of this paper is to discuss the contributions of deep learning at several scales including medical imaging, disease tracing, analysis of protein structure, drug discovery, and virus severity and infectivity to control the ongoing outbreak. A progressive search of the database related to the applications of deep learning was executed on COVID-19. Further, a comprehensive review is done using selective information by assessing the different perspectives of deep learning. This paper attempts to explore and discuss the overall applications of deep learning on multiple dimensions to control novel coronavirus (COVID-19). Though various studies are conducted using deep learning algorithms, there are still some constraints and challenges while applying for real-world problems. The ongoing progress in deep learning contributes to handle coronavirus infection and plays an effective role to develop appropriate solutions. It is expected that this paper would be a great help for the researchers who would like to contribute to the development of remedies\u00a0for this current pandemic in this area.", "journal": "SN computer science", "date": "2020-11-10", "authors": ["AmanullahAsraf", "Md ZabirulIslam", "Md RezwanulHaque", "Md MilonIslam"], "doi": "10.1007/s42979-020-00383-w\n10.1016/j.cmrp.2020.03.011\n10.1080/03772063.2020.1713916\n10.5815/ijieeb.2019.02.03\n10.1007/s42979-020-00305-w\n10.1007/s42979-020-00216-w\n10.1101/2020.08.24.20181339v1\n10.1007/s42979-020-00195-y\n10.18280/ria.330605\n10.1016/j.dsx.2020.05.008\n10.3201/eid1002.030759\n10.1016/j.cmpb.2020.105581\n10.1016/j.ejrad.2020.109041\n10.1101/2020.02.14.20023028\n10.1007/s10489-020-01714-3\n10.1101/2020.03.20.20039834\n10.1101/2020.02.23.20026930\n10.1148/ryct.2020200242\n10.1016/j.chaos.2020.109864\n10.1101/2020.03.25.20043505\n10.1016/S1473-3099(20)30237-1\n10.26434/chemrxiv.11829102.v2\n10.1101/2020.03.03.972133\n10.1016/j.csbj.2020.03.025\n10.1007/s12539-020-00376-6\n10.1101/2020.01.29.925354"}
{"title": "The importance of standardisation - COVID-19 CT & Radiograph Image Data Stock for deep learning purpose.", "abstract": "With the number of affected individuals still growing world-wide, the research on COVID-19 is continuously expanding. The deep learning community concentrates their efforts on exploring if neural networks can potentially support the diagnosis using CT and radiograph images of patients' lungs. The two most popular publicly available datasets for COVID-19 classification are COVID-CT and COVID-19 Image Data Collection. In this work, we propose a new dataset which we call COVID-19 CT & Radiograph Image Data Stock. It contains both CT and radiograph samples of COVID-19 lung findings and combines them with additional data to ensure a sufficient number of diverse COVID-19-negative samples. Moreover, it is supplemented with a carefully defined split. The aim of COVID-19 CT & Radiograph Image Data Stock is to create a public pool of CT and radiograph images of lungs to increase the efficiency of distinguishing COVID-19 disease from other types of pneumonia and from healthy chest. We hope that the creation of this dataset would allow standardisation of the approach taken for training deep neural networks for COVID-19 classification and eventually for building more reliable models.", "journal": "Computers in biology and medicine", "date": "2020-11-09", "authors": ["KrzysztofMisztal", "AgnieszkaPocha", "MartynaDurak-Kozica", "Micha\u0142W\u0105tor", "AleksandraKubica-Misztal", "MarcinHartel"], "doi": "10.1016/j.compbiomed.2020.104092\n10.5281/zeno.do.3723295\n10.5281/zenodo.3723299"}
{"title": "Lung volume reduction and infection localization revealed in Big data CT imaging of COVID-19.", "abstract": "The ongoing worldwide COVID-19 pandemic has become a huge threat to global public health. Using CT image, 3389 COVID-19 patients, 1593 community-acquired pneumonia (CAP) patients, and 1707 nonpneumonia subjects were included to explore the different patterns of lung and lung infection. We found that COVID-19 patients have a significant reduced lung volume with increased density and mass, and the infections tend to present as bilateral lower lobes. The findings provide imaging evidence to improve our understanding of COVID-19.", "journal": "International journal of infectious diseases : IJID : official publication of the International Society for Infectious Diseases", "date": "2020-11-07", "authors": ["FengShi", "YingWei", "LimingXia", "FeiShan", "ZhanhaoMo", "FuhuaYan", "DinggangShen"], "doi": "10.1016/j.ijid.2020.10.095\n10.1148/radiol.2020200823\n10.1016/S0140-6736(20)30183-5\n10.1007/s11604-020-00956-y\n10.2214/AJR.20.22954\n10.1016/S1473-3099(20)30086-4"}
{"title": "A Weakly-Supervised Framework for COVID-19 Classification and Lesion Localization From Chest CT.", "abstract": "Accurate and rapid diagnosis of COVID-19 suspected cases plays a crucial role in timely quarantine and medical treatment. Developing a deep learning-based model for automatic COVID-19 diagnosis on chest CT is helpful to counter the outbreak of SARS-CoV-2. A weakly-supervised deep learning framework was developed using 3D CT volumes for COVID-19 classification and lesion localization. For each patient, the lung region was segmented using a pre-trained UNet; then the segmented 3D lung region was fed into a 3D deep neural network to predict the probability of COVID-19 infectious; the COVID-19 lesions are localized by combining the activation regions in the classification network and the unsupervised connected components. 499 CT volumes were used for training and 131 CT volumes were used for testing. Our algorithm obtained 0.959 ROC AUC and 0.976 PR AUC. When using a probability threshold of 0.5 to classify COVID-positive and COVID-negative, the algorithm obtained an accuracy of 0.901, a positive predictive value of 0.840 and a very high negative predictive value of 0.982. The algorithm took only 1.93 seconds to process a single patient's CT volume using a dedicated GPU. Our weakly-supervised deep learning model can accurately predict the COVID-19 infectious probability and discover lesion regions in chest CT without the need for annotating the lesions for training. The easily-trained and high-performance deep learning algorithm provides a fast way to identify COVID-19 patients, which is beneficial to control the outbreak of SARS-CoV-2. The developed deep learning software is available at https://github.com/sydney0zq/covid-19-detection.", "journal": "IEEE transactions on medical imaging", "date": "2020-11-07", "authors": ["XinggangWang", "XianboDeng", "QingFu", "QiangZhou", "JiapeiFeng", "HuiMa", "WenyuLiu", "ChuanshengZheng"], "doi": "10.1109/TMI.2020.2995965"}
{"title": "Deep learning-based model for detecting 2019 novel coronavirus pneumonia on high-resolution computed tomography.", "abstract": "Computed tomography (CT) is the preferred imaging method for diagnosing 2019 novel coronavirus (COVID19) pneumonia. We aimed to construct a system based on deep learning for detecting COVID-19 pneumonia on high resolution CT. For model development and validation, 46,096 anonymous images from 106 admitted patients, including 51 patients of laboratory confirmed COVID-19 pneumonia and 55 control patients of other diseases in Renmin Hospital of Wuhan University were retrospectively collected. Twenty-seven prospective consecutive patients in Renmin Hospital of Wuhan University were collected to evaluate the efficiency of radiologists against 2019-CoV pneumonia with that of the model. An external test was conducted in Qianjiang Central Hospital to estimate the system's robustness. The model achieved a per-patient accuracy of 95.24% and a per-image accuracy of 98.85% in internal retrospective dataset. For 27 internal prospective patients, the system achieved a comparable performance to that of expert radiologist. In external dataset, it achieved an accuracy of 96%. With the assistance of the model, the reading time of radiologists was greatly decreased by 65%. The deep learning model showed a comparable performance with expert radiologist, and greatly improved the efficiency of radiologists in clinical practice.", "journal": "Scientific reports", "date": "2020-11-07", "authors": ["JunChen", "LianlianWu", "JunZhang", "LiangZhang", "DexinGong", "YilinZhao", "QiuxiangChen", "ShulanHuang", "MingYang", "XiaoYang", "ShanHu", "YongguiWang", "XiaoHu", "BiqingZheng", "KuoZhang", "HuilingWu", "ZehuaDong", "YoumingXu", "YijieZhu", "XiChen", "MengjiaoZhang", "LileiYu", "FanCheng", "HonggangYu"], "doi": "10.1038/s41598-020-76282-0\n10.1016/j.ijid.2020.01.009\n10.1056/NEJMoa2001191\n10.1056/NEJMc2001468\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(19)32501-2\n10.1016/S2468-1253(19)30413-3\n10.1136/gutjnl-2018-317366\n10.1016/j.gie.2019.09.016\n10.1016/j.gie.2019.11.026\n10.1055/a-0855-3532\n10.1016/j.jhin.2020.01.010\n10.3390/jcm9020462\n10.2807/1560-7917.ES.2020.25.4.2000058\n10.1038/s41591-019-0447-x\n10.1016/j.acra.2019.05.018\n10.1007/s12519-020-00345-5"}
{"title": "Artificial Intelligence ECG to Detect Left Ventricular Dysfunction in COVID-19: A Case Series.", "abstract": "Coronavirus disease 2019 (COVID-19) can result in deterioration of cardiac function, which is associated with high mortality. A simple point-of-care diagnostic test to screen for ventricular dysfunction would be clinically useful to guide management. We sought to review the clinical experience with an artificial intelligence electrocardiogram (AI ECG) to screen for ventricular dysfunction in patients with documented COVID-19. We examined all patients in the Mayo Clinic system who underwent clinically indicated electrocardiography and echocardiography within 2 weeks following a positive COVID-19 test and had permitted use of their data for research were included. Of the 27 patients who met the inclusion criteria, one had a history of normal ventricular function who developed COVID-19 myocarditis with rapid clinical decline. The initial AI ECG in this patient indicated normal ventricular function. Repeat AI ECG showed a probability of ejection fraction (EF) less than or equal to 40% of 90.2%, corroborated with an echocardiographic EF of 35%. One other patient had a pre-existing EF less than or equal to 40%, accurately detected by the algorithm before and after COVID-19 diagnosis, and another was found to have a low EF by AI ECG and echocardiography with the COVID-19 diagnosis. The area under the curve for detection of EF less than or equal to 40% was 0.95. This case series suggests that the AI ECG, previously shown to detect ventricular dysfunction in a large general population, may be useful as a screening tool for the detection of cardiac dysfunction in patients with COVID-19.", "journal": "Mayo Clinic proceedings", "date": "2020-11-07", "authors": ["Zachi IAttia", "SurajKapa", "Peter ANoseworthy", "FranciscoLopez-Jimenez", "Paul AFriedman"], "doi": "10.1016/j.mayocp.2020.09.020"}
{"title": "Streamlining follicular monitoring during controlled ovarian stimulation: a data-driven approach to efficient IVF care in the new era of social distancing.", "abstract": "What is the optimal follicular tracking strategy for controlled ovarian stimulation (COS) in order to minimise face-to-face interactions?\nAs data from follicular tracking scans on Days 5, 6 or 7 of stimulation are the most useful to accurately predict trigger timing and risk of over-response, scans on these days should be prioritised if streamlined monitoring is necessary.\nBritish Fertility Society guidance for centres restarting ART following coronavirus disease 2019 (COVID-19) pandemic-related shutdowns recommends reducing the number of patient visits for monitoring during COS. Current evidence on optimal monitoring during ovarian stimulation is sparse, and protocols vary significantly. Small studies of simplifying IVF therapy by minimising monitoring have reported no adverse effects on outcomes, including live birth rate. There are opportunities to learn from the adaptations necessary during these extraordinary times to improve the efficiency of IVF care in the longer term.\nA retrospective database analysis of 9294 ultrasound scans performed during monitoring of 2322 IVF cycles undertaken by 1875 women in a single centre was performed. The primary objective was to identify when in the IVF cycle the data obtained from ultrasound are most predictive of both oocyte maturation trigger timing and an over-response to stimulation. If a reduced frequency of clinic visits is needed due to COVID-19 precautions, prioritising attendance for monitoring scans on the most predictive cycle days may be prudent.\nThe study comprised anonymised retrospective database analysis of IVF/ICSI cycles at a tertiary referral IVF centre. Machine learning models are used in combining demographic and follicular tracking data to predict cycle oocyte maturation trigger timing and over-response. The primary outcome was the day or days in cycle from which scan data yield optimal model prediction performance statistics. The model for predicting trigger day uses patient age, number of follicles at baseline scan and follicle count by size for the current scan. The model to predict over-response uses age and number of follicles of a given size.\nThe earliest cycle day for which our model has high accuracy to predict both trigger day and risk of over-response is stimulation Day 5. The Day 5 model to predict trigger date has a mean squared error 2.16 \u00b1 0.12 and to predict over-response an area under the receiver operating characteristic curve 0.91 \u00b1 0.01.\nThis is a retrospective single-centre study and the results may not be generalisable to centres using different treatment protocols. The results are derived from modelling, and further clinical validation studies will verify the accuracy of the model.\nFollicular tracking starting at Day 5 of stimulation may help to streamline the amount of monitoring required in COS. Previous small studies have shown that minimal monitoring protocols did not adversely impact outcomes. If IVF can safely be made less onerous on the clinic's resources and patient's time, without compromising success, this could help to reduce burden-related treatment drop-out.\nF.P.C. acknowledges funding from the NIHR Applied Research Collaboration Wessex. The authors declare they have no competing interests in relation to this work.\nN/A.", "journal": "Human reproduction (Oxford, England)", "date": "2020-11-05", "authors": ["IRobertson", "F PChmiel", "YCheong"], "doi": "10.1093/humrep/deaa251\n10.1093/hropen/hoz038"}
{"title": "CT and clinical assessment in asymptomatic and pre-symptomatic patients with early SARS-CoV-2 in outbreak settings.", "abstract": "The early infection dynamics of patients with SARS-CoV-2 are not well understood. We aimed to investigate and characterize associations between clinical, laboratory, and imaging features of asymptomatic and pre-symptomatic patients with SARS-CoV-2.\nSeventy-four patients with RT-PCR-proven SARS-CoV-2 infection were asymptomatic at presentation. All were retrospectively identified from 825 patients with chest CT scans and positive RT-PCR following exposure or travel risks in outbreak settings in Japan and China. CTs were obtained for every patient within a day of admission and were reviewed for infiltrate subtypes and percent with assistance from a deep learning tool. Correlations of clinical, laboratory, and imaging features were analyzed and comparisons were performed using univariate and multivariate logistic regression.\nForty-eight of 74 (65%) initially asymptomatic patients had CT infiltrates that pre-dated symptom onset by 3.8 days. The most common CT infiltrates were ground glass opacities (45/48; 94%) and consolidation (22/48; 46%). Patient body temperature (p < 0.01), CRP (p < 0.01), and KL-6 (p = 0.02) were associated with the presence of CT infiltrates. Infiltrate volume (p = 0.01), percent lung involvement (p = 0.01), and consolidation (p = 0.043) were associated with subsequent development of symptoms.\nCOVID-19 CT infiltrates pre-dated symptoms in two-thirds of patients. Body temperature elevation and laboratory evaluations may identify asymptomatic patients with SARS-CoV-2 CT infiltrates at presentation, and the characteristics of CT infiltrates could help identify asymptomatic SARS-CoV-2 patients who subsequently develop symptoms. The role of chest CT in COVID-19 may be illuminated by a better understanding of CT infiltrates in patients with early disease or SARS-CoV-2 exposure.\n\u2022 Forty-eight of 74 (65%) pre-selected asymptomatic patients with SARS-CoV-2 had abnormal chest CT findings. \u2022 CT infiltrates pre-dated symptom onset by 3.8 days (range 1-5). \u2022 KL-6, CRP, and elevated body temperature identified patients with CT infiltrates. Higher infiltrate volume, percent lung involvement, and pulmonary consolidation identified patients who developed symptoms.", "journal": "European radiology", "date": "2020-11-05", "authors": ["NicoleVarble", "MaximeBlain", "MichaelKassin", "ShengXu", "Evrim BTurkbey", "AmelAmalou", "DilaraLong", "StephanieHarmon", "ThomasSanford", "DongYang", "ZiyueXu", "DaguangXu", "MonaFlores", "PengAn", "GianpaoloCarrafiello", "HirofumiObinata", "HitoshiMori", "KakuTamura", "Ashkan AMalayeri", "Steven MHolland", "TaraPalmore", "KaiyuanSun", "BarisTurkbey", "Bradford JWood"], "doi": "10.1007/s00330-020-07401-8\n10.1056/NEJMoa2001316\n10.1016/S1473-3099(20)30114-6\n10.1056/NEJMc2001737\n10.1016/S1473-3099(20)30086-4\n10.2807/1560-7917.ES.2020.25.10.2000180\n10.1148/radiol.2020200642\n10.1086/652241\n10.1016/S0140-6736(20)30566-3\n10.1016/j.crad.2020.03.008\n10.1148/ryct.2020200092\n10.1148/ryct.2020200196"}
{"title": "The study of automatic machine learning base on radiomics of non-focus area in the first chest CT of different clinical types of COVID-19 pneumonia.", "abstract": "To explore the possibility of predicting the clinical types of Corona-Virus-Disease-2019 (COVID-19) pneumonia by analyzing the non-focus area of the lung in the first chest CT image of patients with COVID-19 by using automatic machine learning (Auto-ML). 136 moderate and 83 severe patients were selected from the patients with COVID-19 pneumonia. The clinical and laboratory data were collected for statistical analysis. The texture features of the Non-focus area of the first chest CT of patients with COVID-19 pneumonia were extracted, and then the classification model of the first chest CT of COVID-19 pneumonia was constructed by using these texture features based on the Auto-ML method of radiomics, The area under curve(AUC), true positive rate(TPR), true negative rate (TNR), positive predictive value(PPV) and negative predictive value (NPV) of the operating characteristic curve (ROC) were used to evaluate the accuracy of the first chest CT image classification model in patients with COVID-19 pneumonia. The TPR, TNR, PPV, NPV and AUC of the training cohort and test cohort of the moderate group and the control group, the severe group and the control group, the moderate group and the severe group were all greater than 95% and 0.95 respectively. The non-focus area of the first CT image of COVID-19 pneumonia has obvious difference in different clinical types. The AUTO-ML classification model of Radiomics based on this difference can be used to predict the clinical types of COVID-19 pneumonia.", "journal": "Scientific reports", "date": "2020-11-05", "authors": ["Hui-BinTan", "FeiXiong", "Yuan-LiangJiang", "Wen-CaiHuang", "YeWang", "Han-HanLi", "TaoYou", "Ting-TingFu", "RanLu", "Bi-WenPeng"], "doi": "10.1038/s41598-020-76141-y\n10.1111/tmi.13383\n10.1056/NEJMoa2001017\n10.2214/AJR.20.22959\n10.1148/radiol.2020200230\n10.3233/xst-200689\n10.1016/j.acra.2020.03.003\n10.1016/j.ejrad.2020.108961\n10.2214/ajr.20.22975\n10.2214/ajr.17.18384\n10.1038/nrclinonc.2017.141\n10.1016/j.ijrobp.2014.11.030\n10.1016/j.chest.2016.03.001\n10.1117/1.Jmi.7.1.014504\n10.5152/dir.2019.19321\n10.1093/bioinformatics/btz470\n10.1093/bioinformatics/btz796\n10.1093/neuonc/noz184\n10.5858/arpa.2016-0427-OA\n10.1055/s-0036-1584801\n10.1016/j.ccm.2016.11.013\n10.1378/chest.11-1062"}
{"title": "Lower Circulating Interferon-Gamma Is a Risk Factor for Lung Fibrosis in COVID-19 Patients.", "abstract": "Cytokine storm resulting from SARS-CoV-2 infection is one of the leading causes of acute respiratory distress syndrome (ARDS) and lung fibrosis. We investigated the effect of inflammatory molecules to identify any marker that is related to lung fibrosis in coronavirus disease 2019 (COVID-19). Seventy-six COVID-19 patients who were admitted to Youan Hospital between January 21 and March 20, 2020 and recovered were recruited for this study. Pulmonary fibrosis, represented as fibrotic volume on chest CT images, was computed by an artificial intelligence (AI)-assisted program. Plasma samples were collected from the participants shortly after admission, to measure the basal inflammatory molecules levels. At discharge, fibrosis was present in 46 (60.5%) patients whose plasma interferon-\u03b3 (IFN-\u03b3) levels were twofold lower than those without fibrosis (", "journal": "Frontiers in immunology", "date": "2020-11-03", "authors": ["Zhong-JieHu", "JiaXu", "Ji-MingYin", "LiLi", "WeiHou", "Li-LiZhang", "ZhenZhou", "Yi-ZhouYu", "Hong-JunLi", "Ying-MeiFeng", "Rong-HuaJin"], "doi": "10.3389/fimmu.2020.585647\n10.1016/j.cmi.2020.03.026\n10.1016/j.tmaid.2020.101623\n10.1016/s2213-260070053-5\n10.1183/13993003.01217-2020\n10.1016/s2213-260030225-3\n10.1016/j.cell.2020.02.052\n10.1038/s41586-020-2180-5\n10.1016/s2213-260030076-x\n10.1084/jem.20200652\n10.1007/978-3-319-24574-4_28\n10.1093/ajcp/aqaa052\n10.7326/0003-4819-150-9-200905050-00006\n10.7150/thno.46465\n10.1117/1.jei.27.1.013018\n10.1111/codi.15138\n10.1016/j.cell.2020.04.042\n10.1038/s41591-020-0868-6\n10.1038/s41577-020-0308-3\n10.1016/j.ebiom.2020.102763\n10.1172/JCI137244\n10.1111/luts.12312\n10.1016/j.jaci.2020.04.027\n10.23812/CONTI-E\n10.1002/jmv.25948\n10.1038/nrneph.2016.48\n10.1080/08830180902978120\n10.1681/ASN.2016030306\n10.1165/rcmb.2003-0374OC"}
{"title": "A deep transfer learning model with classical data augmentation and CGAN to detect COVID-19 from chest CT radiography digital images.", "abstract": "The Coronavirus disease 2019 (COVID-19) is the fastest transmittable virus caused by severe acute respiratory syndrome Coronavirus 2 (SARS-CoV-2). The detection of COVID-19 using artificial intelligence techniques and especially deep learning will help to detect this virus in early stages which will reflect in increasing the opportunities of fast recovery of patients worldwide. This will lead to release the pressure off the healthcare system around the world. In this research, classical data augmentation techniques along with Conditional Generative Adversarial Nets (CGAN) based on a deep transfer learning model for COVID-19 detection in chest CT scan images will be presented. The limited benchmark datasets for COVID-19 especially in chest CT images are the main motivation of this research. The main idea is to collect all the possible images for COVID-19 that exists until the very writing of this research and use the classical data augmentations along with CGAN to generate more images to help in the detection of the COVID-19. In this study, five different deep convolutional neural network-based models (AlexNet, VGGNet16, VGGNet19, GoogleNet, and ResNet50) have been selected for the investigation to detect the Coronavirus-infected patient using chest CT radiographs digital images. The classical data augmentations along with CGAN improve the performance of classification in all selected deep transfer models. The outcomes show that ResNet50 is the most appropriate deep learning model to detect the COVID-19 from limited chest CT dataset using the classical data augmentation with testing accuracy of 82.91%, sensitivity 77.66%, and specificity of 87.62%.", "journal": "Neural computing & applications", "date": "2020-11-03", "authors": ["MohamedLoey", "GunasekaranManogaran", "Nour Eldeen MKhalifa"], "doi": "10.1007/s00521-020-05437-x\n10.1016/j.tmrv.2020.02.003\n10.1007/s12098-020-03263-6\n10.1016/j.ijantimicag.2020.105924\n10.1016/S1473-3099(20)30063-3\n10.1001/jama.2020.3864\n10.1016/j.jare.2020.03.005\n10.3390/pathogens9030231\n10.1038/s41579-020-0336-9\n10.1038/s41586-020-2169-0\n10.1002/jmv.25699\n10.1056/NEJMoa2001191\n10.1056/NEJMc2001468\n10.1056/NEJMc2001272\n10.1016/S1473-3099(20)30067-0\n10.1016/j.compag.2019.05.019\n10.1016/j.zemedi.2018.11.002\n10.1016/j.zemedi.2018.12.003\n10.1109/5.726791\n10.3390/sym12040651\n10.1097/RLI.0000000000000574\n10.1038/s41598-019-56989-5"}
{"title": "Misleading Public Statements About COVID-19.", "abstract": null, "journal": "Journal of the American College of Radiology : JACR", "date": "2020-11-02", "authors": ["Carolyn CMeltzer", "Richard HWiggins", "MahmudMossa-Basha", "SusanPalasis", "EricRussell", "DavidMikulis", "PatriciaRhyner", "JamesAnderson", "Ryan BPeterson", "JamesSmirniotopoulos", "A JamesBarkovich", "Robert DZimmerman", "Christopher GFilippi", "Howard ARowley", "Nicholas AKoontz", "Ann KJay", "JoshuaNickerson", "BronwynHamilton", "DanielChow", "Christopher TWhitlow"], "doi": "10.1016/j.jacr.2020.10.012"}
{"title": "[Role of Chest Radiographs and CT Scans and the Application of Artificial Intelligence in Coronavirus Disease 2019].", "abstract": "Coronavirus disease (COVID-19) has threatened public health as a global pandemic. Chest CT and radiography are crucial in managing COVID-19 in addition to reverse transcription-polymerase chain reaction, which is the gold standard for COVID-19 diagnosis. This is a review of the current status of the use of chest CT and radiography in COVID-19 diagnosis and management and an introduction of early representative studies on the application of artificial intelligence to chest CT and radiography. The authors also share their experiences to provide insights into the future value of artificial intelligence.\n\ucf54\ub85c\ub098\ubc14\uc774\ub7ec\uc2a4\uac10\uc5fc\uc99d-19 (coronavirus disease 2019; \uc774\ud558 COVID-19)\ub294 \uc804 \uc138\uacc4\uc801 \ub300\uc720\ud589 \uc9c8\ud658\uc73c\ub85c \uc778\ub958 \ubcf4\uac74\uc744 \uc704\ud611\ud558\uace0 \uc788\ub2e4. \ud749\ubd80 CT \ubc0f \ud749\ubd80X\uc120\uc0ac\uc9c4\uc740 COVID-19\uc758 \ud45c\uc900 \uc9c4\ub2e8\uac80\uc0ac\uc778 \uc5ed\uc804\uc0ac \uc911\ud569\ud6a8\uc18c \uc5f0\uc1c4\ubc18\uc751\uc5d0 \ub354\ud558\uc5ec COVID-19 \uc9c4\ub2e8 \ubc0f \uc911\uc99d\ub3c4 \ud3c9\uac00\uc5d0\uc11c \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud558\uace0 \uc788\ub2e4. \ubcf8 \uc885\uc124\uc5d0\uc11c\ub294 \ud749\ubd80 CT \ubc0f \ud749\ubd80X\uc120\uc0ac\uc9c4\uc758 COVID-19 \ud3d0\ub834\uc5d0 \ub300\ud55c \ud604\uc7ac \uc5ed\ud560\uc5d0 \ub300\ud558\uc5ec \uc0b4\ud3b4\ubcf4\uace0 \uc778\uacf5\uc9c0\ub2a5\uc744 \uc801\uc6a9\ud55c \ub300\ud45c\uc801 \ucd08\uae30 \uc5f0\uad6c\ub4e4\uacfc \uc800\uc790\ub4e4\uc758 \uacbd\ud5d8\uc744 \uc18c\uac1c\ud568\uc73c\ub85c\uc368 \ud5a5\ud6c4 \ud65c\uc6a9\uac00\uce58\uc5d0 \ub300\ud574 \uc0b4\ud3b4\ubcf4\uace0\uc790 \ud55c\ub2e4.", "journal": "Taehan Yongsang Uihakhoe chi", "date": "2020-11-01", "authors": ["Seung-JinYoo", "Jin MoGoo", "Soon HoYoon"], "doi": "10.3348/jksr.2020.0138\n10.1007/s00330-020-07013-2\n10.1148/radiol.2020201754\n10.1007/s00330-020-07035-w\n10.1148/radiol.2020200905\n10.21203/rs.3.rs-48290/v1"}
{"title": "Dual-branch combination network (DCN): Towards accurate diagnosis and lesion segmentation of COVID-19 using CT images.", "abstract": "The recent global outbreak and spread of coronavirus disease (COVID-19) makes it an imperative to develop accurate and efficient diagnostic tools for the disease as medical resources are getting increasingly constrained. Artificial intelligence (AI)-aided tools have exhibited desirable potential; for example, chest computed tomography (CT) has been demonstrated to play a major role in the diagnosis and evaluation of COVID-19. However, developing a CT-based AI diagnostic system for the disease detection has faced considerable challenges, which is mainly due to the lack of adequate manually-delineated samples for training, as well as the requirement of sufficient sensitivity to subtle lesions in the early infection stages. In this study, we developed a dual-branch combination network (DCN) for COVID-19 diagnosis that can simultaneously achieve individual-level classification and lesion segmentation. To focus the classification branch more intensively on the lesion areas, a novel lesion attention module was developed to integrate the intermediate segmentation results. Furthermore, to manage the potential influence of different imaging parameters from individual facilities, a slice probability mapping method was proposed to learn the transformation from slice-level to individual-level classification. We conducted experiments on a large dataset of 1202 subjects from ten institutes in China. The results demonstrated that 1) the proposed DCN attained a classification accuracy of 96.74% on the internal dataset and 92.87% on the external validation dataset, thereby outperforming other models; 2) DCN obtained comparable performance with fewer samples and exhibited higher sensitivity, especially in subtle lesion detection; and 3) DCN provided good interpretability on the loci of infection compared to other deep models due to its classification guided by high-level semantic information. An online CT-based diagnostic platform for COVID-19 derived from our proposed framework is now available.", "journal": "Medical image analysis", "date": "2020-11-01", "authors": ["KaiGao", "JianpoSu", "ZhongbiaoJiang", "Ling-LiZeng", "ZhichaoFeng", "HuiShen", "PengfeiRong", "XinXu", "JianQin", "YuexiangYang", "WeiWang", "DewenHu"], "doi": "10.1016/j.media.2020.101836\n10.1148/radiol.2020200642\n10.1016/j.neunet.2018.07.011\n10.1016/S0140-6736(20)30154-9\n10.1101/2020.02.25.20021568\n10.1109/TPAMI.2017.2699184\n10.1088/1361-6560/ab3103\n10.1148/radiol.2020200230\n10.1148/radiol.2020200432\n10.1109/CVPR.2019.00326\n10.1109/CVPR.2016.90\n10.1109/TPAMI.2019.2913372\n10.1016/S0140-6736(20)30183-5\n10.1109/CVPR.2017.243\n10.1007/s10439-020-02502-3\n10.1016/j.acra.2004.06.005\n10.1101/2020.03.19.20039354\n10.1038/nature14539\n10.1016/j.media.2019.101628\n10.1148/radiol.2020200905\n10.1016/j.media.2019.101595\n10.1109/83.951537\n10.1016/j.media.2017.07.005\n10.1109/CVPR.2015.7298965\n10.1148/radiol.2017161659\n10.1118/1.3611983\n10.1109/3dv.2016.79\n10.1007/978-3-319-24574-4_28\n10.1016/j.idm.2020.02.002\n10.1146/annurev-bioeng-071516-044442\n10.1109/RBME.2020.2987975\n10.1016/S1473-3099(20)30086-4\n10.1109/CVPR.2017.683\n10.1109/cvpr.2018.00813\n10.1101/2020.02.27.20028027\n10.1016/j.cell.2020.04.045\n10.1109/TCI.2016.2644865\n10.1109/CVPR.2017.660\n10.1101/2020.03.12.20027185"}
{"title": "Plasma Proteomics Identify Biomarkers and Pathogenesis of COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) pandemic is a global public health crisis. However, little is known about the pathogenesis and biomarkers of COVID-19. Here, we profiled host responses to COVID-19 by performing plasma proteomics of a cohort of COVID-19 patients, including non-survivors and survivors recovered from mild or severe symptoms, and uncovered numerous COVID-19-associated alterations of plasma proteins. We developed a machine-learning-based pipeline to identify 11 proteins as biomarkers and a set of biomarker combinations, which were validated by an independent cohort and accurately distinguished and predicted COVID-19 outcomes. Some of the biomarkers were further validated by enzyme-linked immunosorbent assay (ELISA) using a larger cohort. These markedly altered proteins, including the biomarkers, mediate pathophysiological pathways, such as immune or inflammatory responses, platelet degranulation and coagulation, and metabolism, that likely contribute to the pathogenesis. Our findings provide valuable knowledge about COVID-19 biomarkers and shed light on the pathogenesis and potential therapeutic targets of COVID-19.", "journal": "Immunity", "date": "2020-11-01", "authors": ["TingShu", "WanshanNing", "DiWu", "JiqianXu", "QiangqiangHan", "MuhanHuang", "XiaojingZou", "QingyuYang", "YangYuan", "YuanyuanBie", "ShangwenPan", "JingfangMu", "YangHan", "XiaoboYang", "HongZhou", "RuitingLi", "YujieRen", "XiChen", "ShanglongYao", "YangQiu", "Ding-YuZhang", "YuXue", "YouShang", "XiZhou"], "doi": "10.1016/j.immuni.2020.10.008\n10.1093/bib/bbaa038"}
{"title": "Computed tomography semi-automated lung volume quantification in SARS-CoV-2-related pneumonia.", "abstract": "To evaluate a semi-automated segmentation and ventilated lung quantification on chest computed tomography\u00a0(CT) to assess lung involvement in patients affected by SARS-CoV-2. Results were compared with clinical and functional parameters and outcomes.\nAll images underwent quantitative analyses with a dedicated workstation using a semi-automatic lung segmentation software to compute ventilated lung volume (VLV), Ground-glass opacity (GGO) volume (GGO-V), and consolidation volume (CONS-V) as absolute volume and as a percentage of total lung volume (TLV). The ratio between CONS-V, GGO-V, and\u00a0VLV (CONS-V/VLV and GGO-V/VLV, respectively), TLV (CONS-V/TLV, GGO-V/TLV, and GGO-V + CONS-V/TLV respectively), and the ratio between VLV and TLV (VLV/TLV) were calculated.\nA total of 108 patients were enrolled. GGO-V/TLV significantly correlated with WBC (r\u2009=\u20090.369), neutrophils (r\u2009=\u20090.446), platelets (r\u2009=\u20090.182), CRP (r\u2009=\u20090.190), PaCO\nThe use of quantitative semi-automated algorithm for lung CT elaboration effectively correlates the severity of SARS-CoV-2-related pneumonia with laboratory parameters and the need for invasive ventilation.\n\u2022 Pathological lung volumes, expressed both as GGO-V and as CONS-V, can be considered a useful tool in SARS-CoV-2-related pneumonia. \u2022 All lung volumes, expressed themselves and as ratio with TLV and VLV, correlate with laboratory data, in particular C-reactive protein and white blood cell count. \u2022 All lung volumes correlate with patient's outcome, in particular concerning invasive ventilation.", "journal": "European radiology", "date": "2020-10-31", "authors": ["DavideIppolito", "MariaRagusi", "DavideGandola", "CesareMaino", "AnnaPecorelli", "SimoneTerrani", "MartaPeroni", "TeresaGiandola", "MarcoPorta", "CammilloTalei Franzesi", "SandroSironi"], "doi": "10.1007/s00330-020-07271-0\n10.1007/s00134-020-05979-7\n10.1097/RTI.0b013e31829f6796\n10.1055/s-0030-1254068\n10.1259/bjr.20170644\n10.21037/jtd.2017.08.17\n10.1097/MD.0000000000005494\n10.1371/journal.pone.0151498\n10.1148/radiol.2462070712\n10.21037/qims-20-564\n10.1007/s12098-020-03263-6\n10.2214/AJR.20.22975"}
{"title": "FLANNEL (Focal Loss bAsed Neural Network EnsembLe) for COVID-19 detection.", "abstract": "The study sought to test the possibility of differentiating chest x-ray images of coronavirus disease 2019 (COVID-19) against other pneumonia and healthy patients using deep neural networks.\nWe construct the radiography (x-ray) imaging data from 2 publicly available sources, which include 5508 chest x-ray images across 2874 patients with 4 classes: normal, bacterial pneumonia, non-COVID-19 viral pneumonia, and COVID-19. To identify COVID-19, we propose a FLANNEL (Focal Loss bAsed Neural Network EnsembLe) model, a flexible module to ensemble several convolutional neural network models and fuse with a focal loss for accurate COVID-19 detection on class imbalance data.\nFLANNEL consistently outperforms baseline models on COVID-19 identification task in all metrics. Compared with the best baseline, FLANNEL shows a higher macro-F1 score, with 6% relative increase on the COVID-19 identification task, in which it achieves precision of 0.7833 \u00b1 0.07, recall of 0.8609 \u00b1 0.03, and F1 score of 0.8168 \u00b1 0.03.\nEnsemble learning that combines multiple independent basis classifiers can increase the robustness and accuracy. We propose a neural weighing module to learn the importance weight for each base model and combine them via weighted ensemble to get the final classification results. In order to handle the class imbalance challenge, we adapt focal loss to our multiple classification task as the loss function.\nFLANNEL effectively combines state-of-the-art convolutional neural network classification models and tackles class imbalance with focal loss to achieve better performance on COVID-19 detection from x-rays.", "journal": "Journal of the American Medical Informatics Association : JAMIA", "date": "2020-10-31", "authors": ["ZhiQiao", "AustinBae", "Lucas MGlass", "CaoXiao", "JimengSun"], "doi": "10.1093/jamia/ocaa280"}
{"title": "Multi-band MR fingerprinting (MRF) ASL imaging using artificial-neural-network trained with high-fidelity experimental data.", "abstract": "We aim to leverage the power of deep-learning with high-fidelity training data to improve the reliability and processing speed of hemodynamic mapping with MR fingerprinting (MRF) arterial spin labeling (ASL).\nA total of 15 healthy subjects were studied on a 3T MRI. Each subject underwent 10 runs of a multi-band multi-slice MRF-ASL sequence for a total scan time of approximately 40 min. MRF-ASL images were averaged across runs to yield a set of high-fidelity data. Training of a fully connected artificial neural network (ANN) was then performed using these data. The results from ANN were compared to those of dictionary matching (DM), ANN trained with single-run experimental data and with simulation data. Initial clinical performance of the technique was also demonstrated in a Moyamoya patient.\nThe use of ANN reduced the processing time of MRF-ASL data to 3.6 s, compared to DM of 3 h 12 min. Parametric values obtained with ANN and DM were strongly correlated (R\nDeep-learning-based parametric reconstruction improves the reliability of MRF-ASL hemodynamic maps and reduces processing time.", "journal": "Magnetic resonance in medicine", "date": "2020-10-28", "authors": ["HongliFan", "PanSu", "JudyHuang", "PeiyingLiu", "HanzhangLu"], "doi": "10.1002/mrm.28560"}
{"title": "Artificial Intelligence for clinical decision support in Critical Care, required and accelerated by COVID-19.", "abstract": null, "journal": "Anaesthesia, critical care & pain medicine", "date": "2020-10-26", "authors": ["MiiaJansson", "JuanjoRubio", "RicardGavald\u00e0", "JordiRello"], "doi": "10.1016/j.accpm.2020.09.010\n10.1016/j.accpm.2018.09.008\n10.1186/s40248-018-0118-7\n10.1016/j.iccn.2020.102812\n10.1097/ALN.000000000000\n10.1016/j.cmi.2019.09.009\n10.1136/bmj.h4438\n10.1186/s12871-020-0942-0\n10.1016/j.jcrc.2020.01.023\n10.1532/hsf.1566\n10.1097/MD.0000000000017392\n10.1136/bmjopen-2019-033898\n10.1136/bmj.m1328\n10.1155/2013/504136\n10.1371/journal.pone.0181448\n10.1109/ICHI.2015.23\n10.1371/journal.pone.0206862\n10.21037/jtd.2020.02.64\n10.3390/app10093233\n10.1016/j.compbiomed.2020.103792\n10.1111/codi.15252\n10.1134/S0362119716080107\n10.1080/17434440.2020.1814742\n10.1111/anae.14945\n10.1186/s12931-020-1285-6\n10.23996/fjhw.82538"}
{"title": "CoVA: An Acuity Score for Outpatient Screening that Predicts Coronavirus Disease 2019 Prognosis.", "abstract": "We sought to develop an automatable score to predict hospitalization, critical illness, or death for patients at risk for coronavirus disease 2019 (COVID-19) presenting for urgent care.\nWe developed the COVID-19 Acuity Score (CoVA) based on a single-center study of adult outpatients seen in respiratory illness clinics or the emergency department. Data were extracted from the Partners Enterprise Data Warehouse, and split into development (n\u2005=\u20059381, 7 March-2 May) and prospective (n\u2005=\u20052205, 3-14 May) cohorts. Outcomes were hospitalization, critical illness (intensive care unit or ventilation), or death within 7 days. Calibration was assessed using the expected-to-observed event ratio (E/O). Discrimination was assessed by area under the receiver operating curve (AUC).\nIn the prospective cohort, 26.1%, 6.3%, and 0.5% of patients experienced hospitalization, critical illness, or death, respectively. CoVA showed excellent performance in prospective validation for hospitalization (expected-to-observed ratio [E/O]: 1.01; AUC: 0.76), for critical illness (E/O: 1.03; AUC: 0.79), and for death (E/O: 1.63; AUC: 0.93). Among 30 predictors, the top 5 were age, diastolic blood pressure, blood oxygen saturation, COVID-19 testing status, and respiratory rate.\nCoVA is a prospectively validated automatable score for the outpatient setting to predict adverse events related to COVID-19 infection.", "journal": "The Journal of infectious diseases", "date": "2020-10-25", "authors": ["HaoqiSun", "AayusheeJain", "Michael JLeone", "Haitham SAlabsi", "Laura NBrenner", "ElissaYe", "WendongGe", "Yu-PingShao", "Christine LBoutros", "RuopengWang", "Ryan ATesh", "ColinMagdamo", "Sarah ICollens", "WolfgangGanglberger", "Ingrid VBassett", "James BMeigs", "JayashreeKalpathy-Cramer", "Matthew DLi", "Jacqueline TChu", "Michael LDougan", "Lawrence WStratton", "JonathanRosand", "BruceFischl", "SudeshnaDas", "Shibani SMukerji", "Gregory KRobbins", "M BrandonWestover"], "doi": "10.1093/infdis/jiaa663"}
{"title": "Implementation of convolutional neural network approach for COVID-19 disease detection.", "abstract": "In this paper, two novel, powerful, and robust convolutional neural network (CNN) architectures are designed and proposed for two different classification tasks using publicly available data sets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 versus normal versus pneumonia) with 98.27% average accuracy. The hyperparameters of both CNN models are automatically determined using Grid Search. Experimental results on large clinical data sets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome the disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue, which is a great improvement of available automatic methods in the literature. To the best of the author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN, whose hyperparameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN-based COVID-19 chest X-ray image classification study that uses the largest possible clinical data set. A total of 1,524 COVID-19, 1,527 pneumonia, and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.", "journal": "Physiological genomics", "date": "2020-10-24", "authors": ["EmrahIrmak"], "doi": "10.1152/physiolgenomics.00084.2020\n10.1007/978-981-10-9035-6_33\n10.1152/physiolgenomics.00029.2020\n10.1007/s13246-020-00865-4\n10.1088/1742-6596/1193/1/012033\n10.3390/ijerph17082690\n10.1515/comp-2019-0011\n10.1109/ACCESS.2020.2981141\n10.1016/j.bbe.2018.10.004\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2020200905\n10.3390/sym12040651\n10.1016/j.compbiomed.2020.103792\n10.1016/j.jocs.2018.12.003\n10.1007/s10096-020-03901-z\n10.1109/ACCESS.2019.2919122\n10.1016/j.compbiomed.2020.103805\n10.1016/j.bbe.2019.11.004\n10.1016/j.physa.2019.123592\n10.1097/EDE.0000000000001027\n10.1155/2019/7289273"}
{"title": "Decoding COVID-19 pneumonia: comparison of deep learning and radiomics CT image signatures.", "abstract": "High-dimensional image features that underlie COVID-19 pneumonia remain opaque. We aim to compare feature engineering and deep learning methods to gain insights into the image features that drive CT-based for COVID-19 pneumonia prediction, and uncover CT image features significant for COVID-19 pneumonia from deep learning and radiomics framework.\nA total of 266 patients with COVID-19 and other viral pneumonia with clinical symptoms and CT signs similar to that of COVID-19 during the outbreak were retrospectively collected from three hospitals in China and the USA. All the pneumonia lesions on CT images were manually delineated by four radiologists. One hundred eighty-four patients (n\u2009=\u200993 COVID-19 positive; n\u2009=\u200991 COVID-19 negative; 24,216 pneumonia lesions from 12,001 CT image slices) from two hospitals from China served as discovery cohort for model development. Thirty-two patients (17 COVID-19 positive, 15 COVID-19 negative; 7883 pneumonia lesions from 3799 CT image slices) from a US hospital served as external validation cohort. A bi-directional adversarial network-based framework and PyRadiomics package were used to extract deep learning and radiomics features, respectively. Linear and Lasso classifiers were used to develop models predictive of COVID-19 versus non-COVID-19 viral pneumonia.\n120-dimensional deep learning image features and 120-dimensional radiomics features were extracted. Linear and Lasso classifiers identified 32 high-dimensional deep learning image features and 4 radiomics features associated with COVID-19 pneumonia diagnosis (P\u2009<\u20090.0001). Both models achieved sensitivity >\u200973% and specificity >\u200975% on external validation cohort with slight superior performance for radiomics Lasso classifier. Human expert diagnostic performance improved (increase by 16.5% and 11.6% in sensitivity and specificity, respectively) when using a combined deep learning-radiomics model.\nWe uncover specific deep learning and radiomics features to add insight into interpretability of machine learning algorithms and compare deep learning and radiomics models for COVID-19 pneumonia that might serve to augment human diagnostic performance.", "journal": "European journal of nuclear medicine and molecular imaging", "date": "2020-10-24", "authors": ["HongmeiWang", "LuWang", "Edward HLee", "JimmyZheng", "WeiZhang", "SafwanHalabi", "ChunleiLiu", "KexueDeng", "JiangdianSong", "Kristen WYeom"], "doi": "10.1007/s00259-020-05075-4\n10.1002/ctm2.17\n10.1158/0008-5472.CAN-17-0339\n10.1038/nrclinonc.2017.141\n10.1016/j.ebiom.2018.09.007\n10.3389/fonc.2019.00340\n10.3389/fonc.2019.00255\n10.1007/s11547-020-01195-x\n10.1021/acs.molpharmaceut.7b00578\n10.1038/s41592-019-0403-1\n10.1158/1078-0432.CCR-17-2507\n10.1007/s10637-017-0524-2\n10.1007/s13139-018-0514-0"}
{"title": "Integrative analysis for COVID-19 patient outcome prediction.", "abstract": "While image analysis of chest computed tomography (CT) for COVID-19 diagnosis has been intensively studied, little work has been performed for image-based patient outcome prediction. Management of high-risk patients with early intervention is a key to lower the fatality rate of COVID-19 pneumonia, as a majority of patients recover naturally. Therefore, an accurate prediction of disease progression with baseline imaging at the time of the initial presentation can help in patient management. In lieu of only size and volume information of pulmonary abnormalities and features through deep learning based image segmentation, here we combine radiomics of lung opacities and non-imaging features from demographic data, vital signs, and laboratory findings to predict need for intensive care unit (ICU) admission. To our knowledge, this is the first study that uses holistic information of a patient including both imaging and non-imaging data for outcome prediction. The proposed methods were thoroughly evaluated on datasets separately collected from three hospitals, one in the United States, one in Iran, and another in Italy, with a total 295 patients with reverse transcription polymerase chain reaction (RT-PCR) assay positive COVID-19 pneumonia. Our experimental results demonstrate that adding non-imaging features can significantly improve the performance of prediction to achieve AUC up to 0.884 and sensitivity as high as 96.1%, which can be valuable to provide clinical decision support in managing COVID-19 patients. Our methods may also be applied to other lung diseases including but not limited to community acquired pneumonia. The source code of our work is available at https://github.com/DIAL-RPI/COVID19-ICUPrediction.", "journal": "Medical image analysis", "date": "2020-10-23", "authors": ["HanqingChao", "XiFang", "JiajinZhang", "FatemehHomayounieh", "Chiara DArru", "Subba RDigumarthy", "RosaBabaei", "Hadi KMobin", "ImanMohseni", "LucaSaba", "AlessandroCarriero", "ZenoFalaschi", "AlessioPasche", "GeWang", "Mannudeep KKalra", "PingkunYan"], "doi": "10.1016/j.media.2020.101844\n10.1148/radiol.2020200642\n10.1148/radiol.2020201343\n10.1148/radiol.2020200905\n10.1038/s42256-020-0180-7\n10.2214/AJR.20.22976"}
{"title": "A model based on CT radiomic features for predicting RT-PCR becoming negative in coronavirus disease 2019 (COVID-19) patients.", "abstract": "Coronavirus disease 2019 (COVID-19) has emerged as a global pandemic. According to the diagnosis and treatment guidelines of China, negative reverse transcription-polymerase chain reaction (RT-PCR) is the key criterion for discharging COVID-19 patients. However, repeated RT-PCR tests lead to medical waste and prolonged hospital stays for COVID-19 patients during the recovery period. Our purpose is to assess a model based on chest computed tomography (CT) radiomic features and clinical characteristics to predict RT-PCR negativity during clinical treatment.\nFrom February 10 to March 10, 2020, 203 mild COVID-19 patients in Fangcang Shelter Hospital were retrospectively included (training: n\u2009=\u2009141; testing: n\u2009=\u200962), and clinical characteristics were collected. Lung abnormalities on chest CT images were segmented with a deep learning algorithm. CT quantitative features and radiomic features were automatically extracted. Clinical characteristics and CT quantitative features were compared between RT-PCR-negative and RT-PCR-positive groups. Univariate logistic regression and Spearman correlation analyses identified the strongest features associated with RT-PCR negativity, and a multivariate logistic regression model was established. The diagnostic performance was evaluated for both cohorts.\nThe RT-PCR-negative group had a longer time interval from symptom onset to CT exams than the RT-PCR-positive group (median 23 vs. 16\u00a0days, p\u2009<\u20090.001). There was no significant difference in the other clinical characteristics or CT quantitative features. In addition to the time interval from symptom onset to CT exams, nine CT radiomic features were selected for the model. ROC curve analysis revealed AUCs of 0.811 and 0.812 for differentiating the RT-PCR-negative group, with sensitivity/specificity of 0.765/0.625 and 0.784/0.600 in the training and testing datasets, respectively.\nThe model combining CT radiomic features and clinical data helped predict RT-PCR negativity during clinical treatment, indicating the proper time for RT-PCR retesting.", "journal": "BMC medical imaging", "date": "2020-10-22", "authors": ["QuanCai", "Si-YaoDu", "SiGao", "Guo-LiangHuang", "ZhengZhang", "ShuLi", "XinWang", "Pei-LingLi", "PengLv", "GangHou", "Li-NaZhang"], "doi": "10.1186/s12880-020-00521-z\n10.1007/s00330-020-06801-0\n10.1007/s11604-020-01010-7\n10.1186/s12880-020-00464-5\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463\n10.1097/RLI.0000000000000672\n10.1148/ryct.2020200047\n10.1007/s00330-020-06817-6\n10.1148/radiol.2020200905\n10.1007/s10096-020-03901-z\n10.1148/radiol.2020200527\n10.1101/2020.02.11.20021493\n10.1148/radiol.2020201433\n10.1148/radiol.2020200230\n10.1148/radiol.2020200274\n10.1001/jama.2020.1585\n10.1148/radiol.2020200269\n10.1148/radiol.2020200323\n10.1148/ryct.2020200031\n10.1016/S2213-2600(20)30079-5\n10.1016/S0140-6736(20)30211-7\n10.1186/s12931-020-01338-8\n10.1016/j.ebiom.2020.102763"}
{"title": "Adoption of Digital Technologies in Health Care During the COVID-19 Pandemic: Systematic Review of Early Scientific Literature.", "abstract": "The COVID-19 pandemic is favoring digital transitions in many industries and in society as a whole. Health care organizations have responded to the first phase of the pandemic by rapidly adopting digital solutions and advanced technology tools.\nThe aim of this review is to describe the digital solutions that have been reported in the early scientific literature to mitigate the impact of COVID-19 on individuals and health systems.\nWe conducted a systematic review of early COVID-19-related literature (from January 1 to April 30, 2020) by searching MEDLINE and medRxiv with appropriate terms to find relevant literature on the use of digital technologies in response to the pandemic. We extracted study characteristics such as the paper title, journal, and publication date, and we categorized the retrieved papers by the type of technology and patient needs addressed. We built a scoring rubric by cross-classifying the patient needs with the type of technology. We also extracted information and classified each technology reported by the selected articles according to health care system target, grade of innovation, and scalability to other geographical areas.\nThe search identified 269 articles, of which 124 full-text articles were assessed and included in the review after screening. Most of the selected articles addressed the use of digital technologies for diagnosis, surveillance, and prevention. We report that most of these digital solutions and innovative technologies have been proposed for the diagnosis of COVID-19. In particular, within the reviewed articles, we identified numerous suggestions on the use of artificial intelligence (AI)-powered tools for the diagnosis and screening of COVID-19. Digital technologies are also useful for prevention and surveillance measures, such as contact-tracing apps and monitoring of internet searches and social media usage. Fewer scientific contributions address the use of digital technologies for lifestyle empowerment or patient engagement.\nIn the field of diagnosis, digital solutions that integrate with traditional methods, such as AI-based diagnostic algorithms based both on imaging and clinical data, appear to be promising. For surveillance, digital apps have already proven their effectiveness; however, problems related to privacy and usability remain. For other patient needs, several solutions have been proposed, such as telemedicine or telehealth tools. These tools have long been available, but this historical moment may actually be favoring their definitive large-scale adoption. It is worth taking advantage of the impetus provided by the crisis; it is also important to keep track of the digital solutions currently being proposed to implement best practices and models of care in future and to adopt at least some of the solutions proposed in the scientific literature, especially in national health systems, which have proved to be particularly resistant to the digital transition in recent years.", "journal": "Journal of medical Internet research", "date": "2020-10-21", "authors": ["DavideGolinelli", "ErikBoetto", "GherardoCarullo", "Andrea GiovanniNuzzolese", "Maria PaolaLandini", "Maria PiaFantini"], "doi": "10.2196/22280\n10.1177/2055207620920083?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dpubmed\n10.1177/2055207620920083\n10.1038/d41586-020-00896-7\n10.1093/jamia/ocaa078\n10.1056/NEJMp2005835\n10.2196/jmir.9498\n10.1038/nrd.2016.265\n10.1126/science.abb5793\n10.7326/0003-4819-151-4-200908180-00135?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dpubmed\n10.7326/0003-4819-151-4-200908180-00135\n10.1001/jama.2010.903\n10.1377/hpb20130214.898775/full/\n10.37473/dac/10.1101/2020.02.20.20025957\n10.1101/2020.02.24.20026682\n10.1001/jama.2020.3151\n10.1101/2020.02.27.20028027\n10.1101/2020.03.14.20035956\n10.2139/ssrn.3551355\n10.1101/2020.03.19.20039354\n10.2196/18848\n10.1101/2020.03.24.20042317\n10.1101/2020.03.12.20027185\n10.1101/2020.03.24.20042234\n10.2139/ssrn.3557984\n10.1038/s41591-020-0832-5\n10.1038/s41591-020-0824-5\n10.3390/ijerph17072309\n10.1101/2020.03.28.20046045\n10.1101/2020.03.24.20043117\n10.3201/eid2607.200574\n10.3201/eid2607.200574\n10.1126/science.abb6936\n10.1016/j.jpainsymman.2020.03.019\n10.1016/j.jhin.2020.03.027\n10.3390/diagnostics10040198\n10.3332/ecancer.2020.ed97\n10.2196/18717\n10.2196/18810\n10.1101/2020.03.31.20048439\n10.1093/jamia/ocaa048\n10.1017/dmp.2020.69\n10.3390/s20072033\n10.1136/bmj.m1373\n10.1101/2020.04.03.20052084\n10.1101/2020.04.02.20051334\n10.1101/2020.04.02.20051284\n10.1101/2020.03.30.20047787\n10.1101/2020.03.25.008805\n10.2196/18936\n10.1101/2020.04.03.20052936\n10.1101/2020.04.02.20048793\n10.1101/2020.04.02.20048793\n10.1101/2020.03.30.20047472\n10.1101/2020.03.29.20046789\n10.2196/17791\n10.1093/jamia/ocaa051\n10.24869/psyd.2020.25\n10.1055/s-0040-1709715\n10.1101/2020.04.08.20040907\n10.1101/2020.04.08.20040907\n10.1002/hep.31276\n10.1111/jrh.12435\n10.1101/2020.04.08.20057679\n10.1111/jrh.12449\n10.1111/jrh.12446\n10.5435/JAAOS-D-20-00380\n10.1101/2020.04.09.20059857\n10.1016/j.dsx.2020.04.012\n10.1101/2020.04.11.20062091\n10.1101/2020.04.11.20062091\n10.1097/JCMA.0000000000000325\n10.1016/j.amjoto.2020.102490\n10.2196/18980\n10.1101/2020.04.08.20057968\n10.1101/2020.04.08.20057968\n10.1089/tmj.2020.0084\n10.1016/j.pedn.2020.04.013\n10.1016/j.pedn.2020.04.013\n10.1101/2020.04.13.20063479\n10.1101/2020.04.13.20063461\n10.1089/dia.2020.0161\n10.1016/j.telpol.2020.101976\n10.1111/ced.14247\n10.1111/ced.14245\n10.1101/2020.04.15.20066720\n10.1101/2020.04.15.20063677\n10.1016/j.clae.2020.04.002\n10.1016/j.arth.2020.04.038\n10.1016/j.ajem.2020.04.025\n10.1007/s10461-020-02870-w\n10.1007/s10072-020-04391-9\n10.1093/jamia/ocaa067\n10.1093/jamia/ocaa064\n10.1089/omi.2020.0047\n10.1016/j.giq.2020.101481\n10.1101/2020.03.30.20047456\n10.1101/2020.04.17.20070094\n10.1101/2020.04.17.20069211\n10.1016/j.jpainsymman.2020.04.017\n10.1016/j.jaccao.2020.04.003\n10.1016/j.hlpt.2020.04.005\n10.3390/ijerph17082906\n10.1101/2020.04.15.20067025\n10.1101/2020.04.05.20054254\n10.1089/omi.2020.0053\n10.1016/j.scitotenv.2020.138858\n10.1001/jama.2020.6602\n10.23736/S1973-9087.20.06331-5\n10.23736/S1973-9087.20.06331-5\n10.2106/JBJS.20.00609\n10.2106/JBJS.20.00609\n10.1371/journal.pone.0232391\n10.1371/journal.pone.0232391\n10.1016/j.dsx.2020.04.032\n10.1016/j.hlpt.2020.04.010\n10.1016/j.dib.2020.105618\n10.3390/ijerph17092997\n10.1093/jamia/ocaa037\n10.3390/s20092479\n10.2196/19547\n10.2196/19297\n10.2196/19139\n10.2196/19106\n10.1101/2020.04.22.20071043\n10.1002/oby.22851\n10.1002/oby.22851\n10.2196/19118\n10.1101/2020.04.26.20073411\n10.1093/ptj/pzaa079\n10.1101/2020.04.25.20079426\n10.1101/2020.04.25.20079129\n10.1101/2020.04.24.20078238\n10.1101/2020.04.23.20077552\n10.1093/jamia/ocaa081\n10.2196/18808\n10.1101/2020.04.19.20067660\n10.1101/2020.04.13.20059691\n10.1101/2020.04.13.20059691\n10.1101/2020.04.09.20059840\n10.1101/2020.04.01.20049684\n10.1089/tmj.2020.0091\n10.1016/j.jamcollsurg.2020.04.030\n10.1007/s10389-020-01295-y\n10.1002/oby.22860\n10.1007/s11739-020-02371-7\n10.1007/s40258-020-00569-6\n10.1038/s42256-020-0194-1\n10.2196/19457\n10.2196/18795\n10.1038/s41591-020-0832-5\n10.1093/intqhc/mzy186\n10.2196/19284\n10.3163/1536-5050.102.3.006\n10.1038/d41586-020-01520-4"}
{"title": "Telerobotic ultrasound to provide obstetrical ultrasound services remotely during the COVID-19 pandemic.", "abstract": "Obstetrical ultrasound imaging is critical in identifying at-risk pregnancies and informing clinical management. The coronavirus disease 2019 (COVID-19) pandemic has exacerbated challenges in accessing obstetrical ultrasound for patients in underserved rural and remote communities where this service is not available. This prospective descriptive study describes our experience of providing obstetrical ultrasound services remotely using a telerobotic ultrasound system in a northern Canadian community isolated due to a COVID-19 outbreak.\nA telerobotic ultrasound system was used to perform obstetrical ultrasound exams remotely in La Loche, Canada, a remote community without regular access to obstetrical ultrasound. Using a telerobotic ultrasound system, a sonographer 605 km away remotely controlled an ultrasound probe and ultrasound settings. Twenty-one exams were performed in a five-week period during a COVID-19 outbreak in the community, including limited first-, second- and third-trimester exams (\nOf 11 limited obstetrical exams, radiologists indicated images were adequate in nine (81%) cases, adequate with some reservations in one (9%) case and inadequate in one (9%) case. Of 10 second-trimester complete obstetrical exams, radiologists indicated images were adequate in two (20%) cases, adequate with some reservations in three (30%) cases and inadequate in five (50%) cases. Second-trimester complete obstetrical exams were limited due to a combination of body habitus, foetal lie and telerobotic technology.\nA telerobotic ultrasound system may be used to answer focused clinical questions such as foetal viability, dating and foetal presentation in a timely manner while minimising patient travel to larger centres and potential exposure to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), during the COVID-19 pandemic.", "journal": "Journal of telemedicine and telecare", "date": "2020-10-21", "authors": ["Scott JAdams", "BrentBurbridge", "LeslieChatterson", "VeronicaMcKinney", "PaulBabyn", "IvarMendez"], "doi": "10.1177/1357633X20965422"}
{"title": "Automatic classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray image: combination of data augmentation methods.", "abstract": "This study aimed to develop and validate computer-aided diagnosis (CXDx) system for classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray (CXR) images. From two public datasets, 1248 CXR images were obtained, which included 215, 533, and 500 CXR images of COVID-19 pneumonia patients, non-COVID-19 pneumonia patients, and the healthy samples, respectively. The proposed CADx system utilized VGG16 as a pre-trained model and combination of conventional method and mixup as data augmentation methods. Other types of pre-trained models were compared with the VGG16-based model. Single type or no data augmentation methods were also evaluated. Splitting of training/validation/test sets was used when building and evaluating the CADx system. Three-category accuracy was evaluated for test set with 125 CXR images. The three-category accuracy of the CAD system was 83.6% between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy. Sensitivity for COVID-19 pneumonia was more than 90%. The combination of conventional method and mixup was more useful than single type or no data augmentation method. In conclusion, this study was able to create an accurate CADx system for the 3-category classification. Source code of our CADx system is available as open source for COVID-19 research.", "journal": "Scientific reports", "date": "2020-10-18", "authors": ["MizuhoNishio", "ShunjiroNoguchi", "HidetoshiMatsuo", "TakamichiMurakami"], "doi": "10.1038/s41598-020-74539-2\n10.1148/radiol.2020200432\n10.1148/radiol.2020200823\n10.1148/radiol.2511081296\n10.1148/radiol.2020201160\n10.1007/s13244-018-0639-9\n10.1016/S0140-6736(18)31645-3\n10.1109/tcsvt.2019.2935128"}
{"title": "Multi-task deep learning based CT imaging analysis for COVID-19 pneumonia: Classification and segmentation.", "abstract": "This paper presents an automatic classification segmentation tool for helping screening COVID-19 pneumonia using chest CT imaging. The segmented lesions can help to assess the severity of pneumonia and follow-up the patients. In this work, we propose a new multitask deep learning model to jointly identify COVID-19 patient and segment COVID-19 lesion from chest CT images. Three learning tasks: segmentation, classification and reconstruction are jointly performed with different datasets. Our motivation is on the one hand to leverage useful information contained in multiple related tasks to improve both segmentation and classification performances, and on the other hand to deal with the problems of small data because each task can have a relatively small dataset. Our architecture is composed of a common encoder for disentangled feature representation with three tasks, and two decoders and a multi-layer perceptron for reconstruction, segmentation and classification respectively. The proposed model is evaluated and compared with other image segmentation techniques using a dataset of 1369 patients including 449 patients with COVID-19, 425 normal ones, 98 with lung cancer and 397 of different kinds of pathology. The obtained results show very encouraging performance of our method with a dice coefficient higher than 0.88 for the segmentation and an area under the ROC curve higher than 97% for the classification.", "journal": "Computers in biology and medicine", "date": "2020-10-17", "authors": ["AmineAmyar", "RomainModzelewski", "HuaLi", "SuRuan"], "doi": "10.1016/j.compbiomed.2020.104037"}
{"title": "Covid-19 classification by FGCNet with deep feature fusion from graph convolutional network and convolutional neural network.", "abstract": "(", "journal": "An international journal on information fusion", "date": "2020-10-15", "authors": ["Shui-HuaWang", "Vishnu VarthananGovindaraj", "Juan ManuelG\u00f3rriz", "XinZhang", "Yu-DongZhang"], "doi": "10.1016/j.inffus.2020.10.004\n10.1007/s00330-020-07044-9\n10.1002/mp.14327"}
{"title": "M ", "abstract": "To counter the outbreak of COVID-19, the accurate diagnosis of suspected cases plays a crucial role in timely quarantine, medical treatment, and preventing the spread of the pandemic. Considering the limited training cases and resources (e.g, time and budget), we propose a Multi-task Multi-slice Deep Learning System (M ", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-10-14", "authors": ["XuelinQian", "HuazhuFu", "WeiyaShi", "TaoChen", "YanweiFu", "FeiShan", "XiangyangXue"], "doi": "10.1109/JBHI.2020.3030853"}
{"title": "Clinical and laboratory data, radiological structured report findings and quantitative evaluation of lung involvement on baseline chest CT in COVID-19 patients to predict prognosis.", "abstract": "To evaluate by means of regression models the relationships between baseline clinical and laboratory data and lung involvement on baseline chest CT and to quantify the thoracic disease using an artificial intelligence tool and a visual scoring system to predict prognosis in patients with COVID-19 pneumonia.\nThis study included 103 (41 women and 62 men; 68.8\u00a0years of mean age-range, 29-93\u00a0years) with suspicious COVID-19 viral infection evaluated by reverse transcription real-time fluorescence polymerase chain reaction (RT-PCR) test. All patients underwent CT examinations at the time of admission in addition to clinical and laboratory findings recording. All chest CT examinations were reviewed using a structured report. Moreover, using an artificial intelligence tool we performed an automatic segmentation on CT images based on Hounsfield unit to calculate residual healthy lung parenchyma, ground-glass opacities (GGO), consolidations and emphysema volumes for both right and left lungs. Two expert radiologists, in consensus, attributed at the CT pulmonary disease involvement a severity score using a scale of 5 levels; the score was attributed for GGO and consolidation for each lung, and then, an overall radiological severity visual score was obtained summing the single score. Univariate and multivariate regression analysis was performed.\nSymptoms and comorbidities did not show differences statistically significant in terms of patient outcome. Instead, SpO2 was significantly lower in patients hospitalized in critical conditions or died while age, HS CRP, leukocyte count, neutrophils, LDH, d-dimer, troponin, creatinine and azotemia, ALT, AST and bilirubin values were significantly higher. GGO and consolidations were the main CT patterns (a variable combination of GGO and consolidations was found in 87.8% of patients). CT COVID-19 disease was prevalently bilateral (77.6%) with peripheral distribution (74.5%) and multiple lobes localizations (52.0%). Consolidation, emphysema and residual healthy lung parenchyma volumes showed statistically significant differences in the three groups of patients based on outcome (patients discharged at home, patients hospitalized in stable conditions and patient hospitalized in critical conditions or died) while GGO volume did not affect the patient's outcome. Moreover, the overall radiological severity visual score (cutoff\u2009\u2265\u20098) was a predictor of patient outcome. The highest value of R-squared (R\nIn conclusion, both CT visual score and computerized software-based quantification of the consolidation, emphysema and residual healthy lung parenchyma on chest CT images were independent predictors of outcome in patients with COVID-19 pneumonia.", "journal": "La Radiologia medica", "date": "2020-10-14", "authors": ["CappabiancaSalvatore", "FuscoRoberta", "de LisioAngela", "PauraCesare", "ClementeAlfredo", "GagliardiGiuliano", "LombardiGiulio", "GiacobbeGiuliana", "Russo GaetanoMaria", "Belfiore MariaPaola", "UrraroFabrizio", "GrassiRoberta", "FeragalliBeatrice", "MieleVittorio"], "doi": "10.1007/s11547-020-01293-w\n10.1002/jmv.25773\n10.1097/RTI.0000000000000516\n10.1007/s00330-020-06817-6\n10.1148/ryct.2020200075\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200236\n10.1016/j.ejrad.2019.108748\n10.1056/NEJMoa052052\n10.1148/radiol.2020200988\n10.21203/rs.3.rs-24312/v1\n10.1007/s00330-012-2683-z\n10.1371/journal.pone.0152505\n10.1007/s11547-020-01195-x\n10.1148/radiol.2020200370\n10.1016/j.ijid.2020.02.043\n10.1016/S0140-6736(20)30211-7\n10.1016/S0140-6736(20)30154-9\n10.1007/s11547-020-01200-3\n10.1056/NEJMoa2002032\n10.1007/s11547-020-01197-9\n10.1007/s11547-020-01202-1\n10.1007/s11547-019-00990-5\n10.1007/s11547-018-0900-9\n10.1136/thx.2010.145995\n10.1183/09031936.00071812\n10.1148/ryct.2020200047\n10.1148/radiol.2020201433\n10.1007/s00134-020-05991-x"}
{"title": "CT Quantification and Machine-learning Models for Assessment of Disease Severity and Prognosis of COVID-19 Patients.", "abstract": "This study was to investigate the CT quantification of COVID-19 pneumonia and its impacts on the assessment of disease severity and the prediction of clinical outcomes in the management of COVID-19 patients.\nNinety-nine COVID-19 patients who were confirmed by positive nucleic acid test (NAT) of RT-PCR and hospitalized from January 19, 2020 to February 19, 2020 were collected for this retrospective study. All patients underwent arterial blood gas test, routine blood test, chest CT examination, and physical examination on admission. In addition, follow-up clinical data including the disease severity, clinical treatment, and clinical outcomes were collected for each patient. Lung volume, lesion volume, nonlesion lung volume (NLLV) (lung volume - lesion volume), and fraction of nonlesion lung volume (%NLLV) (nonlesion lung volume / lung volume) were quantified in CT images by using two U-Net models trained for segmentation of lung and COVID-19 lesions in CT images. Furthermore, we calculated 20 histogram textures for lesions volume and NLLV, respectively. To investigate the validity of CT quantification in the management of COVID-19, we built random forest (RF) models for the purpose of classification and regression to assess the disease severity (Moderate, Severe, and Critical) and to predict the need and length of ICU stay, the duration of oxygen inhalation, hospitalization, sputum NAT-positive, and patient prognosis. The performance of RF classifiers was evaluated using the area under the receiver operating characteristic curves (AUC) and that of RF regressors using the root-mean-square error.\nPatients were classified into three groups of disease severity: moderate (n\u202f=\u202f25), severe (n\u202f=\u202f47) and critical (n\u202f=\u202f27), according to the clinical staging. Of which, a total of 32 patients, 1 (1/25) moderate, 6 (6/47) severe, and 25 critical (25/27), respectively, were admitted to ICU. The median values of ICU stay were 0, 0, and 12 days, the duration of oxygen inhalation 10, 15, and 28 days, the hospitalization 12, 16, and 28 days, and the sputum NAT-positive 8, 9, and 13 days, in three severity groups, respectively. The clinical outcomes were complete recovery (n\u202f=\u202f3), partial recovery with residual pulmonary damage (n\u202f=\u202f80), prolonged recovery (n\u202f=\u202f15), and death (n\u202f=\u202f1). The %NLLV in three severity groups were 92.18 \u00b1 9.89%, 82.94 \u00b1 16.49%, and 66.19 \u00b1 24.15% with p value <0.05 among each two groups. The AUCs of RF classifiers using hybrid models were 0.927 and 0.929 in classification of moderate vs (severe\u202f+\u202fcritical), and severe vs critical, respectively, which were significantly higher than either radiomics models or clinical models (p < 0.05). The root-mean-square errors of RF regressors were 0.88 weeks for prediction of duration of hospitalization (mean: 2.60 \u00b1 1.01 weeks), 0.92 weeks for duration of oxygen inhalation (mean: 2.44 \u00b1 1.08 weeks), 0.90 weeks for duration of sputum NAT-positive (mean: 1.59 \u00b1 0.98 weeks), and 0.69 weeks for stay of ICU (mean: 1.32 \u00b1 0.67 weeks), respectively. The AUCs for prediction of ICU treatment and prognosis (partial recovery vs prolonged recovery) were 0.945 and 0.960, respectively.\nCT quantification and machine-learning models show great potentials for assisting decision-making in the management of COVID-19 patients by assessing disease severity and predicting clinical outcomes.", "journal": "Academic radiology", "date": "2020-10-14", "authors": ["WenliCai", "TianyuLiu", "XingXue", "GuiboLuo", "XiaoliWang", "YihongShen", "QiangFang", "JifangSheng", "FengChen", "TingboLiang"], "doi": "10.1016/j.acra.2020.09.004\n10.1109/RBME.2020.2990959\n10.1007/978-3-319-24574-4_28\n10.1101/2020.02.25.20021568\n10.2139/ssrn.3546089"}
{"title": "Development of a quantitative segmentation model to assess the effect of comorbidity on patients with COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) has brought a global disaster. Quantitative lesions may provide the radiological evidence of the severity of pneumonia and further to assess the effect of comorbidity on patients with COVID-19.\n294 patients with COVID-19 were enrolled from February, 24, 2020 to June, 1, 2020 from six centers. Multi-task Unet network was used to segment the whole lung and lesions from chest CT images. This deep learning method was pre-trained in 650 CT images (550 in primary dataset and 100 in test dataset) with COVID-19 or community-acquired pneumonia and Dice coefficients in test dataset were calculated. 50 CT scans of 50 patients (15 with comorbidity and 35 without comorbidity) were random selected to mark lesions manually. The results will be compared with the automatic segmentation model. Eight quantitative parameters were calculated based on the segmentation results to evaluate the effect of comorbidity on patients with COVID-19.\nQuantitative segmentation model was proved to be effective and accurate with all Dice coefficients more than 0.85 and all accuracies more than 0.95. Of the 294 patients, 52 (17.7%) patients were reported having at least one comorbidity; 14 (4.8%) having more than one comorbidity. Patients with any comorbidity were older (P\u2009<\u20090.001), had longer incubation period (P\u2009<\u20090.001), were more likely to have abnormal laboratory findings (P\u2009<\u20090.05), and be in severity status (P\u2009<\u20090.001). More lesions (including larger volume of lesion, consolidation, and ground-glass opacity) were shown in patients with any comorbidity than patients without comorbidity (all P\u2009<\u20090.001). More lesions were found on CT images in patients with more comorbidities. The median volumes of lesion, consolidation, and ground-glass opacity in diabetes mellitus group were largest among the groups with single comorbidity that had the incidence rate of top three.\nMulti-task Unet network can make quantitative CT analysis of lesions to assess the effect of comorbidity on patients with COVID-19, further to provide the radiological evidence of the severity of pneumonia. More lesions (including GGO and consolidation) were found in CT images of cases with comorbidity. The more comorbidities patients have, the more lesions CT images show.", "journal": "European journal of medical research", "date": "2020-10-14", "authors": ["CuiZhang", "GuangzhaoYang", "ChunxianCai", "ZhihuaXu", "HaiWu", "YouminGuo", "ZongyuXie", "HengfengShi", "GuohuaCheng", "JianWang"], "doi": "10.1186/s40001-020-00450-1\n10.1111/tmi.13383\n10.1056/NEJMoa2001191\n10.1001/jama.2020.5394\n10.1001/jama.2020.4031\n10.18632/aging.103000\n10.1111/joim.13063\n10.1183/13993003.00547-2020\n10.1148/radiol.2020200241\n10.1016/j.jpha.2020.03.004\n10.1186/s40779-020-0233-6\n10.2214/AJR.20.22954\n10.1016/S0140-6736(20)30211-7\n10.1093/cid/ciaa414\n10.1001/jama.2020.1585\n10.1111/all.14238\n10.1016/j.ijid.2016.06.015\n10.1038/s41569-020-0360-5\n10.1101/cshperspect.a007724\n10.1016/S1473-3099(09)70282-8\n10.1111/eci.13259\n10.1016/j.jcv\n10.1097/MEG.0000000000001742"}
{"title": "Active contour regularized semi-supervised learning for COVID-19 CT infection segmentation with limited annotations.", "abstract": "Infection segmentation on chest CT plays an important role in the quantitative analysis of COVID-19. Developing automatic segmentation tools in a short period with limited labelled images has become an urgent need. Pseudo label-based semi-supervised method is a promising way to leverage unlabelled data to improve segmentation performance. Existing methods usually obtain pseudo labels by first training a network with limited labelled images and then inferring unlabelled images. However, these methods may generate obviously inaccurate labels and degrade the subsequent training process. To address these challenges, in this paper, an active contour regularized semi-supervised learning framework was proposed to automatically segment infections with few labelled images. The active contour regularization was realized by the region-scalable fitting (RSF) model which is embedded to the loss function of the network to regularize and refine the pseudo labels of the unlabelled images. We further designed a splitting method to separately optimize the RSF regularization term and the segmentation loss term with iterative convolution-thresholding method and stochastic gradient descent, respectively, which enable fast optimization of each term. Furthermore, we built a statistical atlas to show the infection spatial distribution. Extensive experiments on a small public dataset and a large scale dataset showed that the proposed method outperforms state-of-the-art methods with up to 5% in dice similarity coefficient and normalized surface dice, 10% in relative absolute volume difference and 8 mm in 95% Hausdorff distance. Moreover, we observed that the infections tend to occur at the dorsal subpleural lung and posterior basal segments that are not mentioned in current radiology reports and are meaningful to advance our understanding of COVID-19.", "journal": "Physics in medicine and biology", "date": "2020-10-13", "authors": ["JunMa", "ZiweiNie", "CongcongWang", "GuoqiangDong", "QiongjieZhu", "JianHe", "LuyingGui", "XiaopingYang"], "doi": "10.1088/1361-6560/abc04e"}
{"title": "Severity and Consolidation Quantification of COVID-19 From CT Images Using Deep Learning Based on Hybrid Weak Labels.", "abstract": "Early and accurate diagnosis of Coronavirus disease (COVID-19) is essential for patient isolation and contact tracing so that the spread of infection can be limited. Computed tomography (CT) can provide important information in COVID-19, especially for patients with moderate to severe disease as well as those with worsening cardiopulmonary status. As an automatic tool, deep learning methods can be utilized to perform semantic segmentation of affected lung regions, which is important to establish disease severity and prognosis prediction. Both the extent and type of pulmonary opacities help assess disease severity. However, manually pixel-level multi-class labelling is time-consuming, subjective, and non-quantitative. In this article, we proposed a hybrid weak label-based deep learning method that utilize both the manually annotated pulmonary opacities from COVID-19 pneumonia and the patient-level disease-type information available from the clinical report. A UNet was firstly trained with semantic labels to segment the total infected region. It was used to initialize another UNet, which was trained to segment the consolidations with patient-level information using the Expectation-Maximization (EM) algorithm. To demonstrate the performance of the proposed method, multi-institutional CT datasets from Iran, Italy, South Korea, and the United States were utilized. Results show that our proposed method can predict the infected regions as well as the consolidation regions with good correlation to human annotation.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-10-13", "authors": ["DufanWu", "KuangGong", "Chiara DanielaArru", "FatemehHomayounieh", "BernardoBizzo", "VarunBuch", "HuiRen", "KyungsangKim", "NirNeumark", "PengchengXu", "ZhiyuanLiu", "WeiFang", "NuobeiXie", "Won YoungTak", "Soo YoungPark", "Yu RimLee", "Min KyuKang", "Jung GilPark", "AlessandroCarriero", "LucaSaba", "MahsaMasjedi", "HamidrezaTalari", "RosaBabaei", "Hadi KarimiMobin", "ShadiEbrahimian", "IttaiDayan", "Mannudeep KKalra", "QuanzhengLi"], "doi": "10.1109/JBHI.2020.3030224"}
{"title": "Early prediction of level-of-care requirements in patients with COVID-19.", "abstract": "This study examined records of 2566 consecutive COVID-19 patients at five Massachusetts hospitals and sought to predict level-of-care requirements based on clinical and laboratory data. Several classification methods were applied and compared against standard pneumonia severity scores. The need for hospitalization, ICU care, and mechanical ventilation were predicted with a validation accuracy of 88%, 87%, and 86%, respectively. Pneumonia severity scores achieve respective accuracies of 73% and 74% for ICU care and ventilation. When predictions are limited to patients with more complex disease, the accuracy of the ICU and ventilation prediction models achieved accuracy of 83% and 82%, respectively. Vital signs, age, BMI, dyspnea, and comorbidities were the most important predictors of hospitalization. Opacities on chest imaging, age, admission vital signs and symptoms, male gender, admission laboratory results, and diabetes were the most important risk factors for ICU admission and mechanical ventilation. The factors identified collectively form a signature of the novel COVID-19 disease.\nThe new coronavirus (now named SARS-CoV-2) causing the disease pandemic in 2019 (COVID-19), has so far infected over 35 million people worldwide and killed more than 1 million. Most people with COVID-19 have no symptoms or only mild symptoms. But some become seriously ill and need hospitalization. The sickest are admitted to an Intensive Care Unit (ICU) and may need mechanical ventilation to help them breath. Being able to predict which patients with COVID-19 will become severely ill could help hospitals around the world manage the huge influx of patients caused by the pandemic and save lives. Now, Hao, Sotudian, Wang, Xu et al. show that computer models using artificial intelligence technology can help predict which COVID-19 patients will be hospitalized, admitted to the ICU, or need mechanical ventilation. Using data of 2,566 COVID-19 patients from five Massachusetts hospitals, Hao et al. created three separate models that can predict hospitalization, ICU admission, and the need for mechanical ventilation with more than 86% accuracy, based on patient characteristics, clinical symptoms, laboratory results and chest x-rays. Hao et al. found that the patients\u2019 vital signs, age, obesity, difficulty breathing, and underlying diseases like diabetes, were the strongest predictors of the need for hospitalization. Being male, having diabetes, cloudy chest x-rays, and certain laboratory results were the most important risk factors for intensive care treatment and mechanical ventilation. Laboratory results suggesting tissue damage, severe inflammation or oxygen deprivation in the body's tissues were important warning signs of severe disease. The results provide a more detailed picture of the patients who are likely to suffer from severe forms of COVID-19. Using the predictive models may help physicians identify patients who appear okay but need closer monitoring and more aggressive treatment. The models may also help policy makers decide who needs workplace accommodations such as being allowed to work from home, which individuals may benefit from more frequent testing, and who should be prioritized for vaccination when a vaccine becomes available.", "journal": "eLife", "date": "2020-10-13", "authors": ["BoranHao", "ShahabeddinSotudian", "TaiyaoWang", "TingtingXu", "YangHu", "ApostolosGaitanidis", "KerryBreen", "George CVelmahos", "Ioannis ChPaschalidis"], "doi": "10.7554/eLife.60519\n10.1093/cid/ciaa674\n10.1023/A:1010933404324\n10.1093/cid/ciaa632\n10.1007/BF00994018\n10.1056/NEJM199701233360402\n10.1093/cid/ciaa443\n10.1001/jama.2020.4031\n10.1056/NEJMoa2002032\n10.1093/cid/ciaa414\n10.1162/tacl_a_00101\n10.1016/S1473-3099(20)30144-4\n10.1093/eurheartj/ehaa235\n10.1136/thorax.58.5.377\n10.1093/cid/ciaa270\n10.1001/jama.2020.4683\n10.1016/j.metabol.2020.154262\n10.1093/cid/ciaa627\n10.1001/jama.2020.6775\n10.7554/eLife.57278\n10.1097/CCM.0000000000004410\n10.1093/cid/ciaa538\n10.1001/jama.2020.5046\n10.1093/cid/ciaa793\n10.1038/s42256-020-0180-7\n10.1164/ajrccm/137.4.796\n10.1161/CIRCRESAHA.120.317134"}
{"title": "Zero-shot learning and its applications from autonomous vehicles to COVID-19 diagnosis: A review.", "abstract": "The challenge of learning a new concept, object, or a new medical disease recognition without receiving any examples beforehand is called Zero-Shot Learning (ZSL). One of the major issues in deep learning based methodologies such as in Medical Imaging and other real-world applications is the requirement of large annotated datasets prepared by clinicians or experts to train the model. ZSL is known for having minimal human intervention by relying only on previously known or trained concepts plus currently existing auxiliary information. This is ever-growing research for the cases where we have very limited or no annotated datasets available and the detection ", "journal": "Intelligence-based medicine", "date": "2020-10-13", "authors": ["MahdiRezaei", "MahsaShahidi"], "doi": "10.1016/j.ibmed.2020.100005\n10.1101/2020.03.30.20047456\n10.1109/ACCESS.2020.2989273\n10.1109/CVPR.2016.14\n10.1109/CVPR.2013.111\n10.1109/TPAMI.2015.2487986\n10.1109/CVPR.2015.7298911\n10.1109/CVPR.2016.643\n10.5555/3305381.3305404\n10.1162/tacl_a_00288\n10.1109/RIOS.2017.7956436\n10.1007/978-3-030-01246-5_24\n10.1109/CVPR.2005.117\n10.1016/j.cviu.2007.09.014\n10.1145/1282280.1282340\n10.1007/978-3-319-46454-1_44\n10.1109/ICCVW.2017.308\n10.1109/CVPR.2016.575\n10.1109/ICCV.2017.376\n10.1109/CVPR.2018.00115\n10.1109/WACV45572.2020.9093610\n10.1007/3-540-33486-6_8\n10.1007/978-3-319-10590-1_4\n10.1109/CVPR.2009.5206848\n10.18653/v1/N19-1423\n10.1109/CVPR.2019.00228\n10.1109/CVPR.2019.00523\n10.1109/TPAMI.2016.2643667\n10.1109/ICCV.2013.321\n10.1109/CVPR.2017.666\n10.1148/radiol.2020200432\n10.1109/CVPR.2009.5206772\n10.1109/TPAMI.2006.79\n10.1007/978-3-030-01231-1_2\n10.1109/TPAMI.2015.2408354\n10.1609/aaai.v33i01.33018303\n10.1307/mmj/1029003026\n10.1007/s11263-013-0658-4\n10.5555/2969033.2969125\n10.5555/2976456.2976521\n10.18653/v1/P19-1121\n10.5555/3295222.3295327\n10.5555/3298023.3298158\n10.1162/neco.1997.9.8.1735\n10.1080/00437956.1954.11659520\n10.1109/CVPR.2016.90\n10.1016/j.cmpb.2020.105581\n10.1109/CVPR.2019.00089\n10.5555/2969033.2969213\n10.5555/3327345.3327499\n10.1007/978-3-030-01249-6_8\n10.1162/tacl_a_00065\n10.1109/CVPR.2019.01175\n10.1109/ICCV.2019.00851\n10.1109/CVPR.2012.6248112\n10.1109/CVPR.2017.679\n10.1109/CVPR.2015.7298932\n10.1145/3132635.3132650\n10.1109/ICCV.2015.282\n10.1109/CVPR.2017.473\n10.1145/3065386\n10.1109/CVPR.2018.00450\n10.1126/science.aab3050\n10.1109/CVPR.2009.5206594\n10.1109/TPAMI.2013.140\n10.1126/scirobotics.aav3150\n10.1109/CVPR.2018.00170\n10.1109/ICCV.2015.483\n10.1109/TGRS.2017.2689071\n10.1109/ACCESS.2019.2925093\n10.1109/CVPR.2019.00758\n10.1148/radiol.2020200905\n10.5555/3045118.3045301\n10.1109/CVPR.2017.553\n10.2214/AJR.20.22954\n10.1109/CVPR.2018.00779\n10.1109/LSP.2020.2977498\n10.18653/v1/P19-1335\n10.1109/WACV.2017.110\n10.1109/CVPR.2017.653\n10.1023/B:VISI.0000029664.99615.94\n10.1109/CVPR.2017.10\n10.5555/2999792.2999959\n10.1109/CVPR.2000.855856\n10.1145/219717.219748\n10.1109/CVPRW.2018.00294\n10.1109/WACV.2018.00047\n10.18653/v1/D16-1089\n10.1080/09332480.2014.914768\n10.1109/CVPR.2018.00749\n10.5555/2984093.2984252\n10.1109/ICCV.2011.6126281\n10.1109/CVPRW.2018.00278\n10.1109/CVPR.2012.6247998\n10.3115/v1/D14-1162\n10.18653/v1/N18-1202\n10.1109/CVPR.2016.247\n10.1109/CVPR.2017.117\n10.1007/978-3-030-20887-5_34\n10.1109/CVPR.2016.13\n10.1109/aiar.2018.8769804\n10.1109/CVPR.2014.24\n10.1109/TITS.2015.2421482\n10.5555/2999611.2999617\n10.1109/CVPR.2011.5995627\n10.1109/CVPR.2010.5540121\n10.1007/978-3-319-50077-5_2\n10.23937/2378-3656/1410264\n10.1109/ISMA.2008.4648837\n10.1109/TPAMI.2012.269\n10.1016/0306-4573(88)90021-0\n10.1109/CVPR.2019.00227\n10.1007/3-540-44581-1_27\n10.1109/CVPR.2019.00844\n10.1007/978-3-642-33715-4_18\n10.1109/CVPR.2007.383198\n10.1109/WACV.2018.00181\n10.1109/CVPR.2018.00379\n10.1109/CVPR.2018.00860\n10.1109/CVPR.2018.00329\n10.5555/3294996.3295163\n10.5555/2999611.2999716\n10.5555/2969442.2969628\n10.1109/CVPR.2018.00113\n10.1109/CVPR.2015.7298594\n10.1007/978-3-030-35699-6_25\n10.1007/11957959_18\n10.1109/ICCV.2017.386\n10.1145/1553374.1553509\n10.1109/CVPR.2015.7298658\n10.1155/2019/4180949\n10.5555/3295222.3295349\n10.1007/978-3-319-71246-8_48\n10.7551/mitpress/7503.001.0001\n10.5555/3016100.3016198\n10.1007/s11263-017-1027-5\n10.1109/ICCV.2019.00933\n10.1145/3293318\n10.1109/ICCV.2013.264\n10.1109/CVPR.2017.369\n10.1109/CVPR.2018.00717\n10.1007/978-3-319-46478-7_31\n10.1007/s10994-010-5198-3\n10.1002/ppul.24718\n10.1109/CVPR.2016.15\n10.1109/TPAMI.2018.2857768\n10.1109/CVPR.2018.00581\n10.1109/CVPR.2017.328\n10.1109/CVPR.2019.01052\n10.1109/CVPR.2019.00961\n10.1109/ICIP.2019.8803426\n10.1145/3078971.3078977\n10.1109/CVPR.2017.217\n10.1109/ICME.2017.8019425\n10.1109/TPAMI.2014.2388235\n10.1109/CVPR.2017.542\n10.1148/radiol.2020200343\n10.1007/978-3-642-15555-0_10\n10.1109/ICCV.2019.00124\n10.1109/CVPR.2017.321\n10.1016/j.media.2020.101664\n10.1109/ICCV.2015.474\n10.1109/CVPR.2016.649\n10.1007/978-3-319-46478-7_33\n10.1109/ICCVW.2017.310\n10.1148/radiol.2020200370\n10.1109/CVPR.2019.00311\n10.1109/CVPR.2018.00111\n10.1109/ICCV.2015.11"}
{"title": "A light CNN for detecting COVID-19 from CT scans of the chest.", "abstract": "Computer Tomography (CT) imaging of the chest is a valid diagnosis tool to detect COVID-19 promptly and to control the spread of the disease. In this work we propose a light Convolutional Neural Network (CNN) design, based on the model of the SqueezeNet, for the efficient discrimination of COVID-19 CT images with respect to other community-acquired pneumonia and/or healthy CT images. The architecture allows to an accuracy of 85.03% with an improvement of about 3.2% in the first dataset arrangement and of about 2.1% in the second dataset arrangement. The obtained gain, though of low entity, can be really important in medical diagnosis and, in particular, for Covid-19 scenario. Also the average classification time on a high-end workstation, 1.25\u00a0s, is very competitive with respect to that of more complex CNN designs, 13.41\u00a0s, witch require pre-processing. The proposed CNN can be executed on medium-end laptop without GPU acceleration in 7.81\u00a0s: this is impossible for methods requiring GPU acceleration. The performance of the method can be further improved with efficient pre-processing strategies for witch GPU acceleration is not necessary.", "journal": "Pattern recognition letters", "date": "2020-10-13", "authors": ["MatteoPolsinelli", "LuigiCinque", "GiuseppePlacidi"], "doi": "10.1016/j.patrec.2020.10.001"}
{"title": "Dynamic evaluation of lung involvement during coronavirus disease-2019 (COVID-19) with quantitative lung CT.", "abstract": "To identify and quantify lung changes associated with coronavirus disease-2019 (COVID-19) with quantitative lung CT during the disease.\nThis retrospective study reviewed COVID-19 patients who underwent multiple chest CT scans during their disease course. Quantitative lung CT was used to determine the nature and volume of lung involvement. A semi-quantitative scoring system was also used to evaluate lung lesions.\nThis study included eighteen cases (4 cases in mild type, 10 cases in moderate type, 4 cases in severe type, and without critical type cases) with confirmed COVID-19. Patients had a mean hospitalized period of 24.1 \u00b1 7.1 days (range: 14-38 days) and underwent an average CT scans of 3.9 \u00b1 1.6 (range: 2-8). The total volumes of lung abnormalities reached a peak of 8.8 \u00b1 4.1 days (range: 2-14 days). The ground-glass opacity (GGO) volume percentage was higher than the consolidative opacity (CO) volume percentage on the first CT examination (Z = 2.229, P = 0.026), and there was no significant difference between the GGO volume percentage and that of CO at the peak stage (Z = - 0.628, P = 0.53). The volume percentage of lung involvement identified by AI demonstrated a strong correlation with the total CT scores at each stage (r = 0.873, P = 0.0001).\nQuantitative lung CT can automatically identify the nature of lung involvement and quantify the dynamic changes of lung lesions on CT during COVID-19. For patients who recovered from COVID-19, GGO was the predominant imaging feature on the initial CT scan, while GGO and CO were the main appearances at peak stage.", "journal": "Emergency radiology", "date": "2020-10-11", "authors": ["ChunMa", "Xiao-LingWang", "Dong-MeiXie", "Yu-DanLi", "Yong-JiZheng", "Hai-BingZhang", "BingMing"], "doi": "10.1007/s10140-020-01856-4\n10.7150/thno.45985\n10.1148/radiol.2020200370.M\n10.1148/radiol.2020200230\n10.1007/s00330-019-06344-z\n10.1016/j.ejrad.2019.108774\n10.1126/science.1116480\n10.1148/radiol.2282030593\n10.1016/j.jpha.2020.03.004\n10.1016/j.jrid.2020.04.004\n10.21037/atm-20-3554\n10.2214/AJR.20.23034\n10.1007/s00259-020-04735-9\n10.1148/radiol.2020200236\n10.1148/radiol.2020200323\n10.2214/AJR"}
{"title": "A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks.", "abstract": "The use of imaging data has been reported to be useful for rapid diagnosis of COVID-19. Although computed tomography (CT) scans show a variety of signs caused by the viral infection, given a large amount of images, these visual features are difficult and can take a long time to be recognized by radiologists. Artificial intelligence methods for automated classification of COVID-19 on CT scans have been found to be very promising. However, current investigation of pretrained convolutional neural networks (CNNs) for COVID-19 diagnosis using CT data is limited. This study presents an investigation on 16 pretrained CNNs for classification of COVID-19 using a large public database of CT scans collected from COVID-19 patients and non-COVID-19 subjects. The results show that, using only 6 epochs for training, the CNNs achieved very high performance on the classification task. Among the 16 CNNs, DenseNet-201, which is the deepest net, is the best in terms of accuracy, balance between sensitivity and specificity, [Formula: see text] score, and area under curve. Furthermore, the implementation of transfer learning with the direct input of whole image slices and without the use of data augmentation provided better classification rates than the use of data augmentation. Such a finding alleviates the task of data augmentation and manual extraction of regions of interest on CT images, which are adopted by current implementation of deep-learning models for COVID-19 classification.", "journal": "Scientific reports", "date": "2020-10-11", "authors": ["Tuan DPham"], "doi": "10.1038/s41598-020-74164-z\n10.1007/s00330-020-06827-4\n10.1148/ryai.2020200053\n10.1007/s00330-020-06817-6\n10.1186/s12967-020-02324-w\n10.1007/s00330-020-06975-7\n10.1007/s00330-020-06863-0\n10.1038/s41591-020-0931-3\n10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020200905\n10.1101/2020.02.14.20023028\n10.1186/s40537-019-0197-0\n10.1016/j.cmpb.2020.105475\n10.1093/jbcr/irz103\n10.1109/ACCESS.2019.2919678\n10.1016/j.neucom.2018.05.083\n10.1109/TKDE.2008.239\n10.7763/IJMLC.2013.V3.307\n10.1007/s13748-016-0094-0\n10.1016/j.cmpb.2019.06.023"}
{"title": "Development and evaluation of an artificial intelligence system for COVID-19 diagnosis.", "abstract": "Early detection of COVID-19 based on chest CT enables timely treatment of patients and helps control the spread of the disease. We proposed an artificial intelligence (AI) system for rapid COVID-19 detection and performed extensive statistical analysis of CTs of COVID-19 based on the AI system. We developed and evaluated our system on a large dataset with more than 10 thousand CT volumes from COVID-19, influenza-A/B, non-viral community acquired pneumonia (CAP) and non-pneumonia subjects. In such a difficult multi-class diagnosis task, our deep convolutional neural network-based system is able to achieve an area under the receiver operating characteristic curve (AUC) of 97.81% for multi-way classification on test cohort of 3,199 scans, AUC of 92.99% and 93.25% on two publicly available datasets, CC-CCII and MosMedData respectively. In a reader study involving five radiologists, the AI system outperforms all of radiologists in more challenging tasks at a speed of two orders of magnitude above them. Diagnosis performance of chest x-ray (CXR) is compared to that of CT. Detailed interpretation of deep network is also performed to relate system outputs with CT presentations. The code is available at https://github.com/ChenWWWeixiang/diagnosis_covid19 .", "journal": "Nature communications", "date": "2020-10-11", "authors": ["ChengJin", "WeixiangChen", "YukunCao", "ZhanweiXu", "ZimengTan", "XinZhang", "LeiDeng", "ChuanshengZheng", "JieZhou", "HeshuiShi", "JianjiangFeng"], "doi": "10.1038/s41467-020-18685-1\n10.1148/radiol.2020200823\n10.1148/radiol.2020200642\n10.1016/j.chest.2020.04.003\n10.1148/radiol.2020201160\n10.1016/S1473-3099(20)30086-4\n10.1038/nature14539\n10.1038/nature21056\n10.1016/j.media.2017.07.005\n10.1038/s41591-018-0316-z\n10.1038/s41591-018-0300-7\n10.1038/s41591-019-0447-x\n10.1038/s41598-018-37186-2\n10.1038/s41591-020-0931-3\n10.1016/j.patcog.2018.07.031\n10.1109/TNNLS.2019.2892409\n10.1038/s41598-019-56589-3\n10.1016/j.cell.2020.04.045\n10.1148/radiol.2020200905\n10.1148/radiol.2020201491\n10.1109/TMI.2020.2995965\n10.1109/TMI.2020.2996256\n10.1109/TMI.2020.2995508\n10.1148/radiol.2020201874\n10.1118/1.3528204\n10.1158/0008-5472.CAN-17-0339\n10.1148/radiol.2020200463\n10.1183/16000617.0053-2016"}
{"title": "Severity assessment of COVID-19 using CT image features and laboratory indices.", "abstract": "The coronavirus disease 2019 (COVID-19) is now a global pandemic. Tens of millions of people have been confirmed with infection, and also more people are suspected. Chest computed tomography (CT) is recognized as an important tool for COVID-19 severity assessment. As the number of chest CT images increases rapidly, manual severity assessment becomes a labor-intensive task, delaying appropriate isolation and treatment. In this paper, a study of automatic severity assessment for COVID-19 is presented. Specifically, chest CT images of 118 patients (age 46.5 \u00b1 16.5 years, 64 male and 54 female) with confirmed COVID-19 infection are used, from which 63 quantitative features and 110 radiomics features are derived. Besides the chest CT image features, 36 laboratory indices of each patient are also used, which can provide complementary information from a different view. A random forest (RF) model is trained to assess the severity (non-severe or severe) according to the chest CT image features and laboratory indices. Importance of each chest CT image feature and laboratory index, which reflects the correlation to the severity of COVID-19, is also calculated from the RF model. Using three-fold cross-validation, the RF model shows promising results: 0.910 (true positive ratio), 0.858 (true negative ratio) and 0.890 (accuracy), along with AUC of 0.98. Moreover, several chest CT image features and laboratory indices are found to be highly related to COVID-19 severity, which could be valuable for the clinical diagnosis of COVID-19.", "journal": "Physics in medicine and biology", "date": "2020-10-09", "authors": ["ZhenyuTang", "WeiZhao", "XingzhiXie", "ZhengZhong", "FengShi", "TianminMa", "JunLiu", "DinggangShen"], "doi": "10.1088/1361-6560/abbf9e"}
{"title": "Machine Learning to Predict Mortality and Critical Events in a Cohort of Patients With COVID-19 in New York City: Model Development and Validation.", "abstract": "COVID-19 has infected millions of people worldwide and is responsible for several hundred thousand fatalities. The COVID-19 pandemic has necessitated thoughtful resource allocation and early identification of high-risk patients. However, effective methods to meet these needs are lacking.\nThe aims of this study were to analyze the electronic health records (EHRs) of patients who tested positive for COVID-19 and were admitted to hospitals in the Mount Sinai Health System in New York City; to develop machine learning models for making predictions about the hospital course of the patients over clinically meaningful time horizons based on patient characteristics at admission; and to assess the performance of these models at multiple hospitals and time points.\nWe used Extreme Gradient Boosting (XGBoost) and baseline comparator models to predict in-hospital mortality and critical events at time windows of 3, 5, 7, and 10 days from admission. Our study population included harmonized EHR data from five hospitals in New York City for 4098 COVID-19-positive patients admitted from March 15 to May 22, 2020. The models were first trained on patients from a single hospital (n=1514) before or on May 1, externally validated on patients from four other hospitals (n=2201) before or on May 1, and prospectively validated on all patients after May 1 (n=383). Finally, we established model interpretability to identify and rank variables that drive model predictions.\nUpon cross-validation, the XGBoost classifier outperformed baseline models, with an area under the receiver operating characteristic curve (AUC-ROC) for mortality of 0.89 at 3 days, 0.85 at 5 and 7 days, and 0.84 at 10 days. XGBoost also performed well for critical event prediction, with an AUC-ROC of 0.80 at 3 days, 0.79 at 5 days, 0.80 at 7 days, and 0.81 at 10 days. In external validation, XGBoost achieved an AUC-ROC of 0.88 at 3 days, 0.86 at 5 days, 0.86 at 7 days, and 0.84 at 10 days for mortality prediction. Similarly, the unimputed XGBoost model achieved an AUC-ROC of 0.78 at 3 days, 0.79 at 5 days, 0.80 at 7 days, and 0.81 at 10 days. Trends in performance on prospective validation sets were similar. At 7 days, acute kidney injury on admission, elevated LDH, tachypnea, and hyperglycemia were the strongest drivers of critical event prediction, while higher age, anion gap, and C-reactive protein were the strongest drivers of mortality prediction.\nWe externally and prospectively trained and validated machine learning models for mortality and critical events for patients with COVID-19 at different time horizons. These models identified at-risk patients and uncovered underlying relationships that predicted outcomes.", "journal": "Journal of medical Internet research", "date": "2020-10-08", "authors": ["AkhilVaid", "SulaimanSomani", "Adam JRussak", "Jessica KDe Freitas", "Fayzan FChaudhry", "IshanParanjpe", "Kipp WJohnson", "Samuel JLee", "RiccardoMiotto", "FelixRichter", "ShanZhao", "Noam DBeckmann", "NidhiNaik", "ArashKia", "PremTimsina", "AnuradhaLala", "ManishParanjpe", "EddyeGolden", "MatteoDanieletto", "ManbirSingh", "DaraMeyer", "Paul FO'Reilly", "LauraHuckins", "PatriciaKovatch", "JosephFinkelstein", "Robert MFreeman", "EdgarArgulian", "AndrewKasarskis", "BethanyPercha", "Judith AAberg", "EmiliaBagiella", "Carol RHorowitz", "BarbaraMurphy", "Eric JNestler", "Eric ESchadt", "Judy HCho", "CarlosCordon-Cardo", "ValentinFuster", "Dennis SCharney", "David LReich", "Erwin PBottinger", "Matthew ALevin", "JagatNarula", "Zahi AFayad", "Allan CJust", "Alexander WCharney", "Girish NNadkarni", "Benjamin SGlicksberg"], "doi": "10.2196/24018\n10.1056/NEJMoa2002032\n10.1001/jama.2020.4344\n10.1001/jama.2020.4683\n10.1101/2020.05.04.20090944\n10.1681/ASN.2020050615\n10.1016/j.jacc.2020.06.007\n10.2807/1560-7917.ES.2020.25.10.2000180\n10.2807/1560-7917.ES.2020.25.10.2000180\n10.1007/s10916-020-01562-1\n10.1136/bmj.m1328\n10.1186/s12941-020-00362-2\n10.1186/s12941-020-00362-2\n10.1016/j.jcv.2020.104370\n10.1186/s12967-020-02374-0\n10.1186/s12967-020-02374-0\n10.1136/bmj.m1966\n10.1101/2020.05.19.20103036\n10.1016/S2589-7500(20)30217-X\n10.1101/2020.02.27.20028027\n10.1371/journal.pone.0233328\n10.1371/journal.pone.0233328\n10.1038/s41591-020-0916-2\n10.1101/2020.05.06.20093435\n10.1101/2020.06.03.20121574\n10.1101/2020.04.22.20075416\n10.1101/2020.04.22.20075416\n10.1038/s41467-020-17280-8\n10.1038/s41467-020-17280-8\n10.2196/21439\n10.1161/CIRCOUTCOMES.120.006766\n10.1093/cid/ciaa443\n10.1007/s11427-020-1643-8\n10.1002/jmv.25884\n10.1145/2939672.2939785\n10.1038/s42256-019-0138-9\n10.1186/s12916-014-0241-z\n10.1186/s12916-014-0241-z\n10.1007/s00134-020-05991-x\n10.2139/ssrn.3546115\n10.1016/s2213-2600(20)30116-8\n10.4093/dmj.2020.0091\n10.1016/j.ando.2020.05.001\n10.1016/s0140-6736(20)30920-x\n10.1183/09031936.96.09081736\n10.1515/cclm-2020-0369\n10.1002/ajh.25829\n10.3390/jcm9061718\n10.1016/s2213-2600(20)30229-0\n10.1111/jce.14479\n10.1016/j.thromres.2020.04.013\n10.1056/nejmc2007575\n10.1001/jamacardio.2020.0950\n10.1093/cid/ciaa880\n10.1186/1471-2105-8-25\n10.1186/1471-2105-8-25"}
{"title": "Issues associated with deploying CNN transfer learning to detect COVID-19 from chest X-rays.", "abstract": "Covid-19 first occurred in Wuhan, China in December 2019. Subsequently, the virus spread throughout the world and as of June 2020 the total number of confirmed cases are above 4.7 million with over 315,000 deaths. Machine learning algorithms built on radiography images can be used as a decision support mechanism to aid radiologists to speed up the diagnostic process. The aim of this work is to conduct a critical analysis to investigate the applicability of convolutional neural networks (CNNs) for the purpose of COVID-19 detection in chest X-ray images and highlight the issues of using CNN directly on the whole image. To accomplish this task, we use 12-off-the-shelf CNN architectures in transfer learning mode on 3 publicly available chest X-ray databases together with proposing a shallow CNN architecture in which we train it from scratch. Chest X-ray images are fed into CNN models without any preprocessing to replicate researches used chest X-rays in this manner. Then a qualitative investigation performed to inspect the decisions made by CNNs using a technique known as class activation maps (CAM). Using CAMs, one can map the activations contributed to the decision of CNNs back to the original image to visualize the most discriminating region(s) on the input image. We conclude that CNN decisions should not be taken into consideration, despite their high classification accuracy, until clinicians can visually inspect and approve the region(s) of the input image used by CNNs that lead to its prediction.", "journal": "Physical and engineering sciences in medicine", "date": "2020-10-08", "authors": ["TabanMajeed", "RasberRashid", "DashtiAli", "ArasAsaad"], "doi": "10.1007/s13246-020-00934-8\n10.1148/radiol.2020200642\n10.1007/s13246-020-00865-4\n10.1101/2020.02.14.20023028v5\n10.1101/2020.03.20.20039834\n10.1016/j.compbiomed.2020.103792\n10.1038/s42256-020-0185-2\n10.1038/nature14539\n10.1113/jphysiol.1968.sp008455\n10.1007/s11263-015-0816-y\n10.1109/ACCESS.2017.2784352\n10.1016/j.patcog.2017.10.013\n10.1007/s13244-018-0639-9\n10.1016/j.cell.2018.02.010\n10.1007/s11263-019-01228-7"}
{"title": "How to Improve Compliance with Protective Health Measures during the COVID-19 Outbreak: Testing a Moderated Mediation Model and Machine Learning Algorithms.", "abstract": "In the wake of the sudden spread of COVID-19, a large amount of the Italian population practiced incongruous behaviors with the protective health measures. The present study aimed at examining psychological and psychosocial variables that could predict behavioral compliance. An online survey was administered from 18-22 March 2020 to 2766 participants. Paired sample ", "journal": "International journal of environmental research and public health", "date": "2020-10-07", "authors": ["PaoloRoma", "MerylinMonaro", "LauraMuzi", "MarcoColasanti", "EleonoraRicci", "SilviaBiondi", "ChristianNapoli", "StefanoFerracuti", "CristinaMazza"], "doi": "10.3390/ijerph17197252\n10.1186/1471-2458-11-575\n10.1371/journal.pone.0016460\n10.3390/ijerph17093165\n10.1037/tra0000672\n10.3390/ijerph17176236\n10.3389/fpsyg.2020.567367\n10.1177/109019817400200407\n10.1080/00223980.1975.9915803\n10.1016/0277-9536(87)90367-4\n10.5993/AJHB.29.2.2\n10.1111/jan.13894\n10.1177/002224379503200206\n10.1136/bmj.b2651\n10.29252/qums.12.3.76\n10.1111/pcn.13122\n10.1111/ajsp.12104\n10.1037/0278-6133.19.5.487\n10.1037/0278-6133.26.2.136\n10.1186/1471-2458-11-2\n10.1080/07448481.2011.570398\n10.1016/j.ajic.2010.03.002\n10.1111/j.1468-2958.2003.tb00844.x\n10.1177/0013916512437596\n10.1177/2158244013495542\n10.3201/eid2607.200500\n10.31234/osf.io/5tmsh\n10.1177/1461444809341264\n10.1080/10447318.2016.1220070\n10.18632/aging.103344\n10.1348/135910710X485826\n10.2147/RMHP.S257287\n10.1017/S0008423920000335\n10.3390/ijerph17144924\n10.1016/j.paid.2003.08.018\n10.1371/journal.pone.0013411\n10.1080/00223891.2013.823438\n10.1016/j.paid.2020.110199\n10.1016/j.paid.2020.110346\n10.1177/1745691617693393\n10.1146/annurev-clinpsy-032816-045037\n10.1371/journal.pone.0227113\n10.3389/fpsyt.2019.00389\n10.1038/s41598-020-61636-5\n10.3389/fpsyg.2019.02970\n10.1186/1471-2334-14-169\n10.1080/03637751.2017.1352100\n10.3758/BF03206553\n10.1145/1656274.1656278\n10.1126/science.aaa9375\n10.2307/2347628\n10.1162/089976601300014493\n10.1023/A:1010933404324\n10.3233/IDA-2002-6504\n10.1016/j.jhin.2020.04.035\n10.1371/journal.pone.0233744\n10.1016/j.socscimed.2020.113370\n10.1186/1471-2334-10-296\n10.1016/j.forsciint.2013.08.016"}
{"title": "Clinical Characteristics and Outcomes of Severe and Critical Patients With 2019 Novel Coronavirus Disease (COVID-19) in Wenzhou: A Retrospective Study.", "abstract": "Information about severe cases of 2019 novel coronavirus disease (COVID-19) infection is scarce. The aim of this study was to report the clinical characteristics and outcomes of severe and critical patients with confirmed COVID-19 infection in Wenzhou city. In this single-centered, retrospective cohort study, we consecutively enrolled 37 RT-PCR confirmed positive severe or critical patients from January 28 to February 16, 2020 in a tertiary hospital. Outcomes were followed up until 28-day mortality. Fifteen severe and 22 critical adult patients with the COVID-19 infection were included. Twenty-six (68.4%) were men. Echocardiography data results suggest that normal or increased cardiac output and diastolic dysfunction are the most common manifestations. Compared with severe patients, critical patients were older, more likely to exhibit low platelet counts and high blood urea nitrogen, and were in hospital for longer. Most patients had organ dysfunction during hospitalization, including 11 (29.7%) with ARDS, 8 (21.6%) with acute kidney injury, 17 (45.9%) with acute cardiac injury, and 33 (89.2%) with acute liver dysfunction. Eighteen (48.6%) patients were treated with high-flow ventilation, 9 (13.8%) with non-invasive ventilation, 10 (15.4%) with invasive mechanical ventilation, 7 (18.9%) with prone position ventilation, 6 (16.2%) with extracorporeal membrane oxygenation (ECMO), and 3 (8.1%) with renal replacement therapy. Only 1 (2.7%) patient had died in the 28-day follow up in our study. All patients had bilateral infiltrates on their chest CT scan. Twenty-one (32.3%) patients presented ground glass opacity (GGO) with critical patients more localized in the periphery and the center. The mortality of critical patients with the COVID-19 infection is low in our study. Cardiac function was enhanced in the early stage and less likely to develop into acute cardiac injury, but most patients suffered with acute liver injury. The CT imaging presentations of COVID-19 in critical patients were more likely with consolidation and bilateral lung involvement.", "journal": "Frontiers in medicine", "date": "2020-10-06", "authors": ["Song-ZanQian", "Wan-DongHong", "NoneLingjie-Mao", "NoneChenfeng-Lin", "NoneZhendong-Fang", "Jing-YePan"], "doi": "10.3389/fmed.2020.552002\n10.1136/bmj.m606\n10.1016/j.jinf.2020.02.017\n10.1101/2020.02.06.20020974\n10.1016/j.accpm.2020.03.001\n10.1016/j.jinf.2020.02.016\n10.1016/S0140-6736(20)30183-5\n10.1016/S2213-2600(20)30079-5\n10.1001/jama.2020.1585\n10.7326/M13-2486\n10.1016/S0140-6736(20)30211-7\n10.1086/432007\n10.1016/j.tips.2004.04.001\n10.1038/s41569-020-0360-5\n10.1101/2020.02.03.931766\n10.1001/jama.286.14.1754\n10.1001/jama.2016.0287\n10.1146/annurev.immunol.021908.132625\n10.1101/2020.02.24.20027052"}
{"title": "The Performance of Deep Neural Networks in Differentiating Chest X-Rays of COVID-19 Patients From Other Bacterial and Viral Pneumonias.", "abstract": "Chest radiography is a critical tool in the early detection, management planning, and follow-up evaluation of COVID-19 pneumonia; however, in smaller clinics around the world, there is a shortage of radiologists to analyze large number of examinations especially performed during a pandemic. Limited availability of high-resolution computed tomography and real-time polymerase chain reaction in developing countries and regions of high patient turnover also emphasizes the importance of chest radiography as both a screening and diagnostic tool. In this paper, we compare the performance of 17 available deep learning algorithms to help identify imaging features of COVID19 pneumonia. We utilize an existing diagnostic technology (chest radiography) and preexisting neural networks (DarkNet-19) to detect imaging features of COVID-19 pneumonia. Our approach eliminates the extra time and resources needed to develop new technology and associated algorithms, thus aiding the front-line healthcare workers in the race against the COVID-19 pandemic. Our results show that DarkNet-19 is the optimal pre-trained neural network for the detection of radiographic features of COVID-19 pneumonia, scoring an overall accuracy of 94.28% over 5,854 X-ray images. We also present a custom visualization of the results that can be used to highlight important visual biomarkers of the disease and disease progression.", "journal": "Frontiers in medicine", "date": "2020-10-06", "authors": ["MohamedElgendi", "Muhammad UmerNasir", "QunfengTang", "Richard RibonFletcher", "NewtonHoward", "CarloMenon", "RababWard", "WilliamParker", "SavvasNicolaou"], "doi": "10.3389/fmed.2020.00550\n10.23750/abm.v91i1.9397\n10.1016/S0140-6736(20)30183-5\n10.1128/JCM.00556-09\n10.1001/jama.2020.2648\n10.1097/RLI.0000000000000670\n10.1016/j.acra.2018.02.018\n10.20944/preprints202003.0300.v1\n10.1007/s13246-020-00865-4\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.90\n10.1109/CVPR.2018.00716\n10.1109/CVPR.2018.00907\n10.1109/CVPR.2017.195\n10.1109/CVPR.2018.00474\n10.1109/CVPR.2017.243\n10.1007/s11263-015-0816-y\n10.1016/j.ergon.2011.05.001"}
{"title": "Review on Diagnosis of COVID-19 from Chest CT Images Using Artificial Intelligence.", "abstract": "The COVID-19 diagnostic approach is mainly divided into two broad categories, a laboratory-based and chest radiography approach. The last few months have witnessed a rapid increase in the number of studies use artificial intelligence (AI) techniques to diagnose COVID-19 with chest computed tomography (CT). In this study, we review the diagnosis of COVID-19 by using chest CT toward AI. We searched ArXiv, MedRxiv, and Google Scholar using the terms \"deep learning\", \"neural networks\", \"COVID-19\", and \"chest CT\". At the time of writing (August 24, 2020), there have been nearly 100 studies and 30 studies among them were selected for this review. We categorized the studies based on the classification tasks: COVID-19/normal, COVID-19/non-COVID-19, COVID-19/non-COVID-19 pneumonia, and severity. The sensitivity, specificity, precision, accuracy, area under the curve, and F1 score results were reported as high as 100%, 100%, 99.62, 99.87%, 100%, and 99.5%, respectively. However, the presented results should be carefully compared due to the different degrees of difficulty of different classification tasks.", "journal": "Computational and mathematical methods in medicine", "date": "2020-10-06", "authors": ["IlkerOzsahin", "BoranSekeroglu", "Musa SaniMusa", "Mubarak TaiwoMustapha", "DilberUzun Ozsahin"], "doi": "10.1155/2020/9756518\n10.1109/rbme.2020.2987975\n10.1148/radiol.2020200823\n10.1148/radiol.2020200432\n10.1016/j.crad.2020.06.005\n10.1186/s41747-018-0061-6\n10.1148/radiol.2020200905\n10.1101/2020.04.24.20078998\n10.1007/978-3-030-01264-9_8\n10.1109/cvpr.2018.00474\n10.1109/CVPR.2017.195\n10.1109/cvpr.2016.308\n10.1109/CVPR.2016.90\n10.1080/07391102.2020.1788642\n10.1118/1.3528204\n10.1101/2020.03.20.20039834\n10.1016/j.compmedimag.2011.07.003\n10.1007/s10096-020-03901-z\n10.1101/2020.04.16.20064709\n10.36227/techrxiv.12334265.v2\n10.1109/tmi.2020.2995965\n10.1101/2020.02.25.20021568\n10.1101/2020.03.19.20039354\n10.1109/cvpr.2017.683\n10.1016/j.irbm.2020.05.003\n10.1109/TMI.2020.2996256\n10.1038/s41467-020-17971-2\n10.1148/ryct.2020200026\n10.1101/2020.04.13.20063941\n10.1016/j.eng.2020.04.010\n10.1183/13993003.00775-2020\n10.1148/radiol.2020201491\n10.1109/TMI.2020.2992546\n10.1101/2020.02.23.20026930\n10.2196/19569\n10.1007/s00330-020-07044-9\n10.3389/fbioe.2020.00898\n10.1371/journal.pone.0236621\n10.1007/s00330-020-07156-2"}
{"title": "Effectiveness of COVID-19 diagnosis and management tools: A review.", "abstract": "To review the available literature concerning the effectiveness of the COVID-19 diagnostic tools.\nWith the absence of specific treatment/vaccines for the coronavirus COVID-19, the most appropriate approach to control this infection is to quarantine people and isolate symptomatic people and suspected or infected cases. Although real-time reverse transcription-polymerase chain reaction (RT-PCR) assay is considered the first tool to make a definitive diagnosis of COVID-19 disease, the high false negative rate, low sensitivity, limited supplies and strict requirements for laboratory settings might delay accurate diagnosis. Computed tomography (CT) has been reported as an important tool to identify and investigate suspected patients with COVID-19 disease at early stage.\nRT-PCR shows low sensitivity (60-71%) in diagnosing patients with COVID-19 infection compared to the CT chest. Several studies reported that chest CT scans show typical imaging features in all patients with COVID-19. This high sensitivity and initial presentation in CT chest can be helpful in rectifying false negative results obtained from RT-PCR. As COVID-19 has similar manifestations to other pneumonia diseases, artificial intelligence (AI) might help radiologists to differentiate COVID-19 from other pneumonia diseases.\nAlthough CT scan is a powerful tool in COVID-19 diagnosis, it is not sufficient to detect COVID-19 alone due to the low specificity (25%), and challenges that radiologists might face in differentiating COVID-19 from other viral pneumonia on chest CT scans. AI might help radiologists to differentiate COVID-19 from other pneumonia diseases.\nBoth RT-PCR and CT tests together would increase sensitivity and improve quarantine efficacy, an impact neither could achieve alone.", "journal": "Radiography (London, England : 1995)", "date": "2020-10-04", "authors": ["WAlsharif", "AQurashi"], "doi": "10.1016/j.radi.2020.09.010\n10.1016/S1473-3099(20)30244-9\n10.1101/2020.04.05.20054361\n10.1111/joim.13091\n10.1101/2020.02.06.20020974\n10.1148/radiol.2020200642\n10.1101/2020.02.11.20021493\n10.1101/2020.03.30.20047985\n10.1101/2020.02.14.20023028\n10.1148/radiol.2020201237\n10.1016/j.jinf.2020.02.022\n10.1148/radiol.2020200343\n10.1148/radiol.2020200905\n10.1101/2020.04.11.20062372\n10.1109/RBME.2020.2987975\n10.1148/radiol.2020200527\n10.1148/radiol.2020200432\n10.1148/radiol.2020200490\n10.1101/2020.03.20.20039834\n10.1016/j.acra.2020.01.035\n10.1148/radiol.2020200905\n10.1101/2020.02.25.20021568\n10.1101/2020.03.20.20037325\n10.1148/radiol.2020200823\n10.1148/radiol.2020201491\n10.1101/2020.03.12.20027185"}
{"title": "Artificial intelligence in pulmonary medicine: computer vision, predictive model and COVID-19.", "abstract": "Artificial intelligence (AI) is transforming healthcare delivery. The digital revolution in medicine and healthcare information is prompting a staggering growth of data intertwined with elements from many digital sources such as genomics, medical imaging and electronic health records. Such massive growth has sparked the development of an increasing number of AI-based applications that can be deployed in clinical practice. Pulmonary specialists who are familiar with the principles of AI and its applications will be empowered and prepared to seize future practice and research opportunities. The goal of this review is to provide pulmonary specialists and other readers with information pertinent to the use of AI in pulmonary medicine. First, we describe the concept of AI and some of the requisites of machine learning and deep learning. Next, we review some of the literature relevant to the use of computer vision in medical imaging, predictive modelling with machine learning, and the use of AI for battling the novel severe acute respiratory syndrome-coronavirus-2 pandemic. We close our review with a discussion of limitations and challenges pertaining to the further incorporation of AI into clinical pulmonary practice.", "journal": "European respiratory review : an official journal of the European Respiratory Society", "date": "2020-10-03", "authors": ["DanaiKhemasuwan", "Jeffrey SSorensen", "Henri GColt"], "doi": "10.1183/16000617.0181-2020\n10.1080/17476348.2020.1743181\n10.1136/thoraxjnl-2020-214556\n10.1183/13993003.01216-2019\n10.1007/s41030-020-00110-z\n10.1097/MCP.0000000000000459\n10.1111/resp.13676\n10.1109/TBME.1985.325532\n10.1016/0010-4809(83)90021-6\n10.1097/00004669-198805000-00010\n10.1016/j.jelectrocard.2016.04.010\n10.1016/0004-3702(78)90014-0\n10.1056/NEJM199406233302506\n10.7326/0003-4819-108-1-80\n10.1161/CIRCULATIONAHA.115.001593\n10.1186/s12874-019-0681-4\n10.1056/NEJMp1702071\n10.1016/j.jacr.2019.07.019\n10.1109/72.935086\n10.1186/s13054-017-1836-5\n10.1038/s41591-018-0310-5\n10.1038/s41591-018-0213-5\n10.1007/s11263-015-0816-y\n10.1126/science.aaa8685\n10.1038/s41746-019-0122-0\n10.1038/nature24270\n10.1038/s41568-018-0016-5\n10.1038/nature14539\n10.1038/s41591-019-0536-x\n10.1136/thoraxjnl-2019-214104\n10.1164/rccm.201903-0505OC\n10.1038/srep46479\n10.1148/radiol.2018180237\n10.1186/s13550-017-0260-9\n10.21037/qims.2018.06.03\n10.1158/0008-5472.CAN-18-0696\n10.1097/RLI.0000000000000574\n10.1016/S2213-2600(19)30059-1\n10.1161/CIRCRESAHA.118.313911\n10.1371/journal.pone.0224453\n10.1164/rccm.201808-1543OC\n10.1016/j.chest.2018.01.037\n10.1183/13993003.01660-2018\n10.1148/radiol.2020200905\n10.1001/jama.2016.17216\n10.1038/nature21056\n10.1001/jama.2017.14585\n10.1148/radiol.10091808\n10.1177/0969141317727771\n10.1001/jamaoncol.2016.6416\n10.1016/S2589-7500(19)30123-2\n10.1136/thoraxjnl-2015-207252\n10.1016/j.ejrad.2015.08.016\n10.2214/AJR.15.15674\n10.1016/S2213-2600(15)00140-X\n10.1513/AnnalsATS.201612-947OC\n10.1164/rccm.200711-1754OC\n10.1164/rccm.200906-0896OC\n10.1056/NEJMoa1012740\n10.1159/000454956\n10.1016/S2213-2600(13)70184-X\n10.1183/09031936.05.00035205\n10.1016/S0140-6736(16)00080-5\n10.1016/j.compbiomed.2020.103792\n10.1016/j.ajem.2020.04.016\n10.4049/jimmunol.1900033\n10.1016/j.csbj.2020.03.025\n10.1038/s41586-019-1923-7\n10.1152/physiolgenomics.00029.2020\n10.21037/jtd.2020.02.64\n10.1016/S1473-3099(20)30243-7\n10.1016/S1473-3099(20)30120-1\n10.1007/s00146-020-00978-0\n10.1038/s41591-018-0300-7\n10.1093/annonc/mdx781\n10.1016/S0140-6736(16)32380-7\n10.1001/jama.2019.16489\n10.1186/s40537-014-0007-7"}
{"title": "Application of an Artificial Intelligence Trilogy to Accelerate Processing of Suspected Patients With SARS-CoV-2 at a Smart Quarantine Station: Observational Study.", "abstract": "As the COVID-19 epidemic increases in severity, the burden of quarantine stations outside emergency departments (EDs) at hospitals is increasing daily. To address the high screening workload at quarantine stations, all staff members with medical licenses are required to work shifts in these stations. Therefore, it is necessary to simplify the workflow and decision-making process for physicians and surgeons from all subspecialties.\nThe aim of this paper is to demonstrate how the National Cheng Kung University Hospital artificial intelligence (AI) trilogy of diversion to a smart quarantine station, AI-assisted image interpretation, and a built-in clinical decision-making algorithm improves medical care and reduces quarantine processing times.\nThis observational study on the emerging COVID-19 pandemic included 643 patients. An \"AI trilogy\" of diversion to a smart quarantine station, AI-assisted image interpretation, and a built-in clinical decision-making algorithm on a tablet computer was applied to shorten the quarantine survey process and reduce processing time during the COVID-19 pandemic.\nThe use of the AI trilogy facilitated the processing of suspected cases of COVID-19 with or without symptoms; also, travel, occupation, contact, and clustering histories were obtained with the tablet computer device. A separate AI-mode function that could quickly recognize pulmonary infiltrates on chest x-rays was merged into the smart clinical assisting system (SCAS), and this model was subsequently trained with COVID-19 pneumonia cases from the GitHub open source data set. The detection rates for posteroanterior and anteroposterior chest x-rays were 55/59 (93%) and 5/11 (45%), respectively. The SCAS algorithm was continuously adjusted based on updates to the Taiwan Centers for Disease Control public safety guidelines for faster clinical decision making. Our ex vivo study demonstrated the efficiency of disinfecting the tablet computer surface by wiping it twice with 75% alcohol sanitizer. To further analyze the impact of the AI application in the quarantine station, we subdivided the station group into groups with or without AI. Compared with the conventional ED (n=281), the survey time at the quarantine station (n=1520) was significantly shortened; the median survey time at the ED was 153 minutes (95% CI 108.5-205.0), vs 35 minutes at the quarantine station (95% CI 24-56; P<.001). Furthermore, the use of the AI application in the quarantine station reduced the survey time in the quarantine station; the median survey time without AI was 101 minutes (95% CI 40-153), vs 34 minutes (95% CI 24-53) with AI in the quarantine station (P<.001).\nThe AI trilogy improved our medical care workflow by shortening the quarantine survey process and reducing the processing time, which is especially important during an emerging infectious disease epidemic.", "journal": "Journal of medical Internet research", "date": "2020-10-02", "authors": ["Ping-YenLiu", "Yi-ShanTsai", "Po-LinChen", "Huey-PinTsai", "Ling-WeiHsu", "Chi-ShiangWang", "Nan-YaoLee", "Mu-ShiangHuang", "Yun-ChiaoWu", "Wen-ChienKo", "Yi-ChingYang", "Jung-HsienChiang", "Meng-RuShen"], "doi": "10.2196/19878\n10.1016/s0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1038/s41564-020-0695-z\n10.1056/NEJMoa2001316\n10.1016/j.epidem.2011.01.001\n10.12688/wellcomeopenres.15718.1\n10.2807/1560-7917.ES.2020.25.4.2000058\n10.2807/1560-7917.ES.2020.25.4.2000058\n10.1016/s0140-6736(20)30374-3\n10.2139/ssrn.3524675\n10.1001/jama.2020.3151\n10.3348/kjr.2020.0132\n10.1148/radiol.2020200370\n10.1016/s1473-3099(20)30086-4\n10.1016/j.jacr.2020.02.008\n10.1109/cvpr.2016.319\n10.1109/cvpr.2016.90\n10.1109/cvpr.2018.00745\n10.1056/NEJMp2003539"}
{"title": "Detection Methods of COVID-19.", "abstract": "Since being first detected in China, coronavirus disease 2019 (COVID-19) has spread rapidly across the world, triggering a global pandemic with no viable cure in sight. As a result, national responses have focused on the effective minimization of the spread. Border control measures and travel restrictions have been implemented in a number of countries to limit the import and export of the virus. The detection of COVID-19 is a key task for physicians. The erroneous results of early laboratory tests and their delays led researchers to focus on different options. Information obtained from computed tomography (CT) and radiological images is important for clinical diagnosis. Therefore, it is worth developing a rapid method of detection of viral diseases through the analysis of radiographic images. We propose a novel method of detection of COVID-19. The purpose is to provide clinical decision support to healthcare workers and researchers. The article is to support researchers working on early detection of COVID-19 as well as similar viral diseases.", "journal": "SLAS technology", "date": "2020-10-01", "authors": ["AmiraEchtioui", "WassimZouch", "MohamedGhorbel", "ChokriMhiri", "HabibHamam"], "doi": "10.1177/2472630320962002\n10.1148/radiol\n10.1101/2020.02.14.20023028"}
{"title": "AI for radiographic COVID-19 detection selects shortcuts over signal.", "abstract": "Artificial intelligence (AI) researchers and radiologists have recently reported AI systems that accurately detect COVID-19 in chest radiographs. However, the robustness of these systems remains unclear. Using state-of-the-art techniques in explainable AI, we demonstrate that recent deep learning systems to detect COVID-19 from chest radiographs rely on confounding factors rather than medical pathology, creating an alarming situation in which the systems appear accurate, but fail when tested in new hospitals. We observe that the approach to obtain training data for these AI systems introduces a nearly ideal scenario for AI to learn these spurious \"shortcuts.\" Because this approach to data collection has also been used to obtain training data for detection of COVID-19 in computed tomography scans and for medical imaging tasks related to other diseases, our study reveals a far-reaching problem in medical imaging AI. In addition, we show that evaluation of a model on external data is insufficient to ensure AI systems rely on medically relevant pathology, since the undesired \"shortcuts\" learned by AI systems may not impair performance in new hospitals. These findings demonstrate that explainable AI should be seen as a prerequisite to clinical deployment of ML healthcare models.", "journal": "medRxiv : the preprint server for health sciences", "date": "2020-10-01", "authors": ["Alex JDeGrave", "Joseph DJanizek", "Su-InLee"], "doi": "10.1101/2020.09.13.20193565"}
{"title": "Viral epitope profiling of COVID-19 patients reveals cross-reactivity and correlates of severity.", "abstract": "Understanding humoral responses to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is critical for improving diagnostics, therapeutics, and vaccines. Deep serological profiling of 232 coronavirus disease 2019 (COVID-19) patients and 190 pre-COVID-19 era controls using VirScan revealed more than 800 epitopes in the SARS-CoV-2 proteome, including 10 epitopes likely recognized by neutralizing antibodies. Preexisting antibodies in controls recognized SARS-CoV-2 ORF1, whereas only COVID-19 patient antibodies primarily recognized spike protein and nucleoprotein. A machine learning model trained on VirScan data predicted SARS-CoV-2 exposure history with 99% sensitivity and 98% specificity; a rapid Luminex-based diagnostic was developed from the most discriminatory SARS-CoV-2 peptides. Individuals with more severe COVID-19 exhibited stronger and broader SARS-CoV-2 responses, weaker antibody responses to prior infections, and higher incidence of cytomegalovirus and herpes simplex virus 1, possibly influenced by demographic covariates. Among hospitalized patients, males produce stronger SARS-CoV-2 antibody responses than females.", "journal": "Science (New York, N.Y.)", "date": "2020-10-01", "authors": ["EllenShrock", "EricFujimura", "TomaszKula", "Richard TTimms", "I-HsiuLee", "YumeiLeng", "Matthew LRobinson", "Brandon MSie", "Mamie ZLi", "YuezhouChen", "JenniferLogue", "AdamZuiani", "DeniseMcCulloch", "Felipe J NLelis", "StephanieHenson", "Daniel RMonaco", "MeghanTravers", "ShaghayeghHabibi", "William AClarke", "PatrizioCaturegli", "OliverLaeyendecker", "AlicjaPiechocka-Trocha", "Jonathan ZLi", "AshokKhatri", "Helen YChu", "NoneNone", "Alexandra-Chlo\u00e9Villani", "KyleKays", "Marcia BGoldberg", "NirHacohen", "Michael RFilbin", "Xu GYu", "Bruce DWalker", "Duane RWesemann", "H BenjaminLarman", "James ALederer", "Stephen JElledge"], "doi": "10.1126/science.abd4250\n10.1038/s41579-018-0118-9\n10.4014/jmb.2003.03011\n10.1016/j.clim.2020.108427\n10.1038/nbt.1856\n10.1038/s41596-018-0025-6\n10.1126/science.aaa0698\n10.1126/science.aay6485\n10.1038/s41586-020-2012-7\n10.1128/CVI.00278-10\n10.1038/s42256-019-0138-9\n10.1016/j.ymeth.2019.01.014\n10.1016/j.cell.2020.05.015\n10.1038/s41586-020-2550-z\n10.1128/JVI.02015-19\n10.1016/j.bbrc.2014.07.090\n10.1001/jama.2020.8598\n10.15585/mmwr.mm6915e3\n10.1172/JCI64096\n10.1038/s41467-020-16638-2\n10.1016/j.cell.2020.02.058\n10.1038/s41586-020-2180-5\n10.1126/science.abb2507\n10.1038/s41586-020-2381-y\n10.1371/journal.pone.0200267\n10.1086/652438\n10.1016/j.immuni.2008.09.008\n10.1007/s00262-005-0109-3\n10.1046/j.1365-2567.1997.00310.x\n10.1111/acel.12059\n10.1128/CMR.00102-14\n10.1093/molbev/msy096\n10.1073/pnas.0404206101\n10.1038/s41586-020-2286-9\n10.1093/nar/gkx346\n10.1002/(SICI)1096-987X(199802)19:3<319::AID-JCC6>3.0.CO;2-W"}
{"title": "Improving the performance of CNN to predict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms.", "abstract": "This study aims to develop and test a new computer-aided diagnosis (CAD) scheme of chest X-ray images to detect coronavirus (COVID-19) infected pneumonia.\nCAD scheme first applies two image preprocessing steps to remove the majority of diaphragm regions, process the original image using a histogram equalization algorithm, and a bilateral low-pass filter. Then, the original image and two filtered images are used to form a pseudo color image. This image is fed into three input channels of a transfer learning-based convolutional neural network (CNN) model to classify chest X-ray images into 3 classes of COVID-19 infected pneumonia, other community-acquired no-COVID-19 infected pneumonia, and normal (non-pneumonia) cases. To build and test the CNN model, a publicly available dataset involving 8474 chest X-ray images is used, which includes 415, 5179 and 2,880 cases in three classes, respectively. Dataset is randomly divided into 3 subsets namely, training, validation, and testing with respect to the same frequency of cases in each class to train and test the CNN model.\nThe CNN-based CAD scheme yields an overall accuracy of 94.5 % (2404/2544) with a 95 % confidence interval of [0.93,0.96] in classifying 3 classes. CAD also yields 98.4 % sensitivity (124/126) and 98.0 % specificity (2371/2418) in classifying cases with and without COVID-19 infection. However, without using two preprocessing steps, CAD yields a lower classification accuracy of 88.0 % (2239/2544).\nThis study demonstrates that adding two image preprocessing steps and generating a pseudo color image plays an important role in developing a deep learning CAD scheme of chest X-ray images to improve accuracy in detecting COVID-19 infected pneumonia.", "journal": "International journal of medical informatics", "date": "2020-09-30", "authors": ["MortezaHeidari", "SeyedehnafisehMirniaharikandehei", "Abolfazl ZargariKhuzani", "GopichandhDanala", "YuchenQiu", "BinZheng"], "doi": "10.1016/j.ijmedinf.2020.104284\n10.1101/2020.02.14.20023028\n10.1007/s13246-020-00865-4\n10.17632/rscbjbr9sj.3"}
{"title": "Learning distinctive filters for COVID-19 detection from chest X-ray using shuffled residual CNN.", "abstract": "COVID-19 is a deadly viral infection that has brought a significant threat to human lives. Automatic diagnosis of COVID-19 from medical imaging enables precise medication, helps to control community outbreak, and reinforces coronavirus testing methods in place. While there exist several challenges in manually inferring traces of this viral infection from X-ray, Convolutional Neural Network (CNN) can mine data patterns that capture subtle distinctions between infected and normal X-rays. To enable automated learning of such latent features, a custom CNN architecture has been proposed in this research. It learns unique convolutional filter patterns for each kind of pneumonia. This is achieved by restricting certain filters in a convolutional layer to maximally respond only to a particular class of pneumonia/COVID-19. The CNN architecture integrates different convolution types to aid better context for learning robust features and strengthen gradient flow between layers. The proposed work also visualizes regions of saliency on the X-ray that have had the most influence on CNN's prediction outcome. To the best of our knowledge, this is the first attempt in deep learning to learn custom filters within a single convolutional layer for identifying specific pneumonia classes. Experimental results demonstrate that the proposed work has significant potential in augmenting current testing methods for COVID-19. It achieves an F1-score of 97.20% and an accuracy of 99.80% on the COVID-19 X-ray set.", "journal": "Applied soft computing", "date": "2020-09-30", "authors": ["RKarthik", "RMenaka", "HariharanM"], "doi": "10.1016/j.asoc.2020.106744"}
{"title": "Deep learning-based triage and analysis of lesion burden for COVID-19: a retrospective study with external validation.", "abstract": "Prompt identification of patients suspected to have COVID-19 is crucial for disease control. We aimed to develop a deep learning algorithm on the basis of chest CT for rapid triaging in fever clinics.\nWe trained a U-Net-based model on unenhanced chest CT scans obtained from 2447 patients admitted to Tongji Hospital (Wuhan, China) between Feb 1, 2020, and March 3, 2020 (1647 patients with RT-PCR-confirmed COVID-19 and 800 patients without COVID-19) to segment lung opacities and alert cases with COVID-19 imaging manifestations. The ability of artificial intelligence (AI) to triage patients suspected to have COVID-19 was assessed in a large external validation set, which included 2120 retrospectively collected consecutive cases from three fever clinics inside and outside the epidemic centre of Wuhan (Tianyou Hospital [Wuhan, China; area of high COVID-19 prevalence], Xianning Central Hospital [Xianning, China; area of medium COVID-19 prevalence], and The Second Xiangya Hospital [Changsha, China; area of low COVID-19 prevalence]) between Jan 22, 2020, and Feb 14, 2020. To validate the sensitivity of the algorithm in a larger sample of patients with COVID-19, we also included 761 chest CT scans from 722 patients with RT-PCR-confirmed COVID-19 treated in a makeshift hospital (Guanggu Fangcang Hospital, Wuhan, China) between Feb 21, 2020, and March 6, 2020. Additionally, the accuracy of AI was compared with a radiologist panel for the identification of lesion burden increase on pairs of CT scans obtained from 100 patients with COVID-19.\nIn the external validation set, using radiological reports as the reference standard, AI-aided triage achieved an area under the curve of 0\u00b7953 (95% CI 0\u00b7949-0\u00b7959), with a sensitivity of 0\u00b7923 (95% CI 0\u00b7914-0\u00b7932), specificity of 0\u00b7851 (0\u00b7842-0\u00b7860), a positive predictive value of 0\u00b7790 (0\u00b7777-0\u00b7803), and a negative predictive value of 0\u00b7948 (0\u00b7941-0\u00b7954). AI took a median of 0\u00b755 min (IQR: 0\u00b743-0\u00b763) to flag a positive case, whereas radiologists took a median of 16\u00b721 min (11\u00b767-25\u00b771) to draft a report and 23\u00b706 min (15\u00b767-39\u00b720) to release a report. With regard to the identification of increases in lesion burden, AI achieved a sensitivity of 0\u00b7962 (95% CI 0\u00b7947-1\u00b7000) and a specificity of 0\u00b7875 (95 %CI 0\u00b7833-0\u00b7923). The agreement between AI and the radiologist panel was high (Cohen's kappa coefficient 0\u00b7839, 95% CI 0\u00b7718-0\u00b7940).\nA deep learning algorithm for triaging patients with suspected COVID-19 at fever clinics was developed and externally validated. Given its high accuracy across populations with varied COVID-19 prevalence, integration of this system into the standard clinical workflow could expedite identification of chest CT scans with imaging indications of COVID-19.\nSpecial Project for Emergency of the Science and Technology Department of Hubei Province, China.", "journal": "The Lancet. Digital health", "date": "2020-09-29", "authors": ["MinghuanWang", "ChenXia", "LuHuang", "ShabeiXu", "ChuanQin", "JunLiu", "YingCao", "PengxinYu", "TingtingZhu", "HuiZhu", "ChaonanWu", "RongguoZhang", "XiangyuChen", "JianmingWang", "GuangDu", "ChenZhang", "ShaokangWang", "KuanChen", "ZhengLiu", "LimingXia", "WeiWang"], "doi": "10.1016/S2589-7500(20)30199-0\n10.7326/M20-1495\n10.1148/radiol.2020200905"}
{"title": "CT scan AI-aided triage for patients with COVID-19 in China.", "abstract": null, "journal": "The Lancet. Digital health", "date": "2020-09-29", "authors": ["VarutVardhanabhuti"], "doi": "10.1016/S2589-7500(20)30222-3\n10.1148/radiol.2020202439"}
{"title": "Unveiling COVID-19 from CHEST X-Ray with Deep Learning: A Hurdles Race with Small Data.", "abstract": "The possibility to use widespread and simple chest X-ray (CXR) imaging for early screening of COVID-19 patients is attracting much interest from both the clinical and the AI community. In this study we provide insights and also raise warnings on what is reasonable to expect by applying deep learning to COVID classification of CXR images. We provide a methodological guide and critical reading of an extensive set of statistical results that can be obtained using currently available datasets. In particular, we take the challenge posed by current small size COVID data and show how significant can be the bias introduced by transfer-learning using larger public non-COVID CXR datasets. We also contribute by providing results on a medium size COVID CXR dataset, just collected by one of the major emergency hospitals in Northern Italy during the peak of the COVID pandemic. These novel data allow us to contribute to validate the generalization capacity of preliminary results circulating in the scientific community. Our conclusions shed some light into the possibility to effectively discriminate COVID using CXR.", "journal": "International journal of environmental research and public health", "date": "2020-09-26", "authors": ["EnzoTartaglione", "Carlo AlbertoBarbano", "ClaudioBerzovini", "MarcoCalandri", "MarcoGrangetto"], "doi": "10.3390/ijerph17186933\n10.1148/radiol.2020200490\n10.1101/2020.02.11.20021493\n10.1148/radiol.2020201365\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020201160\n10.1016/S0140-6736(20)30728-5\n10.1080/00313020310001619118\n10.1109/TMI.2016.2528162\n10.1109/TMI.2016.2535865\n10.20944/preprints202003.0300.v1\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2017.08.001\n10.17632/rscbjbr9sj2\n10.1109/TMI.2013.2290491\n10.1109/42.929615\n10.1109/TMI.2014.2337057\n10.1109/TMI.2016.2535302\n10.1007/s00365-006-0663-2"}
{"title": "Imaging Diagnostics and Pathology in SARS-CoV-2-Related Diseases.", "abstract": "In December 2019, physicians reported numerous patients showing pneumonia of unknown origin in the Chinese region of Wuhan. Following the spreading of the infection over the world, The World Health Organization (WHO) on 11 March 2020 declared the novel severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) outbreak a global pandemic. The scientific community is exerting an extraordinary effort to elucidate all aspects related to SARS-CoV-2, such as the structure, ultrastructure, invasion mechanisms, replication mechanisms, or drugs for treatment, mainly through in vitro studies. Thus, the clinical in vivo data can provide a test bench for new discoveries in the field of SARS-CoV-2, finding new solutions to fight the current pandemic. During this dramatic situation, the normal scientific protocols for the development of new diagnostic procedures or drugs are frequently not completely applied in order to speed up these processes. In this context, interdisciplinarity is fundamental. Specifically, a great contribution can be provided by the association and interpretation of data derived from medical disciplines based on the study of images, such as radiology, nuclear medicine, and pathology. Therefore, here, we highlighted the most recent histopathological and imaging data concerning the SARS-CoV-2 infection in lung and other human organs such as the kidney, heart, and vascular system. In addition, we evaluated the possible matches among data of radiology, nuclear medicine, and pathology departments in order to support the intense scientific work to address the SARS-CoV-2 pandemic. In this regard, the development of artificial intelligence algorithms that are capable of correlating these clinical data with the new scientific discoveries concerning SARS-CoV-2 might be the keystone to get out of the pandemic.", "journal": "International journal of molecular sciences", "date": "2020-09-26", "authors": ["ManuelScimeca", "NicolettaUrbano", "RitaBonfiglio", "ManuelaMontanaro", "ElenaBonanno", "OrazioSchillaci", "AlessandroMauriello"], "doi": "10.3390/ijms21186960\n10.1056/NEJMoa2001017\n10.1007/s11427-020-1637-5\n10.3345/cep.2020.00493\n10.1186/s40249-020-00662-x\n10.1186/s12879-020-05010-w\n10.3390/ijms21083004\n10.1007/s00259-020-04735-9\n10.1159/000507423\n10.2217/pme-2018-0050\n10.2217/fon-2017-0698\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30566-3\n10.1038/s41418-020-0530-3\n10.1007/s00428-020-02881-x\n10.1056/NEJMoa2015432\n10.1016/S0140-6736(20)30937-5\n10.1126/science.abb2762\n10.1016/j.trsl.2020.04.007\n10.7326/M20-2566\n10.7326/M20-2003\n10.1016/j.jcv.2020.104362\n10.2169/internalmedicine.2696-19\n10.1111/his.14180\n10.1001/jamacardio.2020.0950\n10.1093/eurheartj/ehaa286\n10.1007/s00059-020-04909-z\n10.5858/arpa.2020-0217-SA\n10.1002/ejhf.1828\n10.1080/15513815.2020.1761491\n10.1016/S0140-6736(20)30211-7\n10.1681/ASN.2020040432\n10.1016/j.jdermsci.2020.04.007\n10.1111/jdv.16569\n10.1016/S0140-6736(20)31103-X\n10.1016/S0140-6736(20)31129-6\n10.1097/PAI.0000000000000860\n10.2174/157016112801784594\n10.1056/NEJMc2019373\n10.1016/j.euf.2020.05.009\n10.1148/radiol.2020200463\n10.1148/radiol.2020200370\n10.1016/S1473-3099(20)30086-4\n10.1007/s00330-020-06748-2\n10.3348/kjr.2020.0132\n10.2214/AJR.19.22372\n10.3348/kjr.2020.0112\n10.1007/s11427-020-1661-4\n10.1016/j.ejrad.2020.108972\n10.4178/epih.e2020006\n10.1148/radiol.2020200230\n10.1148/radiol.2020200241\n10.1148/radiol.2020200274\n10.1007/s11547-020-01200-3\n10.1148/ryct.2020200034\n10.1186/s12880-015-0042-7\n10.1007/s00134-020-05996-6\n10.1155/2019/2045432\n10.1007/s11548-019-02092-z\n10.1097/RLI.0000000000000574\n10.1097/RTI.0000000000000500\n10.1007/s11547-020-01135-9\n10.1038/s41591-020-0931-3\n10.1146/annurev.pathmechdis.3.121806.151534\n10.1016/S0140-6736(80)92989-X\n10.5144/0256-4947.2016.78\n10.1128/JVI.03427-14\n10.1016/j.virol.2008.07.026\n10.1016/j.hrthm.2020.05.001\n10.1161/CIR.0000000000000745\n10.1001/jamacardio.2020.1096\n10.1093/eurheartj/ehaa288\n10.1016/j.bbi.2020.04.077\n10.1161/STROKEAHA.120.030153\n10.1016/j.bbi.2020.04.017\n10.1016/j.bbi.2020.05.002\n10.1007/s00701-020-04374-x\n10.1586/17434440.3.6.699\n10.1016/j.crad.2015.03.010\n10.1053/j.semnuclmed.2012.02.003\n10.1007/s00259-019-04300-z\n10.1183/16000617.0051-2016\n10.1007/s00259-020-04734-w\n10.1007/s00259-020-04819-6\n10.1016/j.jtho.2020.03.022\n10.1007/s11604-020-01006-3\n10.1097/RLU.0000000000003135\n10.1148/radiol.2020200770\n10.2967/jnumed.120.245738\n10.4103/ijri.IJRI_469_16\n10.1186/s13550-015-0143-x\n10.1038/nprot.2017.133\n10.7150/thno.40606\n10.1006/viro.2000.0479\n10.1111/1751-2980.12851\n10.1161/CIRCRESAHA.120.317055\n10.1111/jgs.16472\n10.1016/j.acra.2020.04.030\n10.1002/lary.28692\n10.1097/CCM.0b013e3181aab31f\n10.2967/jnumed.120.246611\n10.1038/s41467-016-0009-6\n10.1186/s13578-019-0315-x\n10.3791/53866"}
{"title": "Detection of COVID-19 Using Deep Learning Algorithms on Chest Radiographs.", "abstract": "To evaluate the performance of a deep learning (DL) algorithm for the detection of COVID-19 on chest radiographs (CXR).\nIn this retrospective study, a DL model was trained on 112,120 CXR images with 14 labeled classifiers (ChestX-ray14) and fine-tuned using initial CXR on hospital admission of 509 patients, who had undergone COVID-19 reverse transcriptase-polymerase chain reaction (RT-PCR). The test set consisted of a CXR on presentation of 248 individuals suspected of COVID-19 pneumonia between February 16 and March 3, 2020 from 4 centers (72 RT-PCR positives and 176 RT-PCR negatives). The CXR were independently reviewed by 3 radiologists and using the DL algorithm. Diagnostic performance was compared with radiologists' performance and was assessed by area under the receiver operating characteristics (AUC).\nThe median age of the subjects in the test set was 61 (interquartile range: 39 to 79) years (51% male). The DL algorithm achieved an AUC of 0.81, sensitivity of 0.85, and specificity of 0.72 in detecting COVID-19 using RT-PCR as the reference standard. On subgroup analyses, the model achieved an AUC of 0.79, sensitivity of 0.80, and specificity of 0.74 in detecting COVID-19 in patients presented with fever or respiratory systems and an AUC of 0.87, sensitivity of 0.85, and specificity of 0.81 in distinguishing COVID-19 from other forms of pneumonia. The algorithm significantly outperforms human readers (P<0.001 using DeLong test) with higher sensitivity (P=0.01 using McNemar test).\nA DL algorithm (COV19NET) for the detection of COVID-19 on chest radiographs can potentially be an effective tool in triaging patients, particularly in resource-stretched health-care systems.", "journal": "Journal of thoracic imaging", "date": "2020-09-25", "authors": ["Wan Hang KeithChiu", "VarutVardhanabhuti", "DmytroPoplavskiy", "Philip Leung HoYu", "RichardDu", "Alistair Yun HeeYap", "SailongZhang", "Ambrose Ho-TungFong", "Thomas Wing-YanChin", "Jonan Chun YinLee", "Siu TingLeung", "Christine Shing YenLo", "Macy Mei-SzeLui", "Benjamin Xin HaoFang", "Ming-YenNg", "Michael DKuo"], "doi": "10.1097/RTI.0000000000000559"}
{"title": "Diagnosis of Coronavirus Disease 2019 Pneumonia by Using Chest Radiography: Value of Artificial Intelligence.", "abstract": "Background Radiologists are proficient in differentiating between chest radiographs with and without symptoms of pneumonia but have found it more challenging to differentiate coronavirus disease 2019 (COVID-19) pneumonia from non-COVID-19 pneumonia on chest radiographs. Purpose To develop an artificial intelligence algorithm to differentiate COVID-19 pneumonia from other causes of abnormalities at chest radiography. Materials and Methods In this retrospective study, a deep neural network, CV19-Net, was trained, validated, and tested on chest radiographs in patients with and without COVID-19 pneumonia. For the chest radiographs positive for COVID-19, patients with reverse transcription polymerase chain reaction results positive for severe acute respiratory syndrome coronavirus 2 with findings positive for pneumonia between February 1, 2020, and May 30, 2020, were included. For the non-COVID-19 chest radiographs, patients with pneumonia who underwent chest radiography between October 1, 2019, and December 31, 2019, were included. Area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were calculated to characterize diagnostic performance. To benchmark the performance of CV19-Net, a randomly sampled test data set composed of 500 chest radiographs in 500 patients was evaluated by the CV19-Net and three experienced thoracic radiologists. Results A total of 2060 patients (5806 chest radiographs; mean age, 62 years \u00b1 16 [standard deviation]; 1059 men) with COVID-19 pneumonia and 3148 patients (5300 chest radiographs; mean age, 64 years \u00b1 18; 1578 men) with non-COVID-19 pneumonia were included and split into training and validation and test data sets. For the test set, CV19-Net achieved an AUC of 0.92 (95% CI: 0.91, 0.93). This corresponded to a sensitivity of 88% (95% CI: 87, 89) and a specificity of 79% (95% CI: 77, 80) by using a high-sensitivity operating threshold, or a sensitivity of 78% (95% CI: 77, 79) and a specificity of 89% (95% CI: 88, 90) by using a high-specificity operating threshold. For the 500 sampled chest radiographs, CV19-Net achieved an AUC of 0.94 (95% CI: 0.93, 0.96) compared with an AUC of 0.85 (95% CI: 0.81, 0.88) achieved by radiologists. Conclusion CV19-Net was able to differentiate coronavirus disease 2019-related pneumonia from other types of pneumonia, with performance exceeding that of experienced thoracic radiologists. \u00a9 RSNA, 2021 ", "journal": "Radiology", "date": "2020-09-25", "authors": ["RanZhang", "XinTie", "ZhihuaQi", "Nicholas BBevins", "ChengzhuZhang", "DaltonGriner", "Thomas KSong", "Jeffrey DNadig", "Mark LSchiebler", "John WGarrett", "KeLi", "Scott BReeder", "Guang-HongChen"], "doi": "10.1148/radiol.2020202944\n10.1109/CVPR.2017.243"}
{"title": "Advancing COVID-19 differentiation with a robust preprocessing and integration of multi-institutional open-repository computer tomography datasets for deep learning analysis.", "abstract": "The coronavirus pandemic and its unprecedented consequences globally has spurred the interest of the artificial intelligence research community. A plethora of published studies have investigated the role of imaging such as chest X-rays and computer tomography in coronavirus disease 2019 (COVID-19) automated diagnosis. \u039fpen repositories of medical imaging data can play a significant role by promoting cooperation among institutes in a world-wide scale. However, they may induce limitations related to variable data quality and intrinsic differences due to the wide variety of scanner vendors and imaging parameters. In this study, a state-of-the-art custom U-Net model is presented with a dice similarity coefficient performance of 99.6% along with a transfer learning VGG-19 based model for COVID-19 versus pneumonia differentiation exhibiting an area under curve of 96.1%. The above was significantly improved over the baseline model trained with no segmentation in selected tomographic slices of the same dataset. The presented study highlights the importance of a robust preprocessing protocol for image analysis within a heterogeneous imaging dataset and assesses the potential diagnostic value of the presented COVID-19 model by comparing its performance to the state of the art.", "journal": "Experimental and therapeutic medicine", "date": "2020-09-25", "authors": ["EleftheriosTrivizakis", "NikosTsiknakis", "Evangelia EVassalou", "Georgios ZPapadakis", "Demetrios ASpandidos", "DimosthenisSarigiannis", "AristidisTsatsakis", "NikolaosPapanikolaou", "Apostolos HKarantanas", "KostasMarias"], "doi": "10.3892/etm.2020.9210\n10.3892/mmr.2020.11127\n10.3892/ijmm.2020.4555\n10.1016/j.toxrep.2020.04.012\n10.1016/j.fct.2020.111418\n10.1148/radiol.2020200343\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200230\n10.1016/S0140-6736(20)30183-5\n10.3892/etm.2020.8797\n10.1007/s13246-020-00865-4\n10.1148/radiol.2020200905\n10.1183/13993003.00775-2020\n10.1016/j.cell.2020.04.045\n10.1016/j.compbiomed.2020.103795\n10.1101/2020.02.23.20026930\n10.1101/2020.03.12.20027185\n10.1101/2020.04.24.20078584\n10.5281/zenodo.3757476\n10.1118/1.3528204\n10.1101/2020.04.13.20063941\n10.1038/s41467-020-17971-2"}
{"title": "COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images.", "abstract": "The novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a major pandemic outbreak recently. Various diagnostic technologies have been under active development. The novel coronavirus disease (COVID-19) may induce pulmonary failures, and chest X-ray imaging becomes one of the major confirmed diagnostic technologies. The very limited number of publicly available samples has rendered the training of the deep neural networks unstable and inaccurate. This study proposed a two-step transfer learning pipeline and a deep residual network framework COVID19XrayNet for the COVID-19 detection problem based on chest X-ray images. COVID19XrayNet firstly tunes the transferred model on a large dataset of chest X-ray images, which is further tuned using a small dataset of annotated chest X-ray images. The final model achieved 0.9108 accuracy. The experimental data also suggested that the model may be improved with more training samples being released. COVID19XrayNet, a two-step transfer learning framework designed for biomedical images.", "journal": "Interdisciplinary sciences, computational life sciences", "date": "2020-09-23", "authors": ["RuochiZhang", "ZhehaoGuo", "YueSun", "QiLu", "ZijianXu", "ZhaominYao", "MeiyuDuan", "ShuaiLiu", "YanjiaoRen", "LanHuang", "FengfengZhou"], "doi": "10.1007/s12539-020-00393-5\n10.1016/j.ijantimicag.2020.105955\n10.1038/s41586-020-2012-7\n10.1001/jama.2020.4683\n10.1016/S0140-6736(20)30633-4\n10.1111/tmi.13383\n10.1093/clinchem/hvaa029\n10.1002/jmv.25727\n10.1148/radiol.2020200343\n10.1109/TIP.2013.2264677\n10.1016/j.compbiomed.2018.06.006\n10.1186/s12859-018-2477-7\n10.1016/j.cell.2018.02.010\n10.1038/s41597-019-0322-0\n10.1109/72.554195\n10.1109/ACCESS.2018.2817593\n10.1109/TMI.2019.2948026\n10.2217/epi-2019-0230\n10.1016/j.compbiomed.2019.103394\n10.11613/BM.2012.031"}
{"title": "Discrimination of pulmonary ground-glass opacity changes in COVID-19 and non-COVID-19 patients using CT radiomics analysis.", "abstract": "The coronavirus disease 2019 (COVID-19) has evolved into a worldwide pandemic. CT although sensitive in detecting changes suffers from poor specificity in discrimination from other causes of ground glass opacities (GGOs). We aimed to develop and validate a CT-based radiomics model to differentiate COVID-19 from other causes of pulmonary GGOs.\nWe retrospectively included COVID-19 patients between 24/01/2020 and 31/03/2020 as case group and patients with pulmonary GGOs between 04/02/2012 and 31/03/2020 as a control group. Radiomics features were extracted from contoured GGOs by PyRadiomics. The least absolute shrinkage and selection operator method was used to establish the radiomics model. We assessed the performance using the area under the curve of the receiver operating characteristic curve (AUC).\nA total of 301 patients (age mean\u202f\u00b1\u202fSD: 64\u202f\u00b1\u202f15 years; male: 52.8 %) from three hospitals were enrolled, including 33 COVID-19 patients in the case group and 268 patients with malignancies or pneumonia in the control group. Thirteen radiomics features out of 474 were selected to build the model. This model achieved an AUC of 0.905, accuracy of 89.5 %, sensitivity of 83.3 %, specificity of 90.0 % in the testing set.\nWe developed a noninvasive radiomics model based on CT imaging for the diagnosis of COVID-19 based on GGO lesions, which could be a promising supplementary tool for improving specificity for COVID-19 in a population confounded by ground glass opacity changes from other etiologies.", "journal": "European journal of radiology open", "date": "2020-09-23", "authors": ["ChenyiXie", "Ming-YenNg", "JieDing", "Siu TingLeung", "Christine Shing YenLo", "Ho Yuen FrankWong", "VarutVardhanabhuti"], "doi": "10.1016/j.ejro.2020.100271"}
{"title": "COVID-CAPS: A capsule network-based framework for identification of COVID-19 cases from X-ray images.", "abstract": "Novel Coronavirus disease (COVID-19) has abruptly and undoubtedly changed the world as we know it at the end of the 2nd decade of the 21st century. COVID-19 is extremely contagious and quickly spreading globally making its early diagnosis of paramount importance. Early diagnosis of COVID-19 enables health care professionals and government authorities to break the chain of transition and flatten the epidemic curve. The common type of COVID-19 diagnosis test, however, requires specific equipment and has relatively low sensitivity. Computed tomography (CT) scans and X-ray images, on the other hand, reveal specific manifestations associated with this disease. Overlap with other lung infections makes human-centered diagnosis of COVID-19 challenging. Consequently, there has been an urgent surge of interest to develop Deep Neural Network (DNN)-based diagnosis solutions, mainly based on Convolutional Neural Networks (CNNs), to facilitate identification of positive COVID-19 cases. CNNs, however, are prone to lose spatial information between image instances and require large datasets. The paper presents an alternative modeling framework based on Capsule Networks, referred to as the COVID-CAPS, being capable of handling small datasets, which is of significant importance due to sudden and rapid emergence of COVID-19. Our results based on a dataset of X-ray images show that COVID-CAPS has advantage over previous CNN-based models. COVID-CAPS achieved an Accuracy of 95.7%, Sensitivity of 90%, Specificity of 95.8%, and Area Under the Curve (AUC) of 0.97, while having far less number of trainable parameters in comparison to its counterparts. To potentially and further improve diagnosis capabilities of the COVID-CAPS, pre-training and transfer learning are utilized based on a new dataset constructed from an external dataset of X-ray images. This is in contrary to existing works on COVID-19 detection where pre-training is performed based on natural images. Pre-training with a dataset of similar nature further improved accuracy to 98.3% and specificity to 98.6%.", "journal": "Pattern recognition letters", "date": "2020-09-23", "authors": ["ParnianAfshar", "ShahinHeidarian", "FarnooshNaderkhani", "AnastasiaOikonomou", "Konstantinos NPlataniotis", "ArashMohammadi"], "doi": "10.1016/j.patrec.2020.09.010"}
{"title": "COVID-19 image classification using deep features and fractional-order marine predators algorithm.", "abstract": "Currently, we witness the severe spread of the pandemic of the new Corona virus, COVID-19, which causes dangerous symptoms to humans and animals, its complications may lead to death. Although convolutional neural networks (CNNs) is considered the current state-of-the-art image classification technique, it needs massive computational cost for deployment and training. In this paper, we propose an improved hybrid classification approach for COVID-19 images by combining the strengths of CNNs (using a powerful architecture called Inception) to extract features and a swarm-based feature selection algorithm (Marine Predators Algorithm) to select the most relevant features. A combination of fractional-order and marine predators algorithm (FO-MPA) is considered an integration among a robust tool in mathematics named fractional-order calculus (FO). The proposed approach was evaluated on two public COVID-19 X-ray datasets which achieves both high performance and reduction of computational complexity. The two datasets consist of X-ray COVID-19 images by international Cardiothoracic radiologist, researchers and others published on Kaggle. The proposed approach selected successfully 130 and 86 out of 51 K features extracted by inception from dataset 1 and dataset 2, while improving classification accuracy at the same time. The results are the best achieved on these datasets when compared to a set of recent feature selection algorithms. By achieving 98.7%, 98.2% and 99.6%, 99% of classification accuracy and F-Score for dataset 1 and dataset 2, respectively, the proposed approach outperforms several CNNs and all recent works on COVID-19 images.", "journal": "Scientific reports", "date": "2020-09-23", "authors": ["Ahmed TSahlol", "DaliaYousri", "Ahmed AEwees", "Mohammed A AAl-Qaness", "RobertasDamasevicius", "Mohamed AbdElaziz"], "doi": "10.1038/s41598-020-71294-2\n10.1038/nature12711\n10.3390/jcm9030674\n10.1148/radiol.2020200330\n10.1155/2018/3052852\n10.1016/j.media.2016.05.004\n10.1016/j.ejca.2011.11.036\n10.1109/TMI.2015.2459064\n10.1016/j.bbe.2019.11.004\n10.1007/s10916-019-1428-9\n10.1007/s10115-006-0043-5\n10.1016/j.irbm.2019.10.006\n10.1016/j.cviu.2010.09.007\n10.1109/LSP.2014.2364612\n10.1002/ima.22118\n10.1109/TMI.2009.2028078\n10.1016/j.media.2017.07.005\n10.1016/j.dss.2011.01.015\n10.1007/s11042-019-7354-5\n10.1007/s11042-020-08699-8\n10.1016/j.eswa.2020.113377\n10.1038/s41598-020-59215-9\n10.1016/j.comnet.2018.01.007\n10.1016/j.engappai.2020.103662\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.future.2019.07.015\n10.1016/j.future.2020.03.055\n10.1016/j.advengsoft.2013.12.007\n10.1016/j.future.2019.02.028\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792"}
{"title": "Cascaded deep learning classifiers for computer-aided diagnosis of COVID-19 and pneumonia diseases in X-ray scans.", "abstract": "Computer-aided diagnosis (CAD) systems are considered a powerful tool for physicians to support identification of the novel Coronavirus Disease 2019 (COVID-19) using medical imaging modalities. Therefore, this article proposes a new framework of cascaded deep learning classifiers to enhance the performance of these CAD systems for highly suspected COVID-19 and pneumonia diseases in X-ray images. Our proposed deep learning framework constitutes two major advancements as follows. First, complicated multi-label classification of X-ray images have been simplified using a series of binary classifiers for each tested case of the health status. That mimics the clinical situation to diagnose potential diseases for a patient. Second, the cascaded architecture of COVID-19 and pneumonia classifiers is flexible to use different fine-tuned deep learning models simultaneously, achieving the best performance of confirming infected cases. This study includes eleven pre-trained convolutional neural network models, such as Visual Geometry Group Network (VGG) and Residual Neural Network (ResNet). They have been successfully tested and evaluated on public X-ray image dataset for normal and three diseased cases. The results of proposed cascaded classifiers showed that VGG16, ResNet50V2, and Dense Neural Network (DenseNet169) models achieved the best detection accuracy of COVID-19, viral (Non-COVID-19) pneumonia, and bacterial pneumonia images, respectively. Furthermore, the performance of our cascaded deep learning classifiers is superior to other multi-label classification methods of COVID-19 and pneumonia diseases in previous studies. Therefore, the proposed deep learning framework presents a good option to be applied in the clinical routine to assist the diagnostic procedures of COVID-19 infection.", "journal": "Complex & intelligent systems", "date": "2020-09-22", "authors": ["Mohamed EsmailKarar", "Ezz El-DinHemdan", "Marwa AShouman"], "doi": "10.1007/s40747-020-00199-4\n10.1016/j.tmaid.2020.101623\n10.1001/jama.2020.0757\n10.1016/j.ijsu.2020.02.034\n10.1016/j.diagmicrobio.2018.11.014\n10.1172/JCI33947\n10.1148/radiol.2020200330\n10.1016/j.jinf.2020.03.007\n10.1148/ryct.2020200034\n10.1016/j.jrid.2020.03.006\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020201160\n10.1097/rti.0000000000000404\n10.1016/S0140-6736(20)30211-7\n10.1016/j.compmedimag.2014.09.005\n10.1097/IMI.0b013e31822c6a77\n10.1016/j.ejrad.2016.10.006\n10.1016/j.media.2020.101666\n10.1016/j.medengphy.2020.02.003\n10.1016/j.bspc.2019.101678\n10.1007/s11548-020-02186-z\n10.1016/j.crad.2019.08.005\n10.1109/TMI.2019.2894349\n10.4018/IJACI.2019070106\n10.1097/SLA.0000000000002693\n10.1016/j.artmed.2018.08.008\n10.1109/ACCESS.2019.2920980\n10.1007/s42979-020-00209-9\n10.1007/s12559-020-09751-3\n10.1016/j.jctube.2019.01.003\n10.1016/j.cmpb.2019.105162\n10.1016/j.measurement.2019.05.076\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.1016/j.cmpb.2020.105608\n10.1016/j.cmpb.2020.105581\n10.1016/j.knosys.2020.106270\n10.1016/j.asoc.2020.106580\n10.1109/LSP.2017.2679608\n10.1016/j.ipm.2009.03.002\n10.1109/TMI.2020.2993291\n10.3233/jifs-201146"}
{"title": "COVID-19 detection in CT images with deep learning: A voting-based scheme and cross-datasets analysis.", "abstract": "Early detection and diagnosis are critical factors to control the COVID-19 spreading. A number of deep learning-based methodologies have been recently proposed for COVID-19 screening in CT scans as a tool to automate and help with the diagnosis. These approaches, however, suffer from at least one of the following problems: (i) they treat each CT scan slice independently and (ii) the methods are trained and tested with sets of images from the same dataset. Treating the slices independently means that the same patient may appear in the training and test sets at the same time which may produce misleading results. It also raises the question of whether the scans from the same patient should be evaluated as a group or not. Moreover, using a single dataset raises concerns about the generalization of the methods. Different datasets tend to present images of varying quality which may come from different types of CT machines reflecting the conditions of the countries and cities from where they come from. In order to address these two problems, in this work, we propose an Efficient Deep Learning Technique for the screening of COVID-19 with a voting-based approach. In this approach, the images from a given patient are classified as group in a voting system. The approach is tested in the two biggest datasets of COVID-19 CT analysis with a patient-based split. A cross dataset study is also presented to assess the robustness of the models in a more realistic scenario in which data comes from different distributions. The cross-dataset analysis has shown that the generalization power of deep learning models is far from acceptable for the task since accuracy drops from 87.68% to 56.16% on the best evaluation scenario. These results highlighted that the methods that aim at COVID-19 detection in CT-images have to improve significantly to be considered as a clinical option and larger and more diverse datasets are needed to evaluate the methods in a realistic scenario.", "journal": "Informatics in medicine unlocked", "date": "2020-09-22", "authors": ["PedroSilva", "EduardoLuz", "GuilhermeSilva", "GladstonMoreira", "RodrigoSilva", "DiegoLucio", "DavidMenotti"], "doi": "10.1016/j.imu.2020.100427\n10.1101/2020.03.30.20047456\n10.1101/2020.02.14.20023028"}
{"title": "OptCoNet: an optimized convolutional neural network for an automatic diagnosis of COVID-19.", "abstract": "The quick spread of coronavirus disease (COVID-19) has become a global concern and affected more than 15 million confirmed patients as of July 2020. To combat this spread, clinical imaging, for example, X-ray images, can be utilized for diagnosis. Automatic identification software tools are essential to facilitate the screening of COVID-19 using X-ray images. This paper aims to classify COVID-19, normal, and pneumonia patients from chest X-ray images. As such, an Optimized Convolutional Neural network (OptCoNet) is proposed in this work for the automatic diagnosis of COVID-19. The proposed OptCoNet architecture is composed of optimized feature extraction and classification components. The Grey Wolf Optimizer (GWO) algorithm is used to optimize the hyperparameters for training the CNN layers. The proposed model is tested and compared with different classification strategies utilizing an openly accessible dataset of COVID-19, normal, and pneumonia images. The presented optimized CNN model provides accuracy, sensitivity, specificity, precision, and F1 score values of 97.78%, 97.75%, 96.25%, 92.88%, and 95.25%, respectively, which are better than those of state-of-the-art models. This proposed CNN model can help in the automatic screening of COVID-19 patients and decrease the burden on medicinal services frameworks.", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2020-09-21", "authors": ["TriptiGoel", "RMurugan", "SeyedaliMirjalili", "Deba KumarChakrabartty"], "doi": "10.1007/s10489-020-01904-z\n10.1001/jama.2020.2565\n10.1016/j.ijsu.2020.02.034\n10.1007/s13246-020-00865-4\n10.1016/j.advengsoft.2013.12.007\n10.1038/scientificamerican0792-66\n10.1145/321062.321069\n10.1016/j.advengsoft.2016.01.008\n10.1016/j.compbiomed.2020.103792"}
{"title": "Detection of COVID-19 from Chest X-Ray Images Using Convolutional Neural Networks.", "abstract": "The detection of severe acute respiratory syndrome coronavirus 2 (SARS CoV-2), which is responsible for coronavirus disease 2019 (COVID-19), using chest X-ray images has life-saving importance for both patients and doctors. In addition, in countries that are unable to purchase laboratory kits for testing, this becomes even more vital. In this study, we aimed to present the use of deep learning for the high-accuracy detection of COVID-19 using chest X-ray images. Publicly available X-ray images (1583 healthy, 4292 pneumonia, and 225 confirmed COVID-19) were used in the experiments, which involved the training of deep learning and machine learning classifiers. Thirty-eight experiments were performed using convolutional neural networks, 10 experiments were performed using five machine learning models, and 14 experiments were performed using the state-of-the-art pre-trained networks for transfer learning. Images and statistical data were considered separately in the experiments to evaluate the performances of models, and eightfold cross-validation was used. A mean sensitivity of 93.84%, mean specificity of 99.18%, mean accuracy of 98.50%, and mean receiver operating characteristics-area under the curve scores of 96.51% are achieved. A convolutional neural network without pre-processing and with minimized layers is capable of detecting COVID-19 in a limited number of, and in imbalanced, chest X-ray images.", "journal": "SLAS technology", "date": "2020-09-20", "authors": ["BoranSekeroglu", "IlkerOzsahin"], "doi": "10.1177/2472630320958376"}
{"title": "Development and validation of risk prediction models for COVID-19 positivity in a hospital setting.", "abstract": "To develop: (1) two validated risk prediction models for coronavirus disease-2019 (COVID-19) positivity using readily available parameters in a general hospital setting; (2) nomograms and probabilities to allow clinical utilisation.\nPatients with and without COVID-19 were included from 4 Hong Kong hospitals. The database was randomly split into 2:1: for model development database (n = 895) and validation database (n = 435). Multivariable logistic regression was utilised for model creation and validated with the Hosmer-Lemeshow (H-L) test and calibration plot. Nomograms and probabilities set at 0.1, 0.2, 0.4 and 0.6 were calculated to determine sensitivity, specificity, positive predictive value (PPV) and negative predictive value (NPV).\nA total of 1330 patients (mean age 58.2 \u00b1 24.5 years; 50.7% males; 296 COVID-19 positive) were recruited. The first prediction model developed had age, total white blood cell count, chest x-ray appearances and contact history as significant predictors (AUC = 0.911 [CI = 0.880-0.941]). The second model developed has the same variables except contact history (AUC = 0.880 [CI = 0.844-0.916]). Both were externally validated on the H-L test (p = 0.781 and 0.155, respectively) and calibration plot. Models were converted to nomograms. Lower probabilities give higher sensitivity and NPV; higher probabilities give higher specificity and PPV.\nTwo simple-to-use validated nomograms were developed with excellent AUCs based on readily available parameters and can be considered for clinical utilisation.", "journal": "International journal of infectious diseases : IJID : official publication of the International Society for Infectious Diseases", "date": "2020-09-19", "authors": ["Ming-YenNg", "Eric Yuk FaiWan", "Ho Yuen FrankWong", "Siu TingLeung", "Jonan Chun YinLee", "Thomas Wing-YanChin", "Christine Shing YenLo", "Macy Mei-SzeLui", "Edward Hung TatChan", "Ambrose Ho-TungFong", "Sau YungFung", "On HangChing", "Keith Wan-HangChiu", "Tom Wai HinChung", "VarutVardhanbhuti", "Hiu Yin SoniaLam", "Kelvin Kai WangTo", "Jeffrey Long FungChiu", "Tina Poy WingLam", "Pek LanKhong", "Raymond Wai ToLiu", "Johnny Wai ManChan", "Alan Ka LunWu", "Kwok-CheungLung", "Ivan Fan NgaiHung", "Chak SingLau", "Michael DKuo", "Mary Sau-ManIp"], "doi": "10.1016/j.ijid.2020.09.022"}
{"title": "Telepsychiatry and other cutting-edge technologies in COVID-19 pandemic: Bridging the distance in mental health assistance.", "abstract": "At the end of 2019, a novel coronavirus (COVID-19) was identified in China. The high potential of human-to-human transmission led to subsequent COVID-19 global pandemic. Public health strategies including reduced social contact and lockdown have been adopted in many countries. Nonetheless, social distancing and isolation could also represent risk factors for mental disorders, resulting in loneliness, reduced social support and under-detection of mental health needs. Along with this, social distancing determines a relevant obstacle for direct access to psychiatric care services. The pandemic generates the urgent need for integrating technology into innovative models of mental healthcare.\nIn this paper, we discuss the potential role of telepsychiatry (TP) and other cutting-edge technologies in the management of mental health assistance. We narratively review the literature to examine the advantages and risks related to the extensive application of these new therapeutic settings, along with the possible limitations and ethical concerns.\nTelemental health services may be particularly feasible and appropriate for the support of patients, family members and healthcare providers during this COVID-19 pandemic. The integration of TP with other technological innovations (eg, mobile apps, virtual reality, big data and artificial intelligence (AI)) opens up interesting future perspectives for the improvement of mental health assistance.\nTelepsychiatry is a promising and growing way to deliver mental health services but is still underused. The COVID-19 pandemic may serve as an opportunity to introduce and promote, among numerous mental health professionals, the knowledge of the possibilities offered by the digital era.", "journal": "International journal of clinical practice", "date": "2020-09-19", "authors": ["FrancescoDi Carlo", "AntonellaSociali", "ElenaPicutti", "MauroPettorruso", "FedericaVellante", "ValeriaVerrastro", "GiovanniMartinotti", "Massimodi Giannantonio"], "doi": "10.1111/ijcp.13716\n10.4088/JCP.20com13361\n10.1089/tmj.2019.0158"}
{"title": "Initial chest radiographs and artificial intelligence (AI) predict clinical outcomes in COVID-19 patients: analysis of 697 Italian patients.", "abstract": "To evaluate whether the initial chest X-ray (CXR) severity assessed by an AI system may have prognostic utility in patients with COVID-19.\nThis retrospective single-center study included adult patients presenting to the emergency department (ED) between February 25 and April 9, 2020, with SARS-CoV-2 infection confirmed on real-time reverse transcriptase polymerase chain reaction (RT-PCR). Initial CXRs obtained on ED presentation were evaluated by a deep learning artificial intelligence (AI) system and compared with the Radiographic Assessment of Lung Edema (RALE) score, calculated by two experienced radiologists. Death and critical COVID-19 (admission to intensive care unit (ICU) or deaths occurring before ICU admission) were identified as clinical outcomes. Independent predictors of adverse outcomes were evaluated by multivariate analyses.\nSix hundred ninety-seven 697 patients were included in the study: 465 males (66.7%), median age of 62 years (IQR 52-75). Multivariate analyses adjusting for demographics and comorbidities showed that an AI system-based score \u2265 30 on the initial CXR was an independent predictor both for mortality (HR 2.60 (95% CI 1.69 - 3.99; p < 0.001)) and critical COVID-19 (HR 3.40 (95% CI 2.35-4.94; p < 0.001)). Other independent predictors were RALE score, older age, male sex, coronary artery disease, COPD, and neurodegenerative disease.\nAI- and radiologist-assessed disease severity scores on CXRs obtained on ED presentation were independent and comparable predictors of adverse outcomes in patients with COVID-19.\nClinicalTrials.gov NCT04318366 ( https://clinicaltrials.gov/ct2/show/NCT04318366 ).\n\u2022 AI system-based score \u2265 30 and a RALE score \u2265 12 at CXRs performed at ED presentation are independent and comparable predictors of death and/or ICU admission in COVID-19 patients. \u2022 Other independent predictors are older age, male sex, coronary artery disease, COPD, and neurodegenerative disease. \u2022 The comparable performance of the AI system in relation to a radiologist-assessed score in predicting adverse outcomes may represent a game-changer in resource-constrained settings.", "journal": "European radiology", "date": "2020-09-19", "authors": ["JunaidMushtaq", "RenatoPennella", "SalvatoreLavalle", "AnnaColarieti", "StephanieSteidler", "Carlo M AMartinenghi", "DiegoPalumbo", "AntonioEsposito", "PatriziaRovere-Querini", "MorenoTresoldi", "GiovanniLandoni", "FabioCiceri", "AlbertoZangrillo", "FrancescoDe Cobelli"], "doi": "10.1007/s00330-020-07269-8\n10.1016/S0140-6736(20)30633-4\n10.1016/j.chest.2020.04.003\n10.3348/kjr.2020.0132\n10.1016/j.amjmed.2004.03.020\n10.1148/radiol.2332031649\n10.2214/ajr.184.3.01840734\n10.1148/radiol.2462070712\n10.1136/thoraxjnl-2017-211280\n10.1007/s11604-020-00975-9"}
{"title": "Obese COVID-19 patients show more severe pneumonia lesions on CT chest imaging.", "abstract": null, "journal": "Diabetes, obesity & metabolism", "date": "2020-09-19", "authors": ["XiaoLuo", "YeerfanJiaerken", "ZhujingShen", "QiyuanWang", "BoLiu", "HaishengZhou", "HanpengZheng", "YongchouLi", "YuantongGao", "SusuHe", "WenbinJi", "YongqiangLiu", "JianbingMa", "LongyunMao", "XiangmingWang", "MeihaoWang", "MiaoguangSu", "PeiyuHuang", "LeiShi", "MinmingZhang"], "doi": "10.1111/dom.14194"}
{"title": "Lung Ultrasonography for Risk Stratification in Patients with Coronavirus Disease 2019 (COVID-19): A Prospective Observational Cohort Study.", "abstract": "Lung ultrasonography (LUS) is a promising pragmatic risk-stratification tool in coronavirus disease 2019 (COVID-19). This study describes and compares LUS characteristics between patients with different clinical outcomes.\nProspective observational study of polymerase chain reaction-confirmed adults with COVID-19 with symptoms of lower respiratory tract infection in the emergency department (ED) of Lausanne University Hospital. A trained physician recorded LUS images using a standardized protocol. Two experts reviewed images blinded to patient outcome. We describe and compare early LUS findings (\u226424 hours of ED presentation) between patient groups based on their 7-day outcome (1) outpatients, (2) hospitalized, and (3) intubated/dead. Normalized LUS score was used to discriminate between groups.\nBetween 6 March and 3 April 2020, we included 80 patients (17 outpatients, 42 hospitalized, and 21 intubated/dead). Seventy-three patients (91%) had abnormal LUS (70% outpatients, 95% hospitalized, and 100% intubated/dead; P\u2005=\u2005.003). The proportion of involved zones was lower in outpatients compared with other groups (median [IQR], 30% [0-40%], 44% [31-70%], 70% [50-88%]; P\u2005<\u2005.001). Predominant abnormal patterns were bilateral and there was multifocal spread thickening of the pleura with pleural line irregularities (70%), confluent B lines (60%), and pathologic B lines (50%). Posterior inferior zones were more often affected. Median normalized LUS score had a good level of discrimination between outpatients and others with area under the ROC of .80 (95% CI, .68-.92).\nSystematic LUS has potential as a reliable, cheap, and easy-to-use triage tool for the early risk stratification in patients with COVID-19 presenting to EDs.", "journal": "Clinical infectious diseases : an official publication of the Infectious Diseases Society of America", "date": "2020-09-18", "authors": ["ThomasBrahier", "Jean-YvesMeuwly", "OlivierPantet", "Marie-Jos\u00e9eBrochu Vez", "H\u00e9l\u00e8neGerhard Donnet", "Mary-AnneHartley", "OlivierHugli", "No\u00e9mieBoillat-Blanco"], "doi": "10.1093/cid/ciaa1408"}
{"title": "Development of a volumetric pancreas segmentation CT dataset for AI applications through trained technologists: a study during the COVID 19 containment phase.", "abstract": "To evaluate the performance of trained technologists vis-\u00e0-vis radiologists for volumetric pancreas segmentation and to assess the impact of supplementary training on their performance.\nIn this IRB-approved study, 22 technologists were trained in pancreas segmentation on portal venous phase CT through radiologist-led interactive videoconferencing sessions based on an image-rich curriculum. Technologists segmented pancreas in 188 CTs using freehand tools on custom image-viewing software. Subsequent supplementary training included multimedia videos focused on common errors, which were followed by second batch of 159 segmentations. Two radiologists reviewed all cases and corrected inaccurate segmentations. Technologists' segmentations were compared against radiologists' segmentations using Dice-Sorenson coefficient (DSC), Jaccard coefficient (JC), and Bland-Altman analysis.\nCorrections were made in 71 (38%) cases from first batch [26 (37%) oversegmentations and 45 (63%) undersegmentations] and in 77 (48%) cases from second batch [12 (16%) oversegmentations and 65 (84%) undersegmentations]. DSC, JC, false positive (FP), and false negative (FN) [mean (SD)] in first versus second batches were 0.63 (0.15) versus 0.63 (0.16), 0.48 (0.15) versus 0.48 (0.15), 0.29 (0.21) versus 0.21 (0.10), and 0.36 (0.20) versus 0.43 (0.19), respectively. Differences were not significant (p\u2009>\u20090.05). However, range of mean pancreatic volume difference reduced in the second batch [-\u00a02.74\u00a0cc (min -\u00a092.96\u00a0cc, max 87.47\u00a0cc) versus -\u00a023.57\u00a0cc (min -\u00a077.32, max 30.19)].\nTrained technologists could\u00a0perform volumetric pancreas segmentation with reasonable accuracy despite its complexity. Supplementary training further reduced range of volume difference in segmentations. Investment into training technologists could augment and accelerate development of body imaging datasets for AI applications.", "journal": "Abdominal radiology (New York)", "date": "2020-09-18", "authors": ["GarimaSuman", "AnanyaPanda", "PanagiotisKorfiatis", "Marie EEdwards", "SushilGarg", "Daniel JBlezek", "Suresh TChari", "Ajit HGoenka"], "doi": "10.1007/s00261-020-02741-x\n10.1148/radiol.2020192224\n10.2214/AJR.18.19914\n10.2214/AJR.18.19970\n10.1016/j.acra.2019.08.014\n10.1007/s00261-018-1793-8\n10.1080/17474124.2018.1496015\n10.1007/s00330-018-5865-5\n10.1038/ajg.2014.1\n10.1148/radiol.2016152547\n10.1016/j.jacr.2020.05.004\n10.1016/j.mri.2012.05.001\n10.11613/BM.2015.015\n10.1148/radiol.11110938\n10.2214/AJR.17.18665\n10.2214/AJR.19.22087\n10.1093/nsr/nwx106"}
{"title": "Development and clinical implementation of tailored image analysis tools for COVID-19 in the midst of the pandemic: The synergetic effect of an open, clinically embedded software development platform and machine learning.", "abstract": "During the emerging COVID-19 pandemic, radiology departments faced a substantial increase in chest CT admissions coupled with the novel demand for quantification of pulmonary opacities. This article describes how our clinic implemented an automated software solution for this purpose into an established software platform in 10 days. The underlying hypothesis was that modern academic centers in radiology are capable of developing and implementing such tools by their own efforts and fast enough to meet the rapidly increasing clinical needs in the wake of a pandemic.\nDeep convolutional neural network algorithms for lung segmentation and opacity quantification on chest CTs were trained using semi-automatically and manually created ground-truth (N\nThe final algorithm was available at day 10 and achieved human-like performance (Dice coefficient\u202f=\u202f0.97). For opacity quantification, a slight underestimation was seen both for the in-house (1.8 %) and for the external algorithm (0.9 %). In contrast to the external reference, the underestimation for the in-house algorithm showed no dependency on total opacity load, making it more suitable for follow-up.\nThe combination of machine learning and a clinically embedded software development platform enabled time-efficient development, instant deployment, and rapid adoption in clinical routine. The algorithm for fully automated lung segmentation and opacity quantification that we developed in the midst of the COVID-19 pandemic was ready for clinical use within just 10 days and achieved human-level performance even in complex cases.", "journal": "European journal of radiology", "date": "2020-09-15", "authors": ["ConstantinAnastasopoulos", "ThomasWeikert", "ShanYang", "AhmedAbdulkadir", "LenaSchm\u00fclling", "ClaudiaB\u00fchler", "FabianoPaciolla", "RaphaelSexauer", "JoshyCyriac", "IvanNesic", "RaphaelTwerenbold", "JensBremerich", "BramStieltjes", "Alexander WSauter", "GregorSommer"], "doi": "10.1016/j.ejrad.2020.109233\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1007/s00330-020-06865-y\n10.1148/radiol.2020201365\n10.1007/s00330-020-06915-5\n10.1148/radiol.2020200230\n10.1148/radiol.2020200823\n10.1148/radiol.2020200463\n10.1148/radiol.2020200843\n10.1148/ryct.2020200044\n10.1148/ryct.2020200082\n10.1148/radiol.2020201473\n10.1148/ryct.2020200047\n10.1148/radiol.2020201433\n10.1148/radiol.2020200905\n10.1164/ajrccm.164.9.2103121\n10.1145/3299887.3299891\n10.1148/ryai.2020200029\n10.1148/ryct.2020200075\n10.1007/s00330-020-06672-5"}
{"title": "A deep learning approach to detect Covid-19 coronavirus with X-Ray images.", "abstract": "Rapid and accurate detection of COVID-19 coronavirus is necessity of time to prevent and control of this pandemic by timely quarantine and medical treatment in absence of any vaccine. Daily increase in cases of COVID-19 patients worldwide and limited number of available detection kits pose difficulty in identifying the presence of disease. Therefore, at this point of time, necessity arises to look for other alternatives. Among already existing, widely available and low-cost resources, X-ray is frequently used imaging modality and on the other hand, deep learning techniques have achieved state-of-the-art performances in computer-aided medical diagnosis. Therefore, an alternative diagnostic tool to detect COVID-19 cases utilizing available resources and advanced deep learning techniques is proposed in this work. The proposed method is implemented in four phases, viz., data augmentation, preprocessing, stage-I and stage-II deep network model designing. This study is performed with online available resources of 1215 images and further strengthen by utilizing data augmentation techniques to provide better generalization of the model and to prevent the model overfitting by increasing the overall length of dataset to 1832 images. Deep network implementation in two stages is designed to differentiate COVID-19 induced pneumonia from healthy cases, bacterial and other virus induced pneumonia on X-ray images of chest. Comprehensive evaluations have been performed to demonstrate the effectiveness of the proposed method with both (i) training-validation-testing and (ii) 5-fold cross validation procedures. High classification accuracy as 97.77%, recall as 97.14% and precision as 97.14% in case of COVID-19 detection shows the efficacy of proposed method in present need of time. Further, the deep network architecture showing averaged accuracy/sensitivity/specificity/precision/F1-score of 98.93/98.93/98.66/96.39/98.15 with 5-fold cross validation makes a promising outcome in COVID-19 detection using X-ray images.", "journal": "Biocybernetics and biomedical engineering", "date": "2020-09-15", "authors": ["GovardhanJain", "DeeptiMittal", "DakshThakur", "Madhup KMittal"], "doi": "10.1016/j.bbe.2020.08.008\n10.1007/s00134-020-05990-y\n10.1038/s41591-020-0817-4\n10.2807/1560-7917\n10.3201/eid2606.200301\n10.1148/radiol.2018180921\n10.1002/jmri.26534\n10.1007/s13246-020-00865-4\n10.1016/j.compbiomed.2020.103792\n10.33889/IJMEMS.2020.5.4.052\n10.3389/fmed.2020.00427\n10.1016/j.chaos.2020.109944\n10.2174/1573405616666200604163954\n10.3390/app10134640\n10.1109/TMI.2020.2995965\n10.1007/s10096-020-03901-z\n10.1007/s10489-020-01826-w\n10.1016/j.imu.2020.100391\n10.1007/s40846-020-00529-4\n10.1109/CVPR.2009.5206848\n10.1109/CVPR.2017.243"}
{"title": "COVID-19 pathways for brain and heart injury in comorbidity patients: A role of medical imaging and artificial intelligence-based COVID severity classification: A review.", "abstract": "Artificial intelligence (AI) has penetrated the field of medicine, particularly the field of radiology. Since its emergence, the highly virulent coronavirus disease 2019 (COVID-19) has infected over 10 million people, leading to over 500,000 deaths as of July 1st, 2020. Since the outbreak began, almost 28,000 articles about COVID-19 have been published (https://pubmed.ncbi.nlm.nih.gov); however, few have explored the role of imaging and artificial intelligence in COVID-19 patients-specifically, those with comorbidities. This paper begins by presenting the four pathways that can lead to heart and brain injuries following a COVID-19 infection. Our survey also offers insights into the role that imaging can play in the treatment of comorbid patients, based on probabilities derived from COVID-19 symptom statistics. Such symptoms include myocardial injury, hypoxia, plaque rupture, arrhythmias, venous thromboembolism, coronary thrombosis, encephalitis, ischemia, inflammation, and lung injury. At its core, this study considers the role of image-based AI, which can be used to characterize the tissues of a COVID-19 patient and classify the severity of their infection. Image-based AI is more important than ever as the pandemic surges and countries worldwide grapple with limited medical resources for detection and diagnosis.", "journal": "Computers in biology and medicine", "date": "2020-09-13", "authors": ["Jasjit SSuri", "AnudeepPuvvula", "MainakBiswas", "MishaMajhail", "LucaSaba", "GavinoFaa", "Inder MSingh", "RonaldOberleitner", "MonikaTurk", "Paramjit SChadha", "Amer MJohri", "J MiguelSanches", "Narendra NKhanna", "KlaudijaViskovic", "SophieMavrogeni", "John RLaird", "GyanPareek", "MartinMiner", "David WSobel", "AntonellaBalestrieri", "Petros PSfikakis", "GeorgeTsoulfas", "AthanasiosProtogerou", "Durga PrasannaMisra", "VikasAgarwal", "George DKitas", "PuneetAhluwalia", "RaghuKolluri", "JagjitTeji", "Mustafa AlMaini", "AnnAgbakoba", "Surinder KDhanjil", "MeyypanSockalingam", "AjitSaxena", "AndrewNicolaides", "AdityaSharma", "VijayRathore", "Janet N AAjuluchukwu", "MostafaFatemi", "AzraAlizad", "VijayViswanathan", "Pudukode RKrishnan", "SubbaramNaidu"], "doi": "10.1016/j.compbiomed.2020.103960"}
{"title": "Contrastive Cross-Site Learning With Redesigned Net for COVID-19 CT Classification.", "abstract": "The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global public health crisis spreading hundreds of countries. With the continuous growth of new infections, developing automated tools for COVID-19 identification with CT image is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. To enlarge the datasets for developing machine learning methods, it is essentially helpful to aggregate the cases from different medical systems for learning robust and generalizable models. This paper proposes a novel joint learning framework to perform accurate COVID-19 identification by effectively learning with heterogeneous datasets with distribution discrepancy. We build a powerful backbone by redesigning the recently proposed COVID-Net in aspects of network architecture and learning strategy to improve the prediction accuracy and learning efficiency. On top of our improved backbone, we further explicitly tackle the cross-site domain shift by conducting separate feature normalization in latent space. Moreover, we propose to use a contrastive training objective to enhance the domain invariance of semantic embeddings for boosting the classification performance on each dataset. We develop and evaluate our method with two public large-scale COVID-19 diagnosis datasets made up of CT images. Extensive experiments show that our approach consistently improves the performanceson both datasets, outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-09-12", "authors": ["ZhaoWang", "QuandeLiu", "QiDou"], "doi": "10.1109/JBHI.2020.3023246"}
{"title": "Response to: 'Correspondence on 'Lung involvement in macrophage activation syndrome and severe COVID-19: results from a cross-sectional study to assess clinical, laboratory and artificial intelligence-radiological differences' by Ruscitti ", "abstract": null, "journal": "Annals of the rheumatic diseases", "date": "2020-09-11", "authors": ["PieroRuscitti", "FedericoBruno", "OnorinaBerardicurti", "ChiaraAcanfora", "ViktoriyaPavlych", "PierpaoloPalumbo", "AlessandroConforti", "FrancescoCarubbi", "IleniaDi Cola", "PaolaDi Benedetto", "PaolaCipriani", "DavideGrassi", "CarloMasciocchi", "AnnamariaIagnocco", "AntonioBarile", "RobertoGiacomelli"], "doi": "10.1136/annrheumdis-2020-218909"}
{"title": "Correspondence on 'Lung involvement in macrophage activation syndrome and severe COVID-19: results from a cross-sectional study to assess clinical, laboratory and artificial intelligence-radiological differences' by Ruscitti ", "abstract": null, "journal": "Annals of the rheumatic diseases", "date": "2020-09-11", "authors": ["Acer I-HungChen", "Sheng-ChiehLin", "James Cheng-ChungWei"], "doi": "10.1136/annrheumdis-2020-218876"}
{"title": "Position paper on COVID-19 imaging and AI: From the clinical needs and technological challenges to initial AI solutions at the lab and national level towards a new era for AI in healthcare.", "abstract": "In this position paper, we provide a collection of views on the role of AI in the COVID-19 pandemic, from clinical requirements to the design of AI-based systems, to the translation of the developed tools to the clinic. We highlight key factors in designing system solutions - per specific task; as well as design issues in managing the disease at the national level. We focus on three specific use-cases for which AI systems can be built: early disease detection, management in a hospital setting, and building patient-specific predictive models that require the combination of imaging with additional clinical data. Infrastructure considerations and population modeling in two European countries will be described. This pandemic has made the practical and scientific challenges of making AI solutions very explicit. A discussion concludes this paper, with a list of challenges facing the community in the AI road ahead.", "journal": "Medical image analysis", "date": "2020-09-06", "authors": ["HayitGreenspan", "Ra\u00falSan Jos\u00e9 Est\u00e9par", "Wiro JNiessen", "EliotSiegel", "MadsNielsen"], "doi": "10.1016/j.media.2020.101800"}
{"title": "Chemoprophylaxis, diagnosis, treatments, and discharge management of COVID-19: An evidence-based clinical practice guideline (updated version).", "abstract": "The novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is the cause of a rapidly spreading illness, coronavirus disease 2019 (COVID-19), affecting more than seventeen million people around the world. Diagnosis and treatment guidelines for clinicians caring for patients are needed. In the early stage, we have issued \"A rapid advice guideline for the diagnosis and treatment of 2019 novel coronavirus (2019-nCoV) infected pneumonia (standard version)\"; now there are many direct evidences emerged and may change some of previous recommendations and it is ripe for develop an evidence-based guideline. We formed a working group of clinical experts and methodologists. The steering group members proposed 29 questions that are relevant to the management of COVID-19 covering the following areas: chemoprophylaxis, diagnosis, treatments, and discharge management. We searched the literature for direct evidence on the management of COVID-19, and assessed its certainty generated recommendations using the Grading of Recommendations, Assessment, Development and Evaluation (GRADE) approach. Recommendations were either strong or weak, or in the form of ungraded consensus-based statement. Finally, we issued 34 statements. Among them, 6 were strong recommendations for, 14 were weak recommendations for, 3 were weak recommendations against and 11 were ungraded consensus-based statement. They covered topics of chemoprophylaxis (including agents and Traditional Chinese Medicine (TCM) agents), diagnosis (including clinical manifestations, reverse transcription-polymerase chain reaction (RT-PCR), respiratory tract specimens, IgM and IgG antibody tests, chest computed tomography, chest x-ray, and CT features of asymptomatic infections), treatments (including lopinavir-ritonavir, umifenovir, favipiravir, interferon, remdesivir, combination of antiviral drugs, hydroxychloroquine/chloroquine, interleukin-6 inhibitors, interleukin-1 inhibitors, glucocorticoid, qingfei paidu decoction, lianhua qingwen granules/capsules, convalescent plasma, lung transplantation, invasive or noninvasive ventilation, and extracorporeal membrane oxygenation (ECMO)), and discharge management (including discharge criteria and management plan in patients whose RT-PCR retesting shows SARS-CoV-2 positive after discharge). We also created two figures of these recommendations for the implementation purpose. We hope these recommendations can help support healthcare workers caring for COVID-19 patients.", "journal": "Military Medical Research", "date": "2020-09-06", "authors": ["Ying-HuiJin", "Qing-YuanZhan", "Zhi-YongPeng", "Xue-QunRen", "Xun-TaoYin", "LinCai", "Yu-FengYuan", "Ji-RongYue", "Xiao-ChunZhang", "Qi-WenYang", "JianguangJi", "JianXia", "Yi-RongLi", "Fu-XiangZhou", "Ya-DongGao", "ZhuiYu", "FengXu", "Ming-LiTu", "Li-MingTan", "MinYang", "FangChen", "Xiao-JuZhang", "MeiZeng", "YuZhu", "Xin-CanLiu", "JianYang", "Dong-ChiZhao", "Yu-FengDing", "NingHou", "Fu-BingWang", "HaoChen", "Yong-GangZhang", "WeiLi", "WenChen", "Yue-XianShi", "Xiu-ZhiYang", "Xue-JunWang", "Yan-JunZhong", "Ming-JuanZhao", "Bing-HuiLi", "Lin-LuMa", "HaoZi", "NaWang", "Yun-YunWang", "Shao-FuYu", "Lu-YaoLi", "QiaoHuang", "HongWeng", "Xiang-YingRen", "Li-ShaLuo", "Man-RuFan", "DiHuang", "Hong-YangXue", "Lin-XinYu", "Jin-PingGao", "TongDeng", "Xian-TaoZeng", "Hong-JunLi", "Zhen-ShunCheng", "XiaomeiYao", "Xing-HuanWang", "NoneNone", "NoneNone"], "doi": "10.1186/s40779-020-00270-8\n10.1186/s40779-020-0233-6\n10.1371/journal.pmed.0040119\n10.1186/s40779-020-00238-8\n10.1136/bmj.i2016\n10.1136/bmj.i2089\n10.1007/s11596-020-2203-3\n10.1371/journal.pone.0234765\n10.3389/fmed.2020.00295\n10.21053/ceo.2020.00570\n10.1016/j.ijid.2020.03.017\n10.20524/aog.2020.0506\n10.1016/s2468-1253(20)30126-6\n10.21037/atm-20-2124\n10.1001/jamaophthalmol.2020.1291\n10.1016/j.cca.2020.03.009\n10.1002/14651858.cd013652\n10.1097/rti.0000000000000533\n10.1002/rmv.2112\n10.26355/eurrev_202003_20706\n10.3760/cma.j.cn311365-20200210-00050\n10.1016/j.cmi.2020.04.026\n10.1016/s0140-6736(20)31042-4\n10.1016/j.jinf.2020.03.002\n10.1016/j.micinf.2020.05.012\n10.1136/bmj.m1849\n10.1016/S2665-9913(20)30164-8\n10.1016/s2665-9913(20)30127-2\n10.1016/j.dsx.2020.06.054\n10.3760/cma.j.cn115673\u204320200225\u204300072\n10.1002/14651858.CD013600.pub2\n10.1101/2020.07.01.20139857v1\n10.1101/2020.06.24.20121905v2\n10.1097/cm9.0000000000000839\n10.1097/sla.0000000000003955\n10.1631/jzus.B2000182\n10.1002/jcla.23392\n10.1001/jamanetworkopen.2020.10475\n10.12968/hmed.2020.0156\n10.3389/fmed.2020.00242\n10.1186/s40779-020-00245-9\n10.1186/s41256-020-00159-y\n10.1186/s41256-018-0084-9\n10.1111/jebm.12314"}
{"title": "Classification of COVID-19 in chest X-ray images using DeTraC deep convolutional neural network.", "abstract": "Chest X-ray is the first imaging technique that plays an important role in the diagnosis of COVID-19 disease. Due to the high availability of large-scale annotated image datasets, great success has been achieved using convolutional neural networks (", "journal": "Applied intelligence (Dordrecht, Netherlands)", "date": "2020-09-05", "authors": ["AsmaaAbbas", "Mohammed MAbdelsamea", "Mohamed MedhatGaber"], "doi": "10.1007/s10489-020-01829-7\n10.1109/ACCESS.2020.2989273\n10.1109/TMI.2016.2535865\n10.1109/TMI.2013.2290491\n10.1080/21681163.2015.1124249\n10.1109/TMI.2013.2284099\n10.1016/j.cmpb.2013.10.011\n10.1007/s10916-016-0539-9\n10.1109/TKDE.2009.191\n10.3846/tede.2010.47\n10.1109/TMI.2016.2528162\n10.1016/j.ipm.2009.03.002\n10.1016/0169-7439(87)80084-9\n10.1007/s10618-009-0146-1\n10.1007/s10115-007-0114-2"}
{"title": "A model for the effective COVID-19 identification in uncertainty environment using primary symptoms and CT scans.", "abstract": "The rapid spread of the COVID-19 virus around the world poses a real threat to public safety. Some COVID-19 symptoms are similar to other viral chest diseases, which makes it challenging to develop models for effective detection of COVID-19 infection. This article advocates a model to differentiate between COVID-19 and other four viral chest diseases under uncertainty environment using the viruses primary symptoms and CT scans. The proposed model is based on a plithogenic set, which provides higher accurate evaluation results in an uncertain environment. The proposed model employs the best-worst method (BWM) and the technique in order of preference by similarity to ideal solution (TOPSIS). Besides, this study discusses how smart Internet of Things technology can assist medical staff in monitoring the spread of COVID-19. Experimental evaluation of the proposed model was conducted on five different chest diseases. Evaluation results demonstrate that the proposed model effectiveness in detecting the COVID-19 in all five cases achieving detection accuracy of up to 98%.", "journal": "Health informatics journal", "date": "2020-09-05", "authors": ["MohamedAbdel-Basst", "RehabMohamed", "MohamedElhoseny"], "doi": "10.1177/1460458220952918\n10.35940/ijrte.D1068.1284S219."}
{"title": "Pilot Study of Robot-Assisted Teleultrasound Based on 5G Network: A New Feasible Strategy for Early Imaging Assessment During COVID-19 Pandemic.", "abstract": "Early diagnosis is critical for the prevention and control of the coronavirus disease 2019 (COVID-19). We attempted to apply a protocol using teleultrasound, which is supported by the 5G network, to explore the feasibility of solving the problem of early imaging assessment of COVID-19. Four male patients with confirmed or suspected COVID-19 were hospitalized in isolation wards in two different cities. Ultrasound specialists, located in two other different cities, carried out the robot-assisted teleultrasound and remote consultation in order to settle the problem of early cardiopulmonary evaluation. Lung ultrasound, brief echocardiography, and blood volume assessment were performed. Whenever difficulties of remote manipulation and diagnosis occurred, the alternative examination was repeated by a specialist from another city, and in sequence, remote consultation was conducted immediately to meet the consensus. The ultrasound specialists successfully completed the telerobotic ultrasound. Lung ultrasound indicated signs of pneumonia with varying degrees in all cases and mild pleural effusion in one case. No abnormalities of cardiac structure and function and blood volume were detected. Remote consultation on the issue of manipulation practice, and the diagnosis in one case was conducted. The cardiopulmonary information was delivered to the frontline clinicians immediately for further treatment. The practice of teleultrasound protocol makes early diagnosis and repeated assessment available in the isolation ward. Ultrasound specialists can be protected from infection, and personal protective equipment can be spared. Quality control can be ensured by remote consultations among doctors. This protocol is worth consideration as a feasible strategy for early imaging assessment in the COVID-19 pandemic.", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2020-09-04", "authors": ["ShengzhengWu", "DuduWu", "RuizhongYe", "KeyanLi", "YuehuaLu", "JufenXu", "LinfeiXiong", "YuanyuanZhao", "AilinCui", "YaqingLi", "ChengzhongPeng", "FaqinLv"], "doi": "10.1109/TUFFC.2020.3020721"}
{"title": "Ultra-low-dose chest CT imaging of COVID-19 patients using a deep residual neural network.", "abstract": "The current study aimed to design an ultra-low-dose CT examination protocol using a deep learning approach suitable for clinical diagnosis of COVID-19 patients.\nIn this study, 800, 170, and 171 pairs of ultra-low-dose and full-dose CT images were used as input/output as training, test, and external validation set, respectively, to implement the full-dose prediction technique. A residual convolutional neural network was applied to generate full-dose from ultra-low-dose CT images. The quality of predicted CT images was assessed using root mean square error (RMSE), structural similarity index (SSIM), and peak signal-to-noise ratio (PSNR). Scores ranging from 1 to 5 were assigned reflecting subjective assessment of image quality and related COVID-19 features, including ground glass opacities (GGO), crazy paving (CP), consolidation (CS), nodular infiltrates (NI), bronchovascular thickening (BVT), and pleural effusion (PE).\nThe radiation dose in terms of CT dose index (CTDI\nThe results demonstrated that the deep learning algorithm is capable of predicting standard full-dose CT images with acceptable quality for the clinical diagnosis of COVID-19 positive patients with substantial radiation dose reduction.\n\u2022 Ultra-low-dose CT imaging of COVID-19 patients would result in the loss of critical information about lesion types, which could potentially affect clinical diagnosis. \u2022 Deep learning-based prediction of full-dose from ultra-low-dose CT images for the diagnosis of COVID-19 could reduce the radiation dose by up to 89%. \u2022 Deep learning algorithms failed to recover the correct lesion structure/density for a number of patients considered outliers, and as such, further research and development is warranted to address these limitations.", "journal": "European radiology", "date": "2020-09-04", "authors": ["IsaacShiri", "AzadehAkhavanallaf", "AmirhosseinSanaat", "YazdanSalimi", "DariushAskari", "ZahraMansouri", "Sajad PShayesteh", "MohammadHasanian", "KiaraRezaei-Kalantari", "AliSalahshour", "SalehSandoughdaran", "HamidAbdollahi", "HosseinArabi", "HabibZaidi"], "doi": "10.1007/s00330-020-07225-6\n10.1001/jama.2020.2648\n10.1016/j.ijantimicag.2020.105924\n10.1128/JCM.00512-20\n10.1016/j.jacr.2020.03.006\n10.1148/radiol.2020190389\n10.1002/mp.14000\n10.1007/s40134-012-0003-7\n10.3348/kjr.2012.13.1.1\n10.1002/mp.13666\n10.1016/j.acra.2020.04.016\n10.1007/s11547-020-01179-x\n10.1007/s00330-020-06809-6\n10.1148/ryct.2020200196\n10.1007/s00330-004-2403-4\n10.1007/s00330-006-0545-2\n10.1016/j.media.2017.07.005\n10.1002/mp.13264\n10.1109/TMI.2018.2827462\n10.1002/mp.13713\n10.3348/kjr.2019.0413\n10.1007/s10278-019-00274-4\n10.2214/AJR.14.13613\n10.1109/TNS.2015.2467219\n10.1002/mp.13619\n10.1001/jama.2010.973\n10.1016/j.ejrad.2016.10.021"}
{"title": "The challenge of COVID-19 low disease prevalence for artificial intelligence models: report of 1,610 patients.", "abstract": null, "journal": "Quantitative imaging in medicine and surgery", "date": "2020-09-04", "authors": ["Carlo CQuattrocchi", "Carlo AMallio", "GabrielePresti", "BrunoBeomonte Zobel", "JacopoCardinale", "MarioIozzino", "Sabino WDella Sala"], "doi": "10.21037/qims-20-782\n10.1038/s41591-020-0931-3\n10.1016/j.dsx.2020.04.012\n10.1007/s00146-020-00978-0\n10.1007/s11547-020-01197-9"}
{"title": "Visual and software-based quantitative chest CT assessment of COVID-19: correlation with clinical findings.", "abstract": "The aim of this study was to evaluate visual and software-based quantitative assessment of parenchymal changes and normal lung parenchyma in patients with coronavirus disease 2019 (COVID-19) pneumonia. The secondary aim of the study was to compare the radiologic findings with clinical and laboratory data.\nPatients with COVID-19 who underwent chest computed tomography (CT) between March 11, 2020 and April 15, 2020 were retrospectively evaluated. Clinical and laboratory findings of patients with abnormal findings on chest CT and PCR-evidence of COVID-19 infection were recorded. Visual quantitative assessment score (VQAS) was performed according to the extent of lung opacities. Software-based quantitative assessment of the normal lung parenchyma percentage (SQNLP) was automatically quantified by a deep learning software. The presence of consolidation and crazy paving pattern (CPP) was also recorded. Statistical analyses were performed to evaluate the correlation between quantitative radiologic assessments, and clinical and laboratory findings, as well as to determine the predictive utility of radiologic findings for estimating severe pneumonia and admission to intensive care unit (ICU).\nA total of 90 patients were enrolled. Both VQAS and SQNLP were significantly correlated with multiple clinical parameters. While VQAS >8.5 (sensitivity, 84.2%; specificity, 80.3%) and SQNLP <82.45% (sensitivity, 83.1%; specificity, 84.2%) were related to severe pneumonia, VQAS >9.5 (sensitivity, 93.3%; specificity, 86.5%) and SQNLP <81.1% (sensitivity, 86.5%; specificity, 86.7%) were predictive of ICU admission. Both consolidation and CPP were more commonly seen in patients with severe pneumonia than patients with nonsevere pneumonia (P = 0.197 for consolidation; P < 0.001 for CPP). Moreover, the presence of CPP showed high specificity (97.2%) for severe pneumonia.\nBoth SQNLP and VQAS were significantly related to the clinical findings, highlighting their clinical utility in predicting severe pneumonia, ICU admission, length of hospital stay, and management of the disease. On the other hand, presence of CPP has high specificity for severe COVID-19 pneumonia.", "journal": "Diagnostic and interventional radiology (Ankara, Turkey)", "date": "2020-09-03", "authors": ["GamzeDurhan", "SelinArdal\u0131 D\u00fczg\u00fcn", "FigenBa\u015faran Demirkaz\u0131k", "\u0130limIrmak", "\u0130lkay\u0130dilman", "MeltemG\u00fcls\u00fcn Akp\u0131nar", "ErhanAkp\u0131nar", "Serpil\u00d6cal", "G\u00fcl\u00e7inTelli", "ArzuTopeli", "Orhan MacitAr\u0131y\u00fcrek"], "doi": "10.5152/dir.2020.20407\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30251-8\n10.1001/jama.2020.1585\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200230\n10.1007/s00330-020-06817-6\n10.1148/radiol.2020201433\n10.1148/ryct.2020200075\n10.1148/rg.2020190099\n10.1148/radiol.2020200370\n10.2214/AJR.20.23078\n10.2214/ajr.175.5.1751329\n10.1016/j.jaci.2020.04.006\n10.1186/s13054-020-2833-7\n10.1007/s11547-020-01202-1\n10.1172/JCI137244\n10.1016/j.medmal.2020.04.006\n10.1148/ryct.2020200047\n10.3348/kjr.2020.0171\n10.1097/RLI.0000000000000689\n10.1097/RLI.0000000000000672\n10.1016/j.jtho.2020.02.010\n10.1101/2020.04.19.20054262\n10.1016/j.trsl.2020.04.007\n10.1148/rg.273065194"}
{"title": "Coronavirus Disease 2019 (COVID-19) diagnostic technologies: A country-based retrospective analysis of screening and containment procedures during the first wave of the pandemic.", "abstract": "Since first report of a novel coronavirus in December of 2019, the Coronavirus Disease 2019 (COVID-19) pandemic has crippled healthcare systems around the world. While many initial screening protocols centered around laboratory detection of the virus, early testing assays were thought to be poorly sensitive in comparison to chest computed tomography, especially in asymptomatic disease. Coupled with shortages of reverse transcription polymerase chain reaction (RT-PCR) testing kits in many parts of the world, these regions instead turned to the use of advanced imaging as a first-line screening modality. However, in contrast to previous Severe Acute Respiratory Syndrome and Middle East Respiratory Syndrome coronavirus epidemics, chest X-ray has not demonstrated optimal sensitivity to be of much utility in first-line screening protocols. Though current national and international guidelines recommend for the use of RT-PCR as the primary screening tool for suspected cases of COVID-19, institutional and regional protocols must consider local availability of resources when issuing universal recommendations. Successful containment and social mitigation strategies worldwide have been thus far predicated on unified governmental responses, though the underlying ideologies of these practices may not be widely applicable in many Western nations. As the strain on the radiology workforce continues to mount, early results indicate a promising role for the use of machine-learning algorithms as risk stratification schema in the months to come.", "journal": "Clinical imaging", "date": "2020-09-02", "authors": ["Brandon K KFields", "Natalie LDemirjian", "AliGholamrezanezhad"], "doi": "10.1016/j.clinimag.2020.08.014\n10.2214/ajr.20.23034\n10.1016/j.ijsu.2020.02.034\n10.2214/ajr.20.22969\n10.1016/j.jacr.2020.02.008\n10.1148/radiol.2020200370\n10.1016/s0140-6736(20)30673-5\n10.1126/science.aba9757\n10.1016/s0140-6736(20)30567-5\n10.1126/science.367.6484.1287\n10.2214/ajr.20.23498\n10.1007/s40484-020-0199-0\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.1007/s00330-020-06809-6\n10.1097/cm9.0000000000000819\n10.1148/radiol.2282030593\n10.2214/AJR.15.14445\n10.6004/jnccn.2012.0023\n10.5607/en20009\n10.1787/comptomoscan-table-2014-1-en\n10.1016/j.jacr.2020.03.015\n10.1016/s2213-2600(20)30071-0\n10.1136/bmj.m1090\n10.1038/d41587-020-00002-2\n10.1016/j.ejrad.2020.108961\n10.1007/s11604-020-00948-y\n10.1016/j.clinimag.2020.05.032\n10.1016/j.clinimag.2020.02.016\n10.1117/12.2256829\n10.2214/ajr.18.19551\n10.1007/s00330-020-06748-2\n10.1148/radiol.2020200230\n10.1002/hpm.2657\n10.5812/iranjradiol.102324\n10.1016/s2214-109x(20)30110-8\n10.1056/NEJMp2005492\n10.1111/anae.15049\n10.3855/jidc.12600\n10.1016/s2468-2667(20)30074-8\n10.1016/j.fertnstert.2020.03.021\n10.1016/j.jse.2012.06.014\n10.5152/dir.2020.30320\n10.1136/bmj.m1065\n10.1148/radiol.2020200847\n10.1016/j.ijid.2020.03.031\n10.3346/jkms.2020.35.e112\n10.1038/d41586-020-00740-y\n10.3346/jkms.2020.35.e123\n10.24171/j.phrp.2020.11.1.09\n10.3348/kjr.2020.0146\n10.1080/01459740.2020.1745482\n10.1016/j.idm.2020.02.003\n10.1007/s00330-020-06865-y\n10.1177/0846537120914428\n10.1177/0846537120913497\n10.1007/s12630-020-01639-y\n10.1148/radiol.2020201365\n10.1148/radiol.2020201754\n10.1007/s10140-020-01818-w"}
{"title": "Toward automated severe pharyngitis detection with smartphone camera using deep learning networks.", "abstract": "Severe pharyngitis is frequently associated with inflammations caused by streptococcal pharyngitis, which can cause immune-mediated and post-infectious complications. The recent global pandemic of coronavirus disease (COVID-19) encourages the use of telemedicine for patients with respiratory symptoms. This study, therefore, purposes automated detection of severe pharyngitis using a deep learning framework with self-taken throat images.\nA dataset composed of two classes of 131 throat images with pharyngitis and 208 normal throat images was collected. Before the training classifier, we constructed a cycle consistency generative adversarial network (CycleGAN) to augment the training dataset. The ResNet50, Inception-v3, and MobileNet-v2 architectures were trained with transfer learning and validated using a randomly selected test dataset. The performance of the models was evaluated based on the accuracy and area under the receiver operating characteristic curve (ROC-AUC).\nThe CycleGAN-based synthetic images reflected the pragmatic characteristic features of pharyngitis. Using the synthetic throat images, the deep learning model demonstrated a significant improvement in the accuracy of the pharyngitis diagnosis. ResNet50 with GAN-based augmentation showed the best ROC-AUC of 0.988 for pharyngitis detection in the test dataset. In the 4-fold cross-validation using the ResNet50, the highest detection accuracy and ROC-AUC achieved were 95.3% and 0.992, respectively.\nThe deep learning model for smartphone-based pharyngitis screening allows fast identification of severe pharyngitis with a potential of the timely diagnosis of pharyngitis. In the recent pandemic of COVID-19, this framework will help patients with upper respiratory symptoms to improve convenience in diagnosis and reduce transmission.", "journal": "Computers in biology and medicine", "date": "2020-09-02", "authors": ["Tae KeunYoo", "Joon YulChoi", "YounilJang", "EinOh", "Ik HeeRyu"], "doi": "10.1016/j.compbiomed.2020.103980\n10.1089/tmj.2020.0099\n10.1177/0194599820931827\n10.4218/etrij.2018-0428\n10.1007/s11042-019-08130-x\n10.1038/s41746-020-0282-y\n10.1128/JCM.00811-19\n10.1542/peds.2009-2648\n10.1186/s12887-019-1393-y\n10.1542/peds.2017-2033\n10.3390/s19153307\n10.1016/S2589-7500(20)30001-7\n10.1016/j.neucom.2018.09.013\n10.1007/s00417-020-04709-5\n10.1007/s42452-020-3000-0\n10.1016/j.media.2020.101794\n10.1371/journal.pone.0187336\n10.1080/07391102.2020.1767212\n10.1016/j.cell.2018.02.010\n10.1007/s11517-018-1915-z\n10.7717/peerj.8668\n10.1148/ryai.2019190015\n10.1109/JBHI.2019.2949601\n10.1109/ACCESS.2018.2874767\n10.1155/2016/6584725\n10.1101/2020.03.20.000133\n10.1111/mice.12387\n10.1136/bmj.m1182\n10.1016/j.jid.2020.01.019\n10.1016/j.oooo.2015.11.005\n10.4103/ijo.IJO_544_18\n10.1016/j.eswa.2019.06.070\n10.1007/978-981-13-8950-4_27\n10.1007/s11548-019-02092-z\n10.1016/j.compbiomed.2020.103698\n10.1038/s42256-019-0137-x\n10.1016/j.compbiomed.2020.103628\n10.1093/sleep/29.7.903\n10.1371/journal.pone.0191493"}
{"title": "The modifications brought about by the COVID-19 pandemic to Nuclear Medicine practice.", "abstract": "Nearly 19.9 million cases and more than 730 thousand disease-related deaths have been confirmed in the months that followed WHO's assessment that the novel coronavirus COVID-19, first emerged in Wuhan China on December 2019, could be characterized as a pandemic. The aforementioned coronavirus affected 188 countries as of 8.10.2020. Despite the continually increasing number of COVID-19 cases reported to CDC, at national level, the percentage of visits to outpatient providers and emergency departments has decreased and mortality rates attributed to COVID-19 have declined compared to the previous weeks, still above the baseline. It is common knowledge that the coronavirus pandemic has reshaped societies and economies around the globe, affecting all aspects of everyday life. Public health systems as a whole have been globally affected since they had to face extraordinary demands over a long period of time, which, in turn, required rapid adjustments in the operating procedures that were already in use, in order to provide high-standard health services, while respecting patients quality of life. Over half of deaths in low-income countries are caused by communicable diseases, maternal causes, conditions arising during pregnancy and childbirth, and nutritional deficiencies. On the contrary, this percentage is less than 7% in high-income countries. Noncommunicable diseases cause 71% of deaths globally, ranging from 37% in low-income countries to 88% in high-income countries. However, in terms of absolute number of deaths, 78% of global NCD deaths occurred in low-and middle-income countries. This partially explains why recent developments in medicine were mostly focused on chronic illnesses, including cardiovascular disease, cancer, chronic respiratory diseases and type 2 diabetes, rather than focusing on infection and inflammation progress. The COVID-19 pandemic and the subsequent burden it placed upon health systems to deal with infectious and non-infectious diseases in a poor environment, can become an opportunity to update the field of medical research and change the governmental policies in place that have been stagnant and/or inefficient and ill-managed. This way, health systems will be equipped with better and faster protocols and best practices in order to manage efficiently any other pandemic that might emerge in the future. In this context, Nuclear Medicine departments should reconsider and update their practices, by altering routines and workflows in order to comply with the new sanitary standards, triaging their appointments, or introducing new diagnostic methods like Tele-Medicine / Tele Nuclear Medicine and Artificial Intelligence applications. This special edition of Hellenic Journal of Nuclear Medicine has as its main purpose to introduce and communicate those new practices and protocols/standard operating procedures, in order for the scientific community, health public institutions, affected individuals and their families to be duly informed.", "journal": "Hellenic journal of nuclear medicine", "date": "2020-08-30", "authors": ["VasilikiChatzipavlidou"], "doi": null}
{"title": "Artificial intelligence and soft skills in radiation oncology: Data versus wisdom.", "abstract": null, "journal": "Journal of medical imaging and radiation sciences", "date": "2020-08-30", "authors": ["Ian SBoon", "Jean SLim", "Moi HYap", "Tracy P TAu Yong", "Cheng SBoon"], "doi": "10.1016/j.jmir.2020.08.011"}
{"title": "Development and Validation of a Deep Learning-Based Model Using Computed Tomography Imaging for Predicting Disease Severity of Coronavirus Disease 2019.", "abstract": "Coronavirus disease 2019 (COVID-19) is sweeping the globe and has resulted in infections in millions of people. Patients with COVID-19 face a high fatality risk once symptoms worsen; therefore, early identification of severely ill patients can enable early intervention, prevent disease progression, and help reduce mortality. This study aims to develop an artificial intelligence-assisted tool using computed tomography (CT) imaging to predict disease severity and further estimate the risk of developing severe disease in patients suffering from COVID-19.\nInitial CT images of 408 confirmed COVID-19 patients were retrospectively collected between January 1, 2020 and March 18, 2020 from hospitals in Honghu and Nanchang. The data of 303 patients in the People's Hospital of Honghu were assigned as the training data, and those of 105 patients in The First Affiliated Hospital of Nanchang University were assigned as the test dataset. A deep learning based-model using multiple instance learning and residual convolutional neural network (ResNet34) was developed and validated. The discrimination ability and prediction accuracy of the model were evaluated using the receiver operating characteristic curve and confusion matrix, respectively.\nThe deep learning-based model had an area under the curve (AUC) of 0.987 (95% confidence interval [CI]: 0.968-1.00) and an accuracy of 97.4% in the training set, whereas it had an AUC of 0.892 (0.828-0.955) and an accuracy of 81.9% in the test set. In the subgroup analysis of patients who had non-severe COVID-19 on admission, the model achieved AUCs of 0.955 (0.884-1.00) and 0.923 (0.864-0.983) and accuracies of 97.0 and 81.6% in the Honghu and Nanchang subgroups, respectively.\nOur deep learning-based model can accurately predict disease severity as well as disease progression in COVID-19 patients using CT imaging, offering promise for guiding clinical treatment.", "journal": "Frontiers in bioengineering and biotechnology", "date": "2020-08-28", "authors": ["Lu-ShanXiao", "PuLi", "FenglongSun", "YanpeiZhang", "ChenghaiXu", "HongboZhu", "Feng-QinCai", "Yu-LinHe", "Wen-FengZhang", "Si-CongMa", "ChenyiHu", "MengchunGong", "LiLiu", "WenzhaoShi", "HongZhu"], "doi": "10.3389/fbioe.2020.00898\n10.1109/tmi.2016.2535865\n10.1038/s41591-019-0508-1\n10.1148/radiol.2363040958\n10.1101/2020.02.25.20021568\n10.1016/s0140-6736(20)30211-7\n10.2214/AJR.14.13671\n10.1097/rli.0000000000000127\n10.1101/2020.02.19.20025296\n10.1101/2020.03.28.20046045\n10.1101/2020.03.17.20037515\n10.1148/radiol.2020200905\n10.7150/thno.46569\n10.1001/jamainternmed.2020.2033\n10.36227/techrxiv.12156522\n10.1097/CM9.0000000000000819\n10.1186/1471-2105-12-77\n10.1007/s10916-020-01562-1\n10.1016/j.neunet.2014.09.003\n10.1016/j.ijsu.2020.02.034\n10.1101/2020.02.23.20026930\n10.1148/radiol.2020200843\n10.7150/thno.46833\n10.1101/2020.02.10.20021675\n10.1148/radiol.2020200370\n10.1101/2020.03.12.20027185\n10.1056/NEJMoa2001017"}
{"title": "Digital pathology and computational image analysis in nephropathology.", "abstract": "The emergence of digital pathology - an image-based environment for the acquisition, management and interpretation of pathology information supported by computational techniques for data extraction and analysis - is changing the pathology ecosystem. In particular, by virtue of our new-found ability to generate and curate digital libraries, the field of machine vision can now be effectively applied to histopathological subject matter by individuals who do not have deep expertise in machine vision techniques. Although these novel approaches have already advanced the detection, classification, and prognostication of diseases in the fields of radiology and oncology, renal pathology is just entering the digital era, with the establishment of consortia and digital pathology repositories for the collection, analysis and integration of pathology data with other domains. The development of machine-learning approaches for the extraction of information from image data, allows for tissue interrogation in a way that was not previously possible. The application of these novel tools are placing pathology centre stage in the process of defining new, integrated, biologically and clinically homogeneous disease categories, to identify patients at risk of progression, and shifting current paradigms for the treatment and prevention of kidney diseases.", "journal": "Nature reviews. Nephrology", "date": "2020-08-28", "authors": ["LauraBarisoni", "Kyle JLafata", "Stephen MHewitt", "AnantMadabhushi", "Ulysses G JBalis"], "doi": "10.1038/s41581-020-0321-6\n10.1117/12.2008695\n10.1093/jnci/djx137"}
{"title": "Adaptive Feature Selection Guided Deep Forest for COVID-19 Classification With Chest CT.", "abstract": "Chest computed tomography (CT) becomes an effective tool to assist the diagnosis of coronavirus disease-19 (COVID-19). Due to the outbreak of COVID-19 worldwide, using the computed-aided diagnosis technique for COVID-19 classification based on CT images could largely alleviate the burden of clinicians. In this paper, we propose an Adaptive Feature Selection guided Deep Forest (AFS-DF) for COVID-19 classification based on chest CT images. Specifically, we first extract location-specific features from CT images. Then, in order to capture the high-level representation of these features with the relatively small-scale data, we leverage a deep forest model to learn high-level representation of the features. Moreover, we propose a feature selection method based on the trained deep forest model to reduce the redundancy of features, where the feature selection could be adaptively incorporated with the COVID-19 classification model. We evaluated our proposed AFS-DF on COVID-19 dataset with 1495 patients of COVID-19 and 1027 patients of community acquired pneumonia (CAP). The accuracy (ACC), sensitivity (SEN), specificity (SPE), AUC, precision and F1-score achieved by our method are 91.79%, 93.05%, 89.95%, 96.35%, 93.10% and 93.07%, respectively. Experimental results on the COVID-19 dataset suggest that the proposed AFS-DF achieves superior performance in COVID-19\u00a0vs. CAP classification, compared with 4 widely used machine learning methods.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-08-28", "authors": ["LiangSun", "ZhanhaoMo", "FuhuaYan", "LimingXia", "FeiShan", "ZhongxiangDing", "BinSong", "WanchunGao", "WeiShao", "FengShi", "HuanYuan", "HuitingJiang", "DijiaWu", "YingWei", "YaozongGao", "HeSui", "DaoqiangZhang", "DinggangShen"], "doi": "10.1109/JBHI.2020.3019505"}
{"title": "A systematic review on recent trends in transmission, diagnosis, prevention and imaging features of COVID-19.", "abstract": "As the new cases of COVID-19 are growing every daysince January 2020, the major way to control the spread wasthrough early diagnosis. Prevention and early diagnosis are the key strategies followed by most countries. This study presents the perspective of different modes of transmission of coronavirus,especially during clinical practices and among the pediatrics. Further, the diagnostic methods and the advancement of the computerized tomography have been discussed. Droplets, aerosol, and close contact are thesignificantfactors to transfer the infection to the suspect. This study predicts the possible transmission of the virus through medical practices such as ophthalmology, dental, and endoscopy procedures. With regard to pediatric transmission, as of now, only afew child fatalities had been reported. Childrenusually respond to the respiratory virus; however, COVID-19 response ison the contrary. The possibility of getting infected is minimal for the newborn. There has been no asymptomatic spread in children until now. Moreover, breastfeedingwould not transmit COVID-19, which is encouraging hygiene news for the pediatric. In addition, the current diagnostic methods for COVID-19 including Immunoglobulin M (IgM) and Immunoglobulin G (IgG)and chest computed topography(CT) scan, reverse transcription-polymerase chain reaction (RT-PCR) andimmunochromatographic fluorescence assay, are also discussed in detail. The introduction of artificial intelligence and deep learning algorithmhas the ability to diagnose COVID-19 in precise. However, the developments of a potential technology for the identification of the infection, such as a drone with thermal screening without human intervention, need to be encouraged.", "journal": "Process biochemistry (Barking, London, England)", "date": "2020-08-28", "authors": ["SManigandan", "Ming-TsangWu", "Vinoth KumarPonnusamy", "Vinay BRaghavendra", "ArivalaganPugazhendhi", "KathirvelBrindhadevi"], "doi": "10.1016/j.procbio.2020.08.016\n10.1016/S0140-6736(20)30360-3\n10.1148/radiol.2020200230\n10.1007/s12630-020-01627-2\n10.1056/NEJMc2022236\n10.1038/s41577-020-0394-2"}
{"title": "[Research on coronavirus disease 2019 (COVID-19) detection method based on depthwise separable DenseNet in chest X-ray images].", "abstract": "Coronavirus disease 2019 (COVID-19) has spread rapidly around the world. In order to diagnose COVID-19 more quickly, in this paper, a depthwise separable DenseNet was proposed. The paper constructed a deep learning model with 2 905 chest X-ray images as experimental dataset. In order to enhance the contrast, the contrast limited adaptive histogram equalization (CLAHE) algorithm was used to preprocess the X-ray image before network training, then the images were put into the training network and the parameters of the network were adjusted to the optimal. Meanwhile, Leaky ReLU was selected as the activation function. VGG16, ResNet18, ResNet34, DenseNet121 and SDenseNet models were used to compare with the model proposed in this paper. Compared with ResNet34, the proposed classification model of pneumonia had improved 2.0%, 2.3% and 1.5% in accuracy, sensitivity and specificity respectively. Compared with the SDenseNet network without depthwise separable convolution, number of parameters of the proposed model was reduced by 43.9%, but the classification effect did not decrease. It can be found that the proposed DWSDenseNet has a good classification effect on the COVID-19 chest X-ray images dataset. Under the condition of ensuring the accuracy as much as possible, the depthwise separable convolution can effectively reduce number of parameters of the model.\n\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u80ba\u708e\u8086\u8650\u5168\u7403\uff0c\u4e3a\u4e86\u66f4\u52a0\u5feb\u901f\u5730\u8bca\u65ad\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u80ba\u708e\uff08COVID-19\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u6df1\u5ea6\u53ef\u5206\u79bb\u7a20\u5bc6\u7f51\u7edc DWSDenseNet\uff0c\u4ee5 2 905 \u4f8b COVID-19 \u80f8\u90e8 X \u7ebf\u5e73\u7247\u5f71\u50cf\u4f5c\u4e3a\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u5728\u7f51\u7edc\u8bad\u7ec3\u524d\u4f7f\u7528\u9650\u5236\u5bf9\u6bd4\u5ea6\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\uff08CLAHE\uff09\u7b97\u6cd5\u5bf9\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff0c\u589e\u5f3a\u56fe\u50cf\u7684\u5bf9\u6bd4\u5ea6\uff0c\u5c06\u9884\u5904\u7406\u4e4b\u540e\u7684\u56fe\u50cf\u653e\u5165\u8bad\u7ec3\u7f51\u7edc\u4e2d\uff0c\u91c7\u7528 Leaky ReLU \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u8c03\u6574\u53c2\u6570\u4ee5\u8fbe\u5230\u6700\u4f18\u3002\u672c\u6587\u5f15\u5165 VGG16\u3001ResNet18\u3001ResNet34\u3001DenseNet121 \u548c SDenseNet \u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u6240\u63d0\u51fa\u7684\u7f51\u7edc\u5728\u4e09\u5206\u7c7b\u5b9e\u9a8c\u4e2d\u76f8\u8f83\u4e8e ResNet34 \u5728\u51c6\u786e\u7387\u3001\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u4e0a\u5206\u522b\u63d0\u9ad8\u4e86 2.0%\u30012.3%\u30011.5%\u3002\u76f8\u5bf9\u4e8e\u6539\u8fdb\u524d\u7684 SDenseNet \u7f51\u7edc\uff0c\u672c\u6587\u6a21\u578b\u7684\u53c2\u6570\u91cf\u51cf\u5c11\u4e86 43.9%\uff0c\u4f46\u5206\u7c7b\u6548\u679c\u5e76\u672a\u4e0b\u964d\u3002\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u53ef\u4ee5\u53d1\u73b0\uff0c\u672c\u6587\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u53ef\u5206\u79bb\u7a20\u5bc6\u7f51\u7edc\u5bf9 COVID-19 \u80f8\u90e8 X \u7ebf\u5e73\u7247\u5f71\u50cf\u6570\u636e\u96c6\u5177\u6709\u826f\u597d\u7684\u5206\u7c7b\u6548\u679c\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u80fd\u591f\u6709\u6548\u5730\u964d\u4f4e\u6a21\u578b\u53c2\u6570\u91cf\u3002.", "journal": "Sheng wu yi xue gong cheng xue za zhi = Journal of biomedical engineering = Shengwu yixue gongchengxue zazhi", "date": "2020-08-26", "authors": ["YiboFeng", "DaweiQiu", "HuiCao", "JunzhongZhang", "ZaihaiXin", "JingLiu"], "doi": "10.7507/1001-5515.202005056"}
{"title": "Radiomics-based model for accurately distinguishing between severe acute respiratory syndrome associated coronavirus 2 (SARS-CoV-2) and influenza A infected pneumonia.", "abstract": "Clinicians have been faced with the challenge of differentiating between severe acute respiratory syndrome associated coronavirus 2 (SARS-CoV-2) infected pneumonia (NCP) and influenza A infected pneumonia (IAP), a seasonal disease that coincided with the outbreak. We aim to develop a machine-learning algorithm based on radiomics to distinguish NCP from IAP by texture analysis based on computed tomography (CT) imaging. Forty-one NCP and 37 IAP patients admitted from January to February 6, 2019 admitted to two hospitals in Wenzhou, China. All patients had undergone chest CT examination and blood routine tests prior to receiving medical treatment. NCP was diagnosed by real-time RT-PCR assays. Eight of 56 radiomic features extracted by LIFEx were selected by least absolute shrinkage and selection operator regression to develop a radiomics score and subsequently constructed into a nomogram to predict NCP with area under the operating characteristics curve of 0.87 (95% confidence interval: 0.77-0.93). The nomogram also showed excellent calibration with Hosmer-Lemeshow test yielding a nonsignificant statistic (", "journal": "MedComm", "date": "2020-08-25", "authors": ["Qi-QiangZeng", "Kenneth IZheng", "JunChen", "Zheng-HaoJiang", "TianTian", "Xiao-BoWang", "Hong-LeiMa", "Ke-HuaPan", "Yun-JunYang", "Yong-PingChen", "Ming-HuaZheng"], "doi": "10.1002/mco2.14\n10.1148/radiol.2020200230"}
{"title": "A Deep Learning System to Screen Novel Coronavirus Disease 2019 Pneumonia.", "abstract": "The real-time reverse transcription-polymerase chain reaction (RT-PCR) detection of viral RNA from sputum or nasopharyngeal swab had a relatively low positive rate in the early stage of coronavirus disease 2019 (COVID-19). Meanwhile, the manifestations of COVID-19 as seen through computed tomography (CT) imaging show individual characteristics that differ from those of other types of viral pneumonia such as influenza-A viral pneumonia (IAVP). This study aimed to establish an early screening model to distinguish COVID-19 from IAVP and healthy cases through pulmonary CT images using deep learning techniques. A total of 618 CT samples were collected: 219 samples from 110 patients with COVID-19 (mean age 50\u00a0years; 63 (57.3%) male patients); 224 samples from 224 patients with IAVP (mean age 61\u00a0years; 156 (69.6%) male patients); and 175 samples from 175 healthy cases (mean age 39\u00a0years; 97 (55.4%) male patients). All CT samples were contributed from three COVID-19-designated hospitals in Zhejiang Province, China. First, the candidate infection regions were segmented out from the pulmonary CT image set using a 3D deep learning model. These separated images were then categorized into the COVID-19, IAVP, and irrelevant to infection (ITI) groups, together with the corresponding confidence scores, using a location-attention classification model. Finally, the infection type and overall confidence score for each CT case were calculated using the Noisy-OR Bayesian function. The experimental result of the benchmark dataset showed that the overall accuracy rate was 86.7% in terms of all the CT cases taken together. The deep learning models established in this study were effective for the early screening of COVID-19 patients and were demonstrated to be a promising supplementary diagnostic method for frontline clinical doctors.", "journal": "Engineering (Beijing, China)", "date": "2020-08-25", "authors": ["XiaoweiXu", "XiangaoJiang", "ChunlianMa", "PengDu", "XukunLi", "ShuangzhiLv", "LiangYu", "QinNi", "YanfeiChen", "JunweiSu", "GuanjingLang", "YongtaoLi", "HongZhao", "JunLiu", "KaijinXu", "LingxiangRuan", "JifangSheng", "YunqingQiu", "WeiWu", "TingboLiang", "LanjuanLi"], "doi": "10.1016/j.eng.2020.04.010"}
{"title": "Deep learning applications in pulmonary medical imaging: recent updates and insights on COVID-19.", "abstract": "Shortly after deep learning algorithms were applied to Image Analysis, and more importantly to medical imaging, their applications increased significantly to become a trend. Likewise, deep learning applications (DL) on pulmonary medical images emerged to achieve remarkable advances leading to promising clinical trials. Yet, coronavirus can be the real trigger to open the route for fast integration of DL in hospitals and medical centers. This paper reviews the development of deep learning applications in medical image analysis targeting pulmonary imaging and giving insights of contributions to COVID-19. It covers more than 160 contributions and surveys in this field, all issued between February 2017 and May 2020 inclusively, highlighting various deep learning tasks such as classification, segmentation, and detection, as well as different pulmonary pathologies like airway diseases, lung cancer, COVID-19 and other infections. It summarizes and discusses the current state-of-the-art approaches in this research domain, highlighting the challenges, especially with COVID-19 pandemic current situation.", "journal": "Machine vision and applications", "date": "2020-08-25", "authors": ["HananFarhat", "George ESakr", "RimaKilany"], "doi": "10.1007/s00138-020-01101-5\n10.1007/s11548-019-01917-1\n10.1146/annurev-bioeng-071516-044442\n10.3322/caac.21552\n10.3390/diagnostics9010029\n10.1186/s12938-018-0544-y\n10.1186/s13550-017-0260-9\n10.1007/s10278-017-0028-9\n10.3390/cancers11020212\n10.14741/Ijcet/22774106/5.2.2015.121"}
{"title": "Identifying COVID19 from Chest CT Images: A Deep Convolutional Neural Networks Based Approach.", "abstract": "Coronavirus Disease (COVID19) is a fast-spreading infectious disease that is currently causing a healthcare crisis around the world. Due to the current limitations of the reverse transcription-polymerase chain reaction (RT-PCR) based tests for detecting COVID19, recently radiology imaging based ideas have been proposed by various works. In this work, various Deep CNN based approaches are explored for detecting the presence of COVID19 from chest CT images. A decision fusion based approach is also proposed, which combines predictions from multiple individual models, to produce a final prediction. Experimental results show that the proposed decision fusion based approach is able to achieve above 86% results across all the performance metrics under consideration, with average AUROC and F1-Score being 0.883 and 0.867, respectively. The experimental observations suggest the potential applicability of such Deep CNN based approach in real diagnostic scenarios, which could be of very high utility in terms of achieving fast testing for COVID19.", "journal": "Journal of healthcare engineering", "date": "2020-08-25", "authors": ["Arnab KumarMishra", "Sujit KumarDas", "PinkiRoy", "SivajiBandyopadhyay"], "doi": "10.1155/2020/8843664\n10.1101/2020.04.16.20064709v1\n10.1101/2020.04.16.20064709\n10.3892/etm.2020.8797\n10.1109/CVPR.2017.243\n10.1080/07391102.2020.1788642\n10.1101/2020.03.12.20027185v2\n10.1101/2020.03.12.20027185\n10.1109/CVPR.2015.7298594\n10.1109/CVPR.2016.308\n10.1109/CVPR.2016.90"}
{"title": "Efficient and Effective Training of COVID-19 Classification Networks With Self-Supervised Dual-Track Learning to Rank.", "abstract": "Coronavirus Disease 2019 (COVID-19) has rapidly spread worldwide since first reported. Timely diagnosis of COVID-19 is crucial both for disease control and patient care. Non-contrast thoracic computed tomography (CT) has been identified as an effective tool for the diagnosis, yet the disease outbreak has placed tremendous pressure on radiologists for reading the exams and may potentially lead to fatigue-related mis-diagnosis. Reliable automatic classification algorithms can be really helpful; however, they usually require a considerable number of COVID-19 cases for training, which is difficult to acquire in a timely manner. Meanwhile, how to effectively utilize the existing archive of non-COVID-19 data (the negative samples) in the presence of severe class imbalance is another challenge. In addition, the sudden disease outbreak necessitates fast algorithm development. In this work, we propose a novel approach for effective and efficient training of COVID-19 classification networks using a small number of COVID-19 CT exams and an archive of negative samples. Concretely, a novel self-supervised learning method is proposed to extract features from the COVID-19 and negative samples. Then, two kinds of soft-labels ('difficulty' and 'diversity') are generated for the negative samples by computing the earth mover's distances between the features of the negative and COVID-19 samples, from which data 'values' of the negative samples can be assessed. A pre-set number of negative samples are selected accordingly and fed to the neural network for training. Experimental results show that our approach can achieve superior performance using about half of the negative samples, substantially reducing model training time.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-08-21", "authors": ["YuexiangLi", "DongWei", "JiaweiChen", "ShileiCao", "HongyuZhou", "YanchunZhu", "JianrongWu", "LanLan", "WenboSun", "TianyiQian", "KaiMa", "HaiboXu", "YefengZheng"], "doi": "10.1109/JBHI.2020.3018181"}
{"title": "Determination of disease severity in COVID-19 patients using deep learning in chest X-ray images.", "abstract": "Chest X-ray plays a key role in diagnosis and management of COVID-19 patients and imaging features associated with clinical elements may assist with the development or validation of automated image analysis tools. We aimed to identify associations between clinical and radiographic features as well as to assess the feasibility of deep learning applied to chest X-rays in the setting of an acute COVID-19 outbreak.\nA retrospective study of X-rays, clinical, and laboratory data was performed from 48 SARS-CoV-2 RT-PCR positive patients (age 60\u00b117 years, 15 women) between February 22 and March 6, 2020 from a tertiary care hospital in Milan, Italy. Sixty-five chest X-rays were reviewed by two radiologists for alveolar and interstitial opacities and classified by severity on a scale from 0 to 3. Clinical factors (age, symptoms, comorbidities) were investigated for association with opacity severity and also with placement of central line or endotracheal tube. Deep learning models were then trained for two tasks: lung segmentation and opacity detection. Imaging characteristics were compared to clinical datapoints using the unpaired student's t-test or Mann-Whitney U test. Cohen's kappa analysis was used to evaluate the concordance of deep learning to conventional radiologist interpretation.\nFifty-six percent of patients presented with alveolar opacities, 73% had interstitial opacities, and 23% had normal X-rays. The presence of alveolar or interstitial opacities was statistically correlated with age (P = 0.008) and comorbidities (P = 0.005). The extent of alveolar or interstitial opacities on baseline X-ray was significantly associated with the presence of endotracheal tube (P = 0.0008 and P = 0.049) or central line (P = 0.003 and P = 0.007). In comparison to human interpretation, the deep learning model achieved a kappa concordance of 0.51 for alveolar opacities and 0.71 for interstitial opacities.\nChest X-ray analysis in an acute COVID-19 outbreak showed that the severity of opacities was associated with advanced age, comorbidities, as well as acuity of care. Artificial intelligence tools based upon deep learning of COVID-19 chest X-rays are feasible in the acute outbreak setting.", "journal": "Diagnostic and interventional radiology (Ankara, Turkey)", "date": "2020-08-21", "authors": ["MaximeBlain", "Michael TKassin", "NicoleVarble", "XiaosongWang", "ZiyueXu", "DaguangXu", "GianpaoloCarrafiello", "ValentinaVespro", "ElviraStellato", "Anna MariaIerardi", "Letizia DiMeglio", "RobertD Suh", "StephanieA Walker", "ShengXu", "ThomasH Sanford", "EvrimB Turkbey", "StephanieHarmon", "BarisTurkbey", "BradfordJ Wood"], "doi": "10.5152/dir.2020.20205\n10.1001/jama.2020.2648\n10.1016/S0140-6736(20)30566-3\n10.1016/S1473-3099(20)30195-X\n10.1001/jamainternmed.2020.0994\n10.1148/radiol.2020200642\n10.1016/j.tmaid.2020.101627\n10.2214/AJR.19.22688\n10.1016/j.jacr.2020.02.008\n10.1148/radiol.2020200823\n10.1148/radiol.2020200432\n10.1148/radiol.2020200330\n10.3348/kjr.2020.0146\n10.1148/radiol.2020200343\n10.1016/j.clinimag.2020.02.008\n10.1097/RLI.0000000000000672\n10.1016/j.ejrad.2020.108941\n10.2214/AJR.20.22976\n10.1016/j.ejrad.2020.108972\n10.1148/radiol.2020200463\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020200370\n10.1148/radiol.2020200843\n10.1148/radiol.2020200230\n10.1148/radiol.2020200241\n10.1148/radiol.2020200274\n10.1007/s00259-020-04720-2\n10.1007/s00330-020-06801-0\n10.2214/AJR.20.23154\n10.1001/jama.2020.4326\n10.1148/ryct.2020200034\n10.3348/kjr.2020.0132\n10.1148/radiol.2020200905\n10.1002/cyto.a.23990\n10.1007/s00330-020-06817-6\n10.1007/978-3-319-24574-4_28\n10.2214/ajr.174.1.1740071\n10.1109/CVPR.2017.243\n10.1109/CVPR.2017.369"}
{"title": "Differentiating novel coronavirus pneumonia from general pneumonia based on machine learning.", "abstract": "Chest CT screening as supplementary means is crucial in diagnosing novel coronavirus pneumonia (COVID-19) with high sensitivity and popularity. Machine learning was adept in discovering intricate structures from CT images and achieved expert-level performance in medical image analysis.\nAn integrated machine learning framework on chest CT images for differentiating COVID-19 from general pneumonia (GP) was developed and validated. Seventy-three confirmed COVID-19 cases were consecutively enrolled together with 27 confirmed general pneumonia patients from Ruian People's Hospital, from January 2020 to March 2020. To accurately classify COVID-19, region of interest (ROI) delineation was implemented based on ground-glass opacities (GGOs) before feature extraction. Then, 34 statistical texture features of COVID-19 and GP ROI images were extracted, including 13 gray-level co-occurrence matrix (GLCM) features, 15 gray-level-gradient co-occurrence matrix (GLGCM) features and 6 histogram features. High-dimensional features impact the classification performance. Thus, ReliefF algorithm was leveraged to select features. The relevance of each feature was the average weights calculated by ReliefF in n times. Features with relevance larger than the empirically set threshold T were selected. After feature selection, the optimal feature set along with 4 other selected feature combinations for comparison were applied to the ensemble of bagged tree (EBT) and four other machine learning classifiers including support vector machine (SVM), logistic regression (LR), decision tree (DT), and K-nearest neighbor with Minkowski distance equal weight (KNN) using tenfold cross-validation.\nThe classification accuracy (ACC), sensitivity (SEN), specificity (SPE) of our proposed method yield 94.16%, 88.62% and 100.00%, respectively. The area under the receiver operating characteristic curve (AUC) was 0.99. The experimental results indicate that the EBT algorithm with statistical textural features based on GGOs for differentiating COVID-19 from general pneumonia achieved high transferability, efficiency, specificity, sensitivity, and impressive accuracy, which is beneficial for inexperienced doctors to more accurately diagnose COVID-19 and essential for controlling the spread of the disease.", "journal": "Biomedical engineering online", "date": "2020-08-21", "authors": ["ChenglongLiu", "XiaoyangWang", "ChenbinLiu", "QingfengSun", "WenxianPeng"], "doi": "10.1186/s12938-020-00809-9"}
{"title": "Interest of the cellular population data analysis as an aid in the early diagnosis of SARS-CoV-2 infection.", "abstract": "Coronavirus disease 2019 (COVID-19) is characterized by a high contagiousness requiring isolation measures. At this time, diagnosis is based on the positivity of specific RT-PCR and/or chest computed tomography scan, which are time-consuming and may delay diagnosis. Complete blood count (CBC) can potentially contribute to the diagnosis of COVID-19. We studied whether the analysis of cellular population data (CPD), provided as part of CBC-Diff analysis by the DxH 800 analyzers (Beckman Coulter), can help to identify SARS-CoV-2 infection.\nCellular population data of the different leukocyte subpopulations were analyzed in 137 controls, 322 patients with proven COVID-19 (COVID+), and 285 patients for whom investigations were negative for SARS-CoV-2 infection (COVID-). When CPD of COVID+ were different from controls and COVID- patients, we used receiver operating characteristic analysis to test the discriminating capacity of the individual parameters. Using a random forest classifier, we developed the algorithm based on the combination of 4 monocyte CPD to discriminate COVID+ from COVID- patients. This algorithm was tested prospectively in a series of 222 patients referred to the emergency unit.\nAmong the 222 patients, 86 were diagnosed as COVID-19 and 60.5% were correctly identified using the discriminating protocol. Among the 136 COVID- patients, 10.3% were misclassified (specificity 89.7%, sensitivity 60.5%). False negatives were observed mainly in patients with a low inflammatory state whereas false positives were mainly seen in patients with sepsis.\nConsideration of CPD could constitute a first step and potentially aid in the early diagnosis of COVID-19.", "journal": "International journal of laboratory hematology", "date": "2020-08-20", "authors": ["MarcVasse", "Marie-ChristineBallester", "DegnileAyaka", "DmitrySukhachev", "Fr\u00e9d\u00e9riqueDelcominette", "FlorenceHabarou", "EmilieJolly", "ElenaSukhacheva", "TiffanyPascreau", "\u00c9ricFarfour"], "doi": "10.1111/ijlh.13312"}
{"title": "AI-Driven COVID-19 Tools to Interpret, Quantify Lung Images.", "abstract": "Qualitative interpretation is a good thing when it comes to reading lung images in the fight against coronavirus 2019 disease (COVID-19), but quantitative analysis makes radiology reporting much more comprehensive. To that end, several research groups have begun looking to artificial intelligence (AI) as a tool for reading and analyzing X-rays and computed tomography (CT) scans, and helping to diagnose and monitor COVID-19.", "journal": "IEEE pulse", "date": "2020-08-18", "authors": ["LeslieMertz"], "doi": "10.1109/MPULS.2020.3008354"}
{"title": "Detection of coronavirus disease from X-ray images using deep learning and transfer learning algorithms.", "abstract": "This study aims to employ the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of novel coronavirus (COVID-19) infected disease.\nThis study applied transfer learning method to develop deep learning models for detecting COVID-19 disease. Three existing state-of-the-art deep learning models namely, Inception ResNetV2, InceptionNetV3 and NASNetLarge, were selected and fine-tuned to automatically detect and diagnose COVID-19 disease using chest X-ray images. A dataset involving 850 images with the confirmed COVID-19 disease, 500 images of community-acquired (non-COVID-19) pneumonia cases and 915 normal chest X-ray images was used in this study.\nAmong the three models, InceptionNetV3 yielded the best performance with accuracy levels of 98.63% and 99.02% with and without using data augmentation in model training, respectively. All the performed networks tend to overfitting (with high training accuracy) when data augmentation is not used, this is due to the limited amount of image data used for training and validation.\nThis study demonstrated that a deep transfer learning is feasible to detect COVID-19 disease automatically from chest X-ray by training the learning model with chest X-ray images mixed with COVID-19 patients, other pneumonia affected patients and people with healthy lungs, which may help doctors more effectively make their clinical decisions. The study also gives an insight to how transfer learning was used to automatically detect the COVID-19 disease. In future studies, as the amount of available dataset increases, different convolution neutral network models could be designed to achieve the goal more efficiently.", "journal": "Journal of X-ray science and technology", "date": "2020-08-18", "authors": ["SalehAlbahli", "WaleedAlbattah"], "doi": "10.3233/XST-200720\n10.1101/2020.02.14.20023028"}
{"title": "Capacity Evaluation of Diagnostic Tests For COVID-19 Using Multicriteria Decision-Making Techniques.", "abstract": "In December 2019, cases of pneumonia were detected in Wuhan, China, which were caused by the highly contagious coronavirus. This study is aimed at comparing the confusion regarding the selection of effective diagnostic methods to make a mutual comparison among existing SARS-CoV-2 diagnostic tests and at determining the most effective one. Based on available published evidence and clinical practice, diagnostic tests of coronavirus disease (COVID-19) were evaluated by multi-criteria decision-making (MCDM) methods, namely, fuzzy preference ranking organization method for enrichment evaluation (fuzzy PROMETHEE) and fuzzy technique for order of preference by similarity to ideal solution (fuzzy TOPSIS). Computerized tomography of chest (chest CT), the detection of viral nucleic acid by polymerase chain reaction, cell culture, CoV-19 antigen detection, CoV-19 antibody IgM, CoV-19 antibody IgG, and chest X-ray were evaluated by linguistic fuzzy scale to compare among the diagnostic tests. This scale consists of selected parameters that possessed different weights which were determined by the experts' opinions of the field. The results of our study with both proposed MCDM methods indicated that the most effective diagnosis method of COVID-19 was chest CT. It is interesting to note that the methods that are consistently used in the diagnosis of viral diseases were ranked in second place for the diagnosis of COVID-19. However, each country should use appropriate diagnostic solutions according to its own resources. Our findings also show which diagnostic systems can be used in combination.", "journal": "Computational and mathematical methods in medicine", "date": "2020-08-18", "authors": ["MuratSayan", "FigenSarigul Yildirim", "TamerSanlidag", "BernaUzun", "DilberUzun Ozsahin", "IlkerOzsahin"], "doi": "10.1155/2020/1560250\n10.1038/s41564-020-0695-z\n10.1056/NEJMoa2001017\n10.3390/pathogens9030231\n10.1101/2020.02.07.937862v1\n10.1001/jama.2020.1623\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.7150/ijbs.45134\n10.1097/CM9.0000000000000722\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.15585/mmwr.mm6905e1\n10.1093/cid/ciaa310\n10.1002/jmv.25800\n10.3346/jkms.2020.35.e84\n10.1148/radiol.11092149\n10.1148/radiol.2020200642\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200343\n10.1016/S1473-3099(20)30241-3\n10.1016/j.ejrad.2020.108961\n10.1287/mnsc.31.6.647\n10.1016/0377-2217(86)90044-5\n10.1016/S0165-0114(99)00021-4\n10.1016/S0019-9958(65)90241-X\n10.1109/ICAL.2008.4636483\n10.1016/j.knosys.2014.04.046\n10.3844/jssp.2008.255.263\n10.1148/radiol.2020200432\n10.1148/radiol.2020200823"}
{"title": "Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets.", "abstract": "Chest CT is emerging as a valuable diagnostic tool for clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to aid in rapid evaluation of CT scans for differentiation of COVID-19 findings from other clinical entities. Here we show\u00a0that a series of deep learning algorithms, trained in a\u00a0diverse multinational cohort of 1280 patients to localize parietal pleura/lung parenchyma followed by classification of COVID-19 pneumonia, can achieve up to 90.8% accuracy, with 84% sensitivity and 93% specificity, as\u00a0evaluated in an independent test set (not included in training and validation) of 1337 patients. Normal controls included chest CTs from oncology, emergency, and pneumonia-related indications. The false positive rate in 140 patients with laboratory confirmed other (non COVID-19) pneumonias was 10%. AI-based algorithms can readily identify CT scans with COVID-19 associated pneumonia, as well as distinguish non-COVID related pneumonias with high specificity in diverse patient populations.", "journal": "Nature communications", "date": "2020-08-17", "authors": ["Stephanie AHarmon", "Thomas HSanford", "ShengXu", "Evrim BTurkbey", "HolgerRoth", "ZiyueXu", "DongYang", "AndriyMyronenko", "VictoriaAnderson", "AmelAmalou", "MaximeBlain", "MichaelKassin", "DilaraLong", "NicoleVarble", "Stephanie MWalker", "UlasBagci", "Anna MariaIerardi", "ElviraStellato", "Guido GiovanniPlensich", "GiuseppeFranceschelli", "CristianoGirlando", "GiovanniIrmici", "DominicLabella", "DimaHammoud", "AshkanMalayeri", "ElizabethJones", "Ronald MSummers", "Peter LChoyke", "DaguangXu", "MonaFlores", "KakuTamura", "HirofumiObinata", "HitoshiMori", "FrancescaPatella", "MaurizioCariati", "GianpaoloCarrafiello", "PengAn", "Bradford JWood", "BarisTurkbey"], "doi": "10.1038/s41467-020-17971-2\n10.1016/j.ijsu.2020.02.034\n10.1101/2020.02.11.20021493v2\n10.1101/2020.1103.1119.20039354v20039351\n10.1101/2020.02.14.20023028v5\n10.1101/2020.03.19.20039354v1\n10.1148/radiol.2020200702\n10.1118/1.3528204\n10.1007/s11263-019-01228-7"}
{"title": "Machine learning-based CT radiomics method for predicting hospital stay in patients with pneumonia associated with SARS-CoV-2 infection: a multicenter study.", "abstract": "The coronavirus disease 2019 (COVID-19) has become a global challenge since the December 2019. The hospital stay is one of the prognostic indicators, and its predicting model based on CT radiomics features is important for assessing the patients' clinical outcome. The study aimed to develop and test machine learning-based CT radiomics models for predicting hospital stay in patients with COVID-19 pneumonia.\nThis retrospective, multicenter study enrolled patients with laboratory-confirmed SARS-CoV-2 infection and their initial CT images from 5 designated hospitals in Ankang, Lishui, Lanzhou, Linxia, and Zhenjiang between January 23, 2020 and February 8, 2020. Patients were classified into short-term (\u226410 days) and long-term hospital stay (>10 days). CT radiomics models based on logistic regression (LR) and random forest (RF) were developed on features from pneumonia lesions in first four centers. The predictive performance was evaluated in fifth center (test dataset) on lung lobe- and patients-level.\nA total of 52 patients were enrolled from designated hospitals. As of February 20, 21 patients remained in hospital or with non-findings in CT were excluded. Therefore, 31 patients with 72 lesion segments were included in analysis. The CT radiomics models based on 6 second-order features were effective in discriminating short- and long-term hospital stay in patients with COVID-19 pneumonia, with areas under the curves of 0.97 (95% CI, 0.83-1.0) and 0.92 (95% CI, 0.67-1.0) by LR and RF, respectively, in test. The LR and RF model showed a sensitivity and specificity of 1.0 and 0.89, 0.75 and 1.0 in test respectively. As of February 28, a prospective cohort of six discharged patients were all correctly recognized as long-term stay using RF and LR models.\nThe machine learning-based CT radiomics features and models showed feasibility and accuracy for predicting hospital stay in patients with COVID-19 pneumonia.", "journal": "Annals of translational medicine", "date": "2020-08-15", "authors": ["HongmeiYue", "QianYu", "ChuanLiu", "YifeiHuang", "ZichengJiang", "ChuxiaoShao", "HongguangZhang", "BaoyiMa", "YuanchengWang", "GuanghangXie", "HaijunZhang", "XiaoguoLi", "NingKang", "XiangpanMeng", "ShanHuang", "DanXu", "JunqiangLei", "HuihongHuang", "JieYang", "JiansongJi", "HongqiuPan", "ShengqiangZou", "ShenghongJu", "XiaolongQi"], "doi": "10.21037/atm-20-3026\n10.1001/jama.2020.1585\n10.1016/S0140-6736(20)30183-5\n10.1136/bmj.m606\n10.1148/radiol.2020200490\n10.1148/radiol.2020200370\n10.1148/radiol.2020200236\n10.21037/atm.2020.02.91\n10.1148/radiol.2015151169\n10.1158/0008-5472.CAN-17-0339\n10.1038/srep13087\n10.1038/s41467-019-11007-0"}
{"title": "COVID-19 Detection Through Transfer Learning Using Multimodal Imaging Data.", "abstract": "Detecting COVID-19 early may help in devising an appropriate treatment plan and disease containment decisions. In this study, we demonstrate how transfer learning from deep learning models can be used to perform COVID-19 detection using images from three most commonly used medical imaging modes X-Ray, Ultrasound, and CT scan. The aim is to provide over-stressed medical professionals a second pair of eyes through intelligent deep learning image classification models. We identify a suitable ", "journal": "IEEE access : practical innovations, open solutions", "date": "2020-08-14", "authors": ["Michael JHorry", "SubrataChakraborty", "ManoranjanPaul", "AnwaarUlhaq", "BiswajeetPradhan", "ManasSaha", "NageshShukla"], "doi": "10.1109/ACCESS.2020.3016780\n10.1007/978-3-319-46976-820\n10.3390/info8030091\n10.1001/jama.2020.3786\n10.7326/M20-0991\n10.1148/radiol.2020200642\n10.2214/AJR.20.22969\n10.1097/RTI.0000000000000404\n10.1148/radiol.2020201160\n10.1016/S2213-2600(20)30120-X\n10.1111/anae.15082\n10.1007/s13244-018-0639-9\n10.1002/jmri.26534\n10.1109/CVPR.2016.90\n10.1109/CVPR.2016.308\n10.1109/CVPR.2017.195\n10.5555/3298023.3298188\n10.1109/CVPR.2017.243\n10.1109/CVPR.2018.00907\n10.4329/wjr.v5.i11.398\n10.1016/j.ijmedinf.2018.06.003\n10.1118/1.2208736\n10.18280/ts.360406\n10.1007/978-3-319-19992-4_46\n10.1016/j.compbiomed.2017.04.006\n10.1109/JTEHM.2019.2955458\n10.1038/s41591-019-0447-x\n10.3390/jcm8040514\n10.1016/j.ajem.2012.08.041\n10.1117/1.jmi.3.1.014501\n10.1371/journal.pone.0063820\n10.1007/s10278-009-9245-1\n10.1007/978-3-642-05177-719\n10.1007/s00134-011-2317-y\n10.1109/icbbe.2011.5780221\n10.1117/12.2254526\n10.1186/s13634-015-0214-1\n10.1016/j.bspc.2012.02.002\n10.1007/s10278-019-00211-5\n10.1007/s10096-020-03901-z\n10.1109/TMI.2020.2995508\n10.1148/radiol.2020200905\n10.1109/CVPR.2017.369\n10.18103/bme.v3i1.1550\n10.1109/BMEiCON.2017.8229130\n10.1007/s11263-015-0816-y\n10.1007/978-3-030-01258-8_39\n10.1109/CVPR.2019.00277\n10.1109/CVPR.2019.00741\n10.1109/ICCV.2019.00200\n10.1109/WACV45572.2020.9093418\n10.1109/CVPR.2019.00497\n10.1109/TPAMI.2019.2938758\n10.1007/978-3-030-01258-8_12\n10.1109/CVPR.2019.00638\n10.1007/978-3-030-01246-5_2\n10.1609/aaai.v33i01.33014780\n10.1109/ICCVW.2019.00246\n10.1109/TPAMI.2018.2858232\n10.1109/CVPR.2019.00060\n10.1007/978-3-030-34879-3_12\n10.1109/CVPR.2009.5206848\n10.3310/hta4050\n10.1109/TPAMI.2018.2798607\n10.1109/LGRS.2017.2704625\n10.1109/TIP.2018.2809606\n10.1109/JSTARS.2016.2634863\n10.1080/21681163.2015.1135299"}
{"title": "Automated quantification of COVID-19 severity and progression using chest CT images.", "abstract": "To develop and test computer software to detect, quantify, and monitor progression of pneumonia associated with COVID-19 using chest CT scans.\nOne hundred twenty chest CT scans from subjects with lung infiltrates were used for training deep learning algorithms to segment lung regions and vessels. Seventy-two serial scans from 24 COVID-19 subjects were used to develop and test algorithms to detect and quantify the presence and progression of infiltrates associated with COVID-19. The algorithm included (1) automated lung boundary and vessel segmentation, (2) registration of the lung boundary between serial scans, (3) computerized identification of the pneumonitis regions, and (4) assessment of disease progression. Agreement between radiologist manually delineated regions and computer-detected regions was assessed using the Dice coefficient. Serial scans were registered and used to generate a heatmap visualizing the change between scans. Two radiologists, using a five-point Likert scale, subjectively rated heatmap accuracy in representing progression.\nThere was strong agreement between computer detection and the manual delineation of pneumonic regions with a Dice coefficient of 81% (CI 76-86%). In detecting large pneumonia regions (>\u2009200\u00a0mm\nThe preliminary results suggested the feasibility of using computer software to detect and quantify pneumonic regions associated with COVID-19 and to generate heatmaps that can be used to visualize and assess progression.\n\u2022 Both computer vision and deep learning technology were used to develop computer software to quantify the presence and progression of pneumonia associated with COVID-19 depicted on CT images. \u2022 The computer software was tested using both quantitative experiments and subjective assessment. \u2022 The computer software has the potential to assist in the detection of the pneumonic regions, monitor disease progression, and assess treatment efficacy related to COVID-19.", "journal": "European radiology", "date": "2020-08-14", "authors": ["JiantaoPu", "Joseph KLeader", "AndriyBandos", "ShiKe", "JingWang", "JunliShi", "PangDu", "YouminGuo", "Sally EWenzel", "Carl RFuhrman", "David OWilson", "Frank CSciurba", "ChenwangJin"], "doi": "10.1007/s00330-020-07156-2\n10.1016/j.clinimag.2020.02.008\n10.1001/jamanetworkopen.2019.1095\n10.1136/jamia.2000.0070593\n10.2196/jmir.6887\n10.2214/ajr.175.5.1751329\n10.1152/japplphysiol.00465.2019\n10.1016/j.compmedimag.2014.01.002\n10.1109/TMI.2010.2076300\n10.1016/S1076-6332(03)00671-8\n10.1016/j.media.2019.101592\n10.1118/1.2948349\n10.1109/TVCG.2010.56"}
{"title": "Comparing different deep learning architectures for classification of chest radiographs.", "abstract": "Chest radiographs are among the most frequently acquired images in radiology and are often the subject of computer vision research. However, most of the models used to classify chest radiographs are derived from openly available deep neural networks, trained on large image datasets. These datasets differ from chest radiographs in that they are mostly color images and have substantially more labels. Therefore, very deep convolutional neural networks (CNN) designed for ImageNet and often representing more\u00a0complex relationships, might not be required for the comparably simpler task of classifying medical image data. Sixteen different architectures of CNN were compared regarding the classification\u00a0performance on two openly available datasets, the CheXpert and COVID-19 Image Data Collection. Areas under the receiver operating characteristics curves (AUROC) between 0.83 and 0.89 could be achieved on the CheXpert dataset. On the COVID-19 Image Data Collection, all models showed an excellent ability to detect COVID-19 and non-COVID pneumonia with AUROC values between 0.983 and 0.998. It could be observed, that more shallow networks may achieve results comparable to their deeper and more complex counterparts with shorter training times, enabling classification performances on medical image data close to the state-of-the-art methods even when using limited hardware.", "journal": "Scientific reports", "date": "2020-08-14", "authors": ["Keno KBressem", "Lisa CAdams", "ChristophErxleben", "BerndHamm", "Stefan MNiehues", "Janis LVahldiek"], "doi": "10.1038/s41598-020-70479-z\n10.1148/ryai.2019190015\n10.3390/info11020108\n10.1093/bioinformatics/bti623\n10.1109/ACCESS.2019.2916849\n10.3348/kjr.2019.0025"}
{"title": "Rapid identification of COVID-19 severity in CT scans through classification of deep features.", "abstract": "Chest CT is used for the assessment of the severity of patients infected with novel coronavirus 2019 (COVID-19). We collected chest CT scans of 202 patients diagnosed with the COVID-19, and try to develop a rapid, accurate and automatic tool for severity screening follow-up therapeutic treatment.\nA total of 729 2D axial plan slices with 246 severe cases and 483 non-severe cases were employed in this study. By taking the advantages of the pre-trained deep neural network, four pre-trained off-the-shelf deep models (Inception-V3, ResNet-50, ResNet-101, DenseNet-201) were exploited to extract the features from these CT scans. These features are then fed to multiple classifiers (linear discriminant, linear SVM, cubic SVM, KNN and Adaboost decision tree) to identify the severe and non-severe COVID-19 cases. Three validation strategies (holdout validation, tenfold cross-validation and leave-one-out) are employed to validate the feasibility of proposed pipelines.\nThe experimental results demonstrate that classification of the features from pre-trained deep models shows the promising application in COVID-19 severity screening, whereas the DenseNet-201 with cubic SVM model achieved the best performance. Specifically, it achieved the highest severity classification accuracy of 95.20% and 95.34% for tenfold cross-validation and leave-one-out, respectively. The established pipeline was able to achieve a rapid and accurate identification of the severity of COVID-19. This may assist the physicians to make more efficient and reliable decisions.", "journal": "Biomedical engineering online", "date": "2020-08-14", "authors": ["ZekuanYu", "XiaohuLi", "HaitaoSun", "JianWang", "TongtongZhao", "HongyiChen", "YichuanMa", "ShujinZhu", "ZongyuXie"], "doi": "10.1186/s12938-020-00807-x\n10.1016/S0140-6736(20)30154-9\n10.1056/NEJMc2001468\n10.1002/jmv.25699\n10.1148/radiol.2020200463\n10.1097/RLI.0000000000000672\n10.1101/2020.03.12.20027185"}
{"title": "Deep-COVID: Predicting COVID-19 from chest X-ray images using deep transfer learning.", "abstract": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3000 images, and most of these networks achieved a sensitivity rate of 98% (\u00a0\u00b1\u00a0 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git.", "journal": "Medical image analysis", "date": "2020-08-12", "authors": ["ShervinMinaee", "RaheleKafieh", "MilanSonka", "ShakibYazdani", "GhazalehJamalipour Soufi"], "doi": "10.1016/j.media.2020.101794\n10.1148/radiol.2020200642\n10.1148/radiol.2020200527\n10.1148/ryct.2020200028"}
{"title": "Identification of COVID-19 samples from chest X-Ray images using deep learning: A comparison of transfer learning approaches.", "abstract": "The novel coronavirus disease 2019 (COVID-19) constitutes a public health emergency globally. The number of infected people and deaths are proliferating every day, which is putting tremendous pressure on our social and healthcare system. Rapid detection of COVID-19 cases is a significant step to fight against this virus as well as release pressure off the healthcare system.\nOne of the critical factors behind the rapid spread of COVID-19 pandemic is a lengthy clinical testing time. The imaging tool, such as Chest X-ray (CXR), can speed up the identification process. Therefore, our objective is to develop an automated CAD system for the detection of COVID-19 samples from healthy and pneumonia cases using CXR images.\nDue to the scarcity of the COVID-19 benchmark dataset, we have employed deep transfer learning techniques, where we examined 15 different pre-trained CNN models to find the most suitable one for this task.\nA total of 860 images (260 COVID-19 cases, 300 healthy and 300 pneumonia cases) have been employed to investigate the performance of the proposed algorithm, where 70% images of each class are accepted for training, 15% is used for validation, and rest is for testing. It is observed that the VGG19 obtains the highest classification accuracy of 89.3% with an average precision, recall, and F1 score of 0.90, 0.89, 0.90, respectively.\nThis study demonstrates the effectiveness of deep transfer learning techniques for the identification of COVID-19 cases using CXR images.", "journal": "Journal of X-ray science and technology", "date": "2020-08-11", "authors": ["Md MamunurRahaman", "ChenLi", "YudongYao", "FrankKulwa", "Mohammad AsadurRahman", "QianWang", "ShouliangQi", "FanjieKong", "XueminZhu", "XinZhao"], "doi": "10.3233/XST-200715"}
{"title": "Analysis of clinical features and imaging signs of COVID-19 with the assistance of artificial intelligence.", "abstract": "To explore the CT imaging features/signs of patients with different clinical types of Coronavirus Disease 2019 (COVID-19) via the application of artificial intelligence (AI), thus improving the understanding of COVID-19.\nClinical data and chest CT imaging features of 58 patients confirmed with COVID-19 in the Fifth Medical Center of PLA General Hospital were retrospectively analyzed. According to the Guidelines on Novel Coronavirus-Infected Pneumonia Diagnosis and Treatment (Provisional 6th Edition), COVID-19 patients were divided into mild type (7), common type (34), severe type (7) and critical type (10 patients). The CT imaging features of the patients with different clinical types of COVID-19 types were analyzed, and the volume percentage of pneumonia lesions with respect to the lung lobes (where the lesion was located) and to the whole lung was calculated with the use of AI software. SPSS 21.0 software was used for statistical analysis.\nCommon clinical manifestations of COVID-19 patients: fever was found in 47 patients (81.0%), cough in 31 (53.4%) and weakness in 10 (17.2%). Laboratory examinations: normal or decreased white blood cell (WBC) counts were observed in 52 patients (89.7%), decreased lymphocyte counts (LCs) in 14 (24.1%) and increased C-reactive protein (CRP) levels in 18 (31.0%). CT imaging features: there were 48 patients (94.1%) with lesions distributed in both lungs and 46 patients (90.2%) had lesions most visible in the lower lungs; the primary manifestations in patients with common type COVID-19 were ground-glass opacities (GGOs) (23/34, 67.6%) or mixed type (17/34, 50.0%), with lesions mainly distributed in the periphery of the lungs (28/34, 82.4%); the primary manifestations of patients with severe/critical type COVID-19 were consolidations (13/17, 76.5%) or mixed type (14/17, 82.4%), with lesions distributed in both the peripheral and central areas of lungs (14/17,82.4%); other common signs, including pleural parallel signs, halo signs, vascular thickening signs, crazy-paving signs and air bronchogram signs, were visible in patients with different clinical types, and pleural effusion was found in 5 patients with severe/critical COVID-19. AI software was used to calculate the volume percentages of pneumonia lesions with respect to the lung lobes (where the lesion was located) and to the whole lung. There were significant differences in the volume percentages of pneumonia lesions for the superior lobe of the left lung, the inferior lobe of the left lung, the superior lobe of the right lung, the inferior lobe of the right lung and the whole lung among patients with different clinical types (p<0.05). The area under the ROC curve (AUC) of the volume percentage of pneumonia lesions for the whole lung for the diagnosis of severe/critical type COVID-19 was 0.740, with sensitivity and specificity of 91.2% and 58.8%, respectively.\nThe clinical and CT imaging features of COVID-19 patients were characteristic to a certain degree; thus, the clinical course and severity of COVID-19 could be evaluated with a combination of an analysis of clinical features and CT imaging features and assistant diagnosis by AI software.", "journal": "European review for medical and pharmacological sciences", "date": "2020-08-09", "authors": ["H-WRen", "YWu", "J-HDong", "W-MAn", "TYan", "YLiu", "C-CLiu"], "doi": "10.26355/eurrev_202008_22510"}
{"title": "Deep Learning-Based Decision-Tree Classifier for COVID-19 Diagnosis From Chest X-ray Imaging.", "abstract": "The global pandemic of coronavirus disease 2019 (COVID-19) has resulted in an increased demand for testing, diagnosis, and treatment. Reverse transcription polymerase chain reaction (RT-PCR) is the definitive test for the diagnosis of COVID-19; however, chest X-ray radiography (CXR) is a fast, effective, and affordable test that identifies the possible COVID-19-related pneumonia. This study investigates the feasibility of using a deep learning-based decision-tree classifier for detecting COVID-19 from CXR images. The proposed classifier comprises three binary decision trees, each trained by a deep learning model with convolution neural network based on the PyTorch frame. The first decision tree classifies the CXR images as normal or abnormal. The second tree identifies the abnormal images that contain signs of tuberculosis, whereas the third does the same for COVID-19. The accuracies of the first and second decision trees are 98 and 80%, respectively, whereas the average accuracy of the third decision tree is 95%. The proposed deep learning-based decision-tree classifier may be used in pre-screening patients to conduct triage and fast-track decision making before RT-PCR results are available.", "journal": "Frontiers in medicine", "date": "2020-08-08", "authors": ["Seung HoonYoo", "HuiGeng", "Tin LokChiu", "Siu KiYu", "Dae ChulCho", "JinHeo", "Min SungChoi", "Il HyunChoi", "CongCung Van", "Nguen VietNhung", "Byung JunMin", "HoLee"], "doi": "10.3389/fmed.2020.00427\n10.1148/radiol.2020200642.\n10.1148/radiol.2020200432\n10.1148/radiol.2020200527.\n10.1148/ryct.2020200028\n10.1148/radiol.2462070712\n10.1016/j.crad.2020.03.003\n10.1148/radiol.2020200230\n10.1016/S0140-6736(20)30183-5\n10.1609/aaai.v33i01.3301590\n10.5588/ijtld.11.0004\n10.1038/srep25265\n10.1109/42.993132\n10.1007/978-3-642-15711-0_81\n10.1109/TBME.2010.2057509\n10.1038/s41598-019-51503-3\n10.1109/ISBI.2019.8759442\n10.1038/s41598-019-42294-8\n10.1109/CVPR.2017.369\n10.21037/jmai.2019.12.01\n10.1109/KSE.2018.8573404\n10.1148/rg.2017160032\n10.1109/CVPR.2016.90\n10.14316/pmp.2019.30.2.49\n10.1118/1.1312192\n10.1016/j.media.2012.06.009\n10.1148/radiol.11100153\n10.3978/j.issn.2223-4292.2013.04.03"}
{"title": "Imaging of COVID-19 pneumonia: Patterns, pathogenesis, and advances.", "abstract": "COVID-19 pneumonia is a newly recognized lung infection. Initially, CT imaging was demonstrated to be one of the most sensitive tests for the detection of infection. Currently, with broader availability of polymerase chain reaction for disease diagnosis, CT is mainly used for the identification of complications and other defined clinical indications in hospitalized patients. Nonetheless, radiologists are interpreting lung imaging in unsuspected patients as well as in suspected patients with imaging obtained to rule out other relevant clinical indications. The knowledge of pathological findings is also crucial for imagers to better interpret various imaging findings. Identification of the imaging findings that are commonly seen with the disease is important to diagnose and suggest confirmatory testing in unsuspected cases. Proper precautionary measures will be important in such unsuspected patients to prevent further spread. In addition to understanding the imaging findings for the diagnosis of the disease, it is important to understand the growing set of tools provided by artificial intelligence. The goal of this review is to highlight common imaging findings using illustrative examples, describe the evolution of disease over time, discuss differences in imaging appearance of adult and pediatric patients and review the available literature on quantitative CT for COVID-19. We briefly address the known pathological findings of the COVID-19 lung disease that may help better understand the imaging appearance, and we provide a demonstration of novel display methodologies and artificial intelligence applications serving to support clinical observations.", "journal": "The British journal of radiology", "date": "2020-08-08", "authors": ["PrashantNagpal", "SabarishNarayanasamy", "AditiVidholia", "JunfengGuo", "Kyung MinShin", "Chang HyunLee", "Eric AHoffman"], "doi": "10.1259/bjr.20200538\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.2648\n10.1148/radiol.2020200642\n10.1148/radiol.2020200823\n10.1148/radiol.2020200463\n10.1148/radiol.2020200843\n10.1093/eurheartj/ehaa254\n10.1183/13993003.00607-2020\n10.1038/s41392-020-0148-4\n10.1002/jmv.25770\n10.1016/j.jtho.2020.02.010\n10.1016/S2213-2600(20)30076-X\n10.1038/s41379-020-0536-x\n10.1093/ajcp/aqaa062\n10.1148/radiol.2020201160\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200274\n10.1016/j.clinimag.2020.04.001\n10.1148/radiol.2020200230\n10.1148/radiol.2433041835\n10.1148/radiol.2462070712\n10.1093/cid/cir122\n10.3348/kjr.2020.0132\n10.1097/RLI.0000000000000672\n10.1007/s00330-020-06731-x\n10.1148/radiol.2020200343\n10.1097/RLI.0000000000000670\n10.1161/CIRCULATIONAHA.120.047430\n10.1111/jth.14817\n10.1007/s11239-020-02129-0\n10.1001/jamapediatrics.2020.1346\n10.1016/S0140-6736(03)13364-8\n10.1002/path.1440\n10.1016/j.ajpath.2015.10.024\n10.2214/AJR.20.22969\n10.2214/AJR.15.14445\n10.1148/radiol.2282030593\n10.1067/j.cpradiol.2020.04.001\n10.1002/jmri.25010\n10.1164/rccm.201107-1317PP\n10.1016/j.jaci.2011.04.051\n10.1164/rccm.201803-0444PP\n10.1148/radiol.2020200905\n10.1164/rccm.201506-1208PP\n10.1016/j.jpha.2020.03.004\n10.1097/RTI.0b013e3182a21969\n10.1016/j.acra.2007.03.009\n10.1007/s00330-019-06402-6\n10.1164/ajrccm.160.2.9804094\n10.1164/ajrccm.156.1.9606093\n10.1016/j.acra.2006.04.017\n10.1109/TMI.2006.870889\n10.1164/rccm.201607-1385OC\n10.1371/journal.pone.0230548\n10.1007/s00330-020-06817-6"}
{"title": "CT Manifestations of Coronavirus Disease (COVID-19) Pneumonia and Influenza Virus Pneumonia: A Comparative Study.", "abstract": "", "journal": "AJR. American journal of roentgenology", "date": "2020-08-07", "authors": ["LiaoyiLin", "GangzeFu", "ShuangliChen", "JiejieTao", "AndanQian", "YunjunYang", "MeihaoWang"], "doi": "10.2214/AJR.20.23304"}
{"title": "Introducing the GEV Activation Function for Highly Unbalanced Data to Develop COVID-19 Diagnostic Models.", "abstract": "Fast and accurate diagnosis is essential for the efficient and effective control of the COVID-19 pandemic that is currently disrupting the whole world. Despite the prevalence of the COVID-19 outbreak, relatively few diagnostic images are openly available to develop automatic diagnosis algorithms. Traditional deep learning methods often struggle when data is highly unbalanced with many cases in one class and only a few cases in another; new methods must be developed to overcome this challenge. We propose a novel activation function based on the generalized extreme value (GEV) distribution from extreme value theory, which improves performance over the traditional sigmoid activation function when one class significantly outweighs the other. We demonstrate the proposed activation function on a publicly available dataset and externally validate on a dataset consisting of 1,909 healthy chest X-rays and 84 COVID-19 X-rays. The proposed method achieves an improved area under the receiver operating characteristic (DeLong's p-value < 0.05) compared to the sigmoid activation. Our method is also demonstrated on a dataset of healthy and pneumonia vs. COVID-19 X-rays and a set of computerized tomography images, achieving improved sensitivity. The proposed GEV activation function significantly improves upon the previously used sigmoid activation for binary classification. This new paradigm is expected to play a significant role in the fight against COVID-19 and other diseases, with relatively few training cases available.", "journal": "IEEE journal of biomedical and health informatics", "date": "2020-08-06", "authors": ["JoshuaBridge", "YandaMeng", "YitianZhao", "YongDu", "MingfengZhao", "RenrongSun", "YalinZheng"], "doi": "10.1109/JBHI.2020.3012383\n10.1109/RBME.2020.2987975"}
{"title": "Deep Bidirectional Classification Model for COVID-19 Disease Infected Patients.", "abstract": "In December of 2019, a novel coronavirus (COVID-19) appeared in Wuhan city, China and has been reported in many countries with millions of people infected within only four months. Chest computed Tomography (CT) has proven to be a useful supplement to reverse transcription polymerase chain reaction (RT-PCR) and has been shown to have high sensitivity to diagnose this condition. Therefore, radiological examinations are becoming crucial in early examination of COVID-19 infection. Currently, CT findings have already been suggested as an important evidence for scientific examination of COVID-19 in Hubei, China. However, classification of patient from chest CT images is not an easy task. Therefore, in this paper, a deep bidirectional long short-term memory network with mixture density network (DBM) model is proposed. To tune the hyperparameters of the DBM model, a Memetic Adaptive Differential Evolution (MADE) algorithm is used. Extensive experiments are drawn by considering the benchmark chest-Computed Tomography (chest-CT) images datasets. Comparative analysis reveals that the proposed MADE-DBM model outperforms the competitive COVID-19 classification approaches in terms of various performance metrics. Therefore, the proposed MADE-DBM model can be used in real-time COVID-19 classification systems.", "journal": "IEEE/ACM transactions on computational biology and bioinformatics", "date": "2020-08-06", "authors": ["YadunathPathak", "Piyush KumarShukla", "K VArya"], "doi": "10.1109/TCBB.2020.3009859"}
{"title": "Automatic Pleural Line Extraction and COVID-19 Scoring From Lung Ultrasound Data.", "abstract": "Recent works highlighted the significant potential of lung ultrasound (LUS) imaging in the management of subjects affected by COVID-19. In general, the development of objective, fast, and accurate automatic methods for LUS data evaluation is still at an early stage. This is particularly true for COVID-19 diagnostic. In this article, we propose an automatic and unsupervised method for the detection and localization of the pleural line in LUS data based on the hidden Markov model and Viterbi Algorithm. The pleural line localization step is followed by a supervised classification procedure based on the support vector machine (SVM). The classifier evaluates the healthiness level of a patient and, if present, the severity of the pathology, i.e., the score value for each image of a given LUS acquisition. The experiments performed on a variety of LUS data acquired in Italian hospitals with both linear and convex probes highlight the effectiveness of the proposed method. The average overall accuracy in detecting the pleura is 84% and 94% for convex and linear probes, respectively. The accuracy of the SVM classification in correctly evaluating the severity of COVID-19 related pleural line alterations is about 88% and 94% for convex and linear probes, respectively. The results as well as the visualization of the detected pleural line and the predicted score chart, provide a significant support to medical staff for further evaluating the patient condition.", "journal": "IEEE transactions on ultrasonics, ferroelectrics, and frequency control", "date": "2020-08-04", "authors": ["LeonardoCarrer", "ElenaDonini", "DanieleMarinelli", "MassimoZanetti", "FedericoMento", "ElenaTorri", "AndreaSmargiassi", "RiccardoInchingolo", "GinoSoldati", "LibertarioDemi", "FrancescaBovolo", "LorenzoBruzzone"], "doi": "10.1109/TUFFC.2020.3005512"}
{"title": "Role of 5G-powered remote robotic ultrasound during the COVID-19 outbreak: insights from two cases.", "abstract": "The 2019 Novel Coronavirus disease (COVID-19) broke out in Wuhan, China in December 2019 and spread throughout the world. Early screening and early diagnosis play key roles in prevention and management of the epidemic. Attention should also be paid to the infection of health workers and shortage of medical resources in high-risk areas. Here, we report two cases of patients diagnosed with COVID-19 and evaluated by robotic ultrasound based on 5G-powered technology 700 km east of Wuhan. We here show the advantages of this kind of remote ultrasound scan, which could become a method for the diagnosis and assessment of COVID-19.", "journal": "European review for medical and pharmacological sciences", "date": "2020-08-04", "authors": ["R-ZYu", "Y-QLi", "C-ZPeng", "R-ZYe", "QHe"], "doi": "10.26355/eurrev_202007_22283"}
{"title": "Interpretable artificial intelligence framework for COVID-19 screening on chest X-rays.", "abstract": "COVID-19 has led to an unprecedented healthcare crisis with millions of infected people across the globe often pushing infrastructures, healthcare workers and entire economies beyond their limits. The scarcity of testing kits, even in developed countries, has led to extensive research efforts towards alternative solutions with high sensitivity. Chest radiological imaging paired with artificial intelligence (AI) can offer significant advantages in diagnosis of novel coronavirus infected patients. To this end, transfer learning techniques are used for overcoming the limitations emanating from the lack of relevant big datasets, enabling specialized models to converge on limited data, as in the case of X-rays of COVID-19 patients. In this study, we present an interpretable AI framework assessed by expert radiologists on the basis on how well the attention maps focus on the diagnostically-relevant image regions. The proposed transfer learning methodology achieves an overall area under the curve of 1 for a binary classification problem across a 5-fold training/testing dataset.", "journal": "Experimental and therapeutic medicine", "date": "2020-08-04", "authors": ["NikosTsiknakis", "EleftheriosTrivizakis", "Evangelia EVassalou", "Georgios ZPapadakis", "Demetrios ASpandidos", "AristidisTsatsakis", "JoseS\u00e1nchez-Garc\u00eda", "RafaelL\u00f3pez-Gonz\u00e1lez", "NikolaosPapanikolaou", "Apostolos HKarantanas", "KostasMarias"], "doi": "10.3892/etm.2020.8797\n10.1016/S0140-6736(20)30211-7\n10.3892/mmr.2020.11127\n10.3892/ijmm.2020.4555\n10.3892/ijmm.2020.4596\n10.3892/ijmm.2020.4575\n10.1148/radiol.2020201160\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020201102\n10.1016/j.mehy.2020.109761\n10.1007/s13246-020-00865-4\n10.1101/2020.04.13.20063941\n10.1101/2020.02.14.20023028\n10.1016/j.cell.2018.02.010\n10.1167/iovs.16-19964"}
{"title": "COVIDiag: a clinical CAD system to diagnose COVID-19 pneumonia based on CT findings.", "abstract": "CT findings of COVID-19 look similar to other atypical and viral (non-COVID-19) pneumonia diseases. This study proposes a clinical computer-aided diagnosis (CAD) system using CT features to automatically discriminate COVID-19 from non-COVID-19 pneumonia patients.\nOverall, 612 patients (306 COVID-19 and 306 non-COVID-19 pneumonia) were recruited. Twenty radiological features were extracted from CT images to evaluate the pattern, location, and distribution of lesions of patients in both groups. All significant CT features were fed in five classifiers namely decision tree, K-nearest neighbor, na\u00efve Bayes, support vector machine, and ensemble to evaluate the best performing CAD system in classifying COVID-19 and non-COVID-19 cases.\nLocation and distribution pattern of involvement, number of the lesion, ground-glass opacity (GGO) and crazy-paving, consolidation, reticular, bronchial wall thickening, nodule, air bronchogram, cavity, pleural effusion, pleural thickening, and lymphadenopathy are the significant features to classify COVID-19 from non-COVID-19 groups. Our proposed CAD system obtained the sensitivity, specificity, and accuracy of 0.965, 93.54%, 90.32%, and 91.94%, respectively, using ensemble (COVIDiag) classifier.\nThis study proposed a COVIDiag model obtained promising results using CT radiological routine features. It can be considered an adjunct tool by the radiologists during the current COVID-19 pandemic to make an accurate diagnosis.\n\u2022 Location and distribution of involvement, number of lesions, GGO and crazy-paving, consolidation, reticular, bronchial wall thickening, nodule, air bronchogram, cavity, pleural effusion, pleural thickening, and lymphadenopathy are the significant features between COVID-19 from non-COVID-19 groups. \u2022 The proposed CAD system, COVIDiag, could diagnose COVID-19 pneumonia cases with an AUC of 0.965 (sensitivity\u2009=\u200993.54%; specificity\u2009=\u200990.32%; and accuracy\u2009=\u200991.94%). \u2022 The AUC, sensitivity, specificity, and accuracy obtained by radiologist diagnosis are 0.879, 87.10%, 88.71%, and 87.90%, respectively.", "journal": "European radiology", "date": "2020-08-03", "authors": ["AliAbbasian Ardakani", "U RajendraAcharya", "SinaHabibollahi", "AfshinMohammadi"], "doi": "10.1007/s00330-020-07087-y\n10.1002/jmv.25678\n10.1016/j.ijid.2020.01.009\n10.1016/S0140-6736(20)30183-5\n10.1016/S2213-2600(20)30079-5\n10.1148/radiol.2020200330\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200230\n10.1007/s00330-020-06748-2\n10.1007/s00330-006-0410-3\n10.1007/s00330-018-5528-6\n10.1007/s00330-012-2437-y\n10.1016/j.ejrad.2013.08.026\n10.1016/j.patrec.2019.11.013\n10.1016/j.cmpb.2019.06.023\n10.1097/RLI.0000000000000574\n10.1016/S2213-2600(18)30286-8\n10.1007/s00330-019-06533-w\n10.1016/j.cmpb.2018.04.001\n10.1016/j.compbiomed.2020.103675\n10.1016/j.compbiomed.2017.11.008\n10.1016/j.ejrad.2020.108961\n10.1016/S0720-048X(97)00157-5\n10.1007/s10462-009-9124-7\n10.1016/j.compbiomed.2020.103795\n10.1016/S0140-6736(20)30673-5\n10.1148/radiol.2020200463\n10.1016/j.jinf.2020.02.016"}
{"title": "Relational Modeling for Robust and Efficient Pulmonary Lobe Segmentation in CT Scans.", "abstract": "Pulmonary lobe segmentation in computed tomography scans is essential for regional assessment of pulmonary diseases. Recent works based on convolution neural networks have achieved good performance for this task. However, they are still limited in capturing structured relationships due to the nature of convolution. The shape of the pulmonary lobes affect each other and their borders relate to the appearance of other structures, such as vessels, airways, and the pleural wall. We argue that such structural relationships play a critical role in the accurate delineation of pulmonary lobes when the lungs are affected by diseases such as COVID-19 or COPD. In this paper, we propose a relational approach (RTSU-Net) that leverages structured relationships by introducing a novel non-local neural network module. The proposed module learns both visual and geometric relationships among all convolution features to produce self-attention weights. With a limited amount of training data available from COVID-19 subjects, we initially train and validate RTSU-Net on a cohort of 5000 subjects from the COPDGene study (4000 for training and 1000 for evaluation). Using models pre-trained on COPDGene, we apply transfer learning to retrain and evaluate RTSU-Net on 470 COVID-19 suspects (370 for retraining and 100 for evaluation). Experimental results show that RTSU-Net outperforms three baselines and performs robustly on cases with severe lung infection due to COVID-19.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["WeiyiXie", "ColinJacobs", "Jean-PaulCharbonnier", "Bramvan Ginneken"], "doi": "10.1109/TMI.2020.2995108\n10.1148/radiol.2020201473"}
{"title": "A Noise-Robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions From CT Images.", "abstract": "Segmentation of pneumonia lesions from CT scans of COVID-19 patients is important for accurate diagnosis and follow-up. Deep learning has a potential to automate this task but requires a large set of high-quality annotations that are difficult to collect. Learning from noisy training labels that are easier to obtain has a potential to alleviate this problem. To this end, we propose a novel noise-robust framework to learn from noisy labels for the segmentation task. We first introduce a noise-robust Dice loss that is a generalization of Dice loss for segmentation and Mean Absolute Error (MAE) loss for robustness against noise, then propose a novel COVID-19 Pneumonia Lesion segmentation network (COPLE-Net) to better deal with the lesions with various scales and appearances. The noise-robust Dice loss and COPLE-Net are combined with an adaptive self-ensembling framework for training, where an Exponential Moving Average (EMA) of a student model is used as a teacher model that is adaptively updated by suppressing the contribution of the student to EMA when the student has a large training loss. The student model is also adaptive by learning from the teacher only when the teacher outperforms the student. Experimental results showed that: (1) our noise-robust Dice loss outperforms existing noise-robust loss functions, (2) the proposed COPLE-Net achieves higher performance than state-of-the-art image segmentation networks, and (3) our framework with adaptive self-ensembling significantly outperforms a standard training process and surpasses other noise-robust training approaches in the scenario of learning from noisy labels for COVID-19 pneumonia lesion segmentation.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["GuotaiWang", "XinglongLiu", "ChaopingLi", "ZhiyongXu", "JiugenRuan", "HaifengZhu", "TaoMeng", "KangLi", "NingHuang", "ShaotingZhang"], "doi": "10.1109/TMI.2020.3000314"}
{"title": "A Rapid, Accurate and Machine-Agnostic Segmentation and Quantification Method for CT-Based COVID-19 Diagnosis.", "abstract": "COVID-19 has caused a global pandemic and become the most urgent threat to the entire world. Tremendous efforts and resources have been invested in developing diagnosis, prognosis and treatment strategies to combat the disease. Although nucleic acid detection has been mainly used as the gold standard to confirm this RNA virus-based disease, it has been shown that such a strategy has a high false negative rate, especially for patients in the early stage, and thus CT imaging has been applied as a major diagnostic modality in confirming positive COVID-19. Despite the various, urgent advances in developing artificial intelligence (AI)-based computer-aided systems for CT-based COVID-19 diagnosis, most of the existing methods can only perform classification, whereas the state-of-the-art segmentation method requires a high level of human intervention. In this paper, we propose a fully-automatic, rapid, accurate, and machine-agnostic method that can segment and quantify the infection regions on CT scans from different sources. Our method is founded upon two innovations: 1) the first CT scan simulator for COVID-19, by fitting the dynamic change of real patients' data measured at different time points, which greatly alleviates the data scarcity issue; and 2) a novel deep learning algorithm to solve the large-scene-small-object problem, which decomposes the 3D segmentation problem into three 2D ones, and thus reduces the model complexity by an order of magnitude and, at the same time, significantly improves the segmentation accuracy. Comprehensive experimental results over multi-country, multi-hospital, and multi-machine datasets demonstrate the superior performance of our method over the existing ones and suggest its important application value in combating the disease.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["LongxiZhou", "ZhongxiaoLi", "JuexiaoZhou", "HaoyangLi", "YupengChen", "YuxinHuang", "DexuanXie", "LintaoZhao", "MingFan", "ShahrukhHashmi", "FaisalAbdelkareem", "RihamEiada", "XigangXiao", "LihuaLi", "ZhaowenQiu", "XinGao"], "doi": "10.1109/TMI.2020.3001810\n10.1148/radiol.2020200642\n10.1183/09031936.01.00213501\n10.1371/journal.pmed.1002707\n10.1016/j.cell.2018.02.010\n10.1016/j.diii.2012.04.001\n10.1097/MCP.0000000000000567\n10.1097/rli.0000000000000574\n10.1016/S2213-2600(18)30286-8\n10.1109/TMI.2016.2535865\n10.1016/j.patrec.2019.11.013\n10.1148/rg.2018170048\n10.1148/radiol.2020200274\n10.1148/radiol.2020200823\n10.1101/2020.02.14.20023028\n10.1101/2020.02.23.20026930\n10.1109/TCBB.2019.2939522\n10.1038/nature24270\n10.1093/bioinformatics/bty241\n10.1093/bioinformatics/bty223.v\n10.1093/bioinformatics/btz963\n10.1016/j.cell.2020.04.045"}
{"title": "Inf-Net: Automatic COVID-19 Lung Infection Segmentation From CT Images.", "abstract": "Coronavirus Disease 2019 (COVID-19) spread globally in early 2020, causing the world to face an existential health crisis. Automated detection of lung infections from computed tomography (CT) images offers a great potential to augment the traditional healthcare strategy for tackling COVID-19. However, segmenting infected regions from CT slices faces several challenges, including high variation in infection characteristics, and low intensity contrast between infections and normal tissues. Further, collecting a large amount of data is impractical within a short time period, inhibiting the training of a deep model. To address these challenges, a novel COVID-19 Lung Infection Segmentation Deep Network (Inf-Net) is proposed to automatically identify infected regions from chest CT slices. In our Inf-Net, a parallel partial decoder is used to aggregate the high-level features and generate a global map. Then, the implicit reverse attention and explicit edge-attention are utilized to model the boundaries and enhance the representations. Moreover, to alleviate the shortage of labeled data, we present a semi-supervised segmentation framework based on a randomly selected propagation strategy, which only requires a few labeled images and leverages primarily unlabeled data. Our semi-supervised framework can improve the learning ability and achieve a higher performance. Extensive experiments on our COVID-SemiSeg and real CT volumes demonstrate that the proposed Inf-Net outperforms most cutting-edge segmentation models and advances the state-of-the-art performance.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["Deng-PingFan", "TaoZhou", "Ge-PengJi", "YiZhou", "GengChen", "HuazhuFu", "JianbingShen", "LingShao"], "doi": "10.1109/TMI.2020.2996645"}
{"title": "Dual-Sampling Attention Network for Diagnosis of COVID-19 From Community Acquired Pneumonia.", "abstract": "The coronavirus disease (COVID-19) is rapidly spreading all over the world, and has infected more than 1,436,000 people in more than 200 countries and territories as of April 9, 2020. Detecting COVID-19 at early stage is essential to deliver proper healthcare to the patients and also to protect the uninfected population. To this end, we develop a dual-sampling attention network to automatically diagnose COVID-19 from the community acquired pneumonia (CAP) in chest computed tomography (CT). In particular, we propose a novel online attention module with a 3D convolutional network (CNN) to focus on the infection regions in lungs when making decisions of diagnoses. Note that there exists imbalanced distribution of the sizes of the infection regions between COVID-19 and CAP, partially due to fast progress of COVID-19 after symptom onset. Therefore, we develop a dual-sampling strategy to mitigate the imbalanced learning. Our method is evaluated (to our best knowledge) upon the largest multi-center CT data for COVID-19 from 8 hospitals. In the training-validation stage, we collect 2186 CT scans from 1588 patients for a 5-fold cross-validation. In the testing stage, we employ another independent large-scale testing dataset including 2796 CT scans from 2057 patients. Results show that our algorithm can identify the COVID-19 images with the area under the receiver operating characteristic curve (AUC) value of 0.944, accuracy of 87.5%, sensitivity of 86.9%, specificity of 90.1%, and F1-score of 82.0%. With this performance, the proposed algorithm could potentially aid radiologists with COVID-19 diagnosis from CAP, especially in the early stage of the COVID-19 outbreak.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["XiOuyang", "JiayuHuo", "LimingXia", "FeiShan", "JunLiu", "ZhanhaoMo", "FuhuaYan", "ZhongxiangDing", "QiYang", "BinSong", "FengShi", "HuanYuan", "YingWei", "XiaohuanCao", "YaozongGao", "DijiaWu", "QianWang", "DinggangShen"], "doi": "10.1109/TMI.2020.2995508"}
{"title": "Accurate Screening of COVID-19 Using Attention-Based Deep 3D Multiple Instance Learning.", "abstract": "Automated Screening of COVID-19 from chest CT is of emergency and importance during the outbreak of SARS-CoV-2 worldwide in 2020. However, accurate screening of COVID-19 is still a massive challenge due to the spatial complexity of 3D volumes, the labeling difficulty of infection areas, and the slight discrepancy between COVID-19 and other viral pneumonia in chest CT. While a few pioneering works have made significant progress, they are either demanding manual annotations of infection areas or lack of interpretability. In this paper, we report our attempt towards achieving highly accurate and interpretable screening of COVID-19 from chest CT with weak labels. We propose an attention-based deep 3D multiple instance learning (AD3D-MIL) where a patient-level label is assigned to a 3D chest CT that is viewed as a bag of instances. AD3D-MIL can semantically generate deep 3D instances following the possible infection area. AD3D-MIL further applies an attention-based pooling approach to 3D instances to provide insight into each instance's contribution to the bag label. AD3D-MIL finally learns Bernoulli distributions of the bag-level labels for more accessible learning. We collected 460 chest CT examples: 230 CT examples from 79 patients with COVID-19, 100 CT examples from 100 patients with common pneumonia, and 130 CT examples from 130 people without pneumonia. A series of empirical studies show that our algorithm achieves an overall accuracy of 97.9%, AUC of 99.0%, and Cohen kappa score of 95.7%. These advantages endow our algorithm as an efficient assisted tool in the screening of COVID-19.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["ZhongyiHan", "BenzhengWei", "YanfeiHong", "TianyangLi", "JinyuCong", "XueZhu", "HaifengWei", "WeiZhang"], "doi": "10.1109/TMI.2020.2996256"}
{"title": "Prior-Attention Residual Learning for More Discriminative COVID-19 Screening in CT Images.", "abstract": "We propose a conceptually simple framework for fast COVID-19 screening in 3D chest CT images. The framework can efficiently predict whether or not a CT scan contains pneumonia while simultaneously identifying pneumonia types between COVID-19 and Interstitial Lung Disease (ILD) caused by other viruses. In the proposed method, two 3D-ResNets are coupled together into a single model for the two above-mentioned tasks via a novel prior-attention strategy. We extend residual learning with the proposed prior-attention mechanism and design a new so-called prior-attention residual learning (PARL) block. The model can be easily built by stacking the PARL blocks and trained end-to-end using multi-task losses. More specifically, one 3D-ResNet branch is trained as a binary classifier using lung images with and without pneumonia so that it can highlight the lesion areas within the lungs. Simultaneously, inside the PARL blocks, prior-attention maps are generated from this branch and used to guide another branch to learn more discriminative representations for the pneumonia-type classification. Experimental results demonstrate that the proposed framework can significantly improve the performance of COVID-19 screening. Compared to other methods, it achieves a state-of-the-art result. Moreover, the proposed method can be easily extended to other similar clinical applications such as computer-aided detection and diagnosis of pulmonary nodules in CT images, glaucoma lesions in Retina fundus images, etc.", "journal": "IEEE transactions on medical imaging", "date": "2020-07-31", "authors": ["JunWang", "YimingBao", "YaofengWen", "HongbingLu", "HuLuo", "YunfeiXiang", "XiaomingLi", "ChenLiu", "DahongQian"], "doi": "10.1109/TMI.2020.2994908"}
{"title": "Automated Assessment of COVID-19 Reporting and Data System and Chest CT Severity Scores in Patients Suspected of Having COVID-19 Using Artificial Intelligence.", "abstract": "Background The coronavirus disease 2019 (COVID-19) pandemic has spread across the globe with alarming speed, morbidity, and mortality. Immediate triage of patients with chest infections suspected to be caused by COVID-19 using chest CT may be of assistance when results from definitive viral testing are delayed. Purpose To develop and validate an artificial intelligence (AI) system to score the likelihood and extent of pulmonary COVID-19 on chest CT scans using the COVID-19 Reporting and Data System (CO-RADS) and CT severity scoring systems. Materials and Methods The CO-RADS AI system consists of three deep-learning algorithms that automatically segment the five pulmonary lobes, assign a CO-RADS score for the suspicion of COVID-19, and assign a CT severity score for the degree of parenchymal involvement per lobe. This study retrospectively included patients who underwent a nonenhanced chest CT examination because of clinical suspicion of COVID-19 at two medical centers. The system was trained, validated, and tested with data from one of the centers. Data from the second center served as an external test set. Diagnostic performance and agreement with scores assigned by eight independent observers were measured using receiver operating characteristic analysis, linearly weighted \u03ba values, and classification accuracy. Results A total of 105 patients (mean age, 62 years \u00b1 16 [standard deviation]; 61 men) and 262 patients (mean age, 64 years \u00b1 16; 154 men) were evaluated in the internal and external test sets, respectively. The system discriminated between patients with COVID-19 and those without COVID-19, with areas under the receiver operating characteristic curve of 0.95 (95% CI: 0.91, 0.98) and 0.88 (95% CI: 0.84, 0.93), for the internal and external test sets, respectively. Agreement with the eight human observers was moderate to substantial, with mean linearly weighted \u03ba values of 0.60 \u00b1 0.01 for CO-RADS scores and 0.54 \u00b1 0.01 for CT severity scores. Conclusion With high diagnostic performance, the CO-RADS AI system correctly identified patients with COVID-19 using chest CT scans and assigned standardized CO-RADS and CT severity scores that demonstrated good agreement with findings from eight independent observers and generalized well to external data. \u00a9 RSNA, 2020 ", "journal": "Radiology", "date": "2020-07-31", "authors": ["NikolasLessmann", "Clara IS\u00e1nchez", "LudoBeenen", "Luuk HBoulogne", "MoniqueBrink", "ErdiCalli", "Jean-PaulCharbonnier", "TonDofferhoff", "Wouter Mvan Everdingen", "Paul KGerke", "BramGeurts", "Hester AGietema", "MiriamGroeneveld", "Louisvan Harten", "NilsHendrix", "WardHendrix", "Henkjan JHuisman", "IvanaI\u0161gum", "ColinJacobs", "RubenKluge", "MichelKok", "JasenkoKrdzalic", "BiancaLassen-Schmidt", "Kickyvan Leeuwen", "JamesMeakin", "MikeOverkamp", "Tjalcovan Rees Vellinga", "Eva Mvan Rikxoort", "RiccardoSamperna", "CorneliaSchaefer-Prokop", "StevenSchalekamp", "Ernst ThScholten", "CherylSital", "J LauranSt\u00f6ger", "JonasTeuwen", "Kiran VaidhyaVenkadesh", "Coende Vente", "MariekeVermaat", "WeiyiXie", "Bramde Wilde", "MathiasProkop", "Bramvan Ginneken"], "doi": "10.1148/radiol.2020202439"}
{"title": "Implementation of a Deep Learning-Based Computer-Aided Detection System for the Interpretation of Chest Radiographs in Patients Suspected for COVID-19.", "abstract": "To describe the experience of implementing a deep learning-based computer-aided detection (CAD) system for the interpretation of chest X-ray radiographs (CXR) of suspected coronavirus disease (COVID-19) patients and investigate the diagnostic performance of CXR interpretation with CAD assistance.\nIn this single-center retrospective study, initial CXR of patients with suspected or confirmed COVID-19 were investigated. A commercialized deep learning-based CAD system that can identify various abnormalities on CXR was implemented for the interpretation of CXR in daily practice. The diagnostic performance of radiologists with CAD assistance were evaluated based on two different reference standards: 1) real-time reverse transcriptase-polymerase chain reaction (rRT-PCR) results for COVID-19 and 2) pulmonary abnormality suggesting pneumonia on chest CT. The turnaround times (TATs) of radiology reports for CXR and rRT-PCR results were also evaluated.\nAmong 332 patients (male:female, 173:159; mean age, 57 years) with available rRT-PCR results, 16 patients (4.8%) were diagnosed with COVID-19. Using CXR, radiologists with CAD assistance identified rRT-PCR positive COVID-19 patients with sensitivity and specificity of 68.8% and 66.7%, respectively. Among 119 patients (male:female, 75:44; mean age, 69 years) with available chest CTs, radiologists assisted by CAD reported pneumonia on CXR with a sensitivity of 81.5% and a specificity of 72.3%. The TATs of CXR reports were significantly shorter than those of rRT-PCR results (median 51 vs. 507 minutes; \nRadiologists with CAD assistance could identify patients with rRT-PCR-positive COVID-19 or pneumonia on CXR with a reasonably acceptable performance. In patients suspected with COVID-19, CXR had much faster TATs than rRT-PCRs.", "journal": "Korean journal of radiology", "date": "2020-07-31", "authors": ["Eui JinHwang", "HyungjinKim", "Soon HoYoon", "Jin MoGoo", "Chang MinPark"], "doi": "10.3348/kjr.2020.0536\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343\n10.1148/radiol.2020201160"}
{"title": "Application of a Robotic Tele-Echography System for COVID-19 Pneumonia.", "abstract": "To date, coronavirus disease 2019 (COVID-19) has infected millions of people worldwide. Ultrasound plays an indispensable role in the diagnosis, monitoring, and follow-up of patients with COVID-19. In this study, we used a robotic tele-echography system based on a 5G communication network for remote diagnosis. The system has great potential for lung, heart, and vasculature information, medical staff protection, and resource sharing, can be a valuable tool for treating patients during the pandemic, and can be expected to expand to more specialized fields.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2020-07-30", "authors": ["JingWang", "ChengzhongPeng", "YanZhao", "RuizhongYe", "JunHong", "HaijunHuang", "LegaoChen"], "doi": "10.1002/jum.15406"}
{"title": "Deep transfer\u00a0learning artificial intelligence accurately stages COVID-19 lung disease severity on portable chest radiographs.", "abstract": "This study employed deep-learning convolutional neural networks to stage lung disease severity of Coronavirus Disease 2019 (COVID-19) infection on portable chest x-ray (CXR) with radiologist score of disease severity as ground truth. This study consisted of 131 portable CXR from 84 COVID-19 patients (51M 55.1\u00b114.9yo; 29F 60.1\u00b114.3yo; 4 missing information). Three expert chest radiologists scored the left and right lung separately based on the degree of opacity (0-3) and geographic extent (0-4). Deep-learning convolutional neural network (CNN) was used to predict lung disease severity scores. Data were split into 80% training and 20% testing datasets. Correlation analysis between AI-predicted versus radiologist scores were analyzed. Comparison was made with traditional and transfer learning. The average opacity score was 2.52 (range: 0-6) with a standard deviation of 0.25 (9.9%) across three readers. The average geographic extent score was 3.42 (range: 0-8) with a standard deviation of 0.57 (16.7%) across three readers. The inter-rater agreement yielded a Fleiss' Kappa of 0.45 for opacity score and 0.71 for extent score. AI-predicted scores strongly correlated with radiologist scores, with the top model yielding a correlation coefficient (R2) of 0.90 (range: 0.73-0.90 for traditional learning and 0.83-0.90 for transfer learning) and a mean absolute error of 8.5% (ranges: 17.2-21.0% and 8.5%-15.5, respectively). Transfer learning generally performed better. In conclusion, deep-learning CNN accurately stages disease severity on portable chest x-ray of COVID-19 lung infection. This approach may prove useful to stage lung disease severity, prognosticate, and predict treatment response and survival, thereby informing risk management and resource allocation.", "journal": "PloS one", "date": "2020-07-30", "authors": ["JocelynZhu", "BeiyiShen", "AlmasAbbasi", "MahsaHoshmand-Kochi", "HaifangLi", "Tim QDuong"], "doi": "10.1371/journal.pone.0236621\n10.1016/j.ijid.2020.01.009\n10.1002/jmv.25678\n10.1056/NEJMoa2001316\n10.1186/s12880-015-0103-y\n10.1148/radiol.2020201160\n10.2214/AJR.20.23034\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020200843\n10.1148/radiol.2020200230\n10.1038/nature14539\n10.1038/s41379-018-0073-z\n10.1088/1361-6560/aa93d4\n10.1007/s11548-018-1843-2\n10.1371/journal.pone.0221339\n10.1016/j.imu.2020.100360\n10.1148/radiol.2020201178\n10.1148/radiol.2020200905\n10.1136/thoraxjnl-2017-211280"}
{"title": "Lung involvement in macrophage activation syndrome and severe COVID-19: results from a cross-sectional study to assess clinical, laboratory and artificial intelligence-radiological differences.", "abstract": "To evaluate the clinical pictures, laboratory tests and imaging of patients with lung involvement, either from severe COVID-19 or macrophage activation syndrome (MAS), in order to assess how similar these two diseases are.\nThe present work has been designed as a cross-sectional single-centre study to compare characteristics of patients with lung involvement either from MAS or severe COVID-19. Chest CT scans were assessed by using an artificial intelligence (AI)-based software.\nTen patients with MAS and 47 patients with severe COVID-19 with lung involvement were assessed. Although all patients showed fever and dyspnoea, patients with MAS were characterised by thrombocytopaenia, whereas patients with severe COVID-19 were characterised by lymphopaenia and neutrophilia. Higher values of H-score characterised patients with MAS when compared with severe COVID-19. AI-reconstructed images of chest CT scan showed that apical, basal, peripheral and bilateral distributions of ground-glass opacities (GGOs), as well as apical consolidations, were more represented in severe COVID-19 than in MAS. C reactive protein directly correlated with GGOs extension in both diseases. Furthermore, lymphopaenia inversely correlated with GGOs extension in severe COVID-19.\nOur data could suggest laboratory and radiological differences between MAS and severe COVID-19, paving the way for further hypotheses to be investigated in future confirmatory studies.", "journal": "Annals of the rheumatic diseases", "date": "2020-07-29", "authors": ["PieroRuscitti", "FedericoBruno", "OnorinaBerardicurti", "ChiaraAcanfora", "ViktoriyaPavlych", "PierpaoloPalumbo", "AlessandroConforti", "FrancescoCarubbi", "IleniaDi Cola", "PaolaDi Benedetto", "PaolaCipriani", "DavideGrassi", "CarloMasciocchi", "AnnamariaIagnocco", "AntonioBarile", "RobertoGiacomelli"], "doi": "10.1136/annrheumdis-2020-218048\n10.1016/j.autrev.2020.102537\n10.1016/j.chest.2015.11.004\n10.1136/annrheumdis-2019-216040\n10.1016/S0140-6736(13)61048-X\n10.3899/jrheum.170955\n10.1002/art.41073\n10.1186/s13075-020-02245-5\n10.1007/s11547-020-01197-9\n10.1038/s41568-018-0016-5\n10.3899/jrheum.200334\n10.1016/j.autrev.2020.102562\n10.1002/art.38690\n10.1002/art.1780300209\n10.1148/radiol.2462070712\n10.1016/S0140-6736(05)66379-9\n10.1038/s41392-020-0148-4\n10.3389/fimmu.2020.01130\n10.1016/S2213-2600(20)30243-5\n10.1016/S2665-9913(20)30121-1"}
{"title": "Deploying Machine and Deep Learning Models for Efficient Data-Augmented Detection of COVID-19 Infections.", "abstract": "This generation faces existential threats because of the global assault of the novel Corona virus 2019 (i.e., COVID-19). With more than thirteen million infected and nearly 600000 fatalities in 188 countries/regions, COVID-19 is the worst calamity since the World War II. These misfortunes are traced to various reasons, including late detection of latent or asymptomatic carriers, migration, and inadequate isolation of infected people. This makes detection, containment, and mitigation global priorities to contain exposure via quarantine, lockdowns, work/stay at home, and social distancing that are focused on \"flattening the curve\". While medical and healthcare givers are at the frontline in the battle against COVID-19, it is a crusade for all of humanity. Meanwhile, machine and deep learning models have been revolutionary across numerous domains and applications whose potency have been exploited to birth numerous state-of-the-art technologies utilised in disease detection, diagnoses, and treatment. Despite these potentials, machine and, particularly, deep learning models are data sensitive, because their effectiveness depends on availability and reliability of data. The unavailability of such data hinders efforts of engineers and computer scientists to fully contribute to the ongoing assault against COVID-19. Faced with a calamity on one side and absence of reliable data on the other, this study presents two data-augmentation models to enhance learnability of the Convolutional Neural Network (CNN) and the Convolutional Long Short-Term Memory (ConvLSTM)-based deep learning models (DADLMs) and, by doing so, boost the accuracy of COVID-19 detection. Experimental results reveal improvement in terms of accuracy of detection, logarithmic loss, and testing time relative to DLMs devoid of such data augmentation. Furthermore, average increases of 4% to 11% in COVID-19 detection accuracy are reported in favour of the proposed data-augmented deep learning models relative to the machine learning techniques. Therefore, the proposed algorithm is effective in performing a rapid and consistent Corona virus diagnosis that is primarily aimed at assisting clinicians in making accurate identification of the virus.", "journal": "Viruses", "date": "2020-07-28", "authors": ["AhmedSedik", "Abdullah MIliyasu", "BasmaAbd El-Rahiem", "Mohammed EAbdel Samea", "AsmaaAbdel-Raheem", "MohamedHammad", "JialiangPeng", "Fathi EAbd El-Samie", "Ahmed AAbd El-Latif"], "doi": "10.3390/v12070769\n10.1016/S0140-6736(20)30360-3\n10.1016/S0140-6736(20)30566-3\n10.1016/j.jinf.2020.03.037\n10.1016/j.jcv.2003.09.011\n10.1111/tbed.12401\n10.1016/S0140-6736(20)30183-5\n10.1186/s40779-020-00240-0\n10.1016/j.joen.2020.03.008\n10.1007/s40121-020-00295-5\n10.1001/jama.2020.4756\n10.1016/S1473-3099(14)70846-1\n10.1016/j.measurement.2018.05.033\n10.1016/j.apacoust.2020.107279\n10.21608/mjeer.2019.76962\n10.21608/mjeer.2019.76998\n10.1007/s11042-020-08769-x\n10.1007/s00521-018-3616-9\n10.1016/j.eswa.2019.01.080\n10.1016/j.knosys.2019.105460\n10.1016/j.bspc.2019.101683\n10.1371/journal.pone.0214365\n10.1016/j.knosys.2019.105153\n10.1109/ACCESS.2020.2994762\n10.3233/JIFS-191438\n10.1162/neco.1997.9.8.1735\n10.1364/OL.20.000767\n10.1109/TIT.2013.2288257\n10.1148/radiology.143.1.7063747\n10.1016/j.bspc.2013.09.001\n10.1080/23311916.2019.1599537\n10.1016/j.cell.2018.02.010\n10.1155/2019/4180949\n10.1016/j.cmpb.2019.06.023\n10.3390/sym12040651"}
{"title": "Quantitative lung lesion features and temporal changes on chest CT in patients with common and severe SARS-CoV-2 pneumonia.", "abstract": "The purpose of this study was to describe the temporal evolution of quantitative lung lesion features on chest computed tomography (CT) in patients with common and severe types of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pneumonia. Records of patients diagnosed with SARS-CoV-2 pneumonia were reviewed retrospectively from 24 January 2020 to 15 March 2020. Patients were classified into common and severe groups according to the diagnostic criteria of severe pneumonia. The quantitative CT features of lung lesions were automatically calculated using artificial intelligence algorithms, and the percentages of ground-glass opacity volume (PGV), consolidation volume (PCV) and total lesion volume (PTV) were determined in both lungs. PGV, PCV and PTV were analyzed based on the time from the onset of initial symptoms in the common and severe groups. In the common group, PTV increased slowly and peaked at approximately 12 days from the onset of the initial symptoms. In the severe group, PTV peaked at approximately 17 days. The severe pneumonia group exhibited increased PGV, PCV and PTV compared with the common group. These features started to appear in Stage 2 (4-7 days from onset of initial symptoms) and were observed in all subsequent stages (p<0.05). In severe SARS-CoV-2 pneumonia patients, PGV, PCV and PTV began to significantly increase in Stage 2 and decrease in Stage 5 (22-30 days). Compared with common SARS-CoV-2 pneumonia patients, the patients in the severe group exhibited increased PGV, PCV and PTV as well as a later peak time of lesion and recovery time.", "journal": "PloS one", "date": "2020-07-25", "authors": ["YueZhang", "YingLiu", "HonghanGong", "LinWu"], "doi": "10.1371/journal.pone.0236858\n10.1093/jamia/ocaa037\n10.23736/S0031-0808.20.03897&%23x2013;5\n10.1016/j.jiph.2020.02.033\n10.1017/ice.2020.86\n10.3346/jkms.2020.35.e112\n10.1016/S0140-6736(20)30211-7\n10.1001/jama.2020.1585\n10.1148/radiol.2020200642:200642\n10.1148/radiol.2020200432:200432\n10.1148/radiol.2020200370:200370\n10.1148/radiol.2020200490:200490\n10.1148/radiol.2020200323\n10.1148/radiol.2020200843:200843\n10.1148/radiol.2020200241\n10.1148/radiol.2020200274\n10.1148/radiol.2020200905:200905\n10.1097/RLI.0000000000000127\n10.1016/j.ejrad.2020.108941"}
{"title": "Diagnostic and Interventional Radiology Case Volume and Education in the Age of Pandemics: Impact Analysis and Potential Future Directions.", "abstract": "To assess the immediate impact of the COVID-19 pandemic on Diagnostic and Interventional Radiology education, and to propose measures to preserve and augment trainee education during future crises.\nDiagnostic Radiology (DR) studies and Interventional Radiology (IR) procedures at a single tertiary-care teaching institution between 2015 and 2020 were reviewed. DR was divided by section: body, cardiothoracic, musculoskeletal (MSK), neuroradiology, nuclear medicine, pediatrics, and women's imaging. IR was divided by procedural types: arterial, venous, lymphatic, core, neuro, pediatrics, dialysis, cancer embolization or ablation, noncancer embolization, portal hypertension, and miscellaneous. Impact on didactic education was also assessed. ANOVA, t test, and multiple comparison correction were used for analysis.\nDR and IR caseloads decreased significantly in April 2020 compared to April of the prior 5 years (both p < 0.0001). Case volumes were reduced in body (49.2%, p < 0.01), MSK (54.2%, p < 0.05), neuro (39.3%, p < 0.05), and women's imaging (75.5%, p < 0.05) in DR, and in arterial (62.6%, p < 0.01), neuro IR (57.6%, p < 0.01) and core IR (42.6%, p < 0.05) in IR. IR trainee average caseload in April 2020 decreased 51.9% compared to April of the prior 5 years (p < 0.01). Utilization of online learning increased in April. Trainees saw significant increases in overall DR didactics (31.3%, p\u202f=\u202f0.02) and no reduction in IR didactics, all online. Twelve major national and international DR and IR meetings were canceled or postponed between March and July.\nDecreases in caseload and widespread cancellation of conferences have had significant impact on DR/IR training during COVID-19 restrictions. Remote learning technologies with annotated case recording, boards-style case reviews, procedural simulation and narrated live cases as well as online lectures and virtual journal clubs increased during this time. Whether remote learning can mitigate lost opportunities from in-person interactions remains uncertain. Optimizing these strategies will be important for potential future restricted learning paradigms and can also be extrapolated to augment trainee education during unrestricted times.", "journal": "Academic radiology", "date": "2020-07-25", "authors": ["Ahmed MGabr", "NingchengLi", "Ryan CSchenning", "AlyElbarbary", "James CAnderson", "John AKaufman", "KhashayarFarsad"], "doi": "10.1016/j.acra.2020.07.014\n10.31616/asj.2020.0197\n10.1016/j.arth.2020.04.052\n10.1148/radiol.2020201495\n10.1148/rycan.2020204011\n10.1148/radiol.2020200988\n10.1148/radiol.2020201326\n10.1007/s00247-016-3675-y\n10.1016/j.jacr.2018.06.014\n10.1016/j.crad.2016.10.014\n10.1055/s-0042-109562\n10.7326/0003-4819-107-5-775\n10.1001/jama.288.9.1112\n10.1056/NEJMsr1200117"}
{"title": "Current Landscape of Imaging and the Potential Role for Artificial Intelligence in the Management of COVID-19.", "abstract": "The clinical management of COVID-19 is challenging. Medical imaging plays a critical role in the early detection, clinical monitoring and outcomes assessment of this disease. Chest x-ray radiography and computed tomography) are the standard imaging modalities used for the structural assessment of the disease status, while functional imaging (namely, positron emission tomography) has had limited application. Artificial intelligence can enhance the predictive power and utilization of these imaging approaches and new approaches focusing on detection, stratification and prognostication are showing encouraging results. We review the current landscape of these imaging modalities and artificial intelligence approaches as applied in COVID-19 management.", "journal": "Current problems in diagnostic radiology", "date": "2020-07-25", "authors": ["FaiqShaikh", "Michael BrunAndersen", "M RizwanSohail", "FranciscaMulero", "OmerAwan", "DianaDupont-Roettger", "OlgaKubassova", "JamshidDehmeshki", "SotiriosBisdas"], "doi": "10.1067/j.cpradiol.2020.06.009\n10.1001/jama.2020.2565\n10.1053/j.gastro.2020.04.008\n10.1016/1473-3099(20)30232-2\n10.1016/S2213-2600(20)30127-2\n10.1093/nsr/nwaa036\n10.1016/j.dsx.2020.04.020\n10.1172/JCI137647\n10.1148/ryct.2020200034\n10.1148/radiol.2020201160\n10.1148/radiol.2020200463\n10.1016/j.thromres.2020.04.011\n10.1148/radiol.2020201544\n10.1016/S1473-3099(20)30367-4\n10.1016/j.crad.2015.03.010"}
{"title": "Drawing insights from COVID-19-infected patients using CT scan images and machine learning techniques: a study on 200 patients.", "abstract": "As the whole world is witnessing what novel coronavirus (COVID-19) can do to the mankind, it presents several unique features also. In the absence of specific vaccine for COVID-19, it is essential to detect the disease at an early stage and isolate an infected patient. Till today there is a global shortage of testing labs and testing kits for COVID-19. This paper discusses about the role of machine learning techniques for getting important insights like whether lung computed tomography (CT) scan should be the first screening/alternative test for real-time reverse transcriptase-polymerase chain reaction (RT-PCR), is COVID-19 pneumonia different from other viral pneumonia and if yes how to distinguish it using lung CT scan images from the carefully selected data of lung CT scan COVID-19-infected patients from the hospitals of Italy, China, Moscow and India? For training and testing the proposed system, custom vision software of Microsoft azure based on machine learning techniques is used. An overall accuracy of almost 91% is achieved for COVID-19 classification using the proposed methodology.", "journal": "Environmental science and pollution research international", "date": "2020-07-24", "authors": ["SachinSharma"], "doi": "10.1007/s11356-020-10133-3\n10.1093/bjaceaccp/mkn041"}
{"title": "Artificial intelligence in ophthalmology during COVID-19 and in the post COVID-19 era.", "abstract": "To highlight artificial intelligence applications in ophthalmology during the COVID-19 pandemic that can be used to: describe ocular findings and changes correlated with COVID-19; extract information from scholarly articles on SARS-CoV-2 and COVID-19 specific to ophthalmology; and implement efficient patient triage and telemedicine care.\nOphthalmology has been leading in artificial intelligence and technology applications. With medical imaging analysis, pixel-annotated distinguishable features on COVID-19 patients may help with noninvasive diagnosis and severity outcome predictions. Using natural language processing (NLP) and data integration methods, topic modeling on more than 200 ophthalmology-related articles on COVID-19 can summarize ocular manifestations, viral transmission, treatment strategies, and patient care and practice management. Artificial intelligence for telemedicine applications can address the high demand, prioritize and triage patients, as well as improve at home-monitoring devices and secure data transfers.\nCOVID-19 is significantly impacting the way we are delivering healthcare. Given the already successful implementation of artificial intelligence applications and telemedicine in ophthalmology, we expect that these systems will be embraced more as tools for research, education, and patient care.", "journal": "Current opinion in ophthalmology", "date": "2020-07-23", "authors": ["Joelle AHallak", "Angelica CScanzera", "Dimitri TAzar", "R V PaulChan"], "doi": "10.1097/ICU.0000000000000685\n10.7326/M20-1260\n10.1148/radiol.2020200905\n10.1038/s41591-020-0931-3\n10.1148/radiol.2020201874\n10.1148/radiol.2020201491\n10.1101/2020.03.20.000141\n10.1016/j.ajo.2020.04.029"}
{"title": "Impact of the COVID-19 outbreak on acute stroke care.", "abstract": "There are concerns that the coronavirus disease 2019 (COVID-19) outbreak negatively affects the quality of care for acute cardiovascular conditions. We assessed the impact of the COVID-19 outbreak on trends in hospital admissions and workflow parameters of acute stroke care in Amsterdam, The Netherlands.\nWe used data from the three hospitals that provide acute stroke care for the Amsterdam region. We compared two 7-week periods: one during the peak of the COVID-19 outbreak (March 16th-May 3th 2020) and one prior to the outbreak (October 21st-December 8th 2019). We included consecutive patients who presented to the emergency departments with a suspected stroke and assessed the change in number of patients as an incidence-rate ratio (IRR) using a Poisson regression analysis. Other outcomes were the IRR for stroke subtypes, change in use of reperfusion therapy, treatment times, and in-hospital complications.\nDuring the COVID-19 period, 309 patients presented with a suspected stroke compared to 407 patients in the pre-COVID-19 period (IRR 0.76 95%CI 0.65-0.88). The proportion of men was higher during the COVID-19 period (59% vs. 47%, p\u2009<\u20090.001). There was no change in the proportion of stroke patients treated with intravenous thrombolysis (28% vs. 30%, p\u2009=\u20090.58) or endovascular thrombectomy (11% vs 12%, p\u2009=\u20090.82) or associated treatment times. Seven patients (all ischemic strokes) were diagnosed with COVID-19.\nWe observed a 24% decrease in suspected stroke presentations during the COVID-19 outbreak, but no evidence for a decrease in quality of acute stroke care.", "journal": "Journal of neurology", "date": "2020-07-22", "authors": ["L ARinkel", "J C MPrick", "R E RSlot", "N M ASombroek", "JBurggraaff", "A EGroot", "B JEmmer", "Y B W E MRoos", "M CBrouwer", "R Mvan den Berg-Vos", "C B L MMajoie", "L F MBeenen", "Dvan de Beek", "M CVisser", "S Mvan Schaik", "J MCoutinho"], "doi": "10.1007/s00415-020-10069-1\n10.1056/NEJMc2009166\n10.1016/S0140-6736(20)30793-5\n10.1016/S2352-4642(20)30108-5\n10.1001/jamaneurol.2020.1127\n10.1056/NEJMc2009787\n10.1136/bmj.g3429\n10.1001/jamaneurol.2015.3886\n10.1016/S2468-2667(20)30061-X\n10.1016/S1473-3099(14)70755-8\n10.1161/01.STR.0000189687.78760.47\n10.1161/STROKEAHA.108.540781\n10.3389/fpubh.2020.00152\n10.1056/NEJMoa1713973\n10.1056/NEJMoa1813046\n10.1093/aje/152.6.558"}
{"title": "Chest CT for triage during COVID-19 on the emergency department: myth or truth?", "abstract": "We aimed to investigate the diagnostic performance of chest CT compared with first RT-PCR results in adult patients suspected of COVID-19 infection in an ED setting. We also constructed a predictive machine learning model based on chest CT and additional data to improve the diagnostic accuracy of chest CT.\nThis study's cohort consisted of 319 patients who underwent chest CT and RT-PCR testing at the ED. Patient characteristics, demographics, symptoms, vital signs, laboratory tests, and chest CT results (CO-RADS) were collected. With first RT-PCR as reference standard, the diagnostic performance of chest CT using the CO-RADS score was assessed. Additionally, a predictive machine learning model was constructed using logistic regression.\nChest CT, with first RT-PCR as a reference, had a sensitivity, specificity, PPV, and NPV of 90.2%, 88.2%, 84.5%, and 92.7%, respectively. The prediction model with CO-RADS, ferritin, leucocyte count, CK, days of complaints, and diarrhea as predictors had a sensitivity, specificity, PPV, and NPV of 89.3%, 93.4%, 90.8%, and 92.3%, respectively.\nChest CT, using the CO-RADS scoring system, is a sensitive and specific method that can aid in the diagnosis of COVID-19, especially if RT-PCR tests are scarce during an outbreak. Combining a predictive machine learning model could further improve the accuracy of diagnostic chest CT for COVID-19. Further candidate predictors should be analyzed to improve our model. However, RT-PCR should remain the primary standard of testing as up to 9% of RT-PCR positive patients are not diagnosed by chest CT or our machine learning model.", "journal": "Emergency radiology", "date": "2020-07-22", "authors": ["Joep J RHermans", "JoostGroen", "EgonZwets", "Bianca MBoxma-De Klerk", "Jacob MVan Werkhoven", "David S YOng", "Wessel E J JHanselaar", "LennekeWaals-Prinzen", "VanessaBrown"], "doi": "10.1007/s10140-020-01821-1\n10.1056/NEJMoa2002032\n10.1016/S0140-6736(20)30183-5\n10.1016/j.ejrad.2020.108961\n10.2214/AJR.20.22975\n10.1148/radiol.2020200230\n10.1007/s11604-020-00967-9\n10.3348/kjr.2020.0132"}
{"title": "From community-acquired pneumonia to COVID-19: a deep learning-based method for quantitative analysis of COVID-19 on thick-section CT scans.", "abstract": "To develop a fully automated AI system to quantitatively assess the disease severity and disease progression of COVID-19 using thick-section chest CT images.\nIn this retrospective study, an AI system was developed to automatically segment and quantify the COVID-19-infected lung regions on thick-section chest CT images. Five hundred thirty-one CT scans from 204 COVID-19 patients were collected from one appointed COVID-19 hospital. The automatically segmented lung abnormalities were compared with manual segmentation of two experienced radiologists using the Dice coefficient on a randomly selected subset (30 CT scans). Two imaging biomarkers were automatically computed, i.e., the portion of infection (POI) and the average infection HU (iHU), to assess disease severity and disease progression. The assessments were compared with patient status of diagnosis reports and key phrases extracted from radiology reports using the area under the receiver operating characteristic curve (AUC) and Cohen's kappa, respectively.\nThe dice coefficient between the segmentation of the AI system and two experienced radiologists for the COVID-19-infected lung abnormalities was 0.74 \u00b1 0.28 and 0.76 \u00b1 0.29, respectively, which were close to the inter-observer agreement (0.79 \u00b1 0.25). The computed two imaging biomarkers can distinguish between the severe and non-severe stages with an AUC of 0.97 (p value < 0.001). Very good agreement (\u03ba = 0.8220) between the AI system and the radiologists was achieved on evaluating the changes in infection volumes.\nA deep learning-based AI system built on the thick-section CT imaging can accurately quantify the COVID-19-associated lung abnormalities and assess the disease severity and its progressions.\n\u2022 A deep learning-based AI system was able to accurately segment the infected lung regions by COVID-19 using the thick-section CT scans (Dice coefficient \u2265 0.74). \u2022 The computed imaging biomarkers were able to distinguish between the non-severe and severe COVID-19 stages (area under the receiver operating characteristic curve 0.97). \u2022 The infection volume changes computed by the AI system were able to assess the COVID-19 progression (Cohen's kappa 0.8220).", "journal": "European radiology", "date": "2020-07-20", "authors": ["ZhangLi", "ZhengZhong", "YangLi", "TianyuZhang", "LiangxinGao", "DakaiJin", "YueSun", "XianghuaYe", "LiYu", "ZheyuHu", "JingXiao", "LingyunHuang", "YulingTang"], "doi": "10.1007/s00330-020-07042-x\n10.1148/ryct.2020200044"}
{"title": "Towards explainable deep neural networks (xDNN).", "abstract": "In this paper, we propose an elegant solution that is directly addressing the bottlenecks of the traditional deep learning approaches and offers an explainable internal architecture that can outperform the existing methods, requires very little computational resources (no need for GPUs) and short training times (in the order of seconds). The proposed approach, xDNN is using prototypes. Prototypes are actual training data samples (images), which are local peaks of the empirical data distribution called typicality as well as of the data density. This generative model is identified in a closed form and equates to the pdf but is derived automatically and entirely from the training data with no user- or problem-specific thresholds, parameters or intervention. The proposed xDNN offers a new deep learning architecture that combines reasoning and learning in a synergy. It is non-iterative and non-parametric, which explains its efficiency in terms of time and computational resources. From the user perspective, the proposed approach is clearly understandable to human users. We tested it on challenging problems as the classification of different lighting conditions for driving scenes (iROADS), object detection (Caltech-256, and Caltech-101), and SARS-CoV-2 identification via computed tomography scan (COVID CT-scans dataset). xDNN outperforms the other methods including deep learning in terms of accuracy, time to train and offers an explainable classifier.", "journal": "Neural networks : the official journal of the International Neural Network Society", "date": "2020-07-19", "authors": ["PlamenAngelov", "EduardoSoares"], "doi": "10.1016/j.neunet.2020.07.010"}
{"title": "Tailoring steroids in the treatment of COVID-19 pneumonia assisted by CT scans: three case reports.", "abstract": "In this article, we analyze and report cases of three patients who were admitted to Renmin Hospital, Wuhan University, China, for treating COVID-19 pneumonia in February 2020 and were unresponsive to initial treatment of steroids. They were then received titrated steroids treatment based on the assessment of computed tomography (CT) images augmented and analyzed with the artificial intelligence (AI) tool and output. Three patients were finally recovered and discharged. The result indicated that sufficient steroids may be effective in treating the COVID-19 patients after frequent evaluation and timely adjustment according to the disease severity assessed based on the quantitative analysis of the images of serial CT scans.", "journal": "Journal of X-ray science and technology", "date": "2020-07-18", "authors": ["YingSu", "YiHan", "JieLiu", "YueQiu", "QianTan", "ZhenZhou", "Yi-ZhouYu", "JunChen", "Maryellen LGiger", "Fleming Y MLure", "ZheLuo"], "doi": "10.3233/XST-200710"}
{"title": "Automated detection and quantification of COVID-19 pneumonia: CT imaging analysis by a deep learning-based software.", "abstract": "The novel coronavirus disease 2019 (COVID-19) is an emerging worldwide threat to public health. While chest computed tomography (CT) plays an indispensable role in its diagnosis, the quantification and localization of lesions cannot be accurately assessed manually. We employed deep learning-based software to aid in detection, localization and quantification of COVID-19 pneumonia.\nA total of 2460 RT-PCR tested SARS-CoV-2-positive patients (1250 men and 1210 women; mean age, 57.7\u2009\u00b1\u200914.0\u00a0years (age range, 11-93\u00a0years) were retrospectively identified from Huoshenshan Hospital in Wuhan from February 11 to March 16, 2020. Basic clinical characteristics were reviewed. The uAI Intelligent Assistant Analysis System was used to assess the CT scans.\nCT scans of 2215 patients (90%) showed multiple lesions of which 36 (1%) and 50 patients (2%) had left and right lung infections, respectively (>\u200950% of each affected lung's volume), while 27 (1%) had total lung infection (>\u200950% of the total volume of both lungs). Overall, 298 (12%), 778 (32%) and 1300 (53%) patients exhibited pure ground glass opacities (GGOs), GGOs with sub-solid lesions and GGOs with both sub-solid and solid lesions, respectively. Moreover, 2305 (94%) and 71 (3%) patients presented primarily with GGOs and sub-solid lesions, respectively. Elderly patients (\u2265\u200960\u00a0years) were more likely to exhibit sub-solid lesions. The generalized linear mixed model showed that the dorsal segment of the right lower lobe was the favoured site of COVID-19 pneumonia.\nChest CT combined with analysis by the uAI Intelligent Assistant Analysis System can accurately evaluate pneumonia in COVID-19 patients.", "journal": "European journal of nuclear medicine and molecular imaging", "date": "2020-07-16", "authors": ["Hai-TaoZhang", "Jin-SongZhang", "Hai-HuaZhang", "Yan-DongNan", "YingZhao", "En-QingFu", "Yong-HongXie", "WeiLiu", "Wang-PingLi", "Hong-JunZhang", "HuaJiang", "Chun-MeiLi", "Yan-YanLi", "Rui-NaMa", "Shao-KangDang", "Bo-BoGao", "Xi-JingZhang", "TaoZhang"], "doi": "10.1007/s00259-020-04953-1\n10.1056/NEJMoa2001017\n10.1186/s40779-020-0233-6\n10.3348/kjr.2020.0146"}
{"title": "CovXNet: A multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization.", "abstract": "With the recent outbreak of COVID-19, fast diagnostic testing has become one of the major challenges due to the critical shortage of test kit. Pneumonia, a major effect of COVID-19, needs to be urgently diagnosed along with its underlying reasons. In this paper, deep learning aided automated COVID-19 and other pneumonia detection schemes are proposed utilizing a small amount of COVID-19 chest X-rays. A deep convolutional neural network (CNN) based architecture, named as CovXNet, is proposed that utilizes depthwise convolution with varying dilation rates for efficiently extracting diversified features from chest X-rays. Since the chest X-ray images corresponding to COVID-19 caused pneumonia and other traditional pneumonias have significant similarities, at first, a large number of chest X-rays corresponding to normal and (viral/bacterial) pneumonia patients are used to train the proposed CovXNet. Learning of this initial training phase is transferred with some additional fine-tuning layers that are further trained with a smaller number of chest X-rays corresponding to COVID-19 and other pneumonia patients. In the proposed method, different forms of CovXNets are designed and trained with X-ray images of various resolutions and for further optimization of their predictions, a stacking algorithm is employed. Finally, a gradient-based discriminative localization is integrated to distinguish the abnormal regions of X-ray images referring to different types of pneumonia. Extensive experimentations using two different datasets provide very satisfactory detection performance with accuracy of 97.4% for COVID/Normal, 96.9% for COVID/Viral pneumonia, 94.7% for COVID/Bacterial pneumonia, and 90.2% for multiclass COVID/normal/Viral/Bacterial pneumonias. Hence, the proposed schemes can serve as an efficient tool in the current state of COVID-19 pandemic. All the architectures are made publicly available at: https://github.com/Perceptron21/CovXNet.", "journal": "Computers in biology and medicine", "date": "2020-07-14", "authors": ["TanvirMahmud", "Md AwsafurRahman", "Shaikh AnowarulFattah"], "doi": "10.1016/j.compbiomed.2020.103869"}
{"title": "Feasibility of a 5G-Based Robot-Assisted Remote Ultrasound System for Cardiopulmonary Assessment of Patients With Coronavirus Disease 2019.", "abstract": "Traditional methods for cardiopulmonary assessment of patients with coronavirus disease 2019 (COVID-19) pose risks to both patients and examiners. This necessitates a remote examination of such patients without sacrificing information quality.\nThe goal of this study was to assess the feasibility of a 5G-based robot-assisted remote ultrasound system in examining patients with COVID-19 and to establish an examination protocol for telerobotic ultrasound scanning.\nTwenty-three patients with COVID-19 were included and divided into two groups. Twelve were nonsevere cases, and 11 were severe cases. All patients underwent a 5G-based robot-assisted remote ultrasound system examination of the lungs and heart following an established protocol. Distribution characteristics and morphology of the lung and surrounding tissue lesions, left ventricular ejection fraction, ventricular area ratio, pericardial effusion, and examination-related complications were recorded. Bilateral lung lesions were evaluated by using a lung ultrasound score.\nThe remote ultrasound system successfully and safely performed cardiopulmonary examinations of all patients. Peripheral lung lesions were clearly evaluated. Severe cases of COVID-19 had significantly more diseased regions (median [interquartile range], 6.0 [2.0-11.0] vs\u00a01.0 [0.0-2.8]) and higher lung ultrasound scores (12.0 [4.0-24.0] vs\u00a02.0 [0.0-4.0]) than nonsevere cases of COVID-19 (both, P\u00a0< .05). One nonsevere case (8.3%; 95%\u00a0CI, 1.5-35.4) and three severe cases (27.3%; 95%\u00a0CI, 9.7-56.6) were complicated by pleural effusions. Four severe cases (36.4%; 95%\u00a0CI, 15.2-64.6) were complicated by pericardial effusions (vs 0%\u00a0of nonsevere cases, P\u00a0< .05). No patients had significant examination-related complications.\nUse of the 5G-based robot-assisted remote ultrasound system is feasible and effectively obtains ultrasound characteristics for cardiopulmonary assessment of patients with COVID-19. By following established protocols and considering medical history, clinical manifestations, and laboratory markers, this system might help to evaluate the severity of COVID-19 remotely.", "journal": "Chest", "date": "2020-07-13", "authors": ["RuizhongYe", "XianlongZhou", "FeiShao", "LinfeiXiong", "JunHong", "HaijunHuang", "WeiweiTong", "JingWang", "ShuangxiChen", "AilinCui", "ChengzhongPeng", "YanZhao", "LegaoChen"], "doi": "10.1016/j.chest.2020.06.068\n10.1002/jmv.25727\n10.1111/resp.13791"}
{"title": "Systematic review of artificial intelligence techniques in the detection and classification of COVID-19 medical images in terms of evaluation and benchmarking: Taxonomy analysis, challenges, future solutions and methodological aspects.", "abstract": "This study presents a systematic review of artificial intelligence (AI) techniques used in the detection and classification of coronavirus disease 2019 (COVID-19) medical images in terms of evaluation and benchmarking. Five reliable databases, namely, IEEE Xplore, Web of Science, PubMed, ScienceDirect and Scopus were used to obtain relevant studies of the given topic. Several filtering and scanning stages were performed according to the inclusion/exclusion criteria to screen the 36 studies obtained; however, only 11 studies met the criteria. Taxonomy was performed, and the 11 studies were classified on the basis of two categories, namely, review and research studies. Then, a deep analysis and critical review were performed to highlight the challenges and critical gaps outlined in the academic literature of the given subject. Results showed that no relevant study evaluated and benchmarked AI techniques utilised in classification tasks (i.e. binary, multi-class, multi-labelled and hierarchical classifications) of COVID-19 medical images. In case evaluation and benchmarking will be conducted, three future challenges will be encountered, namely, multiple evaluation criteria within each classification task, trade-off amongst criteria and importance of these criteria. According to the discussed future challenges, the process of evaluation and benchmarking AI techniques used in the classification of COVID-19 medical images considered multi-complex attribute problems. Thus, adopting multi-criteria decision analysis (MCDA) is an essential and effective approach to tackle the problem complexity. Moreover, this study proposes a detailed methodology for the evaluation and benchmarking of AI techniques used in all classification tasks of COVID-19 medical images as future directions; such methodology is presented on the basis of three sequential phases. Firstly, the identification procedure for the construction of four decision matrices, namely, binary, multi-class, multi-labelled and hierarchical, is presented on the basis of the intersection of evaluation criteria of each classification task and AI classification techniques. Secondly, the development of the MCDA approach for benchmarking AI classification techniques is provided on the basis of the integrated analytic hierarchy process and VlseKriterijumska Optimizacija I Kompromisno Resenje methods. Lastly, objective and subjective validation procedures are described to validate the proposed benchmarking solutions.", "journal": "Journal of infection and public health", "date": "2020-07-11", "authors": ["O SAlbahri", "A AZaidan", "A SAlbahri", "B BZaidan", "Karrar HameedAbdulkareem", "Z TAl-Qaysi", "A HAlamoodi", "A MAleesa", "M AChyad", "R MAlesa", "L CKem", "Muhammad ModiLakulu", "A BIbrahim", "Nazre AbdulRashid"], "doi": "10.1016/j.jiph.2020.06.028\n10.1016/j.afjem.2020.03.002\n10.1109/TMI.2020.2993291\n10.1007/s00521-020-05020-4"}
{"title": "2020 ACR Presidential Address: Quality, Ownership, and Our Role as Physicians.", "abstract": "A story from long ago reminds us of the importance of quality in our practices, of taking ownership of our patients, and of our role as physicians. The coronavirus disease 2019 (COVID-19) pandemic has disrupted our practices. Before the pandemic, many practices were stretched thin by the amount of work that needed to be done. The work stoppage in many locations brought an unwelcome pause but gives us time to reflect on our practices. How can we maintain quality when high volumes return? The role of artificial intelligence, and our role in its development, needs to be considered. At the same time, we need to take more ownership of the patient and be more help to our referring providers. Our own ACR staff are great examples of taking ownership. Finally, we must recognize that patients and their families are important for optimal patient care. Making that connection is significant. Let us start where we began-in the service of our patients as their physicians. This role is rewarding and, together with a focus on quality and taking ownership, can lead to successful practices that are good for everyone, including ourselves.", "journal": "Journal of the American College of Radiology : JACR", "date": "2020-07-10", "authors": ["Debra LMonticciolo"], "doi": "10.1016/j.jacr.2020.06.004"}
{"title": "Efficient GAN-based Chest Radiographs (CXR) augmentation to diagnose coronavirus disease pneumonia.", "abstract": "", "journal": "International journal of medical sciences", "date": "2020-07-07", "authors": ["SalehAlbahli"], "doi": "10.7150/ijms.46684"}
{"title": "Classification of the COVID-19 infected patients using DenseNet201 based deep transfer learning.", "abstract": "Deep learning models are widely used in the automatic analysis of radiological images. These techniques can train the weights of networks on large datasets as well as fine tuning the weights of pre-trained networks on small datasets. Due to the small COVID-19 dataset available, the pre-trained neural networks can be used for diagnosis of coronavirus. However, these techniques applied on chest CT image is very limited till now. Hence, the main aim of this paper to use the pre-trained deep learning architectures as an automated tool to detection and diagnosis of COVID-19 in chest CT. A DenseNet201 based deep transfer learning (DTL) is proposed to classify the patients as COVID infected or not i.e. COVID-19 (+) or COVID (-). The proposed model is utilized to extract features by using its own learned weights on the ImageNet dataset\u00a0along with a convolutional neural structure. Extensive experiments are performed to evaluate the performance of the propose DTL model on COVID-19 chest CT scan images. Comparative analyses reveal that the proposed DTL based COVID-19 classification model outperforms the competitive approaches.Communicated by Ramaswamy H. Sarma.", "journal": "Journal of biomolecular structure & dynamics", "date": "2020-07-04", "authors": ["AayushJaiswal", "NehaGianchandani", "DilbagSingh", "VijayKumar", "ManjitKaur"], "doi": "10.1080/07391102.2020.1788642"}
{"title": "A COVID-19 patient with seven consecutive false-negative rRT-PCR results from sputum specimens.", "abstract": null, "journal": "Internal and emergency medicine", "date": "2020-07-04", "authors": ["Cong-YingSong", "Da-GanYang", "Yuan-QiangLu"], "doi": "10.1007/s11739-020-02423-y\n10.3348/kjr.2020.0195\n10.3348/kjr.2020.0146\n10.1148/radiol.2020200343\n10.1093/cid/ciaa149\n10.1007/s11739-020-02321-3\n10.1007/s00134-020-05996-6\n10.1136/bmj.m1443\n10.1631/jzus.B2010011"}
{"title": "A deep learning approach to characterize 2019 coronavirus disease (COVID-19) pneumonia in chest CT images.", "abstract": "To utilize a deep learning model for automatic detection of abnormalities in chest CT images from COVID-19 patients and compare its quantitative determination performance with radiological residents.\nA deep learning algorithm consisted of lesion detection, segmentation, and location was trained and validated in 14,435 participants with chest CT images and definite pathogen diagnosis. The algorithm was tested in a non-overlapping dataset of 96 confirmed COVID-19 patients in three hospitals across China during the outbreak. Quantitative detection performance of the model was compared with three radiological residents with two experienced radiologists' reading reports as reference standard by assessing the accuracy, sensitivity, specificity, and F1 score.\nOf 96 patients, 88 had pneumonia lesions on CT images and 8 had no abnormities on CT images. For per-patient basis, the algorithm showed superior sensitivity of 1.00 (95% confidence interval (CI) 0.95, 1.00) and F1 score of 0.97 in detecting lesions from CT images of COVID-19 pneumonia patients. While for per-lung lobe basis, the algorithm achieved a sensitivity of 0.96 (95% CI 0.94, 0.98) and a slightly inferior F1 score of 0.86. The median volume of lesions calculated by algorithm was 40.10\u00a0cm\nThe algorithm showed excellent performance in detecting COVID-19 pneumonia on chest CT images compared with resident radiologists.\n\u2022 The higher sensitivity of deep learning model in detecting COVID-19 pneumonia were found compared with radiological residents on a per-lobe and per-patient basis. \u2022 The deep learning model improves diagnosis efficiency by shortening processing time. \u2022 The deep learning model can automatically calculate the volume of the lesions and whole lung.", "journal": "European radiology", "date": "2020-07-04", "authors": ["QianqianNi", "Zhi YuanSun", "LiQi", "WenChen", "YiYang", "LiWang", "XinyuanZhang", "LiuYang", "YiFang", "ZijianXing", "ZhenZhou", "YizhouYu", "Guang MingLu", "Long JiangZhang"], "doi": "10.1007/s00330-020-07044-9\n10.1136/bmj.m406\n10.1056/NEJMoa2001017\n10.1136/bmj.m641\n10.1016/j.meegid.2020.104211\n10.1007/s10916-020-1536-6\n10.1038/s41591-018-0300-7\n10.1038/s41586-019-1390-1\n10.1101/2020.02.14.20023028v2\n10.7150/thno.46465\n10.1038/nature14539\n10.1148/radiol.2018180237\n10.1371/journal.pmed.1002686"}
{"title": "Using imaging to combat a pandemic: rationale for developing the UK National COVID-19 Chest Imaging Database.", "abstract": "", "journal": "The European respiratory journal", "date": "2020-07-04", "authors": ["JosephJacob", "DanielAlexander", "J KennethBaillie", "RosalindBerka", "OttaviaBertolli", "JamesBlackwood", "IainBuchan", "ClaireBloomfield", "DominicCushnan", "AnnemarieDocherty", "AnthonyEdey", "AlbertoFavaro", "FergusGleeson", "MarkHalling-Brown", "SamanjitHare", "EmilyJefferson", "AnnetteJohnstone", "MylesKirby", "RuthMcStay", "ArjunNair", "Peter J MOpenshaw", "GeoffParker", "GerryReilly", "GrahamRobinson", "GilesRoditi", "Jonathan C LRodrigues", "NeilSebire", "Malcolm GSemple", "CatherineSudlow", "NickWoznitza", "IndraJoshi"], "doi": "10.1183/13993003.01809-2020\n10.1016/S0140-6736(20)30183-5\n10.1183/13993003.00597-2020\n10.1016/j.crad.2020.03.003\n10.1038/s41591-018-0107-6\n10.1038/nature21056\n10.1016/j.crad.2020.03.005\n10.1016/S1473-3099(13)70327-X\n10.1016/j.crad.2020.03.008\n10.1148/radiol.2020200463\n10.1183/13993003.00407-2020\n10.1183/13993003.00334-2020\n10.1183/13993003.00398-2020"}
{"title": "Development of a clinical decision support system for severity risk prediction and triage of COVID-19 patients at hospital admission: an international multicentre study.", "abstract": "The outbreak of coronavirus disease 2019 (COVID-19) has globally strained medical resources and caused significant mortality.\nTo develop and validate a machine-learning model based on clinical features for severity risk assessment and triage for COVID-19 patients at hospital admission.\n725 patients were used to train and validate the model. This included a retrospective cohort from Wuhan, China of 299 hospitalised COVID-19 patients from 23 December 2019 to 13 February 2020, and five cohorts with 426 patients from eight centres in China, Italy and Belgium from 20 February 2020 to 21 March 2020. The main outcome was the onset of severe or critical illness during hospitalisation. Model performances were quantified using the area under the receiver operating characteristic curve (AUC) and metrics derived from the confusion matrix.\nIn the retrospective cohort, the median age was 50\u2005years and 137 (45.8%) were male. In the five test cohorts, the median age was 62\u2005years and 236 (55.4%) were male. The model was prospectively validated on five cohorts yielding AUCs ranging from 0.84 to 0.93, with accuracies ranging from 74.4% to 87.5%, sensitivities ranging from 75.0% to 96.9%, and specificities ranging from 55.0% to 88.0%, most of which performed better than the pneumonia severity index. The cut-off values of the low-, medium- and high-risk probabilities were 0.21 and 0.80. The online calculators can be found at www.covid19risk.ai.\nThe machine-learning model, nomogram and online calculator might be useful to access the onset of severe and critical illness among COVID-19 patients and triage at hospital admission.", "journal": "The European respiratory journal", "date": "2020-07-04", "authors": ["GuangyaoWu", "PeiYang", "YuanliangXie", "Henry CWoodruff", "XiangangRao", "JulienGuiot", "Anne-NoelleFrix", "RenaudLouis", "MichelMoutschen", "JiaweiLi", "JingLi", "ChenggongYan", "DanDu", "ShengchaoZhao", "YiDing", "BinLiu", "WenwuSun", "FabrizioAlbarello", "AlessandraD'Abramo", "VincenzoSchinin\u00e0", "EmanueleNicastri", "MariaelenaOcchipinti", "GiovanniBarisione", "EmanuelaBarisione", "IvaHalilaj", "PierreLovinfosse", "XiangWang", "JianlinWu", "PhilippeLambin"], "doi": "10.1183/13993003.01104-2020\n10.1001/jama.2020.3204\n10.1056/NEJMoa2002032\n10.1097/CCM.0000000000004411\n10.1056/NEJM199701233360402\n10.1056/NEJMc1906060\n10.1038/nrclinonc.2017.141\n10.1016/S0140-6736(20)30260-9\n10.1373/49.1.1\n10.1164/rccm.201908-1581ST\n10.18637/jss.v036.i11\n10.2307/2531595\n10.1016/S0140-6736(20)30183-5\n10.1001/jama.2020.1585\n10.1038/s41586-020-2012-7\n10.1016/S2213-2600(20)30116-8\n10.1056/NEJMoa2001282\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020200274\n10.1148/RADIOL.2020200843\n10.1148/radiol.2020200230\n10.1016/S1473-3099(20)30086-4\n10.1136/thorax.58.8.686\n10.1016/S0140-6736(19)33221-0\n10.1200/CCI.19.00047\n10.1016/j.radonc.2019.11.019"}
{"title": "Chest CT Evaluation of 11 Persistent Asymptomatic Patients with SARS-CoV-2 Infection.", "abstract": "In total, 11 asymptomatic carriers who underwent nasal or oropharyngeal swab tests for SARS-CoV-2 after being in close contact with patients who developed symptomatic 2019 coronavirus disease (COVID-19) were enrolled in this study. The chest multidetector computed tomography (CT) images of the enrolled patients were qualitatively and quantitatively analyzed. The findings of the first chest CT were normal in 3 (27.3%) patients, 2 of whom were aged below 15 years. The lesions of 2 (18.2%) patients involved 1 lobe with unifocal presence. Subpleural lesions were observed in 7 (63.6%) patients. Ground glass opacity (GGO) was the most common sign observed in 7 (63.6%) patients. Crazy-paving pattern and consolidation were detected in 2 (18.2%) and 4 (36.4%) patients, respectively. Based on deep learning and quantitative analysis, the mean volume of intrapulmonary lesions in the first CT image was 85.73 \u00b1 84.46 cm", "journal": "Japanese journal of infectious diseases", "date": "2020-07-03", "authors": ["ShuoYan", "HuiChen", "Ru-MingXie", "Chun-ShuangGuan", "MingXue", "Zhi-BinLv", "Lian-GuiWei", "YanBai", "Bu-DongChen"], "doi": "10.7883/yoken.JJID.2020.264"}
{"title": "Setting up an Easy-to-Use Machine Learning Pipeline for Medical Decision Support: A Case Study for COVID-19 Diagnosis Based on Deep Learning with CT Scans.", "abstract": "Coronavirus disease (COVID-19) constitutes an ongoing global health problem with significant morbidity and mortality. It usually presents characteristic findings on a chest CT scan, which may lead to early detection of the disease. A timely and accurate diagnosis of COVID-19 is the cornerstone for the prompt management of the patients. The aim of the present study was to evaluate the performance of an automated machine learning algorithm in the diagnosis of Covid-19 pneumonia using chest CT scans. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity, and positive predictive value. The method's average precision was 0.932. We suggest that auto-ML platforms help users with limited ML expertise train image recognition models by only uploading the examined dataset and performing some basic settings. Such methods could deliver significant potential benefits for patients in the future by allowing for earlier disease detection and care.", "journal": "Studies in health technology and informatics", "date": "2020-07-02", "authors": ["AikateriniSakagianni", "GeorgiosFeretzakis", "DimitrisKalles", "ChristinaKoufopoulou", "VasileiosKaldis"], "doi": "10.3233/SHTI200481"}
{"title": "Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays.", "abstract": "Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays.\nIn particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence.\nExperimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97.", "journal": "Computer methods and programs in biomedicine", "date": "2020-07-01", "authors": ["LucaBrunese", "FrancescoMercaldo", "AlfonsoReginelli", "AntonellaSantone"], "doi": "10.1016/j.cmpb.2020.105608"}
{"title": "Quantitative chest CT analysis in COVID-19 to predict the need for oxygenation support and intubation.", "abstract": "Lombardy (Italy) was the epicentre of the COVID-19 pandemic in March 2020. The healthcare system suffered from a shortage of ICU beds and oxygenation support devices. In our Institution, most patients received chest CT at admission, only interpreted visually. Given the proven value of quantitative CT analysis (QCT) in the setting of ARDS, we tested QCT as an outcome predictor for COVID-19.\nWe performed a single-centre retrospective study on COVID-19 patients hospitalised from January 25, 2020, to April 28, 2020, who received CT at admission prompted by respiratory symptoms such as dyspnea or desaturation. QCT was performed using a semi-automated method (3D Slicer). Lungs were divided by Hounsfield unit intervals. Compromised lung (%CL) volume was the sum of poorly and non-aerated volumes (-\u2009500, 100 HU). We collected patient's clinical data including oxygenation support throughout hospitalisation.\nTwo hundred twenty-two patients (163 males, median age 66, IQR 54-6) were included; 75% received oxygenation support (20% intubation rate). Compromised lung volume was the most accurate outcome predictor (logistic regression, p\u00a0<\u20090.001). %CL values in the 6-23% range increased risk of oxygenation support; values above 23% were at risk for intubation. %CL showed a negative correlation with PaO\nQCT provides new metrics of COVID-19. The compromised lung volume is accurate in predicting the need for oxygenation support and intubation and is a significant risk factor for in-hospital death. QCT may serve as a tool for the triaging process of COVID-19.\n\u2022 Quantitative computer-aided analysis of chest CT (QCT) provides new metrics of COVID-19. \u2022 The compromised lung volume measured in the -\u2009500, 100 HU interval predicts oxygenation support and intubation and is a risk factor for in-hospital death. \u2022 Compromised lung values in the 6-23% range prompt oxygenation therapy; values above 23% increase the need for intubation.", "journal": "European radiology", "date": "2020-06-28", "authors": ["EzioLanza", "RiccardoMuglia", "IsabellaBolengo", "Orazio GiuseppeSantonocito", "CostanzaLisi", "GiovanniAngelotti", "PierandreaMorandini", "VictorSavevski", "Letterio SalvatorePoliti", "LucaBalzarini"], "doi": "10.1007/s00330-020-07013-2\n10.1016/S0140-6736(20)30566-3\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200230\n10.1371/journal.pone.0230548\n10.1016/j.ejrad.2019.108748\n10.1148/radiol.2373041515\n10.1056/NEJMoa2001017\n10.1136/thorax.58.5.377\n10.1016/j.mri.2012.05.001\n10.1371/journal.pone.0178944\n10.1186/cc12738\n10.1186/1471-2105-14-106\n10.1177/1536867X19874237\n10.1007/s00134-010-2016-0\n10.1164/rccm.201610-2156SO"}
{"title": "New machine learning method for image-based diagnosis of COVID-19.", "abstract": "COVID-19 is a worldwide epidemic, as announced by the World Health Organization (WHO) in March 2020. Machine learning (ML) methods can play vital roles in identifying COVID-19 patients by visually analyzing their chest x-ray images. In this paper, a new ML-method proposed to classify the chest x-ray images into two classes, COVID-19 patient or non-COVID-19 person. The features extracted from the chest x-ray images using new Fractional Multichannel Exponent Moments (FrMEMs). A parallel multi-core computational framework utilized to accelerate the computational process. Then, a modified Manta-Ray Foraging Optimization based on differential evolution used to select the most significant features. The proposed method evaluated using two COVID-19 x-ray datasets. The proposed method achieved accuracy rates of 96.09% and 98.09% for the first and second datasets, respectively.", "journal": "PloS one", "date": "2020-06-27", "authors": ["Mohamed AbdElaziz", "Khalid MHosny", "AhmadSalah", "Mohamed MDarwish", "SongfengLu", "Ahmed TSahlol"], "doi": "10.1371/journal.pone.0235187\n10.1162/NECO_a_00990"}
{"title": "Truncated inception net: COVID-19 outbreak screening using chest X-rays.", "abstract": "Since December 2019, the Coronavirus Disease (COVID-19) pandemic has caused world-wide turmoil in a short period of time, and the infection, caused by SARS-CoV-2, is spreading rapidly. AI-driven tools are used to identify Coronavirus outbreaks as well as forecast their nature of spread, where imaging techniques are widely used, such as CT scans and chest X-rays (CXRs). In this paper, motivated by the fact that X-ray imaging systems are more prevalent and cheaper than CT scan systems, a deep learning-based Convolutional Neural Network (CNN) model, which we call Truncated Inception Net, is proposed to screen COVID-19 positive CXRs from other non-COVID and/or healthy cases. To validate our proposal, six different types of datasets were employed by taking the following CXRs: COVID-19 positive, Pneumonia positive, Tuberculosis positive, and healthy cases into account. The proposed model achieved an accuracy of 99.96% (AUC of 1.0) in classifying COVID-19 positive cases from combined Pneumonia and healthy cases. Similarly, it achieved an accuracy of 99.92% (AUC of 0.99) in classifying COVID-19 positive cases from combined Pneumonia, Tuberculosis, and healthy CXRs. To the best of our knowledge, as of now, the achieved results outperform the existing AI-driven tools for screening COVID-19 using the acquired CXRs, and proves the viability of using the proposed Truncated Inception Net as a screening tool.", "journal": "Physical and engineering sciences in medicine", "date": "2020-06-27", "authors": ["DipayanDas", "K CSantosh", "UmapadaPal"], "doi": "10.1007/s13246-020-00888-x\n10.1016/j.acra.2020.03.003\n10.1148/radiol.2020200432\n10.1016/S0140-6736(20)30183-5\n10.1007/s10916-020-01562-1\n10.1007/s11548-016-1359-6\n10.1007/s11548-015-1242-x\n10.1007/s10916-018-0991-9\n10.1109/TMI.2017.2775636"}
{"title": "Chest CT in COVID-19 pneumonia: A review of current knowledge.", "abstract": "The current COVID-19 pandemic has highlighted the essential role of chest computed tomography (CT) examination in patient triage in the emergency departments, allowing them to be referred to \"COVID\" or \"non-COVID\" wards. Initial chest CT examination must be performed without intravenous administration of iodinated contrast material, but contrast material administration is required when pulmonary embolism is suspected, which seems to be frequent in severe forms of the disease. Typical CT features consist of bilateral ground-glass opacities with peripheral, posterior and basal predominance. Lung disease extent on CT correlates with clinical severity. Artificial intelligence could assist radiologists for diagnosis and prognosis evaluation.", "journal": "Diagnostic and interventional imaging", "date": "2020-06-24", "authors": ["CJalaber", "TLapotre", "TMorcet-Delattre", "FRibet", "SJouneau", "MLederlin"], "doi": "10.1016/j.diii.2020.06.001\n10.1001/jama.2020.2648\n10.1056/NEJMoa2002032\n10.1148/radiol.2020200432.200642\n10.1148/radiol.2020200432\n10.1007/s00330-020-06865-y\n10.1148/radiol.2020201237\n10.1148/radiol.2020201343\n10.1148/radiol.2020200823\n10.1007/s00330-020-06925-3\n10.1007/s00330-020-06928-0\n10.5152/dir.2020.20254\n10.1148/radiol.2020200843\n10.1007/s00330-020-06823-8\n10.1007/s00330-020-06879-6\n10.1111/myc.13096\n10.1186/s13613-020-00686-4\n10.1148/radiol.2020201544\n10.1148/radiol.2020201561\n10.1161/circulationaha.120.047430\n10.1183/13993003.01365-2020\n10.1016/j.jinf.2020.04.030\n10.5858/arpa.2020-0901-SA\n10.1148/radiol.2020200905"}
{"title": "COVID-19 Pneumonia Diagnosis Using a Simple 2D Deep Learning Framework With a Single Chest CT Image: Model Development and Validation.", "abstract": "Coronavirus disease (COVID-19) has spread explosively worldwide since the beginning of 2020. According to a multinational consensus statement from the Fleischner Society, computed tomography (CT) is a relevant screening tool due to its higher sensitivity for detecting early pneumonic changes. However, physicians are extremely occupied fighting COVID-19 in this era of worldwide crisis. Thus, it is crucial to accelerate the development of an artificial intelligence (AI) diagnostic tool to support physicians.\nWe aimed to rapidly develop an AI technique to diagnose COVID-19 pneumonia in CT images and differentiate it from non-COVID-19 pneumonia and nonpneumonia diseases.\nA simple 2D deep learning framework, named the fast-track COVID-19 classification network (FCONet), was developed to diagnose COVID-19 pneumonia based on a single chest CT image. FCONet was developed by transfer learning using one of four state-of-the-art pretrained deep learning models (VGG16, ResNet-50, Inception-v3, or Xception) as a backbone. For training and testing of FCONet, we collected 3993 chest CT images of patients with COVID-19 pneumonia, other pneumonia, and nonpneumonia diseases from Wonkwang University Hospital, Chonnam National University Hospital, and the Italian Society of Medical and Interventional Radiology public database. These CT images were split into a training set and a testing set at a ratio of 8:2. For the testing data set, the diagnostic performance of the four pretrained FCONet models to diagnose COVID-19 pneumonia was compared. In addition, we tested the FCONet models on an external testing data set extracted from embedded low-quality chest CT images of COVID-19 pneumonia in recently published papers.\nAmong the four pretrained models of FCONet, ResNet-50 showed excellent diagnostic performance (sensitivity 99.58%, specificity 100.00%, and accuracy 99.87%) and outperformed the other three pretrained models in the testing data set. In the additional external testing data set using low-quality CT images, the detection accuracy of the ResNet-50 model was the highest (96.97%), followed by Xception, Inception-v3, and VGG16 (90.71%, 89.38%, and 87.12%, respectively).\nFCONet, a simple 2D deep learning framework based on a single chest CT image, provides excellent diagnostic performance in detecting COVID-19 pneumonia. Based on our testing data set, the FCONet model based on ResNet-50 appears to be the best model, as it outperformed other FCONet models based on VGG16, Xception, and Inception-v3.", "journal": "Journal of medical Internet research", "date": "2020-06-23", "authors": ["HoonKo", "HeewonChung", "Wu SeongKang", "Kyung WonKim", "YoungbinShin", "Seung JiKang", "Jae HoonLee", "Young JunKim", "Nan YeolKim", "HyunseokJung", "JinseokLee"], "doi": "10.2196/19569\n10.1056/NEJMoa2004500\n10.1080/22221751.2020.1745095\n10.1080/22221751.2020.1745095\n10.1002/jmv.25786\n10.1148/radiol.2020200642\n10.1148/radiol.2020201365\n10.3390/diagnostics10040202\n10.3348/kjr.2020.0146\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020201326\n10.1148/radiol.2020200823\n10.1148/radiol.2020200905\n10.1007/s003300101126\n10.1016/j.ejrad.2017.01.016\n10.1109/CVPR.2016.90\n10.1109/cvpr.2016.308\n10.1109/cvpr.2017.195\n10.1162/neco_a_00990\n10.1109/iccv.2017.74\n10.1148/radiol.2020200905\n10.1101/2020.03.12.20027185\n10.1109/jtehm.2018.2837901\n10.1109/cvpr.2009.5206848\n10.1109/iros.2015.7353481\n10.3348/kjr.2020.0132\n10.1148/radiol.2020200988\n10.3348/kjr.2019.0025\n10.1007/s10916-020-01562-1"}
{"title": "COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches.", "abstract": "Coronavirus causes a wide variety of respiratory infections and it is an RNA-type virus that can infect both humans and animal species. It often causes pneumonia in humans. Artificial intelligence models have been helpful for successful analyses in the biomedical field. In this study, Coronavirus was detected using a deep learning model, which is a sub-branch of artificial intelligence. Our dataset consists of three classes namely: coronavirus, pneumonia, and normal X-ray imagery. In this study, the data classes were restructured using the Fuzzy Color technique as a preprocessing step and the images that were structured with the original images were stacked. In the next step, the stacked dataset was trained with deep learning models (MobileNetV2, SqueezeNet) and the feature sets obtained by the models were processed using the Social Mimic optimization method. Thereafter, efficient features were combined and classified using Support Vector Machines (SVM). The overall classification rate obtained with the proposed approach was 99.27%. With the proposed approach in this study, it is evident that the model can efficiently contribute to the detection of COVID-19 disease.", "journal": "Computers in biology and medicine", "date": "2020-06-23", "authors": ["MesutTo\u011fa\u00e7ar", "BurhanErgen", "ZaferC\u00f6mert"], "doi": "10.1016/j.compbiomed.2020.103805\n10.1056/nejmc2001468\n10.1016/S0140-6736(20)30522-5\n10.1136/bmj.m800\n10.1038/s41368-020-0075-9\n10.1016/j.mehy.2019.109503\n10.1007/s10462-018-9641-3\n10.1016/j.measurement.2019.05.076\n10.1038/s41598-019-42294-8\n10.1186/s40537-019-0276-2\n10.1155/2018/4168538\n10.1155/2019/4180949\n10.3390/app10020559\n10.2214/AJR.20.22969\n10.1109/cvpr.2018.00474\n10.1016/j.mehy.2019.109531\n10.3390/ijgi8120582\n10.1016/j.optlaseng.2019.05.005\n10.3390/s19050982\n10.3906/elk-1801-157\n10.3390/rs12010120\n10.1007/s41745-019-0098-4\n10.1007/978-1-4302-5990-9_3\n10.2339/politeknik.369132\n10.3390/app10010243\n10.1016/j.fss.2019.09.013\n10.13140/2.1.3014.6562\n10.1016/j.ejfs.2016.06.001\n10.1016/j.eswa.2019.05.035\n10.1016/j.bbe.2019.11.001"}
{"title": "Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks.", "abstract": "Fast diagnostic methods can control and prevent the spread of pandemic diseases like coronavirus disease 2019 (COVID-19) and assist physicians to better manage patients in high workload conditions. Although a laboratory test is the current routine diagnostic tool, it is time-consuming, imposing a high cost and requiring a well-equipped laboratory for analysis. Computed tomography (CT) has thus far become a fast method to diagnose patients with COVID-19. However, the performance of radiologists in diagnosis of COVID-19 was moderate. Accordingly, additional investigations are needed to improve the performance in diagnosing COVID-19. In this study is suggested a rapid and valid method for COVID-19 diagnosis using an artificial intelligence technique based. 1020 CT slices from 108 patients with laboratory proven COVID-19 (the COVID-19 group) and 86 patients with other atypical and viral pneumonia diseases (the non-COVID-19 group) were included. Ten well-known convolutional neural networks were used to distinguish infection of COVID-19 from non-COVID-19 groups: AlexNet, VGG-16, VGG-19, SqueezeNet, GoogleNet, MobileNet-V2, ResNet-18, ResNet-50, ResNet-101, and Xception. Among all networks, the best performance was achieved by ResNet-101 and Xception. ResNet-101 could distinguish COVID-19 from non-COVID-19 cases with an AUC of 0.994 (sensitivity, 100%; specificity, 99.02%; accuracy, 99.51%). Xception achieved an AUC of 0.994 (sensitivity, 98.04%; specificity, 100%; accuracy, 99.02%). However, the performance of the radiologist was moderate with an AUC of 0.873 (sensitivity, 89.21%; specificity, 83.33%; accuracy, 86.27%). ResNet-101 can be considered as a high sensitivity model to characterize and diagnose COVID-19 infections, and can be used as an adjuvant tool in radiology departments.", "journal": "Computers in biology and medicine", "date": "2020-06-23", "authors": ["Ali AbbasianArdakani", "Alireza RajabzadehKanafi", "U RajendraAcharya", "NazaninKhadem", "AfshinMohammadi"], "doi": "10.1016/j.compbiomed.2020.103795\n10.1148/radiol.2020200527\n10.1148/radiol.2020200823\n10.1016/S0140-6736(20)30673-5"}
{"title": "Automated detection of COVID-19 cases using deep neural networks with X-ray images.", "abstract": "The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.", "journal": "Computers in biology and medicine", "date": "2020-06-23", "authors": ["TulinOzturk", "MuhammedTalo", "Eylul AzraYildirim", "Ulas BaranBaloglu", "OzalYildirim", "URajendra Acharya"], "doi": "10.1016/j.compbiomed.2020.103792\n10.1148/radiol.2020200490\n10.1148/radiol.2020200527\n10.1148/radiol.2020200343\n10.1148/radiol.2020200463\n10.1148/radiol.2020200370\n10.1038/nature21056\n10.1101/2020.03.12.20027185\n10.1148/radiol.2020200823"}
{"title": "End-to-end automatic differentiation of the coronavirus disease 2019 (COVID-19) from viral pneumonia based on chest CT.", "abstract": "In the absence of a virus nucleic acid real-time reverse transcriptase-polymerase chain reaction (RT-PCR) test and experienced radiologists, clinical diagnosis is challenging for viral pneumonia with clinical symptoms and CT signs similar to that of coronavirus disease 2019 (COVID-19). We developed an end-to-end automatic differentiation method based on CT images to identify COVID-19 pneumonia patients in real time.\nFrom January 18 to February 23, 2020, we conducted a retrospective study and enrolled 201 patients from two hospitals in China\u00a0who underwent chest CT and RT-PCR tests, of which 98 patients tested positive for COVID-19 (118 males and 83 females, with an average age of 42\u00a0years). Patient CT images from one hospital were divided among training, validation and test datasets with an 80%:10%:10% ratio. An end-to-end representation learning method using a large-scale bi-directional generative adversarial network (BigBiGAN) architecture was designed to extract semantic features from the CT images. The semantic feature matrix was input for linear classifier construction. Patients from the other hospital were used for external validation. Differentiation accuracy was evaluated using a receiver operating characteristic curve.\nBased on the 120-dimensional semantic features extracted by BigBiGAN from each image, the linear classifier results indicated that the area under the curve (AUC) in the training, validation and test datasets were 0.979, 0.968 and 0.972, respectively, with an average sensitivity of 92% and specificity of 91%. The AUC for external validation was 0.850, with a sensitivity of 80% and specificity of 75%. Publicly available architecture and computing resources were used throughout the study to ensure reproducibility.\nThis study provides an efficient recognition method for coronavirus disease 2019 pneumonia, using an end-to-end design to implement targeted and effective isolation for the containment of this communicable disease.", "journal": "European journal of nuclear medicine and molecular imaging", "date": "2020-06-23", "authors": ["JiangdianSong", "HongmeiWang", "YuchanLiu", "WenqingWu", "GangDai", "ZongshanWu", "PuheZhu", "WeiZhang", "Kristen WYeom", "KexueDeng"], "doi": "10.1007/s00259-020-04929-1\n10.1016/S0140-6736(20)30323-8\n10.1007/s00259-020-04735-9\n10.1093/cid/ciu053\n10.1016/j.ejrad.2020.108991"}
{"title": "Quantitative analysis of chest CT imaging findings with the risk of ARDS in COVID-19 patients: a preliminary study.", "abstract": "The coronavirus disease 2019 (COVID-19) has rapidly become a pandemic worldwide. The value of chest computed tomography (CT) is debatable during the treatment of COVID-19 patients. Compared with traditional chest X-ray radiography, quantitative CT may supply more information, but its value on COVID-19 patients was still not proven.\nAn automatic quantitative analysis model based on a deep network called VB-Net for infection region segmentation was developed. A quantitative analysis was performed for patients diagnosed as severe COVID 19. The quantitative assessment included volume and density among the infectious area. The primary clinical outcome was the existence of acute respiratory distress syndrome (ARDS). A univariable and multivariable logistic analysis was done to explore the relationship between the quantitative results and ARDS existence.\nThe VB-Ne model was sensitive and stable for pulmonary lesion segmentation, and quantitative analysis indicated that the total volume and average density of the lung lesions were not related to ARDS. However, lesions with specific density changes showed some influence on the risk of ARDS. The proportion of lesion density from -549 to -450 Hounsfield unit (HU) was associated with increased risk of ARDS, while the density was ranging from -149 to -50 HU was related to a lowered risk of ARDS.\nThe automatic quantitative model based on VB-Ne can supply useful information for ARDS risk stratification in COVID-19 patients during treatment.", "journal": "Annals of translational medicine", "date": "2020-06-23", "authors": ["YiWang", "YuntianChen", "YiWei", "ManLi", "YuweiZhang", "NaZhang", "ShuangZhao", "HanjiangZeng", "WenDeng", "ZixingHuang", "ZhengYe", "ShangWan", "BinSong"], "doi": "10.21037/atm-20-3554\n10.1056/NEJMoa2001017\n10.1056/NEJMc2001272\n10.3348/kjr.2020.0112\n10.1148/radiol.2020200843\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020201365\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2020200905\n10.1007/s00330-020-06817-6\n10.1016/j.jpha.2020.03.004\n10.1378/chest.15-0034\n10.1148/radiol.2020200370\n10.1148/radiol.2020200463"}
{"title": "Optical techniques, computed tomography and deep learning role in the diagnosis of COVID-19 pandemic towards increasing the survival rate of vulnerable populations.", "abstract": "\u2022 Severe lung complications can be explored using computed tomography during COVID-19 pandemic. \u2022 Ultra-low dose CT can enhance COVID-19 infected patients diagnostic capability. \u2022 Optically monitored CT along with deep learning is the best solution for diagnosis of COVID-19 during pandemic. \u2022 CT scans sensitivity (88 %) is preferable on clinical approach sensitivity (59 %) for COVID-19 suspected patients. \u2022 CT and Computer aided approaches helps the radiologist to make fast and accurate diagnosis during COVID-19 pandemic.", "journal": "Photodiagnosis and photodynamic therapy", "date": "2020-06-21", "authors": ["Shahzad AhmadQureshi", "Aziz UlRehman"], "doi": "10.1016/j.pdpdt.2020.101880\n10.1007/s00330-020-06801-0\n10.1007/s00330-020-06731-x\n10.1016/j.pdpdt.2020.101836\n10.1016/j.pdpdt.2020.101823\n10.1016/j.ijantimicag.2020.105954\n10.1097/rli.0000000000000670\n10.21037/qims.2018.06.05\n10.1080/14737159.2020.1766968\n10.1007/s10489-020-01714-3\n10.1101/2020.02.14.20023028\n10.1109/tip.2011.2107328\n10.1055/a-1154-8795\n10.1136/bmj.m641\n10.2214/ajr.20.22954"}
{"title": "A Novel Machine Learning-derived Radiomic Signature of the Whole Lung Differentiates Stable From Progressive COVID-19 Infection: A Retrospective Cohort Study.", "abstract": "This study aimed to use the radiomics signatures of a machine learning-based tool to evaluate the prognosis of patients with coronavirus disease 2019 (COVID-19) infection.\nThe clinical and imaging data of 64 patients with confirmed diagnoses of COVID-19 were retrospectively selected and divided into a stable group and a progressive group according to the data obtained from the ongoing treatment process. Imaging features from whole-lung images from baseline computed tomography (CT) scans were extracted and dimensionality reduction was performed. Support vector machines were used to construct radiomics signatures and to compare differences between the 2 groups. We also compared the differences of signature scores in the clinical, laboratory, and CT image feature subgroups and finally analyzed the correlation between the radiomics features of the constructed signature and the other features including clinical, laboratory, and CT imaging features.\nThe signature has a good classification effect for the stable group and the progressive group, with area under curve, sensitivity, and specificity of 0.833, 80.95%, and 74.42%, respectively. Signature score differences in laboratory and CT imaging features between subgroups were not statistically significant (P>0.05); cough was negatively correlated with GLCM Entropy_angle 90_offset4 (r=-0.578), but was positively correlated with ShortRunEmphhasis_AllDirect_offset4_SD (r=0.454); C-reactive protein was positively correlated with Cluster Prominence_ AllDirect_offset 4_ SD (r=0.47).\nThe radiomics signature of the whole lung based on machine learning may reveal the changes of lung microstructure in the early stage and help to indicate the progression of the disease.", "journal": "Journal of thoracic imaging", "date": "2020-06-20", "authors": ["LipingFu", "YongchouLi", "AipingCheng", "PeiPeiPang", "ZhenyuShu"], "doi": "10.1097/RTI.0000000000000544\n10.1002/jmv.25682"}
{"title": "Chest lesion CT radiological features and quantitative analysis in RT-PCR turned negative and clinical symptoms resolved COVID-19 patients.", "abstract": "Many studies have described lung lesion computed tomography (CT) features of coronavirus disease 2019 (COVID-19) patients at the early and progressive stages. In this study, we aim to evaluate lung lesion CT radiological features along with quantitative analysis for the COVID-19 patients ready for discharge.\nFrom February 10 to March 10, 2020, 125 COVID-19 patients (age: 16-67 years, 63 males) ready for discharge, with two consecutive negative reverse transcription-polymerase chain reaction (RT-PCR) and no clinical symptoms for more than 3 days, were included. The pre-discharge CT was performed on all patients 1-3 days after the second negative RT-PCR test, and the follow-up CTs were performed on 44 patients 2-13 days later. The imaging features and quantitative analysis were evaluated on both the pre-discharge and the follow-up CTs, by both radiologists and an artificial intelligence (AI) software.\nOn the pre-discharge CT, the most common CT findings included ground-glass opacity (GGO) (99/125, 79.2%) with bilateral mixed distribution, and fibrosis (56/125, 44.8%) with bilateral subpleural distribution. Enlarged mediastinal lymph nodes were also commonly observed (45/125, 36.0%). AI enabled quantitative analysis showed the right lower lobe was mostly involved, and lesions most commonly had CT value of -570 to -470 HU consistent with GGO. Follow-up CT showed GGO decrease in size and density (40/40, 100%) and fibrosis reduction (17/26, 65.4%). Compared with the pre-discharge CT results, quantitative analysis shows the lung lesion volume regressed significantly at follow-up.\nFor COVID-19 patients ready for discharge, GGO and fibrosis are the main CT features and they further regress at follow-up.", "journal": "Quantitative imaging in medicine and surgery", "date": "2020-06-19", "authors": ["SiyaoDu", "SiGao", "GuoliangHuang", "ShuLi", "WeiChong", "ZiyiJia", "GangHou", "Y\u00ec Xi\u00e1ng JW\u00e1ng", "LinaZhang"], "doi": "10.21037/qims-20-531\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMc2001272\n10.1056/NEJMoa2001191\n10.1002/jmv.25699\n10.1148/radiol.2020200343\n10.1148/radiol.2020200432\n10.1148/radiol.2020200642\n10.21037/qims.2020.02.10\n10.1148/radiol.2020200241\n10.1007/s00330-020-06713-z\n10.1148/ryct.2020200034\n10.1007/s00330-020-06731-x\n10.21037/qims-20-564\n10.1148/radiol.2020200370\n10.1148/radiol.2020200230\n10.1148/radiol.2020200274\n10.1148/radiol.2020200269\n10.1148/radiol.2462070712\n10.1016/j.clinimag.2018.06.013\n10.1097/00004728-200411000-00010\n10.1016/S2213-2600(20)30076-X\n10.1097/RLI.0000000000000674\n10.1016/S1473-3099(20)30086-4\n10.21037/atm.2020.02.71\n10.3346/jkms.2004.19.2.159\n10.21037/qims.2018.06.05"}
{"title": "[Recommendations for the treatment of severe coronavirus disease 2019 based on critical care ultrasound].", "abstract": "Severe patients with coronaviras disease 2019 (COVID-19) are characterized by persistent lung damage, causing respiratory failure, secondary circulatory changes and multiple organ dysfunction after virus invasion. Because of its dynamic, real-time, non-invasive, repeatable and other advantages, critical ultrasonography can be widely used in the diagnosis, assessment and guidance of treatment for severe patients. Based on the recommendations of critical care experts from all over the country who fight against the epidemic in Wuhan, this article summarizes the guidelines for the treatment of COVID-19 based on critical ultrasonography, hoping to provide help for the treatment of severe patients. The recommendations mainly cover the following aspects: (1) lung ultrasound in patients with COVID-19 is mainly manifested by thickened and irregular pleural lines, different types of B-lines, shred signs, and other consolidation like dynamic air bronchogram; (2) Echocardiography may show right heart dysfunction, diffuse cardiac function enhancement, stress cardiomyopathy, diffuse cardiac depression and other multiple abnormalities; (3) Critical ultrasonography helps with initiating early treatment in the suspect patient, screening confirmed patients after intensive care unit admission, early assessment of sudden critical events, rapid grading assessment and treatment based on it; (4) Critical ultrasonography helps to quickly screen for the etiology of respiratory failure in patients with COVID-19, make oxygen therapeutic strategy, guide the implementation of lung protective ventilation, graded management and precise off-ventilator; (5) Critical ultrasonography is helpful for assessing the circulatory status of patients with COVID-19, finding chronic cardiopulmonary diseases and guiding extracorporeal membrane oxygenation management; (6) Critical ultrasonography contributes to the management of organs besides based on cardiopulmonary oxygen transport; (7) Critical ultrasonography can help to improve the success of operation; (8) Critical ultrasonography can help to improve the safety and quality of nursing; (9) When performing critical ultrasonography for patients with COVID-19, it needs to implement three-level protection standard, pay attention to disinfect the machine and strictly obey the rules from nosocomial infection. (10) Telemedicine and artificial intelligence centered on critical ultrasonography may help to improve the efficiency of treatment for the patients with COVID-19. In the face of the global spread of the epidemic, all we can do is to share experience, build a defense line, We hope this recommendations can help COVID-19 patients therapy.\n\u91cd\u75c7\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u80ba\u708e\uff08COVID-19\uff09\u60a3\u8005\u7684\u7279\u70b9\u662f\u75c5\u6bd2\u4fb5\u88ad\u540e\uff0c\u80ba\u90e8\u635f\u5bb3\u6301\u7eed\u52a0\u91cd\uff0c\u5f15\u53d1\u547c\u5438\u8870\u7aed\uff0c\u5e76\u53d1\u6216\u7ee7\u53d1\u5faa\u73af\u6539\u53d8\u4e0e\u591a\u5668\u5b98\u529f\u80fd\u969c\u788d\u3002\u91cd\u75c7\u8d85\u58f0\u56e0\u5176\u52a8\u6001\u3001\u5b9e\u65f6\u3001\u65e0\u521b\u3001\u53ef\u91cd\u590d\u7b49\u4f18\u70b9\uff0c\u53ef\u5e7f\u6cdb\u7528\u4e8e\u91cd\u75c7\u60a3\u8005\u7684\u75c5\u56e0\u8bca\u65ad\u3001\u75c5\u60c5\u8bc4\u4f30\u5e76\u6307\u5bfc\u6cbb\u7597\u3002\u73b0\u4ee5\u5728\u6e56\u5317\u7701\u6b66\u6c49\u5e02\u6765\u81ea\u5168\u56fd\u53c2\u4e0eCOVID-19\u6551\u6cbb\u7684\u91cd\u75c7\u4e13\u5bb6\u5efa\u8bae\u4e3a\u57fa\u7840\uff0c\u603b\u7ed3\u51fa\u57fa\u4e8e\u91cd\u75c7\u8d85\u58f0\u7684\u91cd\u75c7COVID-19\u6551\u6cbb\u7684\u6307\u5bfc\u5efa\u8bae\uff0c\u7686\u5728\u4e3a\u6551\u6cbbCOVID-19\u91cd\u75c7\u60a3\u8005\u63d0\u4f9b\u4f9d\u636e\u3002.", "journal": "Zhonghua nei ke za zhi", "date": "2020-06-17", "authors": ["L NZhang", "M GYin", "WHe", "H MZhang", "L XLiu", "RZhu", "JWu", "S HCai", "Y GChao", "X TWang", "NoneNone", "NoneNone", "NoneNone"], "doi": "10.3760/cma.j.cn112138-20200219-00098"}
{"title": "Application of deep learning for fast detection of COVID-19 in X-Rays using nCOVnet.", "abstract": "Presently, COVID-19 has posed a serious threat to researchers, scientists, health professionals, and administrations around the globe from its detection to its treatment. The whole world is witnessing a lockdown like situation because of COVID-19 pandemic. Persistent efforts are being made by the researchers to obtain the possible solutions to control this pandemic in their respective areas. One of the most common and effective methods applied by the researchers is the use of CT-Scans and X-rays to analyze the images of lungs for COVID-19. However, it requires several ", "journal": "Chaos, solitons, and fractals", "date": "2020-06-17", "authors": ["HarshPanwar", "P KGupta", "Mohammad KhubebSiddiqui", "RubenMorales-Menendez", "VaishnaviSingh"], "doi": "10.1016/j.chaos.2020.109944\n10.22207/JPAM.14.SPL1.40\n10.21203/rs.3.rs-26500/v1\n10.1016/j.chaos.2020.109864\n10.1007/s00521-018-3381-9\n10.1186/s40708-020-00105-1"}
{"title": "CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images.", "abstract": "The novel Coronavirus also called COVID-19 originated in Wuhan, China in December 2019 and has now spread across the world. It has so far infected around 1.8 million people and claimed approximately 114,698 lives overall. As the number of cases are rapidly increasing, most of the countries are facing shortage of testing kits and resources. The limited quantity of testing kits and increasing number of daily cases encouraged us to come up with a Deep Learning model that can aid radiologists and clinicians in detecting COVID-19 cases using chest X-rays.\nIn this study, we propose CoroNet, a Deep Convolutional Neural Network model to automatically detect COVID-19 infection from chest X-ray images. The proposed model is based on Xception architecture pre-trained on ImageNet dataset and trained end-to-end on a dataset prepared by collecting COVID-19 and other chest pneumonia X-ray images from two different publically available databases.\nCoroNet has been trained and tested on the prepared dataset and the experimental results show that our proposed model achieved an overall accuracy of 89.6%, and more importantly the precision and recall rate for COVID-19 cases are 93% and 98.2% for 4-class cases (COVID vs Pneumonia bacterial vs pneumonia viral vs normal). For 3-class classification (COVID vs Pneumonia vs normal), the proposed model produced a classification accuracy of 95%. The preliminary results of this study look promising which can be further improved as more training data becomes available.\nCoroNet achieved promising results on a small prepared dataset which indicates that given more data, the proposed model can achieve better results with minimum pre-processing of data. Overall, the proposed model substantially advances the current radiology based methodology and during COVID-19 pandemic, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis, quantification and follow-up of COVID-19 cases.", "journal": "Computer methods and programs in biomedicine", "date": "2020-06-14", "authors": ["Asif IqbalKhan", "Junaid LatiefShah", "Mohammad MudasirBhat"], "doi": "10.1016/j.cmpb.2020.105581\n10.1136/bmj.m641\n10.1148/radiol.2020200432\n10.1148/radiol.2020200343"}
{"title": "Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks.", "abstract": "In this study, a dataset of X-ray images from patients with common bacterial pneumonia, confirmed Covid-19 disease, and normal incidents, was utilized for the automatic detection of the Coronavirus\u00a0disease. The aim of the study is to evaluate the performance of state-of-the-art convolutional neural network architectures proposed over\u00a0the recent years for medical image classification. Specifically, the procedure called Transfer Learning was adopted. With transfer learning, the detection of various abnormalities in small medical image datasets is an achievable target, often yielding remarkable results. The datasets utilized in this experiment are two. Firstly, a collection of 1427 X-ray images including 224 images with confirmed Covid-19 disease, 700 images with confirmed common bacterial pneumonia, and 504 images of normal conditions. Secondly, a dataset including 224 images with confirmed Covid-19 disease, 714 images with confirmed bacterial and viral pneumonia, and 504 images of normal conditions. The data was collected from the available X-ray images on public medical repositories. The results suggest that Deep Learning\u00a0with X-ray\u00a0imaging may extract significant biomarkers related to the Covid-19 disease, while the best accuracy, sensitivity, and specificity obtained is 96.78%, 98.66%, and 96.46% respectively. Since by now, all diagnostic tests show failure rates such as to raise concerns, the probability of incorporating X-rays into the diagnosis of the disease could be assessed by the medical community, based on the findings, while more research to evaluate the X-ray approach from different aspects may be conducted.", "journal": "Physical and engineering sciences in medicine", "date": "2020-06-12", "authors": ["Ioannis DApostolopoulos", "Tzani AMpesiana"], "doi": "10.1007/s13246-020-00865-4\n10.1016/S2213-2600(20)30076-X\n10.1109/TMI.2016.2553401\n10.1561/2000000039\n10.1016/j.cell.2018.02.010\n10.1186/s40537-016-0043-6\n10.1021/ci0342472"}
{"title": "Dynamic evolution of COVID-19 on chest computed tomography: experience from Jiangsu Province of China.", "abstract": "To determine the patterns of chest computed tomography (CT) evolution according to disease severity in a large coronavirus disease 2019 (COVID-19) cohort in Jiangsu Province, China.\nThis retrospective cohort study was conducted from January 10, 2020, to February 18, 2020. All patients diagnosed with COVID-19 in Jiangsu Province were included, retrospectively. Quantitative CT measurements of pulmonary opacities including volume, density, and location were extracted by deep learning algorithm. Dynamic evolution of these measurements was investigated from symptom onset (day 1) to beyond day 15. Comparison was made between severity groups.\nA total of 484 patients (median age of 47\u00a0years, interquartile range 33-57) with 954 CT examinations were included, and each was assigned to one of the three groups: asymptomatic/mild (n\u2009=\u200963), moderate (n\u2009=\u2009378), severe/critically ill (n\u2009=\u200943). Time series showed different evolution patterns of CT measurements in the groups. Following disease onset, posteroinferior subpleural area of the lung was the most common location for pulmonary opacities. Opacity volume continued to increase beyond 15\u00a0days in the severe/critically ill group, compared with peaking on days 13-15 in the moderate group. Asymptomatic/mild group had the lowest opacity volume which almost resolved after 15\u00a0days. The opacity density began to drop from day 10 to day 12 for moderately ill patients.\nVolume, density, and location of the pulmonary opacity and their evolution on CT varied with disease severity in COVID-19. These findings are valuable in understanding the nature of the disease and monitoring the patient's condition during the course of illness.\n\u2022 Volume, density, and location of the pulmonary opacity on CT change over time in COVID-19. \u2022 The evolution of CT appearance follows specific pattern, varying with disease severity.", "journal": "European radiology", "date": "2020-06-12", "authors": ["Yuan-ChengWang", "HuanyuanLuo", "SongqiaoLiu", "ShanHuang", "ZhenZhou", "QianYu", "ShijunZhang", "ZhenZhao", "YizhouYu", "YiYang", "DuolaoWang", "ShenghongJu"], "doi": "10.1007/s00330-020-06976-6\n10.1016/S1473-3099(20)30086-4\n10.1016/S0140-6736(20)30183-5\n10.1016/S0140-6736(10)61459-6\n10.1148/rg.242035193\n10.1002/jmv.25709"}
{"title": "Radiology of COVID-19 - Imaging the pulmonary damage.", "abstract": "A large part of the world is presently in the grip of the coronavirus disease (COVID-19) by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2 virus), declared a pandemic in March 2020. This document is a brief commentary of the imaging modalities used in the screening, diagnosis and management of COVID-19 pneumonia. Chest x-rays, especially portable, still form a part of majority of official guidelines, with reports of the suggestive radiologic features. The potential of CT scan and ultrasound is also realised, with earlier detection rate. Typical radiologic findings of bilateral, asymmetrical, crazy-paved ground glass opacification, consolidation, reverse halo sign, opacities, progressing to fibrosis are well described for both the X-ray and CT scan. Atypical findings include airway changes, pleural effusion, pulmonary nodules and acute pulmonary embolism. Absence of lymphadenopathy, pleural effusion and pneumothorax is notable. The role of portable lung ultrasound, reported to be useful in emergency, is yet to be established in the guidelines. Disinfection of the equipment is a major concern. Governmental guidelines still advocate X-ray despite professional societies increasingly recommending CT scan.", "journal": "JPMA. The Journal of the Pakistan Medical Association", "date": "2020-06-10", "authors": ["SabaSohail"], "doi": "10.5455/JPMA.21"}
{"title": "COVID-Classifier: An automated machine learning model to assist in the diagnosis of COVID-19 infection in chest x-ray images.", "abstract": "Chest-X ray (CXR) radiography can be used as a first-line triage process for non-COVID-19 patients with pneumonia. However, the similarity between features of CXR images of COVID-19 and pneumonia caused by other infections make the differential diagnosis by radiologists challenging. We hypothesized that machine learning-based classifiers can reliably distinguish the CXR images of COVID-19 patients from other forms of pneumonia. We used a dimensionality reduction method to generate a set of optimal features of CXR images to build an efficient machine learning classifier that can distinguish COVID-19 cases from non-COVID-19 cases with high accuracy and sensitivity. By using global features of the whole CXR images, we were able to successfully implement our classifier using a relatively small dataset of CXR images. We propose that our COVID-Classifier can be used in conjunction with other tests for optimal allocation of hospital resources by rapid triage of non-COVID-19 cases.", "journal": "medRxiv : the preprint server for health sciences", "date": "2020-06-09", "authors": ["Abolfazl ZargariKhuzani", "MortezaHeidari", "S AliShariati"], "doi": "10.1101/2020.05.09.20096560"}
{"title": "Diagnostic methods and potential portable biosensors for coronavirus disease 2019.", "abstract": "Timely detection and diagnosis are urgently needed to guide epidemiological measures, infection control, antiviral treatment, and vaccine research. In this review, biomarkers/indicators for diagnosis of coronavirus disease 2019 (COVID-19) or detection of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) in the environment are summarized and discussed. It is concluded that the detection methods targeting antibodies are not suitable for screening of early and asymptomatic cases since most patients had an antibody response at about 10 days after onset of symptoms. However, antibody detection methods can be combined with quantitative real-time reverse transcriptase-polymerase chain reaction (RT-qPCR) to significantly improve the sensitivity and specificity of diagnosis, and boost vaccine research. Fast, sensitive and accurate detection methods targeting antigens need to be developed urgently. Various specimens for diagnosis or detection are compared and analyzed. Among them, deep throat saliva and induced sputum are desired for RT-qPCR test or other early detection technologies. Chest computerized tomography (CT) scan, RT-qPCR, lateral flow immunochromatographic strip (LFICS) for diagnosis of COVID-19 are summarized and compared. Specially, potential electrochemical (EC) biosensor, surface enhanced Raman scattering (SERS)-based biosensor, field-effect transistor (FET)-based biosensor, surface plasmon resonance (SPR)-based biosensor and artificial intelligence (AI) assisted diagnosis of COVID-19 are emphasized. Finally, some commercialized portable detection device, current challenges and future directions are discussed.", "journal": "Biosensors & bioelectronics", "date": "2020-06-09", "authors": ["FeiyunCui", "H SusanZhou"], "doi": "10.1016/j.bios.2020.112349\n10.1148/radiol.2020200905\n10.1002/jmv.25727\n10.1093/cid/ciaa149\n10.1007/s10096-020-03899-4\n10.20944/preprints202003.0184.v1\n10.1101/2020.02.27.20028787\n10.1093/clinchem/hvaa102\n10.1101/2020.02.20.20025874\n10.1101/2020.02.22.961268\n10.1093/cid/ciaa344\n10.1101/2020.02.22.961268"}
{"title": "Significance of clinical phenomes of patients with COVID-19 infection: A learning from 3795 patients in 80 reports.", "abstract": "A new coronavirus SARS-CoV-2 has caused outbreaks in multiple countries and the number of cases is rapidly increasing through human-to-human transmission. Clinical phenomes of patients with SARS-CoV-2 infection are critical in distinguishing it from other respiratory infections. The extent and characteristics of those phenomes varied depending on the severities of the infection, for example, beginning with fever or a mild cough, progressed with signs of pneumonia, and worsened with severe or even fatal respiratory difficulty in acute respiratory distress syndrome. We summarized clinical phenomes of 3795 patients with COVID-19 based on 80 published reports from the onset of outbreak to March 2020 to emphasize the importance and specificity of those phenomes in diagnosis and treatment of infection, and evaluate the impact on medical services. The data show that the incidence of male patients was higher than that of females and the level of C-reaction protein was increased as well as most patients' imaging included ground-glass opacity. Clinical phenomes of SARS-CoV-2 infection were compared with those of SARS-CoV and MERS-CoV infections. There is an urgent need to develop an artificial intelligence-based machine learning capacity to analyze and integrate radiomics- or imaging-based, patient-based, clinician-based, and molecular measurements-based data to fight the outbreak of COVID-19 and enable more efficient responses to unknown infections in future.", "journal": "Clinical and translational medicine", "date": "2020-06-09", "authors": ["LinlinZhang", "Diane CWang", "QihongHuang", "XiangdongWang"], "doi": "10.1002/ctm2.17\n10.1056/NEJMoa2001316\n10.1148/radiol.2020200236"}
{"title": "A Deep Neural Network to Distinguish COVID-19 from other Chest Diseases Using X-ray Images.", "abstract": "Scanning a patient's lungs to detect Coronavirus 2019 (COVID-19) may lead to similar imaging of other chest diseases. Thus, a multidisciplinary approach is strongly required to confirm the diagnosis. There are only a few works targeted at pathological x-ray images. Most of the works only target single disease detection which is not good enough. Some works have been provided for all classes. However, the results suffer due to lack of data for rare classes and data unbalancing problem.\nDue to the rise in COVID-19 cases, medical facilities in many countries are overwhelmed and there is a need for an intelligent system to detect it. Few works have been done regarding the detection of the coronavirus but there are many cases where it can be misclassified as some techniques are not efficient and can only identify specific diseases. This work is a deep learning- based model to distinguish COVID-19 cases from other chest diseases.\nA Deep Neural Network model provides a significant contribution in terms of detecting COVID-19 and provides an effective analysis of chest-related diseases taking into account both age and gender. Our model achieves 87% accuracy in terms of GAN-based synthetic data and presents four different types of deep learning-based models that provide comparable results to other state-of-the-art techniques.\nThe healthcare industry may face unfavorable consequences if the gap in the identification of all types of pneumonia is not filled with effective automation.", "journal": "Current medical imaging", "date": "2020-06-05", "authors": ["SalehAlbahli"], "doi": "10.2174/1573405616666200604163954"}
{"title": "Artificial Intelligence: Promise, Pitfalls, and Perspective.", "abstract": null, "journal": "JAMA", "date": "2020-06-04", "authors": ["Angel NDesai"], "doi": "10.1001/jama.2020.8737"}
{"title": "Any unique image biomarkers associated with COVID-19?", "abstract": "To define the uniqueness of chest CT infiltrative features associated with COVID-19 image characteristics as potential diagnostic biomarkers.\nWe retrospectively collected chest CT exams including n\u2009=\u2009498 on 151 unique patients RT-PCR positive for COVID-19 and n\u2009=\u2009497 unique patients with community-acquired pneumonia (CAP). Both COVID-19 and CAP image sets were partitioned into three groups for training, validation, and testing respectively. In an attempt to discriminate COVID-19 from CAP, we developed several classifiers based on three-dimensional (3D) convolutional neural networks (CNNs). We also asked two experienced radiologists to visually interpret the testing set and discriminate COVID-19 from CAP. The classification performance of the computer algorithms and the radiologists was assessed using the receiver operating characteristic (ROC) analysis, and the nonparametric approaches with multiplicity adjustments when necessary.\nOne of the considered models showed non-trivial, but moderate diagnostic ability overall (AUC of 0.70 with 99% CI 0.56-0.85). This model allowed for the identification of 8-50% of CAP patients with only 2% of COVID-19 patients.\nProfessional or automated interpretation of CT exams has a moderately low ability to distinguish between COVID-19 and CAP cases. However, the automated image analysis is promising for targeted decision-making due to being able to accurately identify a sizable subsect of non-COVID-19 cases.\n\u2022 Both human experts and artificial intelligent models were used to classify the CT scans. \u2022 ROC analysis and the nonparametric approaches were used to analyze the performance of the radiologists and computer algorithms. \u2022 Unique image features or patterns may not exist for reliably distinguishing all COVID-19 from CAP; however, there may be imaging markers that can identify a sizable subset of non-COVID-19 cases.", "journal": "European radiology", "date": "2020-05-29", "authors": ["JiantaoPu", "JosephLeader", "AndriyBandos", "JunliShi", "PangDu", "JuezhaoYu", "BohanYang", "ShiKe", "YouminGuo", "Jessica BField", "CarlFuhrman", "DavidWilson", "FrankSciurba", "ChenwangJin"], "doi": "10.1007/s00330-020-06956-w\n10.1111/j.1467-9868.2007.00593.x"}
{"title": "Intra-Rater and Inter-Rater Reliability of Tongue Coating Diagnosis in Traditional Chinese Medicine Using Smartphones: Quasi-Delphi Study.", "abstract": "There is a growing trend in the use of mobile health (mHealth) technologies in traditional Chinese medicine (TCM) and telemedicine, especially during the coronavirus disease (COVID-19) outbreak. Tongue diagnosis is an important component of TCM, but also plays a role in Western medicine, for example in dermatology. However, the procedure of obtaining tongue images has not been standardized and the reliability of tongue diagnosis by smartphone tongue images has yet to be evaluated.\nThe first objective of this study was to develop an operating classification scheme for tongue coating diagnosis. The second and main objective of this study was to determine the intra-rater and inter-rater reliability of tongue coating diagnosis using the operating classification scheme.\nAn operating classification scheme for tongue coating was developed using a stepwise approach and a quasi-Delphi method. First, tongue images (n=2023) were analyzed by 2 groups of assessors to develop the operating classification scheme for tongue coating diagnosis. Based on clinicians' (n=17) own interpretations as well as their use of the operating classification scheme, the results of tongue diagnosis on a representative tongue image set (n=24) were compared. After gathering consensus for the operating classification scheme, the clinicians were instructed to use the scheme to assess tongue features of their patients under direct visual inspection. At the same time, the clinicians took tongue images of the patients with smartphones and assessed tongue features observed in the smartphone image using the same classification scheme. The intra-rater agreements of these two assessments were calculated to determine which features of tongue coating were better retained by the image. Using the finalized operating classification scheme, clinicians in the study group assessed representative tongue images (n=24) that they had taken, and the intra-rater and inter-rater reliability of their assessments was evaluated.\nIntra-rater agreement between direct subject inspection and tongue image inspection was good to very good (Cohen \u03ba range 0.69-1.0). Additionally, when comparing the assessment of tongue images on different days, intra-rater reliability was good to very good (\u03ba range 0.7-1.0), except for the color of the tongue body (\u03ba=0.22) and slippery tongue fur (\u03ba=0.1). Inter-rater reliability was moderate for tongue coating (Gwet AC2 range 0.49-0.55), and fair for color and other features of the tongue body (Gwet AC2=0.34).\nTaken together, our study has shown that tongue images collected via smartphone contain some reliable features, including tongue coating, that can be used in mHealth analysis. Our findings thus support the use of smartphones in telemedicine for detecting changes in tongue coating.", "journal": "JMIR mHealth and uHealth", "date": "2020-05-28", "authors": ["Zhi ChunWang", "Shi PingZhang", "Pong ChiYuen", "Kam WaChan", "Yi YiChan", "Chun HoiCheung", "Chi HoChow", "Ka KitChua", "JunHu", "ZhichaoHu", "BeiniLao", "Chun ChuenLeung", "HongLi", "LindaZhong", "XushengLiu", "YulongLiu", "ZhenjieLiu", "XinLun", "WeiMo", "Sheung YuenSiu", "ZhoujianXiong", "Wing FaiYeung", "Run YunZhang", "XuebinZhang"], "doi": "10.2196/16018\n10.2196/11959\n10.1007/s10916-013-9978-8\n10.1007/978-981-10-5717-5_5\n10.1016/j.jep.2012.01.033\n10.1038/srep00936\n10.1038/srep00936\n10.2991/bep-16.2017.42\n10.1155/2015/173729\n10.1155/2015/173729\n10.1155/2016/1971295\n10.1155/2016/1971295\n10.1155/2013/204908\n10.1155/2013/204908\n10.1099/jmm.0.46118-0\n10.1155/2012/505063\n10.1155/2012/505063\n10.1089/acm.2005.11.415\n10.1089/acm.2008.0554\n10.1016/j.cmpb.2017.12.029\n10.1109/cisp-bmei.2017.8302299\n10.1155/2018/8491057\n10.1155/2018/8491057\n10.1089/acm.2007.0079\n10.2196/11432\n10.2188/jea.JE20160169\n10.2188/jea.JE20160169\n10.3892/ijo.2016.3466\n10.1038/s41368-020-0074-x"}
{"title": "COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios.", "abstract": "The COVID-19 can cause severe pneumonia and is estimated to have a high impact on the healthcare system. Early diagnosis is crucial for correct treatment in order to possibly reduce the stress in the healthcare system. The standard image diagnosis tests for pneumonia are chest X-ray (CXR) and computed tomography (CT) scan. Although CT scan is the gold standard, CXR are still useful because it is cheaper, faster and more widespread. This study aims to identify pneumonia caused by COVID-19 from other types and also healthy lungs using only CXR images.\nIn order to achieve the objectives, we have proposed a classification schema considering the following perspectives: i) a multi-class classification; ii) hierarchical classification, since pneumonia can be structured as a hierarchy. Given the natural data imbalance in this domain, we also proposed the use of resampling algorithms in the schema in order to re-balance the classes distribution. We observed that, texture is one of the main visual attributes of CXR images, our classification schema extract features using some well-known texture descriptors and also using a pre-trained CNN model. We also explored early and late fusion techniques in the schema in order to leverage the strength of multiple texture descriptors and base classifiers at once. To evaluate the approach, we composed a database, named RYDLS-20, containing CXR images of pneumonia caused by different pathogens as well as CXR images of healthy lungs. The classes distribution follows a real-world scenario in which some pathogens are more common than others.\nThe proposed approach tested in RYDLS-20 achieved a macro-avg F1-Score of 0.65 using a multi-class approach and a F1-Score of 0.89 for the COVID-19 identification in the hierarchical classification scenario.\nAs far as we know, the top identification rate obtained in this paper is the best nominal rate obtained for COVID-19 identification in an unbalanced environment with more than three classes. We must also highlight the novel proposed hierarchical classification approach for this task, which considers the types of pneumonia caused by the different pathogens and lead us to the best COVID-19 recognition rate obtained here.", "journal": "Computer methods and programs in biomedicine", "date": "2020-05-24", "authors": ["Rodolfo MPereira", "DiegoBertolini", "Lucas OTeixeira", "Carlos NSilla", "Yandre M GCosta"], "doi": "10.1016/j.cmpb.2020.105532\n10.1007/s11263-009-0315-0\n10.1016/j.patcog.2013.11.029\n10.1016/j.eswa.2018.01.038"}
{"title": "A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis.", "abstract": "Coronavirus disease 2019 (COVID-19) has spread globally, and medical resources become insufficient in many regions. Fast diagnosis of COVID-19 and finding high-risk patients with worse prognosis for early prevention and medical resource optimisation is important. Here, we proposed a fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis by routinely used computed tomography.We retrospectively collected 5372 patients with computed tomography images from seven cities or provinces. Firstly, 4106 patients with computed tomography images were used to pre-train the deep learning system, making it learn lung features. Following this, 1266 patients (924 with COVID-19 (471 had follow-up for >5\u2005days) and 342 with other pneumonia) from six cities or provinces were enrolled to train and externally validate the performance of the deep learning system.In the four external validation sets, the deep learning system achieved good performance in identifying COVID-19 from other pneumonia (AUC 0.87 and 0.88, respectively) and viral pneumonia (AUC 0.86). Moreover, the deep learning system succeeded to stratify patients into high- and low-risk groups whose hospital-stay time had significant difference (p=0.013 and p=0.014, respectively). Without human assistance, the deep learning system automatically focused on abnormal areas that showed consistent characteristics with reported radiological findings.Deep learning provides a convenient tool for fast screening of COVID-19 and identifying potential high-risk patients, which may be helpful for medical resource optimisation and early prevention before patients show severe symptoms.", "journal": "The European respiratory journal", "date": "2020-05-24", "authors": ["ShuoWang", "YunfeiZha", "WeiminLi", "QingxiaWu", "XiaohuLi", "MengNiu", "MeiyunWang", "XiaomingQiu", "HongjunLi", "HeYu", "WeiGong", "YanBai", "LiLi", "YongbeiZhu", "LiusuWang", "JieTian"], "doi": "10.1183/13993003.00775-2020\n10.1016/S2213-2600(20)30079-5\n10.1016/S2214-109X(20)30068-1\n10.1183/13993003.00334-2020\n10.1016/S1473-3099(20)30134-1\n10.1183/13993003.00986-2018\n10.1016/S2213-2600(18)30286-8\n10.1016/S2213-2600(20)30003-5\n10.1183/13993003.01216-2019\n10.1016/j.media.2017.06.014\n10.1101/2020.02.14.20023028\n10.1016/j.media.2014.07.003\n10.1016/j.radonc.2018.10.019\n10.1101/2020.03.19.20039354\n10.1101/2020.02.23.20026930\n10.1101/2020.03.20.20039834\n10.1148/radiol.2020200905\n10.1101/2020.03.12.20027185"}
{"title": "Intensive Care Risk Estimation in COVID-19 Pneumonia Based on Clinical and Imaging Parameters: Experiences from the Munich Cohort.", "abstract": "The evolving dynamics of coronavirus disease 2019 (COVID-19) and the increasing infection numbers require diagnostic tools to identify patients at high risk for a severe disease course. Here we evaluate clinical and imaging parameters for estimating the need of intensive care unit (ICU) treatment. We collected clinical, laboratory and imaging data from 65 patients with confirmed COVID-19 infection based on polymerase chain reaction (PCR) testing. Two radiologists evaluated the severity of findings in computed tomography (CT) images on a scale from 1 (no characteristic signs of COVID-19) to 5 (confluent ground glass opacities in over 50% of the lung parenchyma). The volume of affected lung was quantified using commercially available software. Machine learning modelling was performed to estimate the risk for ICU treatment. Patients with a severe course of COVID-19 had significantly increased interleukin (IL)-6, C-reactive protein (CRP), and leukocyte counts and significantly decreased lymphocyte counts. The radiological severity grading was significantly increased in ICU patients. Multivariate random forest modelling showed a mean \u00b1 standard deviation sensitivity, specificity and accuracy of 0.72 \u00b1 0.1, 0.86 \u00b1 0.16 and 0.80 \u00b1 0.1 and a receiver operating characteristic-area under curve (ROC-AUC) of 0.79 \u00b1 0.1. The need for ICU treatment is independently associated with affected lung volume, radiological severity score, CRP, and IL-6.", "journal": "Journal of clinical medicine", "date": "2020-05-24", "authors": ["EgonBurian", "FriederikeJungmann", "Georgios AKaissis", "Fabian KLoh\u00f6fer", "Christoph DSpinner", "TobiasLahmer", "MatthiasTreiber", "MichaelDommasch", "GerhardSchneider", "FabianGeisler", "WolfgangHuber", "UlrikeProtzer", "Roland MSchmid", "MarkusSchwaiger", "Marcus RMakowski", "Rickmer FBraren"], "doi": "10.3390/jcm9051514\n10.1056/NEJMp2005492\n10.1007/s00330-020-06801-0\n10.1148/radiol.2020200463\n10.1007/s00330-020-06816-7\n10.1007/s00330-020-06817-6\n10.1148/radiol.2020201160\n10.1097/RLI.0000000000000674\n10.1172/JCI137244\n10.1136/bmj.m1333\n10.1016/j.ijsu.2014.07.013\n10.1136/gutjnl-2020-320926\n10.1016/S0140-6736(20)30211-7\n10.1093/cid/ciaa272\n10.1016/S1473-3099(20)30200-0\n10.1097/RLI.0000000000000672\n10.1148/radiol.2020200642\n10.1056/NEJMsb2005114\n10.1089/hs.2020.0028\n10.1056/NEJMoa2004500\n10.1016/j.ijid.2020.03.040\n10.1016/S1473-3099(20)30232-2\n10.1038/s41586-020-2196-x\n10.1016/j.ejrad.2010.12.085\n10.1371/journal.pone.0197418"}
{"title": "A review on the use of artificial intelligence for medical imaging of the lungs of patients with coronavirus disease 2019.", "abstract": "The results of research on the use of artificial intelligence (AI) for medical imaging of the lungs of patients with coronavirus disease 2019 (COVID-19) has been published in various forms. In this study, we reviewed the AI for diagnostic imaging of COVID-19 pneumonia. PubMed, arXiv, medRxiv, and Google scholar were used to search for AI studies. There were 15 studies of COVID-19 that used AI for medical imaging. Of these, 11 studies used AI for computed tomography (CT) and 4 used AI for chest radiography. Eight studies presented independent test data, 5 used disclosed data, and 4 disclosed the AI source codes. The number of datasets ranged from 106 to 5941, with sensitivities ranging from 0.67-1.00 and specificities ranging from 0.81-1.00 for prediction of COVID-19 pneumonia. Four studies with independent test datasets showed a breakdown of the data ratio and reported prediction of COVID-19 pneumonia with sensitivity, specificity, and area under the curve (AUC). These 4 studies showed very high sensitivity, specificity, and AUC, in the range of 0.9-0.98, 0.91-0.96, and 0.96-0.99, respectively.", "journal": "Diagnostic and interventional radiology (Ankara, Turkey)", "date": "2020-05-22", "authors": ["RintaroIto", "ShingoIwano", "ShinjiNaganawa"], "doi": "10.5152/dir.2019.20294\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2001191\n10.1056/NEJMoa2001316\n10.1001/jama.2020.1585\n10.1148/radiol.2020200642\n10.1148/radiol.2020200463\n10.1148/radiol.2020200843\n10.1148/radiol.2020200823\n10.1007/s11604-020-00958-w\n10.1007/s11604-020-00977-7\n10.1007/s11604-020-00945-1\n10.5152/dir.2019.19125\n10.5152/dir.2019.19025\n10.1007/s11604-019-00902-7\n10.1007/s11604-019-00831-5\n10.1007/s11604-018-0796-2\n10.1007/s11604-018-0798-0\n10.1007/BF00344251\n10.1148/radiol.2020200905\n10.3348/kjr.2020.0146\n10.1097/RTI.0000000000000512\n10.1148/ryct.2020200082\n10.1109/CVPR.2017.243\n10.1007/s13246-020-00865-4\n10.1101/2020.03.20.20037325\n10.1101/2020.02.25.20021568\n10.1101/2020.03.20.20039834\n10.1101/2020.02.23.20026930\n10.1101/2020.02.14.20023028\n10.1101/2020.03.24.20042317\n10.1101/2020.03.12.20027185\n10.1007/978-3-319-24574-4_28"}
{"title": "Artificial intelligence-enabled rapid diagnosis of patients with COVID-19.", "abstract": "For diagnosis of coronavirus disease 2019 (COVID-19), a SARS-CoV-2 virus-specific reverse transcriptase polymerase chain reaction (RT-PCR) test is routinely used. However, this test can take up to 2\u2009d to complete, serial testing may be required to rule out the possibility of false negative results and there is currently a shortage of RT-PCR test kits, underscoring the urgent need for alternative methods for rapid and accurate diagnosis of patients with COVID-19. Chest computed tomography (CT) is a valuable component in the evaluation of patients with suspected SARS-CoV-2 infection. Nevertheless, CT alone may have limited negative predictive value for ruling out SARS-CoV-2 infection, as some patients may have normal radiological findings at early stages of the disease. In this study, we used artificial intelligence (AI) algorithms to integrate chest CT findings with clinical symptoms, exposure history and laboratory testing to rapidly diagnose patients who are positive for COVID-19. Among a total of 905 patients tested by real-time RT-PCR assay and next-generation sequencing RT-PCR, 419 (46.3%) tested positive for SARS-CoV-2. In a test set of 279 patients, the AI system achieved an area under the curve of 0.92 and had equal sensitivity as compared to a senior thoracic radiologist. The AI system also improved the detection of patients who were positive for COVID-19 via RT-PCR who presented with normal CT scans, correctly identifying 17 of 25 (68%) patients, whereas radiologists classified all of these patients as COVID-19 negative. When CT scans and associated clinical history are available, the proposed AI system can help to rapidly diagnose COVID-19 patients.", "journal": "Nature medicine", "date": "2020-05-20", "authors": ["XueyanMei", "Hao-ChihLee", "Kai-YueDiao", "MingqianHuang", "BinLin", "ChenyuLiu", "ZongyuXie", "YixuanMa", "Philip MRobson", "MichaelChung", "AdamBernheim", "VenkateshMani", "ClaudiaCalcagno", "KunweiLi", "ShaolinLi", "HongShan", "JianLv", "TongtongZhao", "JunliXia", "QihuaLong", "SharonSteinberger", "AdamJacobi", "TimothyDeyer", "MartaLuksza", "FangLiu", "Brent PLittle", "Zahi AFayad", "YangYang"], "doi": "10.1038/s41591-020-0931-3\n10.1148/radiol.2020200343\n10.1148/radiol.2020200642\n10.1148/radiol.2020200905\n10.2139/ssrn.3441821\n10.1109/cvpr.2016.90\n10.1109/cvpr.2015.7298668"}
{"title": "Generalizability of Deep Learning Tuberculosis Classifier to COVID-19 Chest Radiographs: New Tricks for an Old Algorithm?", "abstract": null, "journal": "Journal of thoracic imaging", "date": "2020-05-20", "authors": ["Paul HYi", "Tae KyungKim", "Cheng TingLin"], "doi": "10.1097/RTI.0000000000000532"}
{"title": "Clinically Applicable AI System for Accurate Diagnosis, Quantitative Measurements, and Prognosis of COVID-19 Pneumonia Using Computed Tomography.", "abstract": "Many COVID-19 patients infected by SARS-CoV-2 virus develop pneumonia (called novel coronavirus pneumonia, NCP) and rapidly progress to respiratory failure. However, rapid diagnosis and identification of high-risk patients for early intervention are challenging. Using a large computed tomography (CT) database from 3,777 patients, we developed an AI system that can diagnose NCP and differentiate it from other common pneumonia and normal controls. The AI system can assist radiologists and physicians in performing a quick diagnosis especially when the health system is overloaded. Significantly, our AI system identified important clinical markers that correlated with the NCP lesion properties. Together with the clinical data, our AI system was able to provide accurate clinical prognosis that can aid clinicians to consider appropriate early clinical management and allocate resources appropriately. We have made this AI system available globally to assist the clinicians to combat COVID-19.", "journal": "Cell", "date": "2020-05-18", "authors": ["KangZhang", "XiaohongLiu", "JunShen", "ZhihuanLi", "YeSang", "XingwangWu", "YunfeiZha", "WenhuaLiang", "ChengdiWang", "KeWang", "LinsenYe", "MingGao", "ZhongguoZhou", "LiangLi", "JinWang", "ZehongYang", "HuiminCai", "JieXu", "LeiYang", "WenjiaCai", "WenqinXu", "ShaoxuWu", "WeiZhang", "ShanpingJiang", "LianghongZheng", "XuanZhang", "LiWang", "LiuLu", "JiamingLi", "HaipingYin", "WinstonWang", "OulanLi", "CharlotteZhang", "LiangLiang", "TaoWu", "RuiyunDeng", "KangWei", "YongZhou", "TingChen", "Johnson Yiu-NamLau", "MansonFok", "JianxingHe", "TianxinLin", "WeiminLi", "GuangyuWang"], "doi": "10.1016/j.cell.2020.04.045\n10.1109/ICPR.2018.8546325\n10.1016/s2213-2600(20)30079-5"}
{"title": "Extracting Possibly Representative COVID-19 Biomarkers from X-ray Images with Deep Learning Approach and Image Data Related to Pulmonary Diseases.", "abstract": "While the spread of COVID-19 is increased, new, automatic, and reliable methods for accurate detection are essential to reduce the exposure of the medical experts to the outbreak. X-ray imaging, although limited to specific visualizations, may be helpful for the diagnosis. In this study, the problem of automatic classification of pulmonary diseases, including the recently emerged COVID-19, from X-ray images, is considered.\nDeep Learning has proven to be a remarkable method to extract massive high-dimensional features from medical images. Specifically, in this paper, the state-of-the-art Convolutional Neural Network called Mobile Net is employed and trained from scratch to investigate the importance of the extracted features for the classification task. A large-scale dataset of 3905 X-ray images, corresponding to 6 diseases, is utilized for training MobileNet v2, which has been proven to achieve excellent results in related tasks.\nTraining the CNNs from scratch outperforms the other transfer learning techniques, both in distinguishing the X-rays between the seven classes and between Covid-19 and non-Covid-19. A classification accuracy between the seven classes of 87.66% is achieved. Besides, this method achieves 99.18% accuracy, 97.36% Sensitivity, and 99.42% Specificity in the detection of COVID-19.\nThe results suggest that training CNNs from scratch may reveal vital biomarkers related but not limited to the COVID-19 disease, while the top classification accuracy suggests further examination of the X-ray imaging potential.", "journal": "Journal of medical and biological engineering", "date": "2020-05-16", "authors": ["Ioannis DApostolopoulos", "Sokratis IAznaouridis", "Mpesiana ATzani"], "doi": "10.1007/s40846-020-00529-4\n10.1007/s13246-020-00865\n10.1109/TMI.2016.2553401\n10.1631/FITEE.1700808\n10.1016/j.cell.2018.02.010\n10.1109/TMI.2016.2528162\n10.1109/TMI.2018.2791721\n10.3389/fnins.2018.00804\n10.1164/rccm.201802-0350LE\n10.1097/COH.0b013e32833ed177\n10.1002/mp.12820\n10.2214/AJR.16.17224\n10.1111/tmi.13383"}
{"title": "Deep learning-based multi-view fusion model for screening 2019 novel coronavirus pneumonia: A multicentre study.", "abstract": "To develop a deep learning-based method to assist radiologists to fast and accurately identify patients with COVID-19 by CT images.\nWe retrospectively collected chest CT images of 495 patients from three hospitals in China. 495 datasets were randomly divided into 395 cases (80%, 294 of COVID-19, 101 of other pneumonia) of the training set, 50 cases (10%, 37 of COVID-19, 13 of other pneumonia) of the validation set and 50 cases (10%, 37 of COVID-19, 13 of other pneumonia) of the testing set. We trained a multi-view fusion model using deep learning network to screen patients with COVID-19 using CT images with the maximum lung regions in axial, coronal and sagittal views. The performance of the proposed model was evaluated by both the validation and testing sets.\nThe multi-view deep learning fusion model achieved the area under the receiver-operating characteristics curve (AUC) of 0.732, accuracy of 0.700, sensitivity of 0.730 and specificity of 0.615 in validation set. In the testing set, we can achieve AUC, accuracy, sensitivity and specificity of 0.819, 0.760, 0.811 and 0.615 respectively.\nBased on deep learning method, the proposed diagnosis model trained on multi-view images of chest CT images showed great potential to improve the efficacy of diagnosis and mitigate the heavy workload of radiologists for the initial screening of COVID-19 pneumonia.", "journal": "European journal of radiology", "date": "2020-05-15", "authors": ["XiangjunWu", "HuiHui", "MengNiu", "LiangLi", "LiWang", "BingxiHe", "XinYang", "LiLi", "HongjunLi", "JieTian", "YunfeiZha"], "doi": "10.1016/j.ejrad.2020.109041\n10.1016/S0140-6736(20)30154-9\n10.1016/j.ejrad.2020.108961\n10.1016/S1473-3099(20)30086-4\n10.1148/radiol.2020200642\n10.1109/TMI.2018.2876510\n10.1001/jamanetworkopen.2019.2561\n10.1109/TMI.2017.2759102\n10.1038/s41591-018-0177-5\n10.1183/13993003.00986-2018\n10.1016/j.tranon.2017.08.007\n10.1109/Cvpr.2016.90\n10.1007/s00259-020-04735-9\n10.1056/NEJMoa2002032\n10.1007/s11432-020-2849-3\n10.1148/radiol.2020200905\n10.1148/radiol.2020200241\n10.1148/radiol.2020200823\n10.1093/cid/ciaa247"}
{"title": "Deep Learning for Classification and Localization of COVID-19 Markers in Point-of-Care Lung Ultrasound.", "abstract": "Deep learning (DL) has proved successful in medical imaging and, in the wake of the recent COVID-19 pandemic, some works have started to investigate DL-based solutions for the assisted diagnosis of lung diseases. While existing works focus on CT scans, this paper studies the application of DL techniques for the analysis of lung ultrasonography (LUS) images. Specifically, we present a novel fully-annotated dataset of LUS images collected from several Italian hospitals, with labels indicating the degree of disease severity at a frame-level, video-level, and pixel-level (segmentation masks). Leveraging these data, we introduce several deep models that address relevant tasks for the automatic analysis of LUS images. In particular, we present a novel deep network, derived from Spatial Transformer Networks, which simultaneously predicts the disease severity score associated to a input frame and provides localization of pathological artefacts in a weakly-supervised way. Furthermore, we introduce a new method based on uninorms for effective frame score aggregation at a video-level. Finally, we benchmark state of the art deep models for estimating pixel-level segmentations of COVID-19 imaging biomarkers. Experiments on the proposed dataset demonstrate satisfactory results on all the considered tasks, paving the way to future research on DL for the assisted diagnosis of COVID-19 from LUS data.", "journal": "IEEE transactions on medical imaging", "date": "2020-05-15", "authors": ["SubhankarRoy", "WilliMenapace", "SebastiaanOei", "BenLuijten", "EnricoFini", "CristianoSaltori", "IrisHuijben", "NishithChennakeshava", "FedericoMento", "AlessandroSentelli", "EmanuelePeschiera", "RiccardoTrevisan", "GiovanniMaschietto", "ElenaTorri", "RiccardoInchingolo", "AndreaSmargiassi", "GinoSoldati", "PaoloRota", "AndreaPasserini", "Ruud J Gvan Sloun", "ElisaRicci", "LibertarioDemi"], "doi": "10.1109/TMI.2020.2994459"}
{"title": "Mining the Characteristics of COVID-19 Patients in China: Analysis of Social Media Posts.", "abstract": "In December 2019, pneumonia cases of unknown origin were reported in Wuhan City, Hubei Province, China. Identified as the coronavirus disease (COVID-19), the number of cases grew rapidly by human-to-human transmission in Wuhan. Social media, especially Sina Weibo (a major Chinese microblogging social media site), has become an important platform for the public to obtain information and seek help.\nThis study aims to analyze the characteristics of suspected or laboratory-confirmed COVID-19 patients who asked for help on Sina Weibo.\nWe conducted data mining on Sina Weibo and extracted the data of 485 patients who presented with clinical symptoms and imaging descriptions of suspected or laboratory-confirmed cases of COVID-19. In total, 9878 posts seeking help on Sina Weibo from February 3 to 20, 2020 were analyzed. We used a descriptive research methodology to describe the distribution and other epidemiological characteristics of patients with suspected or laboratory-confirmed SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2) infection. The distance between patients' home and the nearest designated hospital was calculated using the geographic information system ArcGIS.\nAll patients included in this study who sought help on Sina Weibo lived in Wuhan, with a median age of 63.0 years (IQR 55.0-71.0). Fever (408/485, 84.12%) was the most common symptom. Ground-glass opacity (237/314, 75.48%) was the most common pattern on chest computed tomography; 39.67% (167/421) of families had suspected and/or laboratory-confirmed family members; 36.58% (154/421) of families had 1 or 2 suspected and/or laboratory-confirmed members; and 70.52% (232/329) of patients needed to rely on their relatives for help. The median time from illness onset to real-time reverse transcription-polymerase chain reaction (RT-PCR) testing was 8 days (IQR 5.0-10.0), and the median time from illness onset to online help was 10 days (IQR 6.0-12.0). Of 481 patients, 32.22% (n=155) lived more than 3 kilometers away from the nearest designated hospital.\nOur findings show that patients seeking help on Sina Weibo lived in Wuhan and most were elderly. Most patients had fever symptoms, and ground-glass opacities were noted in chest computed tomography. The onset of the disease was characterized by family clustering and most families lived far from the designated hospital. Therefore, we recommend the following: (1) the most stringent centralized medical observation measures should be taken to avoid transmission in family clusters;\u00a0and (2) social media can help these patients get early attention during Wuhan's lockdown. These findings can help the government and the health department identify high-risk patients and accelerate emergency responses following public demands for help.", "journal": "Journal of medical Internet research", "date": "2020-05-14", "authors": ["ChunmeiHuang", "XinjieXu", "YuyangCai", "QinminGe", "GuangwangZeng", "XiaopanLi", "WeideZhang", "ChenJi", "LingYang"], "doi": "10.2196/19087\n10.1038/s41426-018-0155-5\n10.1038/s41426-018-0155-5\n10.1016/S0140-6736(20)30183-5\n10.1056/NEJMoa2002032\n10.1056/NEJMoa2001017\n10.1056/NEJMoa2001316\n10.1016/S1473-3099(20)30147-X\n10.24869/psyd.2020.6\n10.1089/tmj.2017.0189\n10.1177/1460458214568037\n10.2196/18825\n10.2196/18825\n10.2196/jmir.2911\n10.1038/s41598-019-46898-y\n10.1038/s41598-019-46898-y\n10.1016/j.ijid.2013.11.013\n10.1177/003335491613100312\n10.1016/j.ajic.2016.05.011\n10.1017/dmp.2017.69\n10.1017/dmp.2017.29\n10.1016/j.puhe.2015.07.025\n10.1016/j.puhe.2017.07.015\n10.1016/j.ajic.2016.04.253\n10.1186/2049-9957-2-31\n10.1016/j.scitotenv.2018.08.044\n10.1016/j.scitotenv.2018.08.044\n10.6084/m9.figshare.12053583.v4\n10.3760/cma.j.issn.0254-6450.2020.02.003\n10.1016/S0140-6736(20)30154-9\n10.1056/NEJMc2001272\n10.1056/NEJMc2001468\n10.1016/S0140-6736(20)30260-9\n10.3760/cma.j.cn112338-20200223-00152\n10.3760/cma.j.cn112338-20200221-00147\n10.3760/cma.j.cn112338-20200223-00153\n10.1016/j.jinf.2020.02.018\n10.1515/cclm-2020-0332\n10.1515/cclm-2020-0332\n10.1097/CM9.0000000000000744\n10.1016/S2214-109X(20)30068-1\n10.3390/ijerph16162813"}
{"title": "Using X-ray images and deep learning for automated detection of coronavirus disease.", "abstract": "Coronavirus is still the leading cause of death worldwide. There are a set number of COVID-19 test units accessible in emergency clinics because of the expanding cases daily. Therefore, it is important to implement an automatic detection and classification system as a speedy elective finding choice to forestall COVID-19 spreading among individuals. Medical images analysis is one of the most promising research areas, it provides facilities for diagnosis and making decisions of a number of diseases such as Coronavirus. This paper conducts a comparative study of the use of the recent deep learning models (VGG16, VGG19, DenseNet201, Inception_ResNet_V2, Inception_V3, Resnet50, and MobileNet_V2) to deal with detection and classification of coronavirus pneumonia. The experiments were conducted using chest X-ray & CT dataset of 6087 images (2780 images of bacterial pneumonia, 1493 of coronavirus, 231 of Covid19, and 1583 normal) and confusion matrices are used to evaluate model performances. Results found out that the use of inception_Resnet_V2 and Densnet201 provide better results compared to other models used in this work (92.18% accuracy for Inception-ResNetV2 and 88.09% accuracy for Densnet201).Communicated by Ramaswamy H. Sarma.", "journal": "Journal of biomolecular structure & dynamics", "date": "2020-05-14", "authors": ["KhalidEl Asnaoui", "YounessChawki"], "doi": "10.1080/07391102.2020.1767212\n10.1080/07391102.2020.1758790\n10.1080/07391102.2020.1763199\n10.1016/j.cmpb.2018.01.017\n10.1080/07391102.2020.1754293\n10.1016/j.patrec.2019.11.013\n10.1184/R1/6606860.v1\n10.1080/07391102.2020.1758788\n10.1016/j.pdisas.2020.100091\n10.1016/j.onehlt.2020.100124\n10.1080/07391102.2020.1761882\n10.1080/07391102.2020.1761881\n10.1080/07391102.2020.1758789\n10.1016/j.ajem.2020.04.003\n10.1080/07391102.2020.1758791\n10.1080/07391102.2020.1756411\n10.1080/07391102.2020.1760136\n10.1016/j.jpainsymman.2020.03.025\n10.1016/j.dsx.2020.04.001\n10.1109/CVPR.2016.90\n10.1109/CVPR.2017.243\n10.1148/radiol.2020200330\n10.1016/S0140-6736(20)30553-5\n10.17632/rscbjbr9sj.2\n10.1016/j.physio.2020.03.003\n10.1016/j.puhe.2020.03.027\n10.1016/j.ophtha.2020.03.037\n10.1056/NEJMoa2001316\n10.1016/j.jinf.2020.03.007\n10.1016/j.cca.2020.03.009\n10.1016/S0140-6736(20)30313-5\n10.1080/07391102.2020.1751300\n10.1080/07391102.2020.1752802\n10.1148/ryct.2020200034\n10.1080/07391102.2020.1757510\n10.1080/07391102.2020.1753580\n10.1080/07391102.2020.1761883\n10.1080/07391102.2020.1760137\n10.1080/07391102.2020.1753577\n10.1007/978-3-319-24574-4_28\n10.1016/j.idm.2020.02.002\n10.1016/j.jaut.2020.102433\n10.1016/j.scitotenv.2020.138532\n10.1016/j.beproc.2018.01.004\n10.1080/07391102.2020.1751298\n10.1109/CVPR.2018.00474\n10.1080/07391102.2020.1762741\n10.1016/j.nmni.2020.100669\n10.1016/j.ctro.2020.03.009\n10.1109/WACV.2017.58\n10.1080/07391102.2020.1763201\n10.1016/j.molmed.2020.02.008\n10.1080/07391102.2020.1763202\n10.1080/07391102.2020.1762743\n10.1016/S1473-3099(20)30129-8\n10.1016/j.ijid.2020.03.017\n10.1016/j.promfg.2020.01.375"}
{"title": "Deep Learning COVID-19 Features on CXR Using Limited Training Data Sets.", "abstract": "Under the global pandemic of COVID-19, the use of artificial intelligence to analyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is becoming important. Unfortunately, due to the emergent nature of the COVID-19 pandemic, a systematic collection of CXR data set for deep neural network training is difficult. To address this problem, here we propose a patch-based convolutional neural network approach with a relatively small number of trainable parameters for COVID-19 diagnosis. The proposed method is inspired by our statistical analysis of the potential imaging biomarkers of the CXR radiographs. Experimental results show that our method achieves state-of-the-art performance and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage.", "journal": "IEEE transactions on medical imaging", "date": "2020-05-13", "authors": ["YujinOh", "SangjoonPark", "Jong ChulYe"], "doi": "10.1109/TMI.2020.2993291"}
{"title": "Diagnosis of Coronavirus Disease 2019 (COVID-19) With Structured Latent Multi-View Representation Learning.", "abstract": "Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread rapidly across the world. Due to the large number of infected patients and heavy labor for doctors, computer-aided diagnosis with machine learning algorithm is urgently needed, and could largely reduce the efforts of clinicians and accelerate the diagnosis process. Chest computed tomography (CT) has been recognized as an informative tool for diagnosis of the disease. In this study, we propose to conduct the diagnosis of COVID-19 with a series of features extracted from CT images. To fully explore multiple features describing CT images from different views, a unified latent representation is learned which can completely encode information from different aspects of features and is endowed with promising class structure for separability. Specifically, the completeness is guaranteed with a group of backward neural networks (each for one type of features), while by using class labels the representation is enforced to be compact within COVID-19/community-acquired pneumonia (CAP) and also a large margin is guaranteed between different types of pneumonia. In this way, our model can well avoid overfitting compared to the case of directly projecting high-dimensional features into classes. Extensive experimental results show that the proposed method outperforms all comparison methods, and rather stable performances are observed when varying the number of training data.", "journal": "IEEE transactions on medical imaging", "date": "2020-05-10", "authors": ["HengyuanKang", "LimingXia", "FuhuaYan", "ZhibinWan", "FengShi", "HuanYuan", "HuitingJiang", "DijiaWu", "HeSui", "ChangqingZhang", "DinggangShen"], "doi": "10.1109/TMI.2020.2992546"}
{"title": "Value of CT application in the screening,diagnosis,and treatment of COVID-19.", "abstract": "The coronavirus disease 2019 (COVID-19) has attracted extensive attention all around the world recently. Early screening, early diagnosis, early isolation, and early treatment remain the most effective prevention and control measures. Computed tomography (CT) plays a vital role in the screening, diagnosis, treatment, and follow-up of COVID-19, especially in the early screening, with a higher sensitivity than that of real-time fluorescence RT-PCR. The combination of CT and artificial intelligence has the potential to help clinicians in improving the diagnostic accuracy and working efficiency.\n2019\u51a0\u72b6\u75c5\u6bd2\u75c5(coronavirus disease 2019\uff0cCOVID-19)\u7684\u66b4\u53d1\u5f15\u8d77\u4e86\u5168\u7403\u7684\u6301\u7eed\u5173\u6ce8\u3002\u65e9\u7b5b\u67e5\u3001\u65e9\u8bca\u65ad\u3001\u65e9\u9694\u79bb\u53ca\u65e9\u6cbb\u7597\u662f\u76ee\u524d\u6700\u6709\u6548\u7684\u9632\u63a7\u624b\u6bb5\u3002\u4e34\u5e8a\u4e0a\uff0c\u8ba1\u7b97\u673a\u4f53\u5c42\u6444\u5f71(computed tomography\uff0cCT)\u5728\u5bf9COVID-19\u60a3\u8005\u7684\u7b5b\u67e5\u3001\u8bca\u65ad\u3001\u6cbb\u7597\u548c\u968f\u8bbf\u8fc7\u7a0b\u4e2d\u5747\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u7b5b\u67e5\u65b9\u9762\uff0c\u5176\u68c0\u6d4b\u654f\u611f\u6027\u9ad8\u4e8e\u5b9e\u65f6\u8367\u5149RT-PCR\u68c0\u6d4b\u3002CT\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u6709\u671b\u5e2e\u52a9\u4e34\u5e8a\u533b\u5e08\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u7387\u548c\u5de5\u4f5c\u6548\u7387\u3002.", "journal": "Zhong nan da xue xue bao. Yi xue ban = Journal of Central South University. Medical sciences", "date": "2020-05-10", "authors": ["GeLi", "ZengXiong", "HuiZhou", "JiangpingXie", "WeiChen", "MolingZhou", "ZhimingZhu", "GaofengZhou", "JinkangLiu"], "doi": "10.11817/j.issn.1672-7347.2020.200132"}
{"title": "COVID-19 on Chest Radiographs: A Multireader Evaluation of an Artificial Intelligence System.", "abstract": "Background Chest radiography may play an important role in triage for coronavirus disease 2019 (COVID-19), particularly in low-resource settings. Purpose To evaluate the performance of an artificial intelligence (AI) system for detection of COVID-19 pneumonia on chest radiographs. Materials and Methods An AI system (CAD4COVID-XRay) was trained on 24\u2009678 chest radiographs, including 1540 used only for validation while training. The test set consisted of a set of continuously acquired chest radiographs (", "journal": "Radiology", "date": "2020-05-10", "authors": ["KeelinMurphy", "HenkSmits", "Arnoud J GKnoops", "Michael B J MKorst", "TijsSamson", "Ernst TScholten", "StevenSchalekamp", "Cornelia MSchaefer-Prokop", "Rick H H MPhilipsen", "AnnetMeijers", "JaimeMelendez", "Bramvan Ginneken", "MatthieuRutten"], "doi": "10.1148/radiol.2020201874\n10.1016/j.clinimag.2020.04.001"}
{"title": "Multicenter cohort study demonstrates more consolidation in upper lungs on initial CT increases the risk of adverse clinical outcome in COVID-19 patients.", "abstract": "", "journal": "Theranostics", "date": "2020-05-07", "authors": ["QianYu", "YuanchengWang", "ShanHuang", "SongqiaoLiu", "ZhenZhou", "ShijunZhang", "ZhenZhao", "YizhouYu", "YiYang", "ShenghongJu"], "doi": "10.7150/thno.46465"}
{"title": "CT quantification of pneumonia lesions in early days predicts progression to severe illness in a cohort of COVID-19 patients.", "abstract": "", "journal": "Theranostics", "date": "2020-05-07", "authors": ["FengjunLiu", "QiZhang", "ChaoHuang", "ChunziShi", "LinWang", "NannanShi", "CongFang", "FeiShan", "XueMei", "JingShi", "FengxiangSong", "ZhongchengYang", "ZezhenDing", "XiaomingSu", "HongzhouLu", "TongyuZhu", "ZhiyongZhang", "LeiShi", "YuxinShi"], "doi": "10.7150/thno.45985"}
{"title": "Artificial intelligence to codify lung CT in Covid-19 patients.", "abstract": "The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already assumed pandemic proportions, affecting over 100 countries in few weeks. A global response is needed to prepare health systems worldwide. Covid-19 can be diagnosed both on chest X-ray and on computed tomography (CT). Asymptomatic patients may also have lung lesions on imaging. CT investigation in patients with suspicion Covid-19 pneumonia involves the use of the high-resolution technique (HRCT). Artificial intelligence (AI) software has been employed to facilitate CT diagnosis. AI software must be useful categorizing the disease into different severities, integrating the structured report, prepared according to subjective considerations, with quantitative, objective assessments of the extent of the lesions. In this communication, we present an example of a good tool for the radiologist (Thoracic VCAR software, GE Healthcare, Italy) in Covid-19 diagnosis (Pan et al. in Radiology, 2020. https://doi.org/10.1148/radiol.2020200370). Thoracic VCAR offers quantitative measurements of the lung involvement. Thoracic VCAR can generate a clear, fast and concise report that communicates vital medical information to referring physicians. In the post-processing phase, software, thanks to the help of a colorimetric map, recognizes the ground glass and differentiates it from consolidation and quantifies them as a percentage with respect to the healthy parenchyma. AI software therefore allows to accurately calculate the volume of each of these areas. Therefore, keeping in mind that CT has high diagnostic sensitivity in identifying lesions, but not specific for Covid-19 and similar to other infectious viral diseases, it is mandatory to have an AI software that expresses objective evaluations of the percentage of ventilated lung parenchyma compared to the affected one.", "journal": "La Radiologia medica", "date": "2020-05-06", "authors": ["Maria PaolaBelfiore", "FabrizioUrraro", "RobertaGrassi", "GiulianaGiacobbe", "GianluigiPatelli", "SalvatoreCappabianca", "AlfonsoReginelli"], "doi": "10.1007/s11547-020-01195-x\n10.1148/radiol.2020200370\n10.23750/abm.v91i1.9397\n10.1016/S0140-6736(20)30627-9\n10.1016/S0140-6736(20)30183-5\n10.1148/radiol.2020200274\n10.1148/radiol.2020200463\n10.1148/radiol.2020200230\n10.3892/ol.2019.11220\n10.1148/radiol.2020200905\n10.1016/j.artmed.2019.101770\n10.1007/s11547-020-01179-x\n10.1016/j.mri.2019.08.030\n10.1007/s11307-019-01336-3\n10.1007/s11547-020-01135-9\n10.3390/healthcare8010046"}
{"title": "The Role of Imaging in the Detection and Management of COVID-19: A Review.", "abstract": "Coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) is spreading rapidly around the world, resulting in a massive death toll. Lung infection or pneumonia is the common complication of COVID-19, and imaging techniques, especially computed tomography (CT), have played an important role in diagnosis and treatment assessment of the disease. Herein, we review the imaging characteristics and computing models that have been applied for the management of COVID-19. CT, positron emission tomography - CT (PET/CT), lung ultrasound, and magnetic resonance imaging (MRI) have been used for detection, treatment, and follow-up. The quantitative analysis of imaging data using artificial intelligence (AI) is also explored. Our findings indicate that typical imaging characteristics and their changes can play crucial roles in the detection and management of COVID-19. In addition, AI or other quantitative image analysis methods are urgently needed to maximize the value of imaging in the management of COVID-19.", "journal": "IEEE reviews in biomedical engineering", "date": "2020-05-02", "authors": ["DiDong", "ZhenchaoTang", "ShuoWang", "HuiHui", "LixinGong", "YaoLu", "ZhongXue", "HongenLiao", "FangChen", "FanYang", "RonghuaJin", "KunWang", "ZhenyuLiu", "JingweiWei", "WeiMu", "HuiZhang", "JingyingJiang", "JieTian", "HongjunLi"], "doi": "10.1109/RBME.2020.2990959"}
{"title": "Radiological approach to COVID-19 pneumonia with an emphasis on chest CT.", "abstract": "Coronavirus disease 2019 (COVID-19) has recently become a worldwide outbreak with several millions of people infected and more than 160.000 deaths. A fast and accurate diagnosis in this outbreak is critical to isolate and treat patients. Radiology plays an important role in the diagnosis and management of the patients. Among various imaging modalities, chest CT has received attention with its higher sensitivity and specificity rates. Shortcomings of the real-time reverse transcriptase-polymerase chain reaction test, including inappropriate sample collection and analysis methods, initial false negative results, and limited availability has led to widespread use of chest CT in the diagnostic algorithm. This review summarizes the role of radiology in COVID-19 pneumonia, diagnostic accuracy of imaging, and chest CT findings of the disease.", "journal": "Diagnostic and interventional radiology (Ankara, Turkey)", "date": "2020-05-01", "authors": ["SerkanG\u00fcneyli", "ZeynepAt\u00e7eken", "HakanDo\u011fan", "EmreAlt\u0131nmakas", "Kayhan \u00c7etinAtasoy"], "doi": "10.5152/dir.2020.20260\n10.2214/AJR.20.22961\n10.1016/S0140-6736(20)30211-7\n10.1056/NEJMoa2001316\n10.1093/clinchem/hvaa029\n10.1148/radiol.2020200274\n10.1111/apt.15731\n10.1148/radiol.2020201187\n10.1007/s00330-020-06817-6\n10.2807/1560-7917.ES.2020.25.3.2000045\n10.1056/NEJMe2001329\n10.1080/22221751.2020.1745095\n10.1148/radiol.2020200230\n10.1007/s00330-020-06713-z\n10.1097/RTI.0000000000000524\n10.1148/radiol.2020201160\n10.1002/jum.15284\n10.1148/radiol.2020200770\n10.1097/RLI.0000000000000670\n10.2214/AJR.20.22954\n10.2214/AJR.20.22976\n10.1148/radiol.2020200823\n10.1148/radiol.2020200463\n10.1148/radiol.2020200642\n10.1016/S1473-3099(20)30086-4\n10.1097/RLI.0000000000000672\n10.3348/kjr.2020.0132\n10.1016/j.ejrad.2020.108961\n10.1148/radiol.2020201365\n10.1186/s13054-015-0995-5\n10.1148/ryct.2020204002\n10.1148/radiol.2020200432\n10.1016/S1473-3099(20)30134-1\n10.1148/radiol.2020200343\n10.1148/radiol.2020200370\n10.1148/radiol.2462070712\n10.1007/s00330-020-06731-x\n10.1055/a-1142-4094\n10.2214/AJR.14.13021\n10.1148/radiol.2283030541\n10.1038/nrdp.2017.74\n10.1007/s00330-020-06801-0\n10.1148/ryct.2020200028\n10.1148/radiology.157.3.3864189\n10.1148/radiol.2301020649\n10.2214/ajr.180.5.1801251\n10.2214/ajr.184.6.01841932\n10.1148/radiol.2020200330\n10.1148/ryct.2020200026\n10.1148/radiol.11092149\n10.1148/radiol.2223991980\n10.1259/bjr.20160723\n10.1016/j.diii.2020.03.014\n10.1148/radiol.2020200527\n10.2214/AJR.20.23034\n10.3346/jkms.2020.35.e86\n10.3348/kjr.2020.0078\n10.1148/radiol.2020200236\n10.1371/journal.pone.0230548\n10.1148/radiol.2020200905\n10.1007/s10916-020-1536-6\n10.1001/jamacardio.2020.1096\n10.1016/S0140-6736(20)30628-0\n10.1002/ppul.24718\n10.1136/thorax.57.5.438\n10.2214/AJR.15.15363\n10.1148/rg.242035193"}
{"title": "Infection Control for CT Equipment and Radiographers' Personal Protection During the Coronavirus Disease (COVID-19) Outbreak in China.", "abstract": "", "journal": "AJR. American journal of roentgenology", "date": "2020-05-01", "authors": ["JiemingQu", "WenjieYang", "YanzhaoYang", "LeQin", "FuhuaYan"], "doi": "10.2214/AJR.20.23112"}
{"title": "Use of CT and artificial intelligence in suspected or COVID-19 positive patients: statement of the Italian Society of Medical and Interventional Radiology.", "abstract": "The COVID-19 pandemic started in Italy in February 2020 with an exponential growth that has exceeded the number of cases reported in China. Italian radiology departments found themselves at the forefront in the management of suspected and positive COVID cases, both in diagnosis, in estimating the severity of the disease and in follow-up. In this context SIRM recommends chest X-ray as first-line imaging tool, CT as additional tool that shows typical features of COVID pneumonia, and ultrasound of the lungs as monitoring tool. SIRM recommends, as high priority, to ensure appropriate\u00a0sanitation procedures on the scan equipment after detecting any suspected or positive COVID-19 patients. In this emergency situation, several expectations have been raised by the scientific community about the role that artificial intelligence can have in improving the diagnosis and treatment of coronavirus infection, and SIRM wishes to deliver clear statements to the radiological community, on the usefulness of artificial intelligence as a radiological decision support system in COVID-19 positive patients. (1) SIRM supports the research on the use of artificial intelligence as a predictive and prognostic decision support system, especially in hospitalized patients and those admitted to intensive care, and welcomes single center of multicenter studies for a clinical validation of the test. (2) SIRM does not support the use of CT with artificial intelligence for screening or as first-line test to diagnose COVID-19. (3) Chest CT with artificial intelligence cannot replace molecular diagnosis tests with nose-pharyngeal swab (rRT-PCR) in suspected for COVID-19 patients.", "journal": "La Radiologia medica", "date": "2020-05-01", "authors": ["EmanueleNeri", "VittorioMiele", "FrancescaCoppola", "RobertoGrassi"], "doi": "10.1007/s11547-020-01197-9\n10.1056/nejmoa2002032\n10.1001/jama.2020.4344\n10.1148/radiol.2020200642\n10.2214/AJR.20.22954\n10.1371/journal.pone.0230548\n10.1007/s11606-020-05762-w\n10.1186/s13089-019-0139-2\n10.1007/s00134-020-05996-6\n10.1016/j.acra.2015.10.004\n10.1007/s00330-016-4688-5\n10.1186/s13244-019-0738-2\n10.1007/s10916-020-01562-1\n10.1148/radiol.2020200905\n10.1007/s11547-020-01135-9\n10.1016/j.jocn.2019.03.001\n10.1186/s13244-019-0785-8"}
{"title": "COVIDiagnosis-Net: Deep Bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from X-ray images.", "abstract": "The Coronavirus Disease 2019 (COVID-19) outbreak has a tremendous impact on global health and the daily life of people still living in more than two hundred countries. The crucial action to gain the force in the fight of COVID-19 is to have powerful monitoring of the site forming infected patients. Most of the initial tests rely on detecting the genetic material of the coronavirus, and they have a poor detection rate with the time-consuming operation. In the ongoing process, radiological imaging is also preferred where chest X-rays are highlighted in the diagnosis. Early studies express the patients with an abnormality in chest X-rays pointing to the presence of the COVID-19. On this motivation, there are several studies cover the deep learning-based solutions to detect the COVID-19 using chest X-rays. A part of the existing studies use non-public datasets, others perform on complicated Artificial Intelligent (AI) structures. In our study, we demonstrate an AI-based structure to outperform the existing studies. The SqueezeNet that comes forward with its light network design is tuned for the COVID-19 diagnosis with Bayesian optimization additive. Fine-tuned hyperparameters and augmented dataset make the proposed network perform much better than existing network designs and to obtain a higher COVID-19 diagnosis accuracy.", "journal": "Medical hypotheses", "date": "2020-04-29", "authors": ["FerhatUcar", "DenizKorkmaz"], "doi": "10.1016/j.mehy.2020.109761\n10.1016/j.ijantimicag.2020.105924\n10.1001/jama.2020.1585\n10.1016/j.cca.2020.03.009\n10.1093/clinchem/hvaa029\n10.1016/j.cell.2018.02.010\n10.1148/radiol.2017162326\n10.1016/j.mehy.2019.109426\n10.1016/j.mehy.2019.109433\n10.1109/ACCESS.2020.2982017\n10.11989/JEST.1674-862X.80904120\n10.1016/j.patcog.2017.10.013\n10.1016/j.neunet.2020.01.017\n10.1016/j.bbe.2020.01.010\n10.1007/s00500-019-04355-y\n10.1109/TII.2019.2907373\n10.1016/j.catena.2019.104249\n10.1016/s1474-6670(17)67769-3\n10.1109/JPROC.2015.2494218\n10.1016/j.neunet.2018.07.011"}
{"title": "Less is More: Intelligent Intensive Care for SARS-CoV-2 Based on the Imaging Data.", "abstract": null, "journal": "Journal of medical imaging and radiation sciences", "date": "2020-04-29", "authors": ["HamidAbdollahi"], "doi": "10.1016/j.jmir.2020.04.002\n10.1148/radiol.2020200432\n10.1007/s00134-020-05996-6\n10.1148/radiol.2020200905"}
{"title": "Artificial Intelligence Augmentation of Radiologist Performance in Distinguishing COVID-19 from Pneumonia of Other Origin at Chest CT.", "abstract": "Background Coronavirus disease 2019 (COVID-19) and pneumonia of other diseases share similar CT characteristics, which contributes to the challenges in differentiating them with high accuracy. Purpose To establish and evaluate an artificial intelligence (AI) system for differentiating COVID-19 and other pneumonia at chest CT and assessing radiologist performance without and with AI assistance. Materials and Methods A total of 521 patients with positive reverse transcription polymerase chain reaction results for COVID-19 and abnormal chest CT findings were retrospectively identified from 10 hospitals from January 2020 to April 2020. A total of 665 patients with non-COVID-19 pneumonia and definite evidence of pneumonia at chest CT were retrospectively selected from three hospitals between 2017 and 2019. To classify COVID-19 versus other pneumonia for each patient, abnormal CT slices were input into the EfficientNet B4 deep neural network architecture after lung segmentation, followed by a two-layer fully connected neural network to pool slices together. The final cohort of 1186 patients (132\u2009583 CT slices) was divided into training, validation, and test sets in a 7:2:1 and equal ratio. Independent testing was performed by evaluating model performance in separate hospitals. Studies were blindly reviewed by six radiologists without and then with AI assistance. Results The final model achieved a test accuracy of 96% (95% confidence interval [CI]: 90%, 98%), a sensitivity of 95% (95% CI: 83%, 100%), and a specificity of 96% (95% CI: 88%, 99%) with area under the receiver operating characteristic curve of 0.95 and area under the precision-recall curve of 0.90. On independent testing, this model achieved an accuracy of 87% (95% CI: 82%, 90%), a sensitivity of 89% (95% CI: 81%, 94%), and a specificity of 86% (95% CI: 80%, 90%) with area under the receiver operating characteristic curve of 0.90 and area under the precision-recall curve of 0.87. Assisted by the probabilities of the model, the radiologists achieved a higher average test accuracy (90% vs 85%, \u0394 = 5, ", "journal": "Radiology", "date": "2020-04-28", "authors": ["Harrison XBai", "RobinWang", "ZengXiong", "BenHsieh", "KenChang", "KaseyHalsey", "Thi My LinhTran", "Ji WhaeChoi", "Dong-CuiWang", "Lin-BoShi", "JiMei", "Xiao-LongJiang", "IanPan", "Qiu-HuaZeng", "Ping-FengHu", "Yi-HuiLi", "Fei-XianFu", "Raymond YHuang", "RonnieSebro", "Qi-ZhiYu", "Michael KAtalay", "Wei-HuaLiao"], "doi": "10.1148/radiol.2020201491"}
{"title": "Classification of COVID-19 patients from chest CT images using multi-objective differential evolution-based convolutional neural networks.", "abstract": "Early classification of 2019 novel coronavirus disease (COVID-19) is essential for disease cure and control. Compared with reverse-transcription polymerase chain reaction (RT-PCR), chest computed tomography (CT) imaging may be a significantly more trustworthy, useful, and rapid technique to classify and evaluate COVID-19, specifically in the epidemic region. Almost all hospitals have CT imaging machines; therefore, the chest CT images can be utilized for early classification of COVID-19 patients. However, the chest CT-based COVID-19 classification involves a radiology expert and considerable time, which is valuable when COVID-19 infection is growing at rapid rate. Therefore, an automated analysis of chest CT images is desirable to save the medical professionals' precious time. In this paper, a convolutional neural networks (CNN) is used to classify the COVID-19-infected patients as infected (+ve) or not (-ve). Additionally, the initial parameters of CNN are tuned using multi-objective differential evolution (MODE). Extensive experiments are performed by considering the proposed and the competitive machine learning techniques on the chest CT images. Extensive analysis shows that the proposed model can classify the chest CT images at a good accuracy rate.", "journal": "European journal of clinical microbiology & infectious diseases : official publication of the European Society of Clinical Microbiology", "date": "2020-04-28", "authors": ["DilbagSingh", "VijayKumar", "NoneVaishali", "ManjitKaur"], "doi": "10.1007/s10096-020-03901-z"}
{"title": "Imaging of coronavirus disease 2019: A Chinese expert consensus statement.", "abstract": "Coronavirus disease 2019 (COVID-19) is highly contagious, mainly causing inflammatory lesions in the lungs, and can also cause damage to the intestine and liver. The rapid spread of the virus that causes coronavirus disease 2019 (COVID-19) pneumonia has posed complex challenges to global public health. Early detection, isolation, diagnosis, and treatment are the most effective means of prevention and control. At present, the epidemic situation of new coronavirus infection has tended to be controlled in China, and it is still in a period of rapid rise in much of the world. The current gold standard for the diagnosis of COVID-19 is the detection of coronavirus nucleic acids, but imaging has an important role in the detection of lung lesions, stratification, evaluation of treatment strategies, and differentiation of mixed infections. This Chinese expert consensus statement summarizes the imaging features of COVID-19 pneumonia and may help radiologists across the world to understand this disease better.", "journal": "European journal of radiology", "date": "2020-04-27", "authors": ["QiYang", "QiangLiu", "HaiboXu", "HongLu", "ShiyuanLiu", "HongjunLi"], "doi": "10.1016/j.ejrad.2020.109008\n10.1056/NEJMc2001468\n10.1056/NEJMoa2001316\n10.1038/s41564-020-0695-z\n10.2807/1560-7917.Es.2020.25.3.2000045\n10.1086/374559\n10.1148/radiol.2020200823\n10.3760/cma.j.issn.1005-1201.2020.0001\n10.13929/j.issn.1003-3289.2020.03.001\n10.1016/s2213-2600(20)30076-x\n10.3760/cma.j.cn112147-20200311-00312\n10.1056/NEJMoa2002032\n10.1016/s0140-6736(20)30183-5\n10.1016/s2213-2600(20)30079-5\n10.1016/s0140-6736(20)30566-3\n10.1128/JCM.00310-20\n10.1148/radiol.2020200988\n10.1148/ryct.2020200152\n10.1016/s0046-8177(03)00367-8\n10.1007/s00247-003-1042-2\n10.1016/j.ijid.2018.03.005\n10.1056/NEJMoa2002032\n10.1007/s00330-020-06731-x\n10.1007/s00330-020-06731-x\n10.19627/j.cnki.cn31-1700/th.20200309.001\n10.1136/bmj.m606\n10.1148/radiol.2392041968\n10.1148/rg.2018170048\n10.1148/radiol.2020200230\n10.1148/ryct.2020200028\n10.1148/radiol.2020200236\n10.1148/radiol.11092149\n10.1148/radiol.2020200323\n10.3348/kjr.2020.0132\n10.1148/radiol.2020200269\n10.1148/radiol.2020200274\n10.1148/ryct.2020200034\n10.1148/rg.242035193\n10.1259/bjr/18544730\n10.4103/0970-2113.63611\n10.1136/bmjopen-2014-006766\n10.2214/AJR.14.13021\n10.1016/j.ejrad.2019.108774"}
{"title": "Rapid Detection of COVID-19 Coronavirus Using a Reverse Transcriptional Loop-Mediated Isothermal Amplification (RT-LAMP) Diagnostic Platform.", "abstract": null, "journal": "Clinical chemistry", "date": "2020-04-22", "authors": ["LinYu", "ShanshanWu", "XiaowenHao", "XueDong", "LinglingMao", "VicentPelechano", "Wei-HuaChen", "XiushanYin"], "doi": "10.1093/clinchem/hvaa102"}
{"title": "Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation, and Diagnosis for COVID-19.", "abstract": "The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delineation of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19.", "journal": "IEEE reviews in biomedical engineering", "date": "2020-04-20", "authors": ["FengShi", "JunWang", "JunShi", "ZiyanWu", "QianWang", "ZhenyuTang", "KeleiHe", "YinghuanShi", "DinggangShen"], "doi": "10.1109/RBME.2020.2987975"}
{"title": "Progress and prospect on imaging diagnosis of COVID-19.", "abstract": "COVID-19 has become a public health emergency due to its rapid transmission. The appearance of pneumonia is one of the major clues for the diagnosis, progress and therapeutic evaluation. More and more literatures about imaging manifestations and related research have been reported. In order to know about the progress and prospective on imaging of COVID-19, this review focus on interpreting the CT findings, stating the potential pathological basis, proposing the challenge of patients with underlying diseases, differentiating with other diseases and suggesting the future research and clinical directions, which would be helpful for the radiologists in the clinical practice and research.", "journal": "Chinese journal of academic radiology", "date": "2020-04-16", "authors": ["LiFan", "DongLi", "HuadanXue", "LongjiangZhang", "ZaiyiLiu", "BingZhang", "LinaZhang", "WenjieYang", "BaojunXie", "XiaoyiDuan", "XiuhuaHu", "KailiangCheng", "LiqingPeng", "NanYu", "LanSong", "HuaiChen", "XinSui", "NannanZheng", "ShiyuanLiu", "ZhengyuJin"], "doi": "10.1007/s42058-020-00031-5\n10.1016/S0140-6736(20)30183-5\n10.1101/2020.02.06.20020974\n10.3760/cma.j.issn.1005-1201.2020.0001\n10.1186/s40779-020-0233-6\n10.13437/j.cnki.jcr.20200206.002\n10.3760/cma.j.issn.1005-1201.2020.0003\n10.1148/radiol.2020200257\n10.1148/radiol.2020200241\n10.1148/radiol.2020200343\n10.1148/radiol.2020200269\n10.1148/ryct.2020200033\n10.1148/ryct.2020200025\n10.1148/radiol.2020200330\n10.1148/ryct.2020200028\n10.1148/ryct.2020200026\n10.1148/radiol.2020200280\n10.1148/radiol.2020200274\n10.1148/radiol.2020200230\n10.1016/S2213-2600(20)30076-X\n10.1148/radiol.2020200236\n10.1148/radiol.2020200370\n10.1148/radiol.2020200323\n10.3760/cma.j.issn.1005-1201.2020.0008\n10.1148/rg.346130107\n10.5114/pjr.2019.85812\n10.1148/radiol.11092149\n10.1148/rg.2018170048\n10.1146/annurev.pathmechdis.3.121806.154316\n10.1016/S1526-0542(04)90037-1\n10.1128/JCM.05629-11\n10.2214/AJR.05.0128\n10.1016/j.idc.2009.10.005\n10.1097/mcp.0000000000000046\n10.1148/radiol.2282030593\n10.2214/AJR.15.14445"}
{"title": "Coronavirus Disease 2019 Deep Learning Models: Methodologic Considerations.", "abstract": null, "journal": "Radiology", "date": "2020-04-04", "authors": ["Andrew M VDad\u00e1rio", "Joselisa P Qde Paiva", "Rodrigo CChate", "Birajara SMachado", "GilbertoSzarf"], "doi": "10.1148/radiol.2020201178\n10.1148/radiol.2020200905"}
{"title": "Proposal for International Standardization of the Use of Lung Ultrasound for Patients With COVID-19: A Simple, Quantitative, Reproducible Method.", "abstract": "Growing evidence is showing the usefulness of lung ultrasound in patients with the 2019 new coronavirus disease (COVID-19). Severe acute respiratory syndrome coronavirus 2 has now spread in almost every country in the world. In this study, we share our experience and propose a standardized approach to optimize the use of lung ultrasound in patients with COVID-19. We focus on equipment, procedure, classification, and data sharing.", "journal": "Journal of ultrasound in medicine : official journal of the American Institute of Ultrasound in Medicine", "date": "2020-04-01", "authors": ["GinoSoldati", "AndreaSmargiassi", "RiccardoInchingolo", "DaniloBuonsenso", "TizianoPerrone", "Domenica FedericaBriganti", "StefanoPerlini", "ElenaTorri", "AlbertoMariani", "Elisa EleonoraMossolani", "FrancescoTursi", "FedericoMento", "LibertarioDemi"], "doi": "10.1002/jum.15285\n10.7863/ultra.15.08023\n10.1007/s00134-020-05996-6\n10.1002/jum.15284\n10.1016/j.jinf.2020.03.004\n10.1016/S2213-2600(20)30120-X"}
{"title": "CT screening for early diagnosis of SARS-CoV-2 infection.", "abstract": null, "journal": "The Lancet. Infectious diseases", "date": "2020-03-31", "authors": ["YongshunHuang", "WeibinCheng", "NaZhao", "HongyingQu", "JunzhangTian"], "doi": "10.1016/S1473-3099(20)30241-3\n10.1016/S1473-3099(20)30086-4\n10.3760/cma.j.cn112150-20200229-00220\n10.1101/2020.03.03.20028423"}
{"title": "Deep Learning Localization of Pneumonia: 2019 Coronavirus (COVID-19) Outbreak.", "abstract": null, "journal": "Journal of thoracic imaging", "date": "2020-03-25", "authors": ["BrianHurt", "SethKligerman", "AlbertHsiao"], "doi": "10.1097/RTI.0000000000000512"}
{"title": "Coronavirus Disease (COVID-19): Spectrum of CT Findings and Temporal Progression of the Disease.", "abstract": "Coronavirus disease is an emerging infection caused by a novel coronavirus that is moving rapidly. High resolution computed tomography (CT) allows objective evaluation of the lung lesions, thus enabling us to better understand the pathogenesis of the disease. With serial CT examinations, the occurrence, development, and prognosis of the disease can be better understood. The imaging can be sorted into four phases: early phase, progressive phase, severe phase, and dissipative phase. The CT appearance of each phase and temporal progression of the imaging findings are demonstrated.", "journal": "Academic radiology", "date": "2020-03-25", "authors": ["MingzhiLi", "PingguiLei", "BingliangZeng", "ZongliangLi", "PengYu", "BingFan", "ChuanhongWang", "ZicongLi", "JianZhou", "ShaoboHu", "HaoLiu"], "doi": "10.1016/j.acra.2020.03.003\n10.1056/NEJMoa2001316\n10.1038/s41586-020-2008-3\n10.1126/science.abb2507\n10.1148/radiol.2020200370\n10.1148/radiol.2020200432\n10.1148/radiol.2020200257\n10.1148/radiol.2020200490\n10.1007/s00330-020-06731-x\n10.3348/kjr.2020.0078\n10.1148/radiol.2020200463\n10.1148/radiol.2020200241\n10.1148/radiol.2020200236\n10.1148/radiol.2020200230"}
{"title": "Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy.", "abstract": "Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (\u00b1standard deviation) was 49 years \u00b1 15, and there were slightly more men than women (1838 vs 1484, respectively; ", "journal": "Radiology", "date": "2020-03-20", "authors": ["LinLi", "LixinQin", "ZeguoXu", "YoubingYin", "XinWang", "BinKong", "JunjieBai", "YiLu", "ZhenghanFang", "QiSong", "KunlinCao", "DaliangLiu", "GuishengWang", "QizhongXu", "XishengFang", "ShiqinZhang", "JuanXia", "JunXia"], "doi": "10.1148/radiol.2020200905\n10.1148/radiol.2020200642\n10.1148/radiol.2020200432"}
{"title": "False-Negative Results of Real-Time Reverse-Transcriptase Polymerase Chain Reaction for Severe Acute Respiratory Syndrome Coronavirus 2: Role of Deep-Learning-Based CT Diagnosis and Insights from Two Cases.", "abstract": "The epidemic of 2019 novel coronavirus, later named as severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is still gradually spreading worldwide. The nucleic acid test or genetic sequencing serves as the gold standard method for confirmation of infection, yet several recent studies have reported false-negative results of real-time reverse-transcriptase polymerase chain reaction (rRT-PCR). Here, we report two representative false-negative cases and discuss the supplementary role of clinical data with rRT-PCR, including laboratory examination results and computed tomography features. Coinfection with SARS-COV-2 and other viruses has been discussed as well.", "journal": "Korean journal of radiology", "date": "2020-03-17", "authors": ["DashengLi", "DaweiWang", "JianpingDong", "NanaWang", "HeHuang", "HaiwangXu", "ChenXia"], "doi": "10.3348/kjr.2020.0146\n10.1101/2020.02.07.937862\n10.1148/radiol.2020200343\n10.1148/radiol.2020200230"}
{"title": "Correlation of Chest CT and RT-PCR Testing for Coronavirus Disease 2019 (COVID-19) in China: A Report of 1014 Cases.", "abstract": "Background Chest CT is used in the diagnosis of coronavirus disease 2019 (COVID-19) and is an important complement to reverse-transcription polymerase chain reaction (RT-PCR) tests. Purpose To investigate the diagnostic value and consistency of chest CT as compared with RT-PCR assay in COVID-19. Materials and Methods This study included 1014 patients in Wuhan, China, who underwent both chest CT and RT-PCR tests between January 6 and February 6, 2020. With use of RT-PCR as the reference standard, the performance of chest CT in the diagnosis of COVID-19 was assessed. In addition, for patients with multiple RT-PCR assays, the dynamic conversion of RT-PCR results (negative to positive, positive to negative) was analyzed as compared with serial chest CT scans for those with a time interval between RT-PCR tests of 4 days or more. Results Of the 1014 patients, 601 of 1014 (59%) had positive RT-PCR results and 888 of 1014 (88%) had positive chest CT scans. The sensitivity of chest CT in suggesting COVID-19 was 97% (95% confidence interval: 95%, 98%; 580 of 601 patients) based on positive RT-PCR results. In the 413 patients with negative RT-PCR results, 308 of 413 (75%) had positive chest CT findings. Of those 308 patients, 48% (103 of 308) were considered as highly likely cases and 33% (103 of 308) as probable cases. At analysis of serial RT-PCR assays and CT scans, the mean interval between the initial negative to positive RT-PCR results was 5.1 days \u00b1 1.5; the mean interval between initial positive to subsequent negative RT-PCR results was 6.9 days \u00b1 2.3. Of the 1014 patients, 60% (34 of 57) to 93% (14 of 15) had initial positive CT scans consistent with COVID-19 before (or parallel to) the initial positive RT-PCR results. Twenty-four of 57 patients (42%) showed improvement on follow-up chest CT scans before the RT-PCR results turned negative. Conclusion Chest CT has a high sensitivity for diagnosis of coronavirus disease 2019 (COVID-19). Chest CT may be considered as a primary tool for the current COVID-19 detection in epidemic areas. \u00a9 RSNA, 2020 ", "journal": "Radiology", "date": "2020-02-27", "authors": ["TaoAi", "ZhenluYang", "HongyanHou", "ChenaoZhan", "ChongChen", "WenzhiLv", "QianTao", "ZiyongSun", "LimingXia"], "doi": "10.1148/radiol.2020200642\n10.1101/2020.02.11.20021493\n10.1101/2020.02.06.20020974"}
{"title": "Optimizing MRF-ASL scan design for precise quantification of brain hemodynamics using neural network regression.", "abstract": "Arterial Spin Labeling (ASL) is a quantitative, non-invasive alternative for perfusion imaging that does not use contrast agents. The magnetic resonance fingerprinting (MRF) framework can be adapted to ASL to estimate multiple physiological parameters simultaneously. In this work, we introduce an optimization scheme to increase the sensitivity of the ASL fingerprint. We also propose a regression based estimation framework for MRF-ASL.\nTo improve the sensitivity of MRF-ASL signals to underlying parameters, we optimized ASL labeling durations using the Cramer-Rao Lower Bound (CRLB). This paper also proposes a neural network regression based estimation framework trained using noisy synthetic signals generated from our ASL signal model. We tested our methods in silico and in vivo, and compared with multiple post labeling delay (multi-PLD) ASL and unoptimized MRF-ASL. We present comparisons of estimated maps for the six parameters of our signal model.\nThe scan design process facilitated precise estimates of multiple hemodynamic parameters and tissue properties from a single scan, in regions of normal gray and white matter, as well as regions with anomalous perfusion activity in the brain. In particular, there was a 86.7% correlation of perfusion estimates with the ground truth in silico, using our proposed techniques. In vivo, there was roughly a 7 fold improvement in the Coefficient of Variation (CoV) for white matter perfusion, and 2 fold improvement in gray matter perfusion CoV in comparison to a reference Multi PLD method. The regression based estimation approach provided perfusion estimates rapidly, with estimation times of around 1s per map.\nScan design optimization, coupled with regression-based estimation is a powerful tool for improving precision in MRF-ASL.", "journal": "Magnetic resonance in medicine", "date": "2019-11-22", "authors": ["AnishLahiri", "Jeffrey AFessler", "LuisHernandez-Garcia"], "doi": "10.1002/mrm.28051"}
{"title": "Data-driven, projection-based respiratory motion compensation of PET data for cardiac PET/CT and PET/MR imaging.", "abstract": "Respiratory patient motion causes blurring of the PET images that may impact accurate quantification of perfusion and infarction extents in PET myocardial viability studies. In this study, we investigate the feasibility of correcting for respiratory motion directly in the PET-listmode data prior to image reconstruction using a data-driven, projection-based, respiratory motion compensation (DPR-MoCo) technique.\nThe DPR-MoCo method was validated using simulations of a XCAT phantom (Biograph mMR PET/MR) as well as experimental phantom acquisitions (Biograph mCT PET/CT). Seven patient studies following a dual-tracer (\nThe DPR-MoCo and the No-MoCo images presented with similar noise-properties (CoV) (P = .12), while the RTA-MoCo and reference-gate images showed increased noise levels (P = .05). TBR\nThe projection-based DPR-MoCo method helps to improve PET image quality of the myocardium without the need for external devices for motion tracking.", "journal": "Journal of nuclear cardiology : official publication of the American Society of Nuclear Cardiology", "date": "2019-02-15", "authors": ["Martin LyngbyLassen", "ThomasBeyer", "AlexanderBerger", "DietrichBeitzke", "SazanRasul", "FlorianB\u00fcther", "MarcusHacker", "JacoboCal-Gonz\u00e1lez"], "doi": "10.1007/s12350-019-01613-2\n10.2967/jnumed.108.059204\n10.1118/1.3112422\n10.1007/s12350-016-0631-z\n10.1088/0031-9155/54/24/007\n10.1109/TNS.2009.2016341\n10.1016/j.media.2014.08.003\n10.1016/j.cpet.2015.09.004\n10.1118/1.3483784\n10.1088/1361-6560/aaf0bc\n10.1007/s00259-018-4047-7\n10.1016/j.cpet.2012.10.004\n10.2967/jnumed.115.164285\n10.1109/42.563659\n10.2967/jnumed.114.151779\n10.1118/1.4800806\n10.1088/0031-9155/58/6/1759\n10.2967/jnumed.115.171728\n10.1118/1.3480985\n10.1088/1361-6560/aa97c8\n10.1088/0031-9155/57/4/867\n10.1016/j.nima.2012.09.039\n10.1007/s12350-016-0522-3\n10.2967/jnumed.108.054726\n10.1118/1.2748104\n10.2967/jnumed.106.032789\n10.1109/TMI.2010.2053043\n10.1080/01621459.1955.10501294\n10.1148/radiology.219.3.r01jn44828\n10.1016/S0140-6736(13)61754-7"}
